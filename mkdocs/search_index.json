{
    "docs": [
        {
            "location": "/", 
            "text": "This site contains the documentation for the software and data produced by the \nEU STRANDS Project\n. For more information on the scientific aims of the project, please see our \nIEEE RAM overview article\n or \nthe STRANDS Project website\n.\n\n\nThe project created autonomous mobile robots which were successfully deployed for long periods in real user environments. In the process of this we created a great deal of open source software for AI and robotics applications. This software is all available via [the STRANDS GitHub organisation][https://github.com/strands-project]. This site provides a single location where the documentation from across that organisation can be viewed. It is also the main location for software tutorials and guides for creating systems, and provides an entry point into using our software for new users. \n\n\nPlease note that a large amount of this site is automatically generated from our code and package documentation, so the structure is currently not perfect. Our scripts for automatically generating this site are available \nhere\n.\n\n\nGetting Started\n\n\nIf you wish to understand or reuse the full STRANDS system, you should follow \nthe STRANDS system tutorial\n. After completing this tutorial, you will have a computer which has ROS and STRANDS packages installed, and can run a simulation which uses some of the core STRANDS subsystems.\n\n\nCore Subsystems\n\n\nA STRANDS system is formed of many components which provide various pieces of functionality, ranging from navigation to user interaction. A list of all packages with a brief overview of their purpose can be found \nhere\n. The following sections give a brief overview of some of the packages which form the core of the system.\n\n\nSTRANDS Navigation\n\n\nNavigation forms the core of the movement capabilities of robots using the system. Our work provides a \nmonitored navigation\n system which detects failures in navigation and triggers recovery behaviours, and a \ntopological navigation\n system where navigation nodes (waypoints) are linked by edges which the robot can traverse. Topological navigation underpins many of the other STRANDS capabilities.\n\n\nSTRANDS Executive\n\n\nThe STRANDS executive controls the execution of tasks requested by users or generated by the system itself, prioritising them using various metrics such as expected completion time, probability of successful completion, and so on. It provides facilities for both long-term task routines, task scheduling and task planning under uncertainty. There is a \nSTRANDS Executive tutorial\n which covers the main parts of the system and \nan overview document\n.\n\n\nPerson Detection and Tracking\n\n\nWhen operating in populated spaces it is crucial to be able to detect and track people. STRANDS produced an indoor \nmulti-person tracker\n which fuses and tracks upper body detections and leg detections. We also have produced \na wheelchair and walking aid detector\n. \n\n\n3D Mapping and Vision\n\n\nOne of the major outputs of the project is a collection of systems for discovering and learning about objects in everyday environments. These are collected together into the STRANDS 3D Mapping collection, described \nhere\n.\n\n\nSemantic Object Maps (SOMa)\n\n\nThe outputs of person detection and 3D mapping are stored in our Semantic Object Map (SOMa) which captures the information the robot gathers over long durations in a central store which supports a range of visualisations and queries. This is described \nhere\n.\n\n\nLong-Term Data Processing (FreMEn and QSRLib)\n\n\nAfter data is collected in SOMa our systems process it using various techniques. Major outputs of STRANDS include \nFreMen\n which provides frequency-based modelling for the temporal dimension of spatial representations, and \nQSRLib\n, a library for generating qualitative spatial relations from sensor data.\n\n\nDatasets\n\n\nYou can find the datasets generated by the project \nhere\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#getting-started", 
            "text": "If you wish to understand or reuse the full STRANDS system, you should follow  the STRANDS system tutorial . After completing this tutorial, you will have a computer which has ROS and STRANDS packages installed, and can run a simulation which uses some of the core STRANDS subsystems.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#core-subsystems", 
            "text": "A STRANDS system is formed of many components which provide various pieces of functionality, ranging from navigation to user interaction. A list of all packages with a brief overview of their purpose can be found  here . The following sections give a brief overview of some of the packages which form the core of the system.", 
            "title": "Core Subsystems"
        }, 
        {
            "location": "/#strands-navigation", 
            "text": "Navigation forms the core of the movement capabilities of robots using the system. Our work provides a  monitored navigation  system which detects failures in navigation and triggers recovery behaviours, and a  topological navigation  system where navigation nodes (waypoints) are linked by edges which the robot can traverse. Topological navigation underpins many of the other STRANDS capabilities.", 
            "title": "STRANDS Navigation"
        }, 
        {
            "location": "/#strands-executive", 
            "text": "The STRANDS executive controls the execution of tasks requested by users or generated by the system itself, prioritising them using various metrics such as expected completion time, probability of successful completion, and so on. It provides facilities for both long-term task routines, task scheduling and task planning under uncertainty. There is a  STRANDS Executive tutorial  which covers the main parts of the system and  an overview document .", 
            "title": "STRANDS Executive"
        }, 
        {
            "location": "/#person-detection-and-tracking", 
            "text": "When operating in populated spaces it is crucial to be able to detect and track people. STRANDS produced an indoor  multi-person tracker  which fuses and tracks upper body detections and leg detections. We also have produced  a wheelchair and walking aid detector .", 
            "title": "Person Detection and Tracking"
        }, 
        {
            "location": "/#3d-mapping-and-vision", 
            "text": "One of the major outputs of the project is a collection of systems for discovering and learning about objects in everyday environments. These are collected together into the STRANDS 3D Mapping collection, described  here .", 
            "title": "3D Mapping and Vision"
        }, 
        {
            "location": "/#semantic-object-maps-soma", 
            "text": "The outputs of person detection and 3D mapping are stored in our Semantic Object Map (SOMa) which captures the information the robot gathers over long durations in a central store which supports a range of visualisations and queries. This is described  here .", 
            "title": "Semantic Object Maps (SOMa)"
        }, 
        {
            "location": "/#long-term-data-processing-fremen-and-qsrlib", 
            "text": "After data is collected in SOMa our systems process it using various techniques. Major outputs of STRANDS include  FreMen  which provides frequency-based modelling for the temporal dimension of spatial representations, and  QSRLib , a library for generating qualitative spatial relations from sensor data.", 
            "title": "Long-Term Data Processing (FreMEn and QSRLib)"
        }, 
        {
            "location": "/#datasets", 
            "text": "You can find the datasets generated by the project  here .", 
            "title": "Datasets"
        }, 
        {
            "location": "/packages/", 
            "text": "Here you can find all the documentation generated by the STRANDS project, aggregated from the github repositories.\n\n\naaf_deployment\n\n\naaf_deployment/info_terminal\n\n\nThe info_terminal package\n\n\nactivity_analysis\n\n\nannotation_tool_kth\n\n\ndatasets\n\n\nfremen\n\n\nmongodb_store\n\n\nA package to support MongoDB-based storage and analysis for data from a ROS system, eg. saved messages, configurations etc\n\n\nplanning_tutorial\n\n\nThe planning_tutorial package\n\n\nrobblog\n\n\nThe robblog package\n\n\nscitos_2d_navigation\n\n\nThis package contains components for using the ROS move base together\nwith the Scitos G5 robot.\n\n\n\nscitos_apps\n\n\nThe scitos_apps metapackage\n\n\nscitos_common\n\n\nThis package contains robot-specific definitions of the SCITOS robot such\nas the URDF description of the robot's kinematics and dynamics and 3D\nmodels of robot components.\n\n\n\nsemantic_segmentation\n\n\nThe semantic_segmentation package\n\n\nsensortag\n\n\nThe sensortag package\n\n\nsoma\n\n\nThe soma package\n\n\nstrands-docker\n\n\nstrands_3d_mapping\n\n\nstrands_3d_mapping/dynamic_object_retrieval\n\n\n  This is a framework for querying for point clouds within a\n  collection of large 3d maps.\n\n\n\nstrands_3d_mapping/observation_registration\n\n\nstrands_3d_mapping/quasimodo\n\n\nstrands_apps\n\n\nThe strands_apps metapackage\n\n\nstrands_data_to_qsrlib\n\n\nThe strands_data_to_qsrlib package\n\n\nstrands_executive\n\n\nstrands_exploration\n\n\nThe strands_exploration package\n\n\nstrands_hri\n\n\nThe strands_hri metapackage\n\n\nstrands_hri/strands_visualise_speech\n\n\nThe strands_visualise_speech package\n\n\nstrands_morse\n\n\nThe strands_morse package\n\n\nstrands_morse/bham\n\n\nstrands_movebase\n\n\nThis package contains components for using the ROS move base together\nwith the Scitos G5 robot. There is options for running obstacle avoidance\nboth with only laser and with an additional depth-sensing camera\nmounted in front. The additional nodes in the package are for processing\nthe incoming clouds from the camera for obstacle avoidance.\n\n\n\nstrands_navigation\n\n\nThe strands_navigation package\n\n\nstrands_navigation/topological_navigation\n\n\nThe topological_navigation package\n\n\nstrands_perception_people\n\n\nThe strands_perception_people metapackage\n\n\nstrands_perception_people/perception_people_launch\n\n\nThe perception_people_launch package\n\n\nstrands_perception_people/visual_odometry\n\n\nThe visual_odometry package\n\n\nstrands_qsr_lib\n\n\nThe strands_qsr_lib metapackage\n\n\nstrands_qsr_lib/qsr_lib\n\n\nThe qsr_lib package\n\n\nstrands_recovery_behaviours\n\n\nThe strands_recovery_behaviours\n\n\nstrands_social\n\n\nThe strands_social metapackage\n\n\nstrands_ui\n\n\nThe strands_ui metapackage\n\n\ntrajectory_behaviours/relational_learner\n\n\nThe relational_learner package\n\n\nv4r\n\n\nThe Vision for Robotics Library of ACIN TUW (open source STRANDS version)\n\n\nv4r_ros_wrappers\n\n\nThe v4r_ros_wrappers package\n\n\nviper\n\n\nView Planning Environment for Robots (VIPER)", 
            "title": "Packages"
        }, 
        {
            "location": "/packages/#aaf_deployment", 
            "text": "", 
            "title": "aaf_deployment"
        }, 
        {
            "location": "/packages/#aaf_deploymentinfo_terminal", 
            "text": "The info_terminal package", 
            "title": "aaf_deployment/info_terminal"
        }, 
        {
            "location": "/packages/#activity_analysis", 
            "text": "", 
            "title": "activity_analysis"
        }, 
        {
            "location": "/packages/#annotation_tool_kth", 
            "text": "", 
            "title": "annotation_tool_kth"
        }, 
        {
            "location": "/packages/#datasets", 
            "text": "", 
            "title": "datasets"
        }, 
        {
            "location": "/packages/#fremen", 
            "text": "", 
            "title": "fremen"
        }, 
        {
            "location": "/packages/#mongodb_store", 
            "text": "A package to support MongoDB-based storage and analysis for data from a ROS system, eg. saved messages, configurations etc", 
            "title": "mongodb_store"
        }, 
        {
            "location": "/packages/#planning_tutorial", 
            "text": "The planning_tutorial package", 
            "title": "planning_tutorial"
        }, 
        {
            "location": "/packages/#robblog", 
            "text": "The robblog package", 
            "title": "robblog"
        }, 
        {
            "location": "/packages/#scitos_2d_navigation", 
            "text": "This package contains components for using the ROS move base together\nwith the Scitos G5 robot.", 
            "title": "scitos_2d_navigation"
        }, 
        {
            "location": "/packages/#scitos_apps", 
            "text": "The scitos_apps metapackage", 
            "title": "scitos_apps"
        }, 
        {
            "location": "/packages/#scitos_common", 
            "text": "This package contains robot-specific definitions of the SCITOS robot such\nas the URDF description of the robot's kinematics and dynamics and 3D\nmodels of robot components.", 
            "title": "scitos_common"
        }, 
        {
            "location": "/packages/#semantic_segmentation", 
            "text": "The semantic_segmentation package", 
            "title": "semantic_segmentation"
        }, 
        {
            "location": "/packages/#sensortag", 
            "text": "The sensortag package", 
            "title": "sensortag"
        }, 
        {
            "location": "/packages/#soma", 
            "text": "The soma package", 
            "title": "soma"
        }, 
        {
            "location": "/packages/#strands-docker", 
            "text": "", 
            "title": "strands-docker"
        }, 
        {
            "location": "/packages/#strands_3d_mapping", 
            "text": "", 
            "title": "strands_3d_mapping"
        }, 
        {
            "location": "/packages/#strands_3d_mappingdynamic_object_retrieval", 
            "text": "This is a framework for querying for point clouds within a\n  collection of large 3d maps.", 
            "title": "strands_3d_mapping/dynamic_object_retrieval"
        }, 
        {
            "location": "/packages/#strands_3d_mappingobservation_registration", 
            "text": "", 
            "title": "strands_3d_mapping/observation_registration"
        }, 
        {
            "location": "/packages/#strands_3d_mappingquasimodo", 
            "text": "", 
            "title": "strands_3d_mapping/quasimodo"
        }, 
        {
            "location": "/packages/#strands_apps", 
            "text": "The strands_apps metapackage", 
            "title": "strands_apps"
        }, 
        {
            "location": "/packages/#strands_data_to_qsrlib", 
            "text": "The strands_data_to_qsrlib package", 
            "title": "strands_data_to_qsrlib"
        }, 
        {
            "location": "/packages/#strands_executive", 
            "text": "", 
            "title": "strands_executive"
        }, 
        {
            "location": "/packages/#strands_exploration", 
            "text": "The strands_exploration package", 
            "title": "strands_exploration"
        }, 
        {
            "location": "/packages/#strands_hri", 
            "text": "The strands_hri metapackage", 
            "title": "strands_hri"
        }, 
        {
            "location": "/packages/#strands_hristrands_visualise_speech", 
            "text": "The strands_visualise_speech package", 
            "title": "strands_hri/strands_visualise_speech"
        }, 
        {
            "location": "/packages/#strands_morse", 
            "text": "The strands_morse package", 
            "title": "strands_morse"
        }, 
        {
            "location": "/packages/#strands_morsebham", 
            "text": "", 
            "title": "strands_morse/bham"
        }, 
        {
            "location": "/packages/#strands_movebase", 
            "text": "This package contains components for using the ROS move base together\nwith the Scitos G5 robot. There is options for running obstacle avoidance\nboth with only laser and with an additional depth-sensing camera\nmounted in front. The additional nodes in the package are for processing\nthe incoming clouds from the camera for obstacle avoidance.", 
            "title": "strands_movebase"
        }, 
        {
            "location": "/packages/#strands_navigation", 
            "text": "The strands_navigation package", 
            "title": "strands_navigation"
        }, 
        {
            "location": "/packages/#strands_navigationtopological_navigation", 
            "text": "The topological_navigation package", 
            "title": "strands_navigation/topological_navigation"
        }, 
        {
            "location": "/packages/#strands_perception_people", 
            "text": "The strands_perception_people metapackage", 
            "title": "strands_perception_people"
        }, 
        {
            "location": "/packages/#strands_perception_peopleperception_people_launch", 
            "text": "The perception_people_launch package", 
            "title": "strands_perception_people/perception_people_launch"
        }, 
        {
            "location": "/packages/#strands_perception_peoplevisual_odometry", 
            "text": "The visual_odometry package", 
            "title": "strands_perception_people/visual_odometry"
        }, 
        {
            "location": "/packages/#strands_qsr_lib", 
            "text": "The strands_qsr_lib metapackage", 
            "title": "strands_qsr_lib"
        }, 
        {
            "location": "/packages/#strands_qsr_libqsr_lib", 
            "text": "The qsr_lib package", 
            "title": "strands_qsr_lib/qsr_lib"
        }, 
        {
            "location": "/packages/#strands_recovery_behaviours", 
            "text": "The strands_recovery_behaviours", 
            "title": "strands_recovery_behaviours"
        }, 
        {
            "location": "/packages/#strands_social", 
            "text": "The strands_social metapackage", 
            "title": "strands_social"
        }, 
        {
            "location": "/packages/#strands_ui", 
            "text": "The strands_ui metapackage", 
            "title": "strands_ui"
        }, 
        {
            "location": "/packages/#trajectory_behavioursrelational_learner", 
            "text": "The relational_learner package", 
            "title": "trajectory_behaviours/relational_learner"
        }, 
        {
            "location": "/packages/#v4r", 
            "text": "The Vision for Robotics Library of ACIN TUW (open source STRANDS version)", 
            "title": "v4r"
        }, 
        {
            "location": "/packages/#v4r_ros_wrappers", 
            "text": "The v4r_ros_wrappers package", 
            "title": "v4r_ros_wrappers"
        }, 
        {
            "location": "/packages/#viper", 
            "text": "View Planning Environment for Robots (VIPER)", 
            "title": "viper"
        }, 
        {
            "location": "/setup/", 
            "text": "Setting up a STRANDS system\n\n\nThis readme will guide you through the setup and use of a system which uses the\nSTRANDS packages. It is assumed that you have a system that is running Ubuntu\n14.04.\n\n\nROS and package setup\n\n\nInstalling ROS\n\n\nThe first step is to follow the instructions found at\n\nhttp://wiki.ros.org/indigo/Installation/Ubuntu\n.\nThe full install of ROS with the package \nros-indigo-desktop-full\n should\ncontain all the packages that are required.\n\n\nInstalling STRANDS packages\n\n\nTo install the strands packages, you should follow the instructions at\n\nhttps://github.com/strands-project-releases/strands-releases/wiki\n.\n\n\nExample system\n\n\nBefore setting up your own system, you might like to see what a system might\nlook like. Instructions for setting up an example system can be found at\n\nhttps://github.com/strands-project-releases/strands-releases/wiki/Example-STRANDS-System\n\n\nCustom system setup\n\n\nDatabase\n\n\nWhile the system is running, it will store data in a mongodb database so that\nvarious components can access it. Depending on what tasks you are running, the\ndatabase can grow quite large (on the order hundreds of gigabytes), so you\nshould create the database on a drive with a lot of space.\n\n\nThe database will be automatically created by the database node, but requires\nthe creation of a directory before it is launched. Running the following will\ninitialise the database. The database is launched in the same way when the full\nsystem is running - it should be run before any other part of the system runs,\nas many of them require it to be up.\n\n\nDATA_DIR=/my/data/directory\nmkdir -p $DATA_DIR/my_database_dir\nroslaunch mongodb_store mongodb_store.launch db_path:=$DATA_DIR/my_database_dir\n\n\n\n\nYou can see more information about the database system at\n\nhttps://github.com/strands-project/mongodb_store/tree/hydro-devel/mongodb_store\n\n\nUsing a simulation\n\n\nYou can use a simulated environment in order to test the system, either with\nexisting simulated environments, or you can set things up to simulate the\nenvironment you wish to operate in. Later on in this document, there are\ninstructions on how to set up a metric map and topological map. These can both\nbe applied in a simulated environment as well.\n\n\nExisting simulations\n\n\nSimulations are based on the \nstrands_morse\n package. If you look in there,\nyou'll find several real environments which have already been set up. We'll look\nat the \nstrands_morse/bham\n directory here.\n\n\nThe \nmaps\n directory contains maps, as you might expect. There are yaml and\npgm map pairs for the ROS grid maps. The tplg file is a yaml export of the\ntopological map for that area.\n\n\nThe \nurdf\n directory contains urdf files for the environment, which make use of\ndae files in the \nmeshes\n directory.\n\n\nThe \nlaunch\n directory contains some launch files which can be used to launch\neverything with just one command.\n\n\nThe top level python files are used to construct various environments, setting\nup the robot position, model and so on.\n\n\nBefore starting the simulation, you'll need to add the topological map to your\ndatabase using\n\n\nrosrun topological_utils load_yaml_map.py `rospack find strands_morse`/bham/maps/cs_lg_sim.tplg\n\n\n\n\nOnce this is done, run\n\n\nroslaunch strands_morse bham_cs_morse.launch\n\n\n\n\nThis will bring up the blender window with the environment loaded and populated\nwith objects and a robot.\n\n\nroslaunch strands_morse bham_cs_nav2d.launch\n\n\n\n\nWill bring up all navigation and related nodes.\n\n\nFinally, use rviz to visualise the map and topological map:\n\n\nroslaunch strands_morse bham_default_rviz.launch\n\n\n\n\nTo allow the robot to move, you'll have to move it backwards first - you can do\nthis by pressing the down arrow on your keyboard when focusing the blender\nwindow. Once you've done that, you can click the green arrows to make the robot\nnavigate there, or you can specify a navigation location using the 2d nav goal\ntool in rviz.\n\n\nCustom simulations\n\n\nTo set up a custom simulation, you'll first need to generate a 3D environment to\nuse. A simple environment is easy to construct. You'll need to have GIMP,\ninkscape and blender installed to create it.\n\n\nIf you want to skip the details, you can find the files created by this section\n\nhere\n.\nYou will still have to add the topological maps to your mongo database with\n\nrosrun topological_utils load_yaml_map.py maps/basic_map.tpl\n.\n\n\nPNG map\n\n\nThe first step is to use GIMP to create an image of the walls in the map that\nyou want to use. The image below is made using the pencil tool and holding\nctrl+shift to make straight lines. It has a scale of 35px/cm. We'll use this\nlater to scale the environment to our robot.\n\n\n\n\nYou can make something like this map using blueprints for the area you are\nworking in and the process below should be similar.\n\n\nSVG map\n\n\nOnce you have an image which contains only black and white pixels, open inkscape\nand import the image, with the \nembed\n option.\n\n\nMake sure the image is selected, and then open the \ntrace bitmap\n dialogue with\nalt+shift+b. Select the single scan colour quantisation option, with 2 colours,\nand check the box to invert the image. Click the update button on the right hand\nside and you should see the result of the tracing. This tracing will convert the\nimage into a vector graphic which can be used in blender. You can fiddle with\nthe options until you get a result that looks good. Once you're satisfied, press\nthe OK button. Then, save the image as an svg file.\n\n\nBlender model\n\n\nOpen blender, and delete the cube that is in the space with the delete key.\nThen, with \nfile\nimport\n import the svg that you just created. You should see it\nin the space as some black lines. You can select it in the top right hand side,\nwhere it will exist as a curve. The image we started with had a scale of\n35px/cm, which will be very small for our robot, which is around 80cm across\n(assuming that we're using the Scitos G5). On the right hand toolbar, you should\nsee a set of icons - a camera, some photos, a chain link, a cube, and so on.\nSelect the cube icon. This Will bring up a set of options which include scaling.\nIn the image, some openings which could represent doors are approximately 50\npixels wide. We'll make these openings 1.5 metres wide, to make them easy to get\nthrough. This means that each pixel has to be 0.03 (1.5/50) metres. At 35px/cm,\neach pixel in the image was 0.000286 metres. So, in order to get the size we\nwant, we should scale each pixel by approximately 105 (0.03/0.000286). We'll\napply this scaling to both the x and y axes.\n\n\nOnce that scaling is done, we also need to make some proper 3D walls with\nheight. Select the curve icon on the right hand side, and you should see under\nthe geometry section the option to extrude. Set this value to 1.5 and it should\nbe OK. Since extruding goes in both vertical directions, shift the model up by\n1.5 in the transform options in the cube section.\n\n\nWe also need a floor, so add a plane in using \nadd\nmesh\nplane\n. Scale it so that\nit covers approximately the area needed to cover all the floor space in the map,\nand transform it so that it sits below the imported map. You may wish to\ntranslate or rotate the map so that it sits in the positive quadrant of the\nspace - when it is imported it will sit on the positive x axis but negative y\naxis.\n\n\nThere also needs to be a light source so you can see what is going on. Instead\nof the weak lamp that currently exists in the space, you should add a sun\n(\nadd\nlamp\nsun\n), which provides much more light.\n\n\nThe final step is to convert the curve to a mesh so that it is correctly\ndisplayed. With the curve selected, press alt+c, which will bring up a\nconversion menu. Use \"mesh from curve\" option, and then save the blender file.\n\n\nYou can find example files created from this process\n\nhere\n.\n\n\nCreating the simulation files\n\n\nNow that we have all these models, we need to create some files to run\neverything. We'll put everything into a new package for convenience.\n\n\nFirst, create a new ros package in your workspace.\n\n\nroscd\ncd src\ncatkin_create_pkg basic_example\ncd basic_example\nmkdir scripts\n\n\n\n\nIn the scripts directory, create a script \nsimulator.sh\n that will be used to\nrun the simulation. Its basic contents should look something like the following.\nIt sets up various paths and then runs another file which defines what the\nsimulation environment actually looks like:\n\n\n#!/bin/bash\nenvironment_name=\nbasic_example\n\nstrands_morse=`rospack find strands_morse`\nexample=`rospack find basic_example`\npath=\n$example\n\ncommon=\n$strands_morse/strands_sim\n\n\nPYTHONPATH=\n$path/src:$common/src:$PYTHONPATH\n\nMORSE_RESOURCE_PATH=\n$strands_morse:$common/data:$common/robots:$path:$MORSE_RESOURCE_PATH\n\nexport MORSE_RESOURCE_PATH PYTHONPATH\nadded=`$strands_morse/morse_config.py $environment_name $path`\necho \nRunning morse on $path with PYTHONPATH=$PYTHONPATH and MORSE_RESOURCE_PATH=$MORSE_RESOURCE_PATH\n\nPATH=/opt/strands-morse-simulator/bin:$PATH\n\nmorse run thermo `rospack find basic_example`/example_sim.py\n\n\n\n\nDon't forget to run \nchmod +x scripts/simulator.sh\n to make it executable.\n\n\nIn the top level directory, create the simulation definition (\nexample_sim.py\n)\n\n\n#! /usr/bin/env morseexec\n\nimport sys\nimport subprocess \nimport os\nimport random\n\nfrom morse.builder import *\nfrom strands_sim.builder.robots import Scitosa5\n\nrobot = Scitosa5(with_cameras = Scitosa5.WITHOUT_DEPTHCAMS)\n# Specify the initial position and rotation of the robot\nrobot.translate(x=2,y=2, z=0)\nrobot.rotate(z=-1.57)\n\n# Specify where the model of the environment is\nmodel_file=os.path.join(os.path.dirname(os.path.abspath( __file__)),'maps/basic_map.blend')\n# Create the environment with the model file, and use fast mode - you can do\n# this to speed things up a little when you're using the scitos A5 without\n# depthcams.\nenv = Environment(model_file,fastmode=True)\n# Place the camera in the environment\nenv.set_camera_location([0, 0, 10])\n# Aim the camera so that it's looking at the environment\nenv.set_camera_rotation([0.5, 0, -0.5])\n\n\n\n\nDownload the basic map created above from github into the maps directory.\n\n\nroscd basic_example\nmkdir maps\ncd maps\nwget https://github.com/strands-project/strands_documentation/raw/master/resources/basic_map.blend\n\n\n\n\nCreate a launch file which will be used to launch the simulator (\nlaunch/basic_example.launch\n)\n\n\nlaunch\n\n\n   \n!-- Scitos robot --\n\n   \ninclude file=\n$(find strands_morse)/launch/scitos.launch\n/\n\n\n   \nnode pkg=\nbasic_example\n type=\nsimulator.sh\n respawn=\nfalse\n name=\nbasic_example\n output=\nscreen\n/\n\n\n\n/launch\n\n\n\n\n\nFinally, compile the package with \ncatkin build basic_example\n. You should then\nbe able to run \nroslaunch basic_example basic_example.launch\n, and see a robot\nin the world.\n\n\nAt this point, the robot will not be able to move. The following file\n(\nlaunch/basic_example_nav.launch\n) will launch the required parts of the\nstrands system.\n\n\nlaunch\n\n  \n!-- declare arg to be passed in --\n\n  \narg name=\nwith_chest_xtion\n default=\nfalse\n/\n\n  \narg name=\nmon_nav_config_file\n  default=\n /\n\n  \narg name=\nmax_bumper_recoveries\n default=\n.inf\n/\n\n  \narg name=\nwait_reset_bumper_duration\n default=\n0.0\n/\n\n  \narg name=\ntopological_navigation_retries\n default=\n3\n/\n\n  \narg name=\ntopological_map_name\n default=\nbasic_map\n/\n\n  \narg name=\nmap\n default=\n$(find strands_morse)/basic_example/maps/basic_map.yaml\n/\n\n\n  \n!-- 2D Navigation --\n\n  \ninclude file=\n$(find strands_movebase)/launch/movebase.launch\n\n      \narg name=\nmap\n value=\n$(arg map)\n/\n\n      \narg name=\nwith_chest_xtion\n value=\n$(arg with_chest_xtion)\n/\n\n  \n/include\n\n\n  \nnode pkg=\nmonitored_navigation\n type=\nmonitored_nav.py\n name=\nmonitored_nav\n output=\nscreen\n args=\n$(arg mon_nav_config_file)\n\n    \nparam name=\nwait_reset_bumper_duration\n value=\n$(arg wait_reset_bumper_duration)\n/\n\n    \nrosparam param=\n/monitored_navigation/recover_states/recover_bumper\n subst_value=\nTrue\n[True, $(arg max_bumper_recoveries)]\n/rosparam\n\n  \n/node\n\n\n  \nnode pkg=\ntopological_navigation\n type=\nmap_manager.py\n name=\ntopological_map_manager\n args=\n$(arg topological_map_name)\n respawn=\ntrue\n/\n\n  \nnode pkg=\ntopological_navigation\n name=\ntopological_localisation\n type=\nlocalisation.py\n output=\nscreen\n respawn=\ntrue\n/\n\n  \nnode pkg=\ntopological_navigation\n type=\nvisualise_map.py\n name=\nvisualise_map\n args=\n$(arg topological_map_name)\n respawn=\ntrue\n/\n\n\n  \nnode pkg=\ntopological_navigation\n name=\ntopological_navigation\n type=\nnavigation.py\n output=\nscreen\n respawn=\ntrue\n\n    \nparam name=\nretries\n type=\nint\n value=\n$(arg topological_navigation_retries)\n/\n\n  \n/node\n\n\n  \nnode pkg=\ntf\n type=\nstatic_transform_publisher\n name=\nenv_broadcaster\n \n        args=\n0 0 0 0 0 0 /odom /map 200\n\n  \n/node\n\n\n/launch\n\n\n\n\n\nYou can also use the following launch file (\nlaunch/basic_example_rviz.launch\n)\nto launch an rviz instance with various interactive markers set up.\n\n\nlaunch\n\n  \nnode pkg=\nrviz\n type=\nrviz\n name=\nrviz\n args=\n-d $(find strands_morse)/basic_example/default.rviz\n/\n\n\n/launch\n\n\n\n\n\nTo use this, you'll first have to construct a pgm map. You can do this by\ncolouring the image you used to create the simulation map with the correct\ncolours for ROS map usage (e.g.\n\nthe basic map\n).\nAlternatively, you can also use gmapping - see below for instructions. You\nshould save the map to \nmaps/basic_map.pgm\n and \nmaps/basic_map.yaml\n, or save\nit elsewhere and point the above launch file to the correct location. If you\nmake a map from the image, you will have to create a corresponding yaml file to\ndescribe it and give the scaling of the image and some other details. See\n\nmap server\n for details on the yaml format.\n\n\nIf you use gmapping, you can use \nrosrun teleop_twist_keyboard\nteleop_twist_keyboard.py\n to control the motion of the robot. You may have to\ninstall this package first. You should also run rviz so that you can see the map\nbeing constructed and make sure you haven't missed any part of it. You can leave\nthe map as it is, or trim it to remove some of the excess parts if your map is\nsmall. In that case you will need to change the origin of the map so that it\ncorresponds with where you want your origin to be.\n\n\nYou should follow the instructions in the topological map section below to\ncreate a topological map for the environment. Once you've created it and\ninserted it into the mongo database, you should change the default \nmap_name\n to\nthe name of the map in your database. You can find an example\n\nhere\n.\n\n\nWhen running the system, you may have to set the position of the robot in rviz\nto the correct location on the map, as the origin of the map there and in the\nsimulation does not align.\n\n\nYou can find documentation for the MORSE simulator\n\nhere\n, which gives more\ndetails about what you can do in the \nexample_sim.py\n file.\n\n\nMetric map\n\n\nThe metric map is a 2D map of the operating environment, where each cell of a\ngrid is populated with a value which represents whether that that cell is\noccupied by an obstacle, is empty, or has an unknown occupancy state. The\nquickest and easiest way to map your environment is using the\n\nROS gmapping\n package. How you use this package\nwill depend on the type of robot you have. The package requires that you have\nlaser and odometry data being published to the ROS system.\n\n\nAssuming that your laser data is being published on \n/base_scan\n, and odometry\non \n/odom\n, you can start the mapping process as below. The\n\nmaxUrange\n parameter defines a threshold on the distance of usable range\nmeasurements received. For example, setting this to 20 will discard any readings\nreceived which are beyond 20 metres.\n\n\nrosrun gmapping slam_gmapping scan:=base_scan maxUrange:=20\n\n\n\n\nWhile this runs, you can observe the map being built in the \nrviz\n utility by\nadding a display for the \n/map\n topic. Push the robot around in your operation\narea. You should try to move relatively slowly. You should also try to ensure\nthat you revisit previously mapped areas after going around the environment, so\nthat the map can be properly adjusted.\n\n\nOnce you are happy with your map, you should save it using the\n\nmap_server\n:\n\n\nrosrun map_server map_saver -f my_map map:=my_map_topic\n\n\n\n\nThis will produce a \n.pgm\n file and a \n.yaml\n file. The \n.pgm\n file contains an\nimage of the map, which you can manipulate with an image editing program if\nnecessary. The \n.yaml\n file contains information about the map. If something\nstrange is happening with your map, then it might be worth checking that this\nfile is set up to point to the correct \n.pgm\n file. You can also adjust the\nresolution of the map and its origin in the \n.yaml\n file.\n\n\nAt this point, you may wish to clean up the image to remove dynamic obstacles\nfrom the map. In GIMP, you can do this by using the pencil tool with white\nselected.\n\n\nAlong with the base map, it is also possible to provide a \"nogo\" map, which is\nused to more strictly define which areas are passable and which are not. You can\nuse this to restrict the robot's movement in an area where there are no walls to\nobstruct the robot's motion. The nogo map should duplicate the base map. You can\nthen draw obstacles onto the map in black (255) in the places which you would\nlike to have a phantom obstacle. The pencil tool in GIMP is again useful. We\nrecommend creating a new GIMP file with the nogo map on a new layer so that it\ncan be more easily modified if necessary. GIMP can export the file to a \n.pgm\n\nfile.\n\n\nOnce you are happy with your map, you can use\n\n\nrosrun map_server map_server my_map.yaml map:=mymap\n\n\n\n\nto make the map available on the \n/mymap\n topic.\n\n\nAdding to the map\n\n\nSometimes it may be necessary to remap parts of the map due to changes in the\nenvironment. You can use gmapping to do this, and then stitch the images\ntogether in an image editing program. Sometimes, you may need to rotate the\nimages. You should change the interpolation settings when you do this to \"none\"\nto prevent blurring. If you somehow end up with a blurred map, it is possible to\nfix it as follows.\n\n\nUsing GIMP, first make two duplicates of the layer containing the map. In the\nfirst layer duplicated layers, use \ncolours\nthreshold\n, to extract out the black\nregions. A lower threshold of between 190 and 203 seems to be effective, with\nthe upper at 255. You should tweak the lower threshold so that the grey unknown\nareas are white, and most of the obstacles are black. Then, using \nselect\nby\ncolour\n, select the white part of the layer and cut and paste it onto a new\nlayer (\nC-x C-v\n). When you paste, a floating selection layer will come up.\nRight click this floating selection in the layer list, and send it to a new\nlayer. You now have two layers, one with free space, and one with obstacles.\n\n\nIn the other duplicated layer, do the same thing, but now extract the obstacles\nand unknown regions by thresholding. A lower threshold of between 230 and 240\nshould work. Select the white region with the colour select tool again, and\ndelete it. Select the black pixels, and then use \nselect\nshrink\n to shrink the\nselection by a couple of pixels. 2 or 3 should be sufficient. With this\nselection still active, create a new layer. Use the pipette tool, to sample the\n\"unknown\" cell colour from the original map. Paint over the selected area with\nthe pencil tool so that it has the \"unknown\" colour. Arrange the three layers so\nthat the obstacles are on top, unknown regions below, and free space below that.\nFinally, merge the three layers by right clicking the obstacle layer and\nclicking \"merge down\". Do this again to merge the newly created layer and the\nfree space layer.\n\n\nTopological map\n\n\nOnce you have a metric map, you need to set up a topological map for the robot\nto use for path planning and other actions. The topological map is made up of\nnodes, which represent some point of interest or navigation location, and edges,\nwhich are connections between the nodes. The easiest way to set up the\ntopological map is using the strands utilities created for that purpose.\n\n\nIf you already have a map, you can add it into the database with\n\n\nrosrun topological_utils load_yaml_map.py /path/to/my.yaml\n\n\n\n\nThis yaml file can be produced for a map that exists in the database using\n\n\nrosrun topological_utils map_to_yaml.py map_pointset my_topmap.yaml\n\n\n\n\nYou can see which topological maps already exist in the database with\n\n\nrosrun topological_utils list_maps\n\n\n\n\nIf you haven't yet created a map you can add an empty map to the database with\n\n\nrosrun topological_utils insert_empty_map.py my_pointset_name\n\n\n\n\nModifying the map\n\n\nThe best way to modify the map is the use the \ntopological_rviz_tools\n package.\nThis provides some tools and a panel in \nrviz\n which will allow you to quickly\nand easily modify things. If you need more direct access, you can always dump\nthe map in the database (with \nmap_to_yaml.py\n), edit things in the file, and\nthen replace the values in the database with the modified values. This can\nresult in internal inconsistencies, so it is not recommended.\n\n\nYou can launch the rviz tool as follows:\n\n\nroslaunch topological_rviz_tools strands_rviz_topmap.launch map:=/path/to/map.yaml topmap:=topmap_pointset db_path:=/path/to/db\n\n\n\n\nOnce you have added a new node to the map, you should delete the \ntemp_node\n.\nFor instructions on using the rviz topological map editor, see the readme\n\nhere\n.\n\n\nLaunching the core nodes\n\n\nIn order to run the system, core nodes need to run. In general, this is the\nnavigation, executive and database. You will also need to ensure that there are\nnodes providing odometry data and laser scans from your robot setup on the\n\n/odom\n and \n/scan\n topics. You should also ensure that you have battery data\nbeing published on the \n/battery_status\n topic using the Scitos message format:\n\n\nstd_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\nfloat32 voltage\nfloat32 current\nint8 lifePercent\nint16 lifeTime\nbool charging\nbool powerSupplyPresent\nfloat32[] cellVoltage\n\n\n\n\nIf you wish to use your own battery message, you will need to change some things\nin the routine classes in \nstrands_executive_behaviours\n. You will need to\nmodify\n\nthis file\n\nin order to set things up for your required voltages.\n\n\nWe'll assume here that the system is a scitos A5 robot.\n\n\nThe first thing to start is \nroscore\n as usual. We prefer to start roscore\nindependently of other launch files so that they can be restarted if necessary\nwithout breaking the system.\n\n\nAfter that, the robot drivers should be started\n\n\nroslaunch --wait strands_bringup strands_robot.launch with_mux:=false with_magnetic_barrier:=false\n\n\n\n\nThen, the database.\n\n\nroslaunch --wait strands_bringup strands_core.launch db_path:=$DB_PATH\n\n\n\n\nThe navigation requires the UI to be started before it is fully initialised.\n\n\nHOST_IP=$EXTERNAL_UI_IP roslaunch --wait strands_bringup strands_ui.launch\n\n\n\n\nThe \nEXTERNAL_UI_IP\n is the IP at which the interface will be displayed. You can\nchoose localhost, but you should specify the IP that the machine is assigned.\nYou can check this with \nifconfig\n. You should then open a browser and access\n\nEXTERNAL_UI_IP:8090\n. For example, if you have the IP 10.0.11.161, then you\nwould access 10.0.11.161:8090.\n\n\nBasic navigation is launched with\n\n\nroslaunch --wait strands_bringup strands_navigation.launch positionUpdate:=false map:=$NAV_MAP with_no_go_map:=$WITH_NO_GO no_go_map:=$NOGO_MAP topological_map:=$TOP_MAP\n\n\n\n\nNAV_MAP\n is the map to use for navigation, and should point to a yaml file,\nsuch as that created by the \nmap_saver\n.\n\n\nNO_GO_MAP\n is a map that is used to specify nogo areas. It should point to a\nyaml map. This can be used to draw lines in open space which the robot will not\ncross, which can be useful for doorways or other areas which the robot should\nnot enter.\n\n\nTOP_MAP\n is the name of the topological map corresponding to the navigation\nmap. This name should exist in the database that has been loaded above.\n\n\nFinally, the executive deals with tasks.\n\n\nroslaunch --wait task_executor mdp-executor.launch interruptible_wait:=false combined_sort:=true\n\n\n\n\nRoutine\n\n\nThe routine allows tasks to be scheduled on a regular basis. A task can be\npretty much anything you define. You can schedule tasks to be performed within a\nspecific time window each day. The routine also defines when the robot is\nactive. You can specify when the robot should be active and when it should\nremain on the charging station for the day.\n\n\nWhile you can set up your own routine in a python script, it is also possible to\ndo it using the \nautomated_routine\n package. You will need to set up a yaml file\ncontaining various settings for timings, actions and so on. An example with\ncomments can be found\n\nhere\n.\n\n\nThe routine requires that other parts of the system are already running, so it\nshould be launched last.\n\n\nroslaunch --wait automated_routine automated_routine.launch routine_config:=$ROUTINE_CONFIG\n\n\n\n\nROUTINE_CONFIG\n refers to the location of the yaml file which defines the routine.\n\n\nTo have a task run, all you need is an action server which will perform the\nrequired action, and a srv message corresponding to it. The task objects created\nby the routine define parameters for the population of the action object, and\nwhich actionserver the populated message should be passed to in order for the\ntask to be executed.\n\n\nTo see an example of what more complex code for a custom task might look like,\nsee \nhere\n.\nYou can see more about tasks \nhere\n.\n\n\nExample task in simulation\n\n\nIt's also possible to run the routine in simulation. You'll need to run the\nexecutor first with \nroslaunch strands_morse basic_example_executor.launch\n. The\nroutine makes use of the file\n\nstrands_morse/basic_example/conf/basic_routine.yaml\n. If you follow the\ninstructions below to create a basic test action, you can leave this as is, but\nif you'd like to do something else you can modify it however you like.\n\n\nHere is a small example task that you can use to test the routine. Create a\npackage in your workspace with \ncatkin_create_pkg print_string rospy std_msgs\nmessage_generation\n.\n\n\nIn the scripts directory, create a \nprint_string.py\n script and make sure it's\nexecutable with \nchmod +x nav_action.py\n\n\ncd print_string\nmkdir scripts\ncd scripts\ntouch print_string.py\nchmod +x print_string.py\n\n\n\n\nThe script should contain the following code:\n\n\n#! /usr/bin/env python\nimport rospy\nimport actionlib\nfrom pr_str.msg import PrintMessageAction\n\nclass print_string(object):\n\n    def __init__(self):\n        self.server = actionlib.SimpleActionServer('print_string_action', PrintMessageAction, self.process_request, False)\n        self.server.start()\n\n    def process_request(self, req):\n        rospy.loginfo(\nHello, here's a message at waypoint {0}: {1}\n.format(req.waypoint, req.message))\n        self.server.set_succeeded()\n\nif __name__ == '__main__':\n    rospy.init_node('print_string_action')\n    ps = print_string()\n    rospy.loginfo(\nWaiting for action requests.\n)\n    rospy.spin()\n\n\n\n\nTasks will be created by the routine which will send the robot to the waypoints\nrequested in the routine definition, and then a string will be printed wherever\nyou run this script. The \nPrintMessage\n service is defined as follows:\n\n\nstring waypoint\nstring message\n----\n----\nbool result\n\n\n\n\nYou should create an \naction\n directory in the package and create a file\n\nPrintMessage.action\n with the above contents.\n\n\nYou'll also need to populate the \nCMakeLists.txt\n and \npackage.xml\n files like\nthis:\n\n\ncmake_minimum_required(VERSION 2.8.3)\nproject(pr_str)\n\nfind_package(catkin REQUIRED COMPONENTS\n  rospy\n  std_msgs\n  message_generation\n  actionlib_msgs\n  actionlib\n)\n\nadd_action_files(\n  DIRECTORY action\n  FILES\n  PrintMessage.action\n)\n\ngenerate_messages(\n  DEPENDENCIES\n  std_msgs  # Or other packages containing msgs\n  actionlib_msgs\n)\n\ncatkin_package()\n\ninclude_directories(\n  ${catkin_INCLUDE_DIRS}\n)\n\n\n\n\n?xml version=\n1.0\n?\n\n\npackage\n\n  \nname\npr_str\n/name\n\n  \nversion\n0.0.0\n/version\n\n  \ndescription\nThe print_string package\n/description\n\n\n  \nmaintainer email=\nme@mail.net\nme\n/maintainer\n\n\n  \nlicense\nTODO\n/license\n\n\n  \nbuildtool_depend\ncatkin\n/buildtool_depend\n\n  \nbuild_depend\nactionlib\n/build_depend\n\n  \nbuild_depend\nactionlib_msgs\n/build_depend\n\n  \nbuild_depend\nrospy\n/build_depend\n\n  \nbuild_depend\nmessage_generation\n/build_depend\n\n  \nrun_depend\nrospy\n/run_depend\n\n  \nrun_depend\nmessage_runtime\n/run_depend\n\n  \nrun_depend\nactionlib\n/run_depend\n\n  \nrun_depend\nactionlib_msgs\n/run_depend\n\n\n/package\n\n\n\n\n\nCompile the package with \ncatkin build print_string\n, and then run the script\nwith \nrosrun print_string print_string.py\n\n\nFinally, launch the routine with \nroslaunch strands_morse\nbasic_example_routine.launch\n. You should see activity in the executor window\nand in the routine. You can monitor tasks currently in the routine with \nrosrun\ntask_executor schedule_status.py\n.\n\n\nTmux\n\n\nDuring the project we have found tmux to be very useful, as it allows\npersistent terminal sessions which can be accessed remotely. Here is a short\ntmuxinator script that can be used to start off the sessions\n\n\n# ~/.tmuxinator/strands.yml\n\nname: strands\nroot: ~/\npre_window: source `rospack find strands_bringup`/conf/env_vars.sh\nwindows:\n  - ros: roscore\n  - robot: roslaunch --wait strands_bringup strands_robot.launch with_mux:=false with_magnetic_barrier:=false\n  - core:\n      panes:\n        - HOSTNAME=$DB_MACHINE roslaunch --wait strands_bringup strands_core.launch machine:=$DB_MACHINE user:=$RUNTIME_USER db_path:=$\\\nDB_PATH\n        - HOST_IP=$EXTERNAL_UI_IP $DISPLAY_SETTING roslaunch --wait strands_bringup strands_ui.launch mary_machine:=$MARY_MACHINE mary_\\\nmachine_user:=$RUNTIME_USER\n  - navigation: roslaunch --wait strands_bringup strands_navigation.launch positionUpdate:=false map:=$NAV_MAP with_no_go_map:=$WITH_NO\\\nGO no_go_map:=$NOGO_MAP topological_map:=$TOP_MAP chest_xtion_machine:=$CHEST_CAM_MACHINE\n  - executive:\n      panes:\n        - roslaunch --wait task_executor mdp-executor.launch interruptible_wait:=false combined_sort:=true\n        - roslaunch --wait automated_routine automated_routine.launch routine_config:=$ROUTINE_CONFIG\n\n\n\n\nIt can also be found\n\nhere\n.\n\n\nHere is the script that runs in each tmux pane before the commands are passed:\n\n\n#!/usr/bin/env bash\n\nexport EXTERNAL_UI_IP=10.0.11.161\n\n# Database path\nexport DB_PATH=/data/y4_pre_dep/mongo\n# Path to yaml files specifying defaults to load when the db is started\nexport DB_DEFAULTS=/data/y4_pre_dep/defaults\n\n# Topological map to use. This value should exist in the database\nexport TOP_MAP=lg_march2016\n\n# Location of the map to use for navigation\nexport NAV_MAP=/home/strands/tsc_y4_ws/maps/lg_march2016/cropped.yaml\n\n# Whether or not to use nogo map\nexport WITH_NOGO_MAP=false\n\n# Location of the map to use to define no-go areas\n#export NOGO_MAP=\n\nexport ROUTINE_CONFIG=`rospack find automated_routine`/conf/bham_routine.yml\n\n\n\n\nThe file for environment variable setup can be found\n\nhere", 
            "title": "Setup"
        }, 
        {
            "location": "/setup/#setting-up-a-strands-system", 
            "text": "This readme will guide you through the setup and use of a system which uses the\nSTRANDS packages. It is assumed that you have a system that is running Ubuntu\n14.04.", 
            "title": "Setting up a STRANDS system"
        }, 
        {
            "location": "/setup/#ros-and-package-setup", 
            "text": "", 
            "title": "ROS and package setup"
        }, 
        {
            "location": "/setup/#installing-ros", 
            "text": "The first step is to follow the instructions found at http://wiki.ros.org/indigo/Installation/Ubuntu .\nThe full install of ROS with the package  ros-indigo-desktop-full  should\ncontain all the packages that are required.", 
            "title": "Installing ROS"
        }, 
        {
            "location": "/setup/#installing-strands-packages", 
            "text": "To install the strands packages, you should follow the instructions at https://github.com/strands-project-releases/strands-releases/wiki .", 
            "title": "Installing STRANDS packages"
        }, 
        {
            "location": "/setup/#example-system", 
            "text": "Before setting up your own system, you might like to see what a system might\nlook like. Instructions for setting up an example system can be found at https://github.com/strands-project-releases/strands-releases/wiki/Example-STRANDS-System", 
            "title": "Example system"
        }, 
        {
            "location": "/setup/#custom-system-setup", 
            "text": "", 
            "title": "Custom system setup"
        }, 
        {
            "location": "/setup/#database", 
            "text": "While the system is running, it will store data in a mongodb database so that\nvarious components can access it. Depending on what tasks you are running, the\ndatabase can grow quite large (on the order hundreds of gigabytes), so you\nshould create the database on a drive with a lot of space.  The database will be automatically created by the database node, but requires\nthe creation of a directory before it is launched. Running the following will\ninitialise the database. The database is launched in the same way when the full\nsystem is running - it should be run before any other part of the system runs,\nas many of them require it to be up.  DATA_DIR=/my/data/directory\nmkdir -p $DATA_DIR/my_database_dir\nroslaunch mongodb_store mongodb_store.launch db_path:=$DATA_DIR/my_database_dir  You can see more information about the database system at https://github.com/strands-project/mongodb_store/tree/hydro-devel/mongodb_store", 
            "title": "Database"
        }, 
        {
            "location": "/setup/#using-a-simulation", 
            "text": "You can use a simulated environment in order to test the system, either with\nexisting simulated environments, or you can set things up to simulate the\nenvironment you wish to operate in. Later on in this document, there are\ninstructions on how to set up a metric map and topological map. These can both\nbe applied in a simulated environment as well.", 
            "title": "Using a simulation"
        }, 
        {
            "location": "/setup/#existing-simulations", 
            "text": "Simulations are based on the  strands_morse  package. If you look in there,\nyou'll find several real environments which have already been set up. We'll look\nat the  strands_morse/bham  directory here.  The  maps  directory contains maps, as you might expect. There are yaml and\npgm map pairs for the ROS grid maps. The tplg file is a yaml export of the\ntopological map for that area.  The  urdf  directory contains urdf files for the environment, which make use of\ndae files in the  meshes  directory.  The  launch  directory contains some launch files which can be used to launch\neverything with just one command.  The top level python files are used to construct various environments, setting\nup the robot position, model and so on.  Before starting the simulation, you'll need to add the topological map to your\ndatabase using  rosrun topological_utils load_yaml_map.py `rospack find strands_morse`/bham/maps/cs_lg_sim.tplg  Once this is done, run  roslaunch strands_morse bham_cs_morse.launch  This will bring up the blender window with the environment loaded and populated\nwith objects and a robot.  roslaunch strands_morse bham_cs_nav2d.launch  Will bring up all navigation and related nodes.  Finally, use rviz to visualise the map and topological map:  roslaunch strands_morse bham_default_rviz.launch  To allow the robot to move, you'll have to move it backwards first - you can do\nthis by pressing the down arrow on your keyboard when focusing the blender\nwindow. Once you've done that, you can click the green arrows to make the robot\nnavigate there, or you can specify a navigation location using the 2d nav goal\ntool in rviz.", 
            "title": "Existing simulations"
        }, 
        {
            "location": "/setup/#custom-simulations", 
            "text": "To set up a custom simulation, you'll first need to generate a 3D environment to\nuse. A simple environment is easy to construct. You'll need to have GIMP,\ninkscape and blender installed to create it.  If you want to skip the details, you can find the files created by this section here .\nYou will still have to add the topological maps to your mongo database with rosrun topological_utils load_yaml_map.py maps/basic_map.tpl .", 
            "title": "Custom simulations"
        }, 
        {
            "location": "/setup/#png-map", 
            "text": "The first step is to use GIMP to create an image of the walls in the map that\nyou want to use. The image below is made using the pencil tool and holding\nctrl+shift to make straight lines. It has a scale of 35px/cm. We'll use this\nlater to scale the environment to our robot.   You can make something like this map using blueprints for the area you are\nworking in and the process below should be similar.", 
            "title": "PNG map"
        }, 
        {
            "location": "/setup/#svg-map", 
            "text": "Once you have an image which contains only black and white pixels, open inkscape\nand import the image, with the  embed  option.  Make sure the image is selected, and then open the  trace bitmap  dialogue with\nalt+shift+b. Select the single scan colour quantisation option, with 2 colours,\nand check the box to invert the image. Click the update button on the right hand\nside and you should see the result of the tracing. This tracing will convert the\nimage into a vector graphic which can be used in blender. You can fiddle with\nthe options until you get a result that looks good. Once you're satisfied, press\nthe OK button. Then, save the image as an svg file.", 
            "title": "SVG map"
        }, 
        {
            "location": "/setup/#blender-model", 
            "text": "Open blender, and delete the cube that is in the space with the delete key.\nThen, with  file import  import the svg that you just created. You should see it\nin the space as some black lines. You can select it in the top right hand side,\nwhere it will exist as a curve. The image we started with had a scale of\n35px/cm, which will be very small for our robot, which is around 80cm across\n(assuming that we're using the Scitos G5). On the right hand toolbar, you should\nsee a set of icons - a camera, some photos, a chain link, a cube, and so on.\nSelect the cube icon. This Will bring up a set of options which include scaling.\nIn the image, some openings which could represent doors are approximately 50\npixels wide. We'll make these openings 1.5 metres wide, to make them easy to get\nthrough. This means that each pixel has to be 0.03 (1.5/50) metres. At 35px/cm,\neach pixel in the image was 0.000286 metres. So, in order to get the size we\nwant, we should scale each pixel by approximately 105 (0.03/0.000286). We'll\napply this scaling to both the x and y axes.  Once that scaling is done, we also need to make some proper 3D walls with\nheight. Select the curve icon on the right hand side, and you should see under\nthe geometry section the option to extrude. Set this value to 1.5 and it should\nbe OK. Since extruding goes in both vertical directions, shift the model up by\n1.5 in the transform options in the cube section.  We also need a floor, so add a plane in using  add mesh plane . Scale it so that\nit covers approximately the area needed to cover all the floor space in the map,\nand transform it so that it sits below the imported map. You may wish to\ntranslate or rotate the map so that it sits in the positive quadrant of the\nspace - when it is imported it will sit on the positive x axis but negative y\naxis.  There also needs to be a light source so you can see what is going on. Instead\nof the weak lamp that currently exists in the space, you should add a sun\n( add lamp sun ), which provides much more light.  The final step is to convert the curve to a mesh so that it is correctly\ndisplayed. With the curve selected, press alt+c, which will bring up a\nconversion menu. Use \"mesh from curve\" option, and then save the blender file.  You can find example files created from this process here .", 
            "title": "Blender model"
        }, 
        {
            "location": "/setup/#creating-the-simulation-files", 
            "text": "Now that we have all these models, we need to create some files to run\neverything. We'll put everything into a new package for convenience.  First, create a new ros package in your workspace.  roscd\ncd src\ncatkin_create_pkg basic_example\ncd basic_example\nmkdir scripts  In the scripts directory, create a script  simulator.sh  that will be used to\nrun the simulation. Its basic contents should look something like the following.\nIt sets up various paths and then runs another file which defines what the\nsimulation environment actually looks like:  #!/bin/bash\nenvironment_name= basic_example \nstrands_morse=`rospack find strands_morse`\nexample=`rospack find basic_example`\npath= $example \ncommon= $strands_morse/strands_sim \n\nPYTHONPATH= $path/src:$common/src:$PYTHONPATH \nMORSE_RESOURCE_PATH= $strands_morse:$common/data:$common/robots:$path:$MORSE_RESOURCE_PATH \nexport MORSE_RESOURCE_PATH PYTHONPATH\nadded=`$strands_morse/morse_config.py $environment_name $path`\necho  Running morse on $path with PYTHONPATH=$PYTHONPATH and MORSE_RESOURCE_PATH=$MORSE_RESOURCE_PATH \nPATH=/opt/strands-morse-simulator/bin:$PATH\n\nmorse run thermo `rospack find basic_example`/example_sim.py  Don't forget to run  chmod +x scripts/simulator.sh  to make it executable.  In the top level directory, create the simulation definition ( example_sim.py )  #! /usr/bin/env morseexec\n\nimport sys\nimport subprocess \nimport os\nimport random\n\nfrom morse.builder import *\nfrom strands_sim.builder.robots import Scitosa5\n\nrobot = Scitosa5(with_cameras = Scitosa5.WITHOUT_DEPTHCAMS)\n# Specify the initial position and rotation of the robot\nrobot.translate(x=2,y=2, z=0)\nrobot.rotate(z=-1.57)\n\n# Specify where the model of the environment is\nmodel_file=os.path.join(os.path.dirname(os.path.abspath( __file__)),'maps/basic_map.blend')\n# Create the environment with the model file, and use fast mode - you can do\n# this to speed things up a little when you're using the scitos A5 without\n# depthcams.\nenv = Environment(model_file,fastmode=True)\n# Place the camera in the environment\nenv.set_camera_location([0, 0, 10])\n# Aim the camera so that it's looking at the environment\nenv.set_camera_rotation([0.5, 0, -0.5])  Download the basic map created above from github into the maps directory.  roscd basic_example\nmkdir maps\ncd maps\nwget https://github.com/strands-project/strands_documentation/raw/master/resources/basic_map.blend  Create a launch file which will be used to launch the simulator ( launch/basic_example.launch )  launch \n\n    !-- Scitos robot -- \n    include file= $(find strands_morse)/launch/scitos.launch / \n\n    node pkg= basic_example  type= simulator.sh  respawn= false  name= basic_example  output= screen /  /launch   Finally, compile the package with  catkin build basic_example . You should then\nbe able to run  roslaunch basic_example basic_example.launch , and see a robot\nin the world.  At this point, the robot will not be able to move. The following file\n( launch/basic_example_nav.launch ) will launch the required parts of the\nstrands system.  launch \n   !-- declare arg to be passed in -- \n   arg name= with_chest_xtion  default= false / \n   arg name= mon_nav_config_file   default=  / \n   arg name= max_bumper_recoveries  default= .inf / \n   arg name= wait_reset_bumper_duration  default= 0.0 / \n   arg name= topological_navigation_retries  default= 3 / \n   arg name= topological_map_name  default= basic_map / \n   arg name= map  default= $(find strands_morse)/basic_example/maps/basic_map.yaml / \n\n   !-- 2D Navigation -- \n   include file= $(find strands_movebase)/launch/movebase.launch \n       arg name= map  value= $(arg map) / \n       arg name= with_chest_xtion  value= $(arg with_chest_xtion) / \n   /include \n\n   node pkg= monitored_navigation  type= monitored_nav.py  name= monitored_nav  output= screen  args= $(arg mon_nav_config_file) \n     param name= wait_reset_bumper_duration  value= $(arg wait_reset_bumper_duration) / \n     rosparam param= /monitored_navigation/recover_states/recover_bumper  subst_value= True [True, $(arg max_bumper_recoveries)] /rosparam \n   /node \n\n   node pkg= topological_navigation  type= map_manager.py  name= topological_map_manager  args= $(arg topological_map_name)  respawn= true / \n   node pkg= topological_navigation  name= topological_localisation  type= localisation.py  output= screen  respawn= true / \n   node pkg= topological_navigation  type= visualise_map.py  name= visualise_map  args= $(arg topological_map_name)  respawn= true / \n\n   node pkg= topological_navigation  name= topological_navigation  type= navigation.py  output= screen  respawn= true \n     param name= retries  type= int  value= $(arg topological_navigation_retries) / \n   /node \n\n   node pkg= tf  type= static_transform_publisher  name= env_broadcaster  \n        args= 0 0 0 0 0 0 /odom /map 200 \n   /node  /launch   You can also use the following launch file ( launch/basic_example_rviz.launch )\nto launch an rviz instance with various interactive markers set up.  launch \n   node pkg= rviz  type= rviz  name= rviz  args= -d $(find strands_morse)/basic_example/default.rviz /  /launch   To use this, you'll first have to construct a pgm map. You can do this by\ncolouring the image you used to create the simulation map with the correct\ncolours for ROS map usage (e.g. the basic map ).\nAlternatively, you can also use gmapping - see below for instructions. You\nshould save the map to  maps/basic_map.pgm  and  maps/basic_map.yaml , or save\nit elsewhere and point the above launch file to the correct location. If you\nmake a map from the image, you will have to create a corresponding yaml file to\ndescribe it and give the scaling of the image and some other details. See map server  for details on the yaml format.  If you use gmapping, you can use  rosrun teleop_twist_keyboard\nteleop_twist_keyboard.py  to control the motion of the robot. You may have to\ninstall this package first. You should also run rviz so that you can see the map\nbeing constructed and make sure you haven't missed any part of it. You can leave\nthe map as it is, or trim it to remove some of the excess parts if your map is\nsmall. In that case you will need to change the origin of the map so that it\ncorresponds with where you want your origin to be.  You should follow the instructions in the topological map section below to\ncreate a topological map for the environment. Once you've created it and\ninserted it into the mongo database, you should change the default  map_name  to\nthe name of the map in your database. You can find an example here .  When running the system, you may have to set the position of the robot in rviz\nto the correct location on the map, as the origin of the map there and in the\nsimulation does not align.  You can find documentation for the MORSE simulator here , which gives more\ndetails about what you can do in the  example_sim.py  file.", 
            "title": "Creating the simulation files"
        }, 
        {
            "location": "/setup/#metric-map", 
            "text": "The metric map is a 2D map of the operating environment, where each cell of a\ngrid is populated with a value which represents whether that that cell is\noccupied by an obstacle, is empty, or has an unknown occupancy state. The\nquickest and easiest way to map your environment is using the ROS gmapping  package. How you use this package\nwill depend on the type of robot you have. The package requires that you have\nlaser and odometry data being published to the ROS system.  Assuming that your laser data is being published on  /base_scan , and odometry\non  /odom , you can start the mapping process as below. The maxUrange  parameter defines a threshold on the distance of usable range\nmeasurements received. For example, setting this to 20 will discard any readings\nreceived which are beyond 20 metres.  rosrun gmapping slam_gmapping scan:=base_scan maxUrange:=20  While this runs, you can observe the map being built in the  rviz  utility by\nadding a display for the  /map  topic. Push the robot around in your operation\narea. You should try to move relatively slowly. You should also try to ensure\nthat you revisit previously mapped areas after going around the environment, so\nthat the map can be properly adjusted.  Once you are happy with your map, you should save it using the map_server :  rosrun map_server map_saver -f my_map map:=my_map_topic  This will produce a  .pgm  file and a  .yaml  file. The  .pgm  file contains an\nimage of the map, which you can manipulate with an image editing program if\nnecessary. The  .yaml  file contains information about the map. If something\nstrange is happening with your map, then it might be worth checking that this\nfile is set up to point to the correct  .pgm  file. You can also adjust the\nresolution of the map and its origin in the  .yaml  file.  At this point, you may wish to clean up the image to remove dynamic obstacles\nfrom the map. In GIMP, you can do this by using the pencil tool with white\nselected.  Along with the base map, it is also possible to provide a \"nogo\" map, which is\nused to more strictly define which areas are passable and which are not. You can\nuse this to restrict the robot's movement in an area where there are no walls to\nobstruct the robot's motion. The nogo map should duplicate the base map. You can\nthen draw obstacles onto the map in black (255) in the places which you would\nlike to have a phantom obstacle. The pencil tool in GIMP is again useful. We\nrecommend creating a new GIMP file with the nogo map on a new layer so that it\ncan be more easily modified if necessary. GIMP can export the file to a  .pgm \nfile.  Once you are happy with your map, you can use  rosrun map_server map_server my_map.yaml map:=mymap  to make the map available on the  /mymap  topic.", 
            "title": "Metric map"
        }, 
        {
            "location": "/setup/#adding-to-the-map", 
            "text": "Sometimes it may be necessary to remap parts of the map due to changes in the\nenvironment. You can use gmapping to do this, and then stitch the images\ntogether in an image editing program. Sometimes, you may need to rotate the\nimages. You should change the interpolation settings when you do this to \"none\"\nto prevent blurring. If you somehow end up with a blurred map, it is possible to\nfix it as follows.  Using GIMP, first make two duplicates of the layer containing the map. In the\nfirst layer duplicated layers, use  colours threshold , to extract out the black\nregions. A lower threshold of between 190 and 203 seems to be effective, with\nthe upper at 255. You should tweak the lower threshold so that the grey unknown\nareas are white, and most of the obstacles are black. Then, using  select by\ncolour , select the white part of the layer and cut and paste it onto a new\nlayer ( C-x C-v ). When you paste, a floating selection layer will come up.\nRight click this floating selection in the layer list, and send it to a new\nlayer. You now have two layers, one with free space, and one with obstacles.  In the other duplicated layer, do the same thing, but now extract the obstacles\nand unknown regions by thresholding. A lower threshold of between 230 and 240\nshould work. Select the white region with the colour select tool again, and\ndelete it. Select the black pixels, and then use  select shrink  to shrink the\nselection by a couple of pixels. 2 or 3 should be sufficient. With this\nselection still active, create a new layer. Use the pipette tool, to sample the\n\"unknown\" cell colour from the original map. Paint over the selected area with\nthe pencil tool so that it has the \"unknown\" colour. Arrange the three layers so\nthat the obstacles are on top, unknown regions below, and free space below that.\nFinally, merge the three layers by right clicking the obstacle layer and\nclicking \"merge down\". Do this again to merge the newly created layer and the\nfree space layer.", 
            "title": "Adding to the map"
        }, 
        {
            "location": "/setup/#topological-map", 
            "text": "Once you have a metric map, you need to set up a topological map for the robot\nto use for path planning and other actions. The topological map is made up of\nnodes, which represent some point of interest or navigation location, and edges,\nwhich are connections between the nodes. The easiest way to set up the\ntopological map is using the strands utilities created for that purpose.  If you already have a map, you can add it into the database with  rosrun topological_utils load_yaml_map.py /path/to/my.yaml  This yaml file can be produced for a map that exists in the database using  rosrun topological_utils map_to_yaml.py map_pointset my_topmap.yaml  You can see which topological maps already exist in the database with  rosrun topological_utils list_maps  If you haven't yet created a map you can add an empty map to the database with  rosrun topological_utils insert_empty_map.py my_pointset_name", 
            "title": "Topological map"
        }, 
        {
            "location": "/setup/#modifying-the-map", 
            "text": "The best way to modify the map is the use the  topological_rviz_tools  package.\nThis provides some tools and a panel in  rviz  which will allow you to quickly\nand easily modify things. If you need more direct access, you can always dump\nthe map in the database (with  map_to_yaml.py ), edit things in the file, and\nthen replace the values in the database with the modified values. This can\nresult in internal inconsistencies, so it is not recommended.  You can launch the rviz tool as follows:  roslaunch topological_rviz_tools strands_rviz_topmap.launch map:=/path/to/map.yaml topmap:=topmap_pointset db_path:=/path/to/db  Once you have added a new node to the map, you should delete the  temp_node .\nFor instructions on using the rviz topological map editor, see the readme here .", 
            "title": "Modifying the map"
        }, 
        {
            "location": "/setup/#launching-the-core-nodes", 
            "text": "In order to run the system, core nodes need to run. In general, this is the\nnavigation, executive and database. You will also need to ensure that there are\nnodes providing odometry data and laser scans from your robot setup on the /odom  and  /scan  topics. You should also ensure that you have battery data\nbeing published on the  /battery_status  topic using the Scitos message format:  std_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\nfloat32 voltage\nfloat32 current\nint8 lifePercent\nint16 lifeTime\nbool charging\nbool powerSupplyPresent\nfloat32[] cellVoltage  If you wish to use your own battery message, you will need to change some things\nin the routine classes in  strands_executive_behaviours . You will need to\nmodify this file \nin order to set things up for your required voltages.  We'll assume here that the system is a scitos A5 robot.  The first thing to start is  roscore  as usual. We prefer to start roscore\nindependently of other launch files so that they can be restarted if necessary\nwithout breaking the system.  After that, the robot drivers should be started  roslaunch --wait strands_bringup strands_robot.launch with_mux:=false with_magnetic_barrier:=false  Then, the database.  roslaunch --wait strands_bringup strands_core.launch db_path:=$DB_PATH  The navigation requires the UI to be started before it is fully initialised.  HOST_IP=$EXTERNAL_UI_IP roslaunch --wait strands_bringup strands_ui.launch  The  EXTERNAL_UI_IP  is the IP at which the interface will be displayed. You can\nchoose localhost, but you should specify the IP that the machine is assigned.\nYou can check this with  ifconfig . You should then open a browser and access EXTERNAL_UI_IP:8090 . For example, if you have the IP 10.0.11.161, then you\nwould access 10.0.11.161:8090.  Basic navigation is launched with  roslaunch --wait strands_bringup strands_navigation.launch positionUpdate:=false map:=$NAV_MAP with_no_go_map:=$WITH_NO_GO no_go_map:=$NOGO_MAP topological_map:=$TOP_MAP  NAV_MAP  is the map to use for navigation, and should point to a yaml file,\nsuch as that created by the  map_saver .  NO_GO_MAP  is a map that is used to specify nogo areas. It should point to a\nyaml map. This can be used to draw lines in open space which the robot will not\ncross, which can be useful for doorways or other areas which the robot should\nnot enter.  TOP_MAP  is the name of the topological map corresponding to the navigation\nmap. This name should exist in the database that has been loaded above.  Finally, the executive deals with tasks.  roslaunch --wait task_executor mdp-executor.launch interruptible_wait:=false combined_sort:=true", 
            "title": "Launching the core nodes"
        }, 
        {
            "location": "/setup/#routine", 
            "text": "The routine allows tasks to be scheduled on a regular basis. A task can be\npretty much anything you define. You can schedule tasks to be performed within a\nspecific time window each day. The routine also defines when the robot is\nactive. You can specify when the robot should be active and when it should\nremain on the charging station for the day.  While you can set up your own routine in a python script, it is also possible to\ndo it using the  automated_routine  package. You will need to set up a yaml file\ncontaining various settings for timings, actions and so on. An example with\ncomments can be found here .  The routine requires that other parts of the system are already running, so it\nshould be launched last.  roslaunch --wait automated_routine automated_routine.launch routine_config:=$ROUTINE_CONFIG  ROUTINE_CONFIG  refers to the location of the yaml file which defines the routine.  To have a task run, all you need is an action server which will perform the\nrequired action, and a srv message corresponding to it. The task objects created\nby the routine define parameters for the population of the action object, and\nwhich actionserver the populated message should be passed to in order for the\ntask to be executed.  To see an example of what more complex code for a custom task might look like,\nsee  here .\nYou can see more about tasks  here .", 
            "title": "Routine"
        }, 
        {
            "location": "/setup/#example-task-in-simulation", 
            "text": "It's also possible to run the routine in simulation. You'll need to run the\nexecutor first with  roslaunch strands_morse basic_example_executor.launch . The\nroutine makes use of the file strands_morse/basic_example/conf/basic_routine.yaml . If you follow the\ninstructions below to create a basic test action, you can leave this as is, but\nif you'd like to do something else you can modify it however you like.  Here is a small example task that you can use to test the routine. Create a\npackage in your workspace with  catkin_create_pkg print_string rospy std_msgs\nmessage_generation .  In the scripts directory, create a  print_string.py  script and make sure it's\nexecutable with  chmod +x nav_action.py  cd print_string\nmkdir scripts\ncd scripts\ntouch print_string.py\nchmod +x print_string.py  The script should contain the following code:  #! /usr/bin/env python\nimport rospy\nimport actionlib\nfrom pr_str.msg import PrintMessageAction\n\nclass print_string(object):\n\n    def __init__(self):\n        self.server = actionlib.SimpleActionServer('print_string_action', PrintMessageAction, self.process_request, False)\n        self.server.start()\n\n    def process_request(self, req):\n        rospy.loginfo( Hello, here's a message at waypoint {0}: {1} .format(req.waypoint, req.message))\n        self.server.set_succeeded()\n\nif __name__ == '__main__':\n    rospy.init_node('print_string_action')\n    ps = print_string()\n    rospy.loginfo( Waiting for action requests. )\n    rospy.spin()  Tasks will be created by the routine which will send the robot to the waypoints\nrequested in the routine definition, and then a string will be printed wherever\nyou run this script. The  PrintMessage  service is defined as follows:  string waypoint\nstring message\n----\n----\nbool result  You should create an  action  directory in the package and create a file PrintMessage.action  with the above contents.  You'll also need to populate the  CMakeLists.txt  and  package.xml  files like\nthis:  cmake_minimum_required(VERSION 2.8.3)\nproject(pr_str)\n\nfind_package(catkin REQUIRED COMPONENTS\n  rospy\n  std_msgs\n  message_generation\n  actionlib_msgs\n  actionlib\n)\n\nadd_action_files(\n  DIRECTORY action\n  FILES\n  PrintMessage.action\n)\n\ngenerate_messages(\n  DEPENDENCIES\n  std_msgs  # Or other packages containing msgs\n  actionlib_msgs\n)\n\ncatkin_package()\n\ninclude_directories(\n  ${catkin_INCLUDE_DIRS}\n)  ?xml version= 1.0 ?  package \n   name pr_str /name \n   version 0.0.0 /version \n   description The print_string package /description \n\n   maintainer email= me@mail.net me /maintainer \n\n   license TODO /license \n\n   buildtool_depend catkin /buildtool_depend \n   build_depend actionlib /build_depend \n   build_depend actionlib_msgs /build_depend \n   build_depend rospy /build_depend \n   build_depend message_generation /build_depend \n   run_depend rospy /run_depend \n   run_depend message_runtime /run_depend \n   run_depend actionlib /run_depend \n   run_depend actionlib_msgs /run_depend  /package   Compile the package with  catkin build print_string , and then run the script\nwith  rosrun print_string print_string.py  Finally, launch the routine with  roslaunch strands_morse\nbasic_example_routine.launch . You should see activity in the executor window\nand in the routine. You can monitor tasks currently in the routine with  rosrun\ntask_executor schedule_status.py .", 
            "title": "Example task in simulation"
        }, 
        {
            "location": "/setup/#tmux", 
            "text": "During the project we have found tmux to be very useful, as it allows\npersistent terminal sessions which can be accessed remotely. Here is a short\ntmuxinator script that can be used to start off the sessions  # ~/.tmuxinator/strands.yml\n\nname: strands\nroot: ~/\npre_window: source `rospack find strands_bringup`/conf/env_vars.sh\nwindows:\n  - ros: roscore\n  - robot: roslaunch --wait strands_bringup strands_robot.launch with_mux:=false with_magnetic_barrier:=false\n  - core:\n      panes:\n        - HOSTNAME=$DB_MACHINE roslaunch --wait strands_bringup strands_core.launch machine:=$DB_MACHINE user:=$RUNTIME_USER db_path:=$\\\nDB_PATH\n        - HOST_IP=$EXTERNAL_UI_IP $DISPLAY_SETTING roslaunch --wait strands_bringup strands_ui.launch mary_machine:=$MARY_MACHINE mary_\\\nmachine_user:=$RUNTIME_USER\n  - navigation: roslaunch --wait strands_bringup strands_navigation.launch positionUpdate:=false map:=$NAV_MAP with_no_go_map:=$WITH_NO\\\nGO no_go_map:=$NOGO_MAP topological_map:=$TOP_MAP chest_xtion_machine:=$CHEST_CAM_MACHINE\n  - executive:\n      panes:\n        - roslaunch --wait task_executor mdp-executor.launch interruptible_wait:=false combined_sort:=true\n        - roslaunch --wait automated_routine automated_routine.launch routine_config:=$ROUTINE_CONFIG  It can also be found here .  Here is the script that runs in each tmux pane before the commands are passed:  #!/usr/bin/env bash\n\nexport EXTERNAL_UI_IP=10.0.11.161\n\n# Database path\nexport DB_PATH=/data/y4_pre_dep/mongo\n# Path to yaml files specifying defaults to load when the db is started\nexport DB_DEFAULTS=/data/y4_pre_dep/defaults\n\n# Topological map to use. This value should exist in the database\nexport TOP_MAP=lg_march2016\n\n# Location of the map to use for navigation\nexport NAV_MAP=/home/strands/tsc_y4_ws/maps/lg_march2016/cropped.yaml\n\n# Whether or not to use nogo map\nexport WITH_NOGO_MAP=false\n\n# Location of the map to use to define no-go areas\n#export NOGO_MAP=\n\nexport ROUTINE_CONFIG=`rospack find automated_routine`/conf/bham_routine.yml  The file for environment variable setup can be found here", 
            "title": "Tmux"
        }, 
        {
            "location": "/aaf_deployment/aaf_simulation/", 
            "text": "aaf_sim\n\n\nThis package contains files that are necessary for running STRANDS simulations on the University of Lincoln environments.\n\n\nSetting up Autonomous Patrolling Simulation\n\n\n\n\nCalibrate charging station parameters:\n\n\nLaunch strands_datacentre:\n           ```\n           roslaunch mongodb_store mongodb_store.launch db_path:=/opt/strands/mongodb_store\n   ```\n\n\n\n\n\n\n\nLaunch simulation:\n       ```\n       roslaunch strands_morse aaf_sim_morse.launch\n\n\n* Launch scitos_docking:\n\n   roslaunch scitos_docking charging.launch\n\n\n* Drive the robot to the charging station\n   * Calibrate charging parameters running:\n\n   rosrun scitos_docking visual_charging_client calibrate 100\n\n\n2. Insert waypoints on database:\n   * Launch strands_datacentre:\n\n   roslaunch mongodb_store mongodb_store.launch db_path:=/opt/strands/mongodb_store\n\n\n* Insert waypoints in DB\n\n    rosrun topological_utils insert_map.py $(rospack find aaf_simulation)/maps/aaf_sim.tmap aaf_sim aaf_sim\n\n\n```\n   NOTE: You can also create your own topological map following the instructions on: https://github.com/strands-project/strands_navigation/tree/hydro-devel/topological_navigation\n\n\n\n\n\n\n\n\n\n\nLaunching Autonomous Patrolling Simulation\n\n\nIf all previous steps are done launch simulation by running:\n\n\n   ```\n   rosrun aaf_simulation start.sh\n\n   ```", 
            "title": "Aaf simulation"
        }, 
        {
            "location": "/aaf_deployment/aaf_simulation/#aaf_sim", 
            "text": "This package contains files that are necessary for running STRANDS simulations on the University of Lincoln environments.", 
            "title": "aaf_sim"
        }, 
        {
            "location": "/aaf_deployment/aaf_simulation/#setting-up-autonomous-patrolling-simulation", 
            "text": "Calibrate charging station parameters:  Launch strands_datacentre:\n           ```\n           roslaunch mongodb_store mongodb_store.launch db_path:=/opt/strands/mongodb_store    ```    Launch simulation:\n       ```\n       roslaunch strands_morse aaf_sim_morse.launch  * Launch scitos_docking: \n   roslaunch scitos_docking charging.launch  * Drive the robot to the charging station\n   * Calibrate charging parameters running: \n   rosrun scitos_docking visual_charging_client calibrate 100  2. Insert waypoints on database:\n   * Launch strands_datacentre: \n   roslaunch mongodb_store mongodb_store.launch db_path:=/opt/strands/mongodb_store  * Insert waypoints in DB \n    rosrun topological_utils insert_map.py $(rospack find aaf_simulation)/maps/aaf_sim.tmap aaf_sim aaf_sim  ```\n   NOTE: You can also create your own topological map following the instructions on: https://github.com/strands-project/strands_navigation/tree/hydro-devel/topological_navigation", 
            "title": "Setting up Autonomous Patrolling Simulation"
        }, 
        {
            "location": "/aaf_deployment/aaf_simulation/#launching-autonomous-patrolling-simulation", 
            "text": "If all previous steps are done launch simulation by running:     ```\n   rosrun aaf_simulation start.sh\n\n   ```", 
            "title": "Launching Autonomous Patrolling Simulation"
        }, 
        {
            "location": "/aaf_deployment/aaf_walking_group/", 
            "text": "Walking Group\n\n\nThis AAF task is meant to accompany the walking group, driving infront of the therapist and the patients, providing entertainment during rests.\n\n\nFunctionality\n\n\nThe interaction between the robot and therapist is based on the so-called key card (TODO: Upload key-card) which has to be worn around his/her neck. There can be several therpists, each wearing a key card, but there has to be at least one. The key card has to be worn visably, as the robot relies on it to identify the therapist.\n\n\n\n\nThe robot will be waiting for the group to start at the predefined starting location. During this phase it will provide entertainment, i.e. play music, play videos, or show images via a simple to use and clearly structured interface which is supposed to be used by the patients.\n\n\nTo start the group, the therapist shows the robot the key card in the correct orientation. This will trigger the so called guide interface. In the guide interface,\n\n\nthe therapist can toggle playing of music while the groups is walking along the corridors. This will drown out every other auditive feedback. If music is turned off, the robot will play jingles and nature sounds at predefined locations and events.\n\n\nthe next waypoint can be chosen, either by just pressing \"Weiter\" which lets the robot drive to the next waypoint in the tour, or by selecting them from a list of all possible waypoints.\n\n\nthe guide interface can be cancelled which results in showing the entertainment interface again.\n\n\nthe whole tour can be cancelled which results in the robot going away, doing a science.\n\n\nDuring the actual walking, the robot is driving in front of the group, playing either music or jingles and nature sounds. After a predefined distance the robot stops, waiting for the therapist to be \"close\" (using the key card to meassure distance), and then shows a large \"Weiter\" button, which can be bressed by the patients to send the robot off and to elicit interaction.\n\n\nWhen a resting area is reached, the robot waits for the guide to come \"close\" (using the key card to meassure distance), and then shows two buttons, i.e. \"Witer\" and \"Rast\", which either send the robot to the next waypoint or shows the entertainment interface. \n\n\n\n\nThis will continue until the final waypoint is reached. During the whole tour, whenever the guide shows the key card in the specific orientation, the robot will stop and show the guide interface. This way, the music can be toggled, the waypoint chosen or the tour preempted at any point in time.\n\n\nUsage\n\n\nThe whole behaviour is run via the \naxlaunch server\n , starting the components, and \nthe abstract task server\n to create scheduable tasks.\n\n\nConfiguration\n\n\nThe most basic and important configuration happens via a datacentre entry storing the waypoints at which to rest and the distance at which the robot should stop in regular intervals. An example file could look like this:\n\n\nslow:\n    stopping_distance: 3.0\n    waypoints:\n        1: \nWayPoint34\n\n        2: \nWayPoint14\n\n        3: \nWayPoint26\n\n        4: \nWayPoint15\n\n        5: \nWayPoint46\n\nfast:\n    stopping_distance: 5.0\n    waypoints:\n        1: \nWayPoint34\n\n        2: \nWayPoint14\n\n        3: \nWayPoint26\n\n        4: \nWayPoint15\n\n        5: \nWayPoint46\n\n\n\n\n\nslow\n and \nfast\n are internally used identifiers. They don't have to have specific names but have to be unique and consistent with the \ngroup\n entry described below. The \nstopping_distance\n is the distance after which the robot will wait for the therapist to come close, to show a button, which can be pressed by the patients to send it off again. The \nwaypoints\n entry specifies the waypoints at which the group usually rests. Waypoints not specified in this list can never be selected as resting areas during a tour; the index specifies the order of points.\n\n\nThis file has to be inserted into the datacentre using a provided script:\n\n\n$ rosrun aaf_walking_group insert_yaml.py --help\nusage: insert_yaml.py [-h] -i INPUT [--collection_name COLLECTION_NAME]\n                      [--meta_name META_NAME]\n                      dataset_name\n\npositional arguments:\n  dataset_name          The name of the dataset. Saved in meta information\n                        using 'meta_name'\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i INPUT, --input INPUT\n                        Input yaml file\n  --collection_name COLLECTION_NAME\n                        The collection name. Default: aaf_walking_group\n  --meta_name META_NAME\n                        The name of the meta filed to store 'dataset_name' in.\n                        Default: waypoint_set\n\n\n\n\n\nshows it's functionalities. The defaults for \nmeta_name\n and \ncollection_name\n can be changed, but will then have to be specified at start-up. Otherwise, the defaults in the launch file correspond with the defaults in the script. The \ndataset_name\n name is a mendatory argument which is used to identify the configuration you stored. With \n-i \nmy_file\n you specify the input file. Once that has been inserted we can move on to the second config file, loaded on start-up.\nAn example file could look like this:\n\n\nwalking_group:\n    walking_group_slow:\n        group: 'slow'\n        start_location: 'WayPoint34'\n        max_duration: 3600\n        parameters: \n            - 'music_set_group'\n            - 'waypoint_set'\n        values: \n            - 'my_set'\n            - 'aaf_waypoints'\n    walking_group_fast:\n        group: 'fast'\n        start_location: 'WayPoint34'\n        max_duration: 3600\n        parameters: \n            - 'music_set_group'\n            - 'waypoint_set'\n        values: \n            - 'my_set'\n            - 'aaf_waypoints'\n\n\n\n\nwalking_group_slow\n and \nwalking_group_fast\n is used to create two action servers with the corresponding name, therefore these names have to be unique. The number of action servers is dynamic and can be changed by adding another entry to this file. \ngroup\n is used to identify the specific set of waypoints from the datacentre (see above); \nslow\n or \nfast\n in our case. \nstart_location\n is the waypoint to which the scheduler sends the robot at the beginning of the task and should be the same as the first waypoint in the list above; \nWayPoint34\n in our case. The \nmax_duration\n is another argument for the schaduler which tells it how long the group task lasts in the worst case. If this time is reached it will assume that the task failed and preempt it. The \nparameters\n and \nvalues\n fields are used together to create the start-up parameters for the internally used launch file:\n\n\n\n\nhead_machine\n \ndefault=\"localhost\"\n: The machine to which the head_camera is connected. The social card reader and position of card will be started there.\n\n\nhead_user\n \ndefault=\"\"\n: The user of the machine to which the head_camera is connected. The social card reader and position of card will be started there.\n\n\nwaypoint_set\n \ndefault=\"aaf_waypoints\"\n: The \ndataset_name\n used when inserting the yaml file using \ninsert_yaml.py\n\n\nmeta_name\n \ndefault=\"waypoint_set\"\n: The \nmeta_name\n used when inserting the yaml file using \ninsert_yaml.py\n\n\ncollection_name\n \ndefault=\"aaf_walking_group\"\n: The \ncollection_name\n used when inserting the yaml file using \ninsert_yaml.py\n\n\nwaypoint_sounds_file\n \ndefault=\"$(find aaf_walking_group)/conf/waypoint_sounds.yaml\"\n: The waypoint sounds file, describing the waypoints at which to play sounds and which sounds to play; see below.\n\n\nmusic_set_group\n \ndefault=\"aaf_walking_group_music\"\n: The media server music set to play during the walking phase and via the entertainment interface.\n\n\nmusic_set_waypoints\n \ndefault=\"aaf_waypoint_sounds\"\n: The media server music set containing the waypoint sounds.\n\n\nmusic_set_jingles\n \ndefault=\"aaf_jingles\"\n: The media server music set containing the jingles used.\n\n\nmusic_set_recovery\n \ndefault=\"aaf_walking_group_recovery_sounds\"\n: The media server music set containing the jingles used.\n\n\nvideo_set\n \ndefault=\"aaf_walking_group_videos\"\n: The media server video set containing the videos shown during entertainment.\n\n\nimage_set\n \ndefault=\"aaf_walking_group_pictures\"\n: The media server image set containing the pictures shown during entertainment.\n\n\n\n\nConfiguring the recovery behaviours\n\n\nRecovery behaviours are dynamically turend on and off during start-up and after the end of the walking group to prevent some of them from kicking in, making the robot drive backwards. Additionally, a custom recovery behaviour, playing a sounds when in trouble, is added. To tell the navigation which behaviour should be used during the group, we create a so-called whitelist which could look like this:\n\n\nrecover_states:\n  sleep_and_retry: [true, .inf]\n  walking_group_help: [true, .inf]\n\n\n\n\nThis enables, the \nsleep_and_retry\n and \nwalking_group_help\n recovery states and sets the possible retries to \ninf\n. Every behaviour not in this list, will be disabled during the group and reenabled afterwards.\n\n\nConfiguring the media sets\n\n\nThe video and image set can contain any form of images and videos and just have to be passed by name during start-up. The \nmusic_set_group\n can contain any kind of music and just has to be passed by name during start-up. The jingles and waypoint sets are a bit more special. The jingles used have to have the following filenames:\n\n\n\n\njingle_stop.mp3\n: Played when the robot stops and waits for someone to press the \"Weiter\" button.\n\n\njingle_patient_continue.mp3\n: Sound played when someone presses the \"Weiter\" button\n\n\njingle_therapist_continue.mp3\n: Sound played when the robot starts navigating after a therapist interaction.\n\n\njingle_waypoint_reached.mp3\n: Sound played when a resting point is reached.\n\n\n\n\nCurrently these names are hard coded. For the waypoint sounds, we provide a config file loaded from the \nwaypoint_sounds_file\n parameter. An example file could look like this:\n\n\nwaypoint_sounds: '{\nLift1\n: \ngreat_tit.mp3\n, \nWayPoint7\n: \ncrested_tit.mp3\n, \nCafeteria\n: \nnorthern_tit.mp3\n, \nWayPoint8\n: \nmarsh_tit.mp3\n}'\n\n\n\n\nThe keys of the dictornary are topological waypoint names and the values are the filenames of the music played when reaching that waypoint. In this case we play a selection of bird sounds.\n\n\nRunning\n\n\nStart the managing action server(s):\n\n\nroslaunch aaf_walking_group task_servers.launch\n\n\n\n\nthis launch file has one parameter: \nconfig_file\n which specifies the location of the yaml file used to specify parameters needed to run the behaviour. This file is the one described above setting the parameters in the \nwalking_group\n namespace.\n\n\nOnce the launch file is started it provides the respective number of action servers which have an empty goals since everything is defined in the config file. These can easily be scheduled using the google calendar interface since they inherit from \nthe abstract task server\n. An example would be, using all the above config files, we have two action servers, one called \nwalking_group_slow\n and the other \nwalking_group_fast\n. By creating a new event in the google calendar of the robot called either \nwalking_group_slow\n or \nwalking_group_fast\n it will schedule the task and start the specific action server. No additional configuration required. This makes it easy to schedule these events, the only thing that has to be observed is that the actual time window is larger than the \nmax_duration\n set in the launch file. Otherwise the duration can be overridden in the calendar by using yaml style arguments in the event description.\n\n\nComponent Behaviour\n\n\nThe actually started action servers don't do anything until they receive a goal. When a new goal is sent, they\n\n start the necessary components using the \naxlaunch server\n \n\n start the task immediately after the components are launched. \n\n\nAfter the task is preempted or successful, the components are stopped to not use any memory or CPU if the task is not running.\n\n\nThe servers communicate via a topic if an instance of the walking group is already running or not. When trying to start a second instance, e.g. slow is already running and you want to start fast, the goal will be abborted.\n\n\nTesting\n\n\nTo start the statemachine, run above launch file and configuration. Use, e.g.\n\n\nrosrun actionlib axclient.py /walking_group_fast\n\n\n\n\nTo emulate the therapist being close, publish:\n\n\nrostopic pub /socialCardReader/QSR_generator std_msgs/String \ndata: 'near'\n\n\n\n\n\nto emulate the therpist showing the key card in the specified orientation, publish:\n\n\nrostopic pub /socialCardReader/commands std_msgs/String \ndata: 'PAUSE_WALK'", 
            "title": "Aaf walking group"
        }, 
        {
            "location": "/aaf_deployment/aaf_walking_group/#walking-group", 
            "text": "This AAF task is meant to accompany the walking group, driving infront of the therapist and the patients, providing entertainment during rests.", 
            "title": "Walking Group"
        }, 
        {
            "location": "/aaf_deployment/aaf_walking_group/#functionality", 
            "text": "The interaction between the robot and therapist is based on the so-called key card (TODO: Upload key-card) which has to be worn around his/her neck. There can be several therpists, each wearing a key card, but there has to be at least one. The key card has to be worn visably, as the robot relies on it to identify the therapist.   The robot will be waiting for the group to start at the predefined starting location. During this phase it will provide entertainment, i.e. play music, play videos, or show images via a simple to use and clearly structured interface which is supposed to be used by the patients.  To start the group, the therapist shows the robot the key card in the correct orientation. This will trigger the so called guide interface. In the guide interface,  the therapist can toggle playing of music while the groups is walking along the corridors. This will drown out every other auditive feedback. If music is turned off, the robot will play jingles and nature sounds at predefined locations and events.  the next waypoint can be chosen, either by just pressing \"Weiter\" which lets the robot drive to the next waypoint in the tour, or by selecting them from a list of all possible waypoints.  the guide interface can be cancelled which results in showing the entertainment interface again.  the whole tour can be cancelled which results in the robot going away, doing a science.  During the actual walking, the robot is driving in front of the group, playing either music or jingles and nature sounds. After a predefined distance the robot stops, waiting for the therapist to be \"close\" (using the key card to meassure distance), and then shows a large \"Weiter\" button, which can be bressed by the patients to send the robot off and to elicit interaction.  When a resting area is reached, the robot waits for the guide to come \"close\" (using the key card to meassure distance), and then shows two buttons, i.e. \"Witer\" and \"Rast\", which either send the robot to the next waypoint or shows the entertainment interface.    This will continue until the final waypoint is reached. During the whole tour, whenever the guide shows the key card in the specific orientation, the robot will stop and show the guide interface. This way, the music can be toggled, the waypoint chosen or the tour preempted at any point in time.", 
            "title": "Functionality"
        }, 
        {
            "location": "/aaf_deployment/aaf_walking_group/#usage", 
            "text": "The whole behaviour is run via the  axlaunch server  , starting the components, and  the abstract task server  to create scheduable tasks.", 
            "title": "Usage"
        }, 
        {
            "location": "/aaf_deployment/aaf_walking_group/#configuration", 
            "text": "The most basic and important configuration happens via a datacentre entry storing the waypoints at which to rest and the distance at which the robot should stop in regular intervals. An example file could look like this:  slow:\n    stopping_distance: 3.0\n    waypoints:\n        1:  WayPoint34 \n        2:  WayPoint14 \n        3:  WayPoint26 \n        4:  WayPoint15 \n        5:  WayPoint46 \nfast:\n    stopping_distance: 5.0\n    waypoints:\n        1:  WayPoint34 \n        2:  WayPoint14 \n        3:  WayPoint26 \n        4:  WayPoint15 \n        5:  WayPoint46   slow  and  fast  are internally used identifiers. They don't have to have specific names but have to be unique and consistent with the  group  entry described below. The  stopping_distance  is the distance after which the robot will wait for the therapist to come close, to show a button, which can be pressed by the patients to send it off again. The  waypoints  entry specifies the waypoints at which the group usually rests. Waypoints not specified in this list can never be selected as resting areas during a tour; the index specifies the order of points.  This file has to be inserted into the datacentre using a provided script:  $ rosrun aaf_walking_group insert_yaml.py --help\nusage: insert_yaml.py [-h] -i INPUT [--collection_name COLLECTION_NAME]\n                      [--meta_name META_NAME]\n                      dataset_name\n\npositional arguments:\n  dataset_name          The name of the dataset. Saved in meta information\n                        using 'meta_name'\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i INPUT, --input INPUT\n                        Input yaml file\n  --collection_name COLLECTION_NAME\n                        The collection name. Default: aaf_walking_group\n  --meta_name META_NAME\n                        The name of the meta filed to store 'dataset_name' in.\n                        Default: waypoint_set  shows it's functionalities. The defaults for  meta_name  and  collection_name  can be changed, but will then have to be specified at start-up. Otherwise, the defaults in the launch file correspond with the defaults in the script. The  dataset_name  name is a mendatory argument which is used to identify the configuration you stored. With  -i  my_file  you specify the input file. Once that has been inserted we can move on to the second config file, loaded on start-up.\nAn example file could look like this:  walking_group:\n    walking_group_slow:\n        group: 'slow'\n        start_location: 'WayPoint34'\n        max_duration: 3600\n        parameters: \n            - 'music_set_group'\n            - 'waypoint_set'\n        values: \n            - 'my_set'\n            - 'aaf_waypoints'\n    walking_group_fast:\n        group: 'fast'\n        start_location: 'WayPoint34'\n        max_duration: 3600\n        parameters: \n            - 'music_set_group'\n            - 'waypoint_set'\n        values: \n            - 'my_set'\n            - 'aaf_waypoints'  walking_group_slow  and  walking_group_fast  is used to create two action servers with the corresponding name, therefore these names have to be unique. The number of action servers is dynamic and can be changed by adding another entry to this file.  group  is used to identify the specific set of waypoints from the datacentre (see above);  slow  or  fast  in our case.  start_location  is the waypoint to which the scheduler sends the robot at the beginning of the task and should be the same as the first waypoint in the list above;  WayPoint34  in our case. The  max_duration  is another argument for the schaduler which tells it how long the group task lasts in the worst case. If this time is reached it will assume that the task failed and preempt it. The  parameters  and  values  fields are used together to create the start-up parameters for the internally used launch file:   head_machine   default=\"localhost\" : The machine to which the head_camera is connected. The social card reader and position of card will be started there.  head_user   default=\"\" : The user of the machine to which the head_camera is connected. The social card reader and position of card will be started there.  waypoint_set   default=\"aaf_waypoints\" : The  dataset_name  used when inserting the yaml file using  insert_yaml.py  meta_name   default=\"waypoint_set\" : The  meta_name  used when inserting the yaml file using  insert_yaml.py  collection_name   default=\"aaf_walking_group\" : The  collection_name  used when inserting the yaml file using  insert_yaml.py  waypoint_sounds_file   default=\"$(find aaf_walking_group)/conf/waypoint_sounds.yaml\" : The waypoint sounds file, describing the waypoints at which to play sounds and which sounds to play; see below.  music_set_group   default=\"aaf_walking_group_music\" : The media server music set to play during the walking phase and via the entertainment interface.  music_set_waypoints   default=\"aaf_waypoint_sounds\" : The media server music set containing the waypoint sounds.  music_set_jingles   default=\"aaf_jingles\" : The media server music set containing the jingles used.  music_set_recovery   default=\"aaf_walking_group_recovery_sounds\" : The media server music set containing the jingles used.  video_set   default=\"aaf_walking_group_videos\" : The media server video set containing the videos shown during entertainment.  image_set   default=\"aaf_walking_group_pictures\" : The media server image set containing the pictures shown during entertainment.   Configuring the recovery behaviours  Recovery behaviours are dynamically turend on and off during start-up and after the end of the walking group to prevent some of them from kicking in, making the robot drive backwards. Additionally, a custom recovery behaviour, playing a sounds when in trouble, is added. To tell the navigation which behaviour should be used during the group, we create a so-called whitelist which could look like this:  recover_states:\n  sleep_and_retry: [true, .inf]\n  walking_group_help: [true, .inf]  This enables, the  sleep_and_retry  and  walking_group_help  recovery states and sets the possible retries to  inf . Every behaviour not in this list, will be disabled during the group and reenabled afterwards.  Configuring the media sets  The video and image set can contain any form of images and videos and just have to be passed by name during start-up. The  music_set_group  can contain any kind of music and just has to be passed by name during start-up. The jingles and waypoint sets are a bit more special. The jingles used have to have the following filenames:   jingle_stop.mp3 : Played when the robot stops and waits for someone to press the \"Weiter\" button.  jingle_patient_continue.mp3 : Sound played when someone presses the \"Weiter\" button  jingle_therapist_continue.mp3 : Sound played when the robot starts navigating after a therapist interaction.  jingle_waypoint_reached.mp3 : Sound played when a resting point is reached.   Currently these names are hard coded. For the waypoint sounds, we provide a config file loaded from the  waypoint_sounds_file  parameter. An example file could look like this:  waypoint_sounds: '{ Lift1 :  great_tit.mp3 ,  WayPoint7 :  crested_tit.mp3 ,  Cafeteria :  northern_tit.mp3 ,  WayPoint8 :  marsh_tit.mp3 }'  The keys of the dictornary are topological waypoint names and the values are the filenames of the music played when reaching that waypoint. In this case we play a selection of bird sounds.", 
            "title": "Configuration"
        }, 
        {
            "location": "/aaf_deployment/aaf_walking_group/#running", 
            "text": "Start the managing action server(s):  roslaunch aaf_walking_group task_servers.launch  this launch file has one parameter:  config_file  which specifies the location of the yaml file used to specify parameters needed to run the behaviour. This file is the one described above setting the parameters in the  walking_group  namespace.  Once the launch file is started it provides the respective number of action servers which have an empty goals since everything is defined in the config file. These can easily be scheduled using the google calendar interface since they inherit from  the abstract task server . An example would be, using all the above config files, we have two action servers, one called  walking_group_slow  and the other  walking_group_fast . By creating a new event in the google calendar of the robot called either  walking_group_slow  or  walking_group_fast  it will schedule the task and start the specific action server. No additional configuration required. This makes it easy to schedule these events, the only thing that has to be observed is that the actual time window is larger than the  max_duration  set in the launch file. Otherwise the duration can be overridden in the calendar by using yaml style arguments in the event description.", 
            "title": "Running"
        }, 
        {
            "location": "/aaf_deployment/aaf_walking_group/#component-behaviour", 
            "text": "The actually started action servers don't do anything until they receive a goal. When a new goal is sent, they  start the necessary components using the  axlaunch server    start the task immediately after the components are launched.   After the task is preempted or successful, the components are stopped to not use any memory or CPU if the task is not running.  The servers communicate via a topic if an instance of the walking group is already running or not. When trying to start a second instance, e.g. slow is already running and you want to start fast, the goal will be abborted.", 
            "title": "Component Behaviour"
        }, 
        {
            "location": "/aaf_deployment/aaf_walking_group/#testing", 
            "text": "To start the statemachine, run above launch file and configuration. Use, e.g.  rosrun actionlib axclient.py /walking_group_fast  To emulate the therapist being close, publish:  rostopic pub /socialCardReader/QSR_generator std_msgs/String  data: 'near'   to emulate the therpist showing the key card in the specified orientation, publish:  rostopic pub /socialCardReader/commands std_msgs/String  data: 'PAUSE_WALK'", 
            "title": "Testing"
        }, 
        {
            "location": "/aaf_deployment/expert_interventions/", 
            "text": "Date: 18/05/15\n\n\nExpert: Christian Dondrup\n\n\nComments: -\n\n\nFailures:\n\n\n13:00 GMT: Robot mislocalised before start of walking group at 'WalkingGruppeStart'\n\n\n13:45 GMT: scitos_node died at cafeteria during walking group. Respawned but neede relocalistaion\n\n\nParamsets: \n\n\nChanged second walking group starting time in calendar as the scheduler threw it away.\n\n\nChanged infremen params, because the default has not been changed yet.\n\n\nChanged speed of future walking groups\n\n\nRestarts:\n\n\nimage_server of walking group due to missing images when group satrted.", 
            "title": "Expert interventions"
        }, 
        {
            "location": "/aaf_deployment/", 
            "text": "aaf_deployment\n\n\n\nAll components for the STRANDS AAF deployment\n\n\nInfo-Terminal", 
            "title": "Home"
        }, 
        {
            "location": "/aaf_deployment/#aaf_deployment", 
            "text": "All components for the STRANDS AAF deployment", 
            "title": "aaf_deployment"
        }, 
        {
            "location": "/aaf_deployment/#info-terminal", 
            "text": "", 
            "title": "Info-Terminal"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/", 
            "text": "Walking Group\n\n\nThis AAF task is meant to accompany the walking group, driving infront of the therapist and the patients, providing entertainment during rests.\n\n\nFunctionality\n\n\nThe interaction between the robot and therapist is based on the so-called key card (TODO: Upload key-card) which has to be worn around his/her neck. There can be several therpists, each wearing a key card, but there has to be at least one. The key card has to be worn visably, as the robot relies on it to identify the therapist.\n\n\n\n\nThe robot will be waiting for the group to start at the predefined starting location. During this phase it will provide entertainment, i.e. play music, play videos, or show images via a simple to use and clearly structured interface which is supposed to be used by the patients.\n\n\nTo start the group, the therapist shows the robot the key card in the correct orientation. This will trigger the so called guide interface. In the guide interface,\n\n\nthe therapist can toggle playing of music while the groups is walking along the corridors. This will drown out every other auditive feedback. If music is turned off, the robot will play jingles and nature sounds at predefined locations and events.\n\n\nthe next waypoint can be chosen, either by just pressing \"Weiter\" which lets the robot drive to the next waypoint in the tour, or by selecting them from a list of all possible waypoints.\n\n\nthe guide interface can be cancelled which results in showing the entertainment interface again.\n\n\nthe whole tour can be cancelled which results in the robot going away, doing a science.\n\n\nDuring the actual walking, the robot is driving in front of the group, playing either music or jingles and nature sounds. After a predefined distance the robot stops, waiting for the therapist to be \"close\" (using the key card to meassure distance), and then shows a large \"Weiter\" button, which can be bressed by the patients to send the robot off and to elicit interaction.\n\n\nWhen a resting area is reached, the robot waits for the guide to come \"close\" (using the key card to meassure distance), and then shows two buttons, i.e. \"Witer\" and \"Rast\", which either send the robot to the next waypoint or shows the entertainment interface. \n\n\n\n\nThis will continue until the final waypoint is reached. During the whole tour, whenever the guide shows the key card in the specific orientation, the robot will stop and show the guide interface. This way, the music can be toggled, the waypoint chosen or the tour preempted at any point in time.\n\n\nUsage\n\n\nThe whole behaviour is run via the \naxlaunch server\n , starting the components, and \nthe abstract task server\n to create scheduable tasks.\n\n\nConfiguration\n\n\nThe most basic and important configuration happens via a datacentre entry storing the waypoints at which to rest and the distance at which the robot should stop in regular intervals. An example file could look like this:\n\n\nslow:\n    stopping_distance: 3.0\n    waypoints:\n        1: \nWayPoint34\n\n        2: \nWayPoint14\n\n        3: \nWayPoint26\n\n        4: \nWayPoint15\n\n        5: \nWayPoint46\n\nfast:\n    stopping_distance: 5.0\n    waypoints:\n        1: \nWayPoint34\n\n        2: \nWayPoint14\n\n        3: \nWayPoint26\n\n        4: \nWayPoint15\n\n        5: \nWayPoint46\n\n\n\n\n\nslow\n and \nfast\n are internally used identifiers. They don't have to have specific names but have to be unique and consistent with the \ngroup\n entry described below. The \nstopping_distance\n is the distance after which the robot will wait for the therapist to come close, to show a button, which can be pressed by the patients to send it off again. The \nwaypoints\n entry specifies the waypoints at which the group usually rests. Waypoints not specified in this list can never be selected as resting areas during a tour; the index specifies the order of points.\n\n\nThis file has to be inserted into the datacentre using a provided script:\n\n\n$ rosrun aaf_walking_group insert_yaml.py --help\nusage: insert_yaml.py [-h] -i INPUT [--collection_name COLLECTION_NAME]\n                      [--meta_name META_NAME]\n                      dataset_name\n\npositional arguments:\n  dataset_name          The name of the dataset. Saved in meta information\n                        using 'meta_name'\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i INPUT, --input INPUT\n                        Input yaml file\n  --collection_name COLLECTION_NAME\n                        The collection name. Default: aaf_walking_group\n  --meta_name META_NAME\n                        The name of the meta filed to store 'dataset_name' in.\n                        Default: waypoint_set\n\n\n\n\n\nshows it's functionalities. The defaults for \nmeta_name\n and \ncollection_name\n can be changed, but will then have to be specified at start-up. Otherwise, the defaults in the launch file correspond with the defaults in the script. The \ndataset_name\n name is a mendatory argument which is used to identify the configuration you stored. With \n-i \nmy_file\n you specify the input file. Once that has been inserted we can move on to the second config file, loaded on start-up.\nAn example file could look like this:\n\n\nwalking_group:\n    walking_group_slow:\n        group: 'slow'\n        start_location: 'WayPoint34'\n        max_duration: 3600\n        parameters: \n            - 'music_set_group'\n            - 'waypoint_set'\n        values: \n            - 'my_set'\n            - 'aaf_waypoints'\n    walking_group_fast:\n        group: 'fast'\n        start_location: 'WayPoint34'\n        max_duration: 3600\n        parameters: \n            - 'music_set_group'\n            - 'waypoint_set'\n        values: \n            - 'my_set'\n            - 'aaf_waypoints'\n\n\n\n\nwalking_group_slow\n and \nwalking_group_fast\n is used to create two action servers with the corresponding name, therefore these names have to be unique. The number of action servers is dynamic and can be changed by adding another entry to this file. \ngroup\n is used to identify the specific set of waypoints from the datacentre (see above); \nslow\n or \nfast\n in our case. \nstart_location\n is the waypoint to which the scheduler sends the robot at the beginning of the task and should be the same as the first waypoint in the list above; \nWayPoint34\n in our case. The \nmax_duration\n is another argument for the schaduler which tells it how long the group task lasts in the worst case. If this time is reached it will assume that the task failed and preempt it. The \nparameters\n and \nvalues\n fields are used together to create the start-up parameters for the internally used launch file:\n\n\n\n\nwaypoint_set\n \ndefault=\"aaf_waypoints\"\n: The \ndataset_name\n used when inserting the yaml file using \ninsert_yaml.py\n\n\nmeta_name\n \ndefault=\"waypoint_set\"\n: The \nmeta_name\n used when inserting the yaml file using \ninsert_yaml.py\n\n\ncollection_name\n \ndefault=\"aaf_walking_group\"\n: The \ncollection_name\n used when inserting the yaml file using \ninsert_yaml.py\n\n\nwaypoint_sounds_file\n \ndefault=\"$(find aaf_walking_group)/conf/waypoint_sounds.yaml\"\n: The waypoint sounds file, describing the waypoints at which to play sounds and which sounds to play; see below.\n\n\nmusic_set_group\n \ndefault=\"aaf_walking_group_music\"\n: The media server music set to play during the walking phase and via the entertainment interface.\n\n\nmusic_set_waypoints\n \ndefault=\"aaf_waypoint_sounds\"\n: The media server music set containing the waypoint sounds.\n\n\nmusic_set_jingles\n \ndefault=\"aaf_jingles\"\n: The media server music set containing the jingles used.\n\n\nmusic_set_recovery\n \ndefault=\"aaf_walking_group_recovery_sounds\"\n: The media server music set containing the jingles used.\n\n\nvideo_set\n \ndefault=\"aaf_walking_group_videos\"\n: The media server video set containing the videos shown during entertainment.\n\n\nimage_set\n \ndefault=\"aaf_walking_group_pictures\"\n: The media server image set containing the pictures shown during entertainment.\n\n\n\n\nConfiguring the recovery behaviours\n\n\nRecovery behaviours are dynamically turend on and off during start-up and after the end of the walking group to prevent some of them from kicking in, making the robot drive backwards. Additionally, a custom recovery behaviour, playing a sounds when in trouble, is added. To tell the navigation which behaviour should be used during the group, we create a so-called whitelist which could look like this:\n\n\nrecover_states:\n  sleep_and_retry: [true, .inf]\n  walking_group_help: [true, .inf]\n\n\n\n\nThis enables, the \nsleep_and_retry\n and \nwalking_group_help\n recovery states and sets the possible retries to \ninf\n. Every behaviour not in this list, will be disabled during the group and reenabled afterwards.\n\n\nConfiguring the media sets\n\n\nThe video and image set can contain any form of images and videos and just have to be passed by name during start-up. The \nmusic_set_group\n can contain any kind of music and just has to be passed by name during start-up. The jingles and waypoint sets are a bit more special. The jingles used have to have the following filenames:\n\n\n\n\njingle_stop.mp3\n: Played when the robot stops and waits for someone to press the \"Weiter\" button.\n\n\njingle_patient_continue.mp3\n: Sound played when someone presses the \"Weiter\" button\n\n\njingle_therapist_continue.mp3\n: Sound played when the robot starts navigating after a therapist interaction.\n\n\njingle_waypoint_reached.mp3\n: Sound played when a resting point is reached.\n\n\n\n\nCurrently these names are hard coded. For the waypoint sounds, we provide a config file loaded from the \nwaypoint_sounds_file\n parameter. An example file could look like this:\n\n\nwaypoint_sounds: '{\nLift1\n: \ngreat_tit.mp3\n, \nWayPoint7\n: \ncrested_tit.mp3\n, \nCafeteria\n: \nnorthern_tit.mp3\n, \nWayPoint8\n: \nmarsh_tit.mp3\n}'\n\n\n\n\nThe keys of the dictornary are topological waypoint names and the values are the filenames of the music played when reaching that waypoint. In this case we play a selection of bird sounds.\n\n\nRunning\n\n\nStart the managing action server(s):\n\n\nroslaunch aaf_walking_group task_servers.launch\n\n\n\n\nthis launch file has one parameter: \nconfig_file\n which specifies the location of the yaml file used to specify parameters needed to run the behaviour. This file is the one described above setting the parameters in the \nwalking_group\n namespace.\n\n\nOnce the launch file is started it provides the respective number of action servers which have an empty goals since everything is defined in the config file. These can easily be scheduled using the google calendar interface since they inherit from \nthe abstract task server\n. An example would be, using all the above config files, we have two action servers, one called \nwalking_group_slow\n and the other \nwalking_group_fast\n. By creating a new event in the google calendar of the robot called either \nwalking_group_slow\n or \nwalking_group_fast\n it will schedule the task and start the specific action server. No additional configuration required. This makes it easy to schedule these events, the only thing that has to be observed is that the actual time window is larger than the \nmax_duration\n set in the launch file. Otherwise the duration can be overridden in the calendar by using yaml style arguments in the event description.\n\n\nComponent Behaviour\n\n\nThe actually started action servers don't do anything until they receive a goal. When a new goal is sent, they\n\n start the necessary components using the \naxlaunch server\n \n\n start the task immediately after the components are launched. \n\n\nAfter the task is preempted or successful, the components are stopped to not use any memory or CPU if the task is not running.\n\n\nThe servers communicate via a topic if an instance of the walking group is already running or not. When trying to start a second instance, e.g. slow is already running and you want to start fast, the goal will be abborted.\n\n\nTesting\n\n\nTo start the statemachine, run above launch file and configuration. Use, e.g.\n\n\nrosrun actionlib axclient.py /walking_group_fast\n\n\n\n\nTo emulate the therapist being close, publish:\n\n\nrostopic pub /socialCardReader/QSR_generator std_msgs/String \ndata: 'near'\n\n\n\n\n\nto emulate the therpist showing the key card in the specified orientation, publish:\n\n\nrostopic pub /socialCardReader/commands std_msgs/String \ndata: 'PAUSE_WALK'", 
            "title": "Home"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/#walking-group", 
            "text": "This AAF task is meant to accompany the walking group, driving infront of the therapist and the patients, providing entertainment during rests.", 
            "title": "Walking Group"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/#functionality", 
            "text": "The interaction between the robot and therapist is based on the so-called key card (TODO: Upload key-card) which has to be worn around his/her neck. There can be several therpists, each wearing a key card, but there has to be at least one. The key card has to be worn visably, as the robot relies on it to identify the therapist.   The robot will be waiting for the group to start at the predefined starting location. During this phase it will provide entertainment, i.e. play music, play videos, or show images via a simple to use and clearly structured interface which is supposed to be used by the patients.  To start the group, the therapist shows the robot the key card in the correct orientation. This will trigger the so called guide interface. In the guide interface,  the therapist can toggle playing of music while the groups is walking along the corridors. This will drown out every other auditive feedback. If music is turned off, the robot will play jingles and nature sounds at predefined locations and events.  the next waypoint can be chosen, either by just pressing \"Weiter\" which lets the robot drive to the next waypoint in the tour, or by selecting them from a list of all possible waypoints.  the guide interface can be cancelled which results in showing the entertainment interface again.  the whole tour can be cancelled which results in the robot going away, doing a science.  During the actual walking, the robot is driving in front of the group, playing either music or jingles and nature sounds. After a predefined distance the robot stops, waiting for the therapist to be \"close\" (using the key card to meassure distance), and then shows a large \"Weiter\" button, which can be bressed by the patients to send the robot off and to elicit interaction.  When a resting area is reached, the robot waits for the guide to come \"close\" (using the key card to meassure distance), and then shows two buttons, i.e. \"Witer\" and \"Rast\", which either send the robot to the next waypoint or shows the entertainment interface.    This will continue until the final waypoint is reached. During the whole tour, whenever the guide shows the key card in the specific orientation, the robot will stop and show the guide interface. This way, the music can be toggled, the waypoint chosen or the tour preempted at any point in time.", 
            "title": "Functionality"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/#usage", 
            "text": "The whole behaviour is run via the  axlaunch server  , starting the components, and  the abstract task server  to create scheduable tasks.", 
            "title": "Usage"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/#configuration", 
            "text": "The most basic and important configuration happens via a datacentre entry storing the waypoints at which to rest and the distance at which the robot should stop in regular intervals. An example file could look like this:  slow:\n    stopping_distance: 3.0\n    waypoints:\n        1:  WayPoint34 \n        2:  WayPoint14 \n        3:  WayPoint26 \n        4:  WayPoint15 \n        5:  WayPoint46 \nfast:\n    stopping_distance: 5.0\n    waypoints:\n        1:  WayPoint34 \n        2:  WayPoint14 \n        3:  WayPoint26 \n        4:  WayPoint15 \n        5:  WayPoint46   slow  and  fast  are internally used identifiers. They don't have to have specific names but have to be unique and consistent with the  group  entry described below. The  stopping_distance  is the distance after which the robot will wait for the therapist to come close, to show a button, which can be pressed by the patients to send it off again. The  waypoints  entry specifies the waypoints at which the group usually rests. Waypoints not specified in this list can never be selected as resting areas during a tour; the index specifies the order of points.  This file has to be inserted into the datacentre using a provided script:  $ rosrun aaf_walking_group insert_yaml.py --help\nusage: insert_yaml.py [-h] -i INPUT [--collection_name COLLECTION_NAME]\n                      [--meta_name META_NAME]\n                      dataset_name\n\npositional arguments:\n  dataset_name          The name of the dataset. Saved in meta information\n                        using 'meta_name'\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i INPUT, --input INPUT\n                        Input yaml file\n  --collection_name COLLECTION_NAME\n                        The collection name. Default: aaf_walking_group\n  --meta_name META_NAME\n                        The name of the meta filed to store 'dataset_name' in.\n                        Default: waypoint_set  shows it's functionalities. The defaults for  meta_name  and  collection_name  can be changed, but will then have to be specified at start-up. Otherwise, the defaults in the launch file correspond with the defaults in the script. The  dataset_name  name is a mendatory argument which is used to identify the configuration you stored. With  -i  my_file  you specify the input file. Once that has been inserted we can move on to the second config file, loaded on start-up.\nAn example file could look like this:  walking_group:\n    walking_group_slow:\n        group: 'slow'\n        start_location: 'WayPoint34'\n        max_duration: 3600\n        parameters: \n            - 'music_set_group'\n            - 'waypoint_set'\n        values: \n            - 'my_set'\n            - 'aaf_waypoints'\n    walking_group_fast:\n        group: 'fast'\n        start_location: 'WayPoint34'\n        max_duration: 3600\n        parameters: \n            - 'music_set_group'\n            - 'waypoint_set'\n        values: \n            - 'my_set'\n            - 'aaf_waypoints'  walking_group_slow  and  walking_group_fast  is used to create two action servers with the corresponding name, therefore these names have to be unique. The number of action servers is dynamic and can be changed by adding another entry to this file.  group  is used to identify the specific set of waypoints from the datacentre (see above);  slow  or  fast  in our case.  start_location  is the waypoint to which the scheduler sends the robot at the beginning of the task and should be the same as the first waypoint in the list above;  WayPoint34  in our case. The  max_duration  is another argument for the schaduler which tells it how long the group task lasts in the worst case. If this time is reached it will assume that the task failed and preempt it. The  parameters  and  values  fields are used together to create the start-up parameters for the internally used launch file:   waypoint_set   default=\"aaf_waypoints\" : The  dataset_name  used when inserting the yaml file using  insert_yaml.py  meta_name   default=\"waypoint_set\" : The  meta_name  used when inserting the yaml file using  insert_yaml.py  collection_name   default=\"aaf_walking_group\" : The  collection_name  used when inserting the yaml file using  insert_yaml.py  waypoint_sounds_file   default=\"$(find aaf_walking_group)/conf/waypoint_sounds.yaml\" : The waypoint sounds file, describing the waypoints at which to play sounds and which sounds to play; see below.  music_set_group   default=\"aaf_walking_group_music\" : The media server music set to play during the walking phase and via the entertainment interface.  music_set_waypoints   default=\"aaf_waypoint_sounds\" : The media server music set containing the waypoint sounds.  music_set_jingles   default=\"aaf_jingles\" : The media server music set containing the jingles used.  music_set_recovery   default=\"aaf_walking_group_recovery_sounds\" : The media server music set containing the jingles used.  video_set   default=\"aaf_walking_group_videos\" : The media server video set containing the videos shown during entertainment.  image_set   default=\"aaf_walking_group_pictures\" : The media server image set containing the pictures shown during entertainment.   Configuring the recovery behaviours  Recovery behaviours are dynamically turend on and off during start-up and after the end of the walking group to prevent some of them from kicking in, making the robot drive backwards. Additionally, a custom recovery behaviour, playing a sounds when in trouble, is added. To tell the navigation which behaviour should be used during the group, we create a so-called whitelist which could look like this:  recover_states:\n  sleep_and_retry: [true, .inf]\n  walking_group_help: [true, .inf]  This enables, the  sleep_and_retry  and  walking_group_help  recovery states and sets the possible retries to  inf . Every behaviour not in this list, will be disabled during the group and reenabled afterwards.  Configuring the media sets  The video and image set can contain any form of images and videos and just have to be passed by name during start-up. The  music_set_group  can contain any kind of music and just has to be passed by name during start-up. The jingles and waypoint sets are a bit more special. The jingles used have to have the following filenames:   jingle_stop.mp3 : Played when the robot stops and waits for someone to press the \"Weiter\" button.  jingle_patient_continue.mp3 : Sound played when someone presses the \"Weiter\" button  jingle_therapist_continue.mp3 : Sound played when the robot starts navigating after a therapist interaction.  jingle_waypoint_reached.mp3 : Sound played when a resting point is reached.   Currently these names are hard coded. For the waypoint sounds, we provide a config file loaded from the  waypoint_sounds_file  parameter. An example file could look like this:  waypoint_sounds: '{ Lift1 :  great_tit.mp3 ,  WayPoint7 :  crested_tit.mp3 ,  Cafeteria :  northern_tit.mp3 ,  WayPoint8 :  marsh_tit.mp3 }'  The keys of the dictornary are topological waypoint names and the values are the filenames of the music played when reaching that waypoint. In this case we play a selection of bird sounds.", 
            "title": "Configuration"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/#running", 
            "text": "Start the managing action server(s):  roslaunch aaf_walking_group task_servers.launch  this launch file has one parameter:  config_file  which specifies the location of the yaml file used to specify parameters needed to run the behaviour. This file is the one described above setting the parameters in the  walking_group  namespace.  Once the launch file is started it provides the respective number of action servers which have an empty goals since everything is defined in the config file. These can easily be scheduled using the google calendar interface since they inherit from  the abstract task server . An example would be, using all the above config files, we have two action servers, one called  walking_group_slow  and the other  walking_group_fast . By creating a new event in the google calendar of the robot called either  walking_group_slow  or  walking_group_fast  it will schedule the task and start the specific action server. No additional configuration required. This makes it easy to schedule these events, the only thing that has to be observed is that the actual time window is larger than the  max_duration  set in the launch file. Otherwise the duration can be overridden in the calendar by using yaml style arguments in the event description.", 
            "title": "Running"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/#component-behaviour", 
            "text": "The actually started action servers don't do anything until they receive a goal. When a new goal is sent, they  start the necessary components using the  axlaunch server    start the task immediately after the components are launched.   After the task is preempted or successful, the components are stopped to not use any memory or CPU if the task is not running.  The servers communicate via a topic if an instance of the walking group is already running or not. When trying to start a second instance, e.g. slow is already running and you want to start fast, the goal will be abborted.", 
            "title": "Component Behaviour"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/#testing", 
            "text": "To start the statemachine, run above launch file and configuration. Use, e.g.  rosrun actionlib axclient.py /walking_group_fast  To emulate the therapist being close, publish:  rostopic pub /socialCardReader/QSR_generator std_msgs/String  data: 'near'   to emulate the therpist showing the key card in the specified orientation, publish:  rostopic pub /socialCardReader/commands std_msgs/String  data: 'PAUSE_WALK'", 
            "title": "Testing"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/infremen/", 
            "text": "Overview\n\n\nThe \nInfremen\n package contains a waypoint proposer for the \nInfo Terminal\n.\nThe tries to establish the probabilities of people presence at the individual waypoints and times and use this information to navigate to these waypoints when people are likely to be present.\n\n\nEvery midnight, it processes the information about the interactions it achieved during the day and creates dynamic probabilistic models of people presence at the individual waypoints. \nThese models are then used to create a \nschedule\n that determines at which waypoints and times the robot offers its \nInfo Terminal\n service.\n\n\nPractical\n\n\nYou can edit the schedule manually.\n\n\nParameters\n\n\nThe  \ncollectionName\n determines what name will be used when saving and retrieving interaction history from the \nmonngodb\n.\nThe  \nscheduleDirectory\n determines the place to store text files with the schedules.\n\n\nDynamically reconfigurable parameters\n\n\nThe \nexplorationRatio\n determines the balance between exploration (trying to establish the probabilitic functions of people presence) and exploitation (trying to meet people to actually interact).\nIf the battery level drops below the \nminimalBatteryLevel\n, the robot will go and recharge itself.\nThe robot will not leave the waypoint if there was an interaction during the last \ninterationTimeout\n seconds. \nThe \ntaskDuration\n determines how long the robot waits for an interaction. \nThe \nmaxTaskNumber\n sets how much \nInfo Terminal\n tasks are maintained at the same time. Setting \nmaxTaskNumber\n causes the \nInfremen\n to stop proposing tasks.", 
            "title": "Infremen"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/infremen/#overview", 
            "text": "The  Infremen  package contains a waypoint proposer for the  Info Terminal .\nThe tries to establish the probabilities of people presence at the individual waypoints and times and use this information to navigate to these waypoints when people are likely to be present.  Every midnight, it processes the information about the interactions it achieved during the day and creates dynamic probabilistic models of people presence at the individual waypoints. \nThese models are then used to create a  schedule  that determines at which waypoints and times the robot offers its  Info Terminal  service.", 
            "title": "Overview"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/infremen/#practical", 
            "text": "You can edit the schedule manually.", 
            "title": "Practical"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/infremen/#parameters", 
            "text": "The   collectionName  determines what name will be used when saving and retrieving interaction history from the  monngodb .\nThe   scheduleDirectory  determines the place to store text files with the schedules.", 
            "title": "Parameters"
        }, 
        {
            "location": "/aaf_deployment/info_terminal/infremen/#dynamically-reconfigurable-parameters", 
            "text": "The  explorationRatio  determines the balance between exploration (trying to establish the probabilitic functions of people presence) and exploitation (trying to meet people to actually interact).\nIf the battery level drops below the  minimalBatteryLevel , the robot will go and recharge itself.\nThe robot will not leave the waypoint if there was an interaction during the last  interationTimeout  seconds. \nThe  taskDuration  determines how long the robot waits for an interaction. \nThe  maxTaskNumber  sets how much  Info Terminal  tasks are maintained at the same time. Setting  maxTaskNumber  causes the  Infremen  to stop proposing tasks.", 
            "title": "Dynamically reconfigurable parameters"
        }, 
        {
            "location": "/aaf_deployment/wiki/AAF-Deployment-Feedback/", 
            "text": "Feedback Y2\n\n\ncoming soon...", 
            "title": "AAF Deployment Feedback"
        }, 
        {
            "location": "/aaf_deployment/wiki/AAF-Deployment-Feedback/#feedback-y2", 
            "text": "coming soon...", 
            "title": "Feedback Y2"
        }, 
        {
            "location": "/aaf_deployment/wiki/Home/", 
            "text": "This is a public Wiki, no confidential information to go in here!\n\n\nAAF Deployment 2016\n\n\nDiscussions of improvements in Google doc shared via mailing list (can't link here, as it's public)\n\n\n\n\nCan run during night in Administration Wing, including the lobby (effectively y1 area)\n\n\nsee \nissues\n\n\n\n\nAAF Press Dates\n\n\n\n\n05.04. 13:00 CET Short interview for local press (Die ganze Woche\")\n\n\n05.04. 15:00 CET Filming/short interview for RT (Russian Television abroad)\n\n\n06.04. 13:00 Short interview and photo for local press (\"Bezirksblatt\")\n\n\n14.04. 14:30 Interviews for Autrian radio\n\n\n\n\n\n\nAAF Deployment 2015\n\n\nA number of pages on \nstrands_management\n (restricted access) describe the AAF scenario in greater detail:\n\n \nGeneral Scenario description\n\n  * Bellbot (lead by Yiannis)\n  * Walking Group (lead by Christian D)\n  * Info Terminal (lead by Michi)\n\n \nSetup Network etc. at AAF\n\n\n \nGeneral task and Deliverables for yr2\n\n\n \nOverview of STRANDS packaging and releasing procedures\n\n\nTasks that can be scheduled\n\n\nBesides the normal info_task that is scheduled by \ninfremen\n every 10 minutes to run for 5 minutes, we can schedule tasks in the \nGoogle Calendar\n. Here's a list of possible tasks, the \"Where\" field of an event must name a valid waypoint.\n\n\n\n\ninfo_task_server\n: Run the info terminal at a given waypoint\n\n\ncharging_task\n: effectively waits at a given waypoint being non-interruptible. The duration is taken from the actual window and by default 10 minutes are subtracted for the execution time. The waypoint for charging on the docking station should be \"ChargingPoint\", but we could manually charge somewhere else. \n\n\nmaintenance_task\n: pretty much the same as \ncharging_task\n, but interruptible\n\n\nbellbot\n Start a bellbot task to guide a person to a destination. The Waypoint given is the one where the robot is to pick up the person (usually \"Rezeption\").\n\n\nwalking_group_slow\n: Start the \nslow\n walking group. The Waypoint given here is pretty useless, as the tour is hard coded.\n\n\nwalking_group_fast\n: Start the \nfast\n walking group. The Waypoint given here is pretty useless, as the tour is hard coded.\n\n\nstore_logs\n: runs the message_store replication, configured \nhere\n\n\n/topological_prediction/build_temporal_model\n should be run every day in the evening in a ~5 minute window.\n\n\n\n\nGeneral structure\n\n\n\n\nThe deployment shall be based on released packages on indigo amd64\n\n\nThis \naaf_deployment\n repository is meant solely for \nAAF-specific packages\n, anything that has a wider use outside the AAF deployment shall go in dedicated packages, e.g. in \nstrands_apps\n.\n\n\nContributions are via pull requests only and rigorous code review will take place, in particular after the feature freeze milestone.\n\n\n\n\nMilestones\n\n\n\n\nAAF 2015 Feature Freeze\n \n2/4/15\n:\n\n\nAll features implemented for the different task\n\n\nA \"global\" scheduling approach working\n\n\nBasic GUIs in place with functionality\n\n\nThe system will be ran continuously at UoL and Vienna from this point onwards, with bug fixes being integrated as soon as they arrive. \n\n\nAAF 2015 Pre-Deployment\n \n13/4/15\n:\n\n\nHenry will perform 3 days at AAF, based on the released packages\n\n\nThe pre-deployment will comprise a staff training session, to empower AAF members of staff to operate the robot and fill confident about using it\n\n\nThe interfaces and tasks need to be completed by this milestone, as staff training will be based on these interfaces. Minor design tweaks allowed based on feedback from staff. No structural changes in program logic or code structure beyond this point. No new features to be added either.\n\n\nAAF 2015 Deployment\n \n11/5/15\n:\n\n\nStart of the actual deployment for 30 days (including weekends)\n\n\n\n\nHenry@AAF\n\n\n\n\nFor the deployment, Henry will be remotely administrated by the STRANDS team (check https://github.com/strands-project/strands_management/wiki/Y2-Integration-Scenarios for details to log in)\n\n\nOn site, two laptops are provided for staff interactions and robot monitoring, one in Tobias' and Denise's AAF office, and one at the reception desk\n\n\nThe control interface (web service) shall be running on this: \n\n\nscheduling bellbot tasks\n\n\ndisplaying the robots current screen\n\n\nseeing the robot on a map\n\n\nscheduling other tasks\n\n\nmonitoring the robot's health (battery, disk space)\n\n\n\n\n\n\nThe laptop in the AAF office is also the control-PC, running mongodb replication and serving the websites for the control interfaces (see above)\n\n\nThe docking station will be in the reception for Henry to approach whenever needed autonomously\n\n\nAn additional charging opportunity is in the AAF office by cable, an non-interruptable \"maintenance\" task shall be implemented to call the robot into the office for maintenance and charging, where it will be explicitly released again for other jobs by clicking in the GUI\n\n\n(Video-)Recording will be disabled in the therapy wing (AAF to indicate the topological nodes that should be excluded)\n\n\nAn additional web cam shall be place above the screen for recording interactions. This shall be continuously recording data (using data_compression node) whenever permitted at low framerate.\n\n\nData will be uploaded during charging period to the control-PC \n\n\n\n\nSetup Henry\n\n\n\n\nNetworking:\n\n\nwhen WIFI network connection drops we shall try to reconnect automatically by pasting \nnm-connect.sh\n into root's crontab (\nsudo crontab -e\n)\n\n\nVoice: \nscript\n added that sets Henry's voice\n\n\n\n\nDeployment Feedback", 
            "title": "Home"
        }, 
        {
            "location": "/aaf_deployment/wiki/Home/#aaf-deployment-2016", 
            "text": "Discussions of improvements in Google doc shared via mailing list (can't link here, as it's public)   Can run during night in Administration Wing, including the lobby (effectively y1 area)  see  issues", 
            "title": "AAF Deployment 2016"
        }, 
        {
            "location": "/aaf_deployment/wiki/Home/#aaf-press-dates", 
            "text": "05.04. 13:00 CET Short interview for local press (Die ganze Woche\")  05.04. 15:00 CET Filming/short interview for RT (Russian Television abroad)  06.04. 13:00 Short interview and photo for local press (\"Bezirksblatt\")  14.04. 14:30 Interviews for Autrian radio", 
            "title": "AAF Press Dates"
        }, 
        {
            "location": "/aaf_deployment/wiki/Home/#aaf-deployment-2015", 
            "text": "A number of pages on  strands_management  (restricted access) describe the AAF scenario in greater detail:   General Scenario description \n  * Bellbot (lead by Yiannis)\n  * Walking Group (lead by Christian D)\n  * Info Terminal (lead by Michi)   Setup Network etc. at AAF    General task and Deliverables for yr2    Overview of STRANDS packaging and releasing procedures", 
            "title": "AAF Deployment 2015"
        }, 
        {
            "location": "/aaf_deployment/wiki/Home/#tasks-that-can-be-scheduled", 
            "text": "Besides the normal info_task that is scheduled by  infremen  every 10 minutes to run for 5 minutes, we can schedule tasks in the  Google Calendar . Here's a list of possible tasks, the \"Where\" field of an event must name a valid waypoint.   info_task_server : Run the info terminal at a given waypoint  charging_task : effectively waits at a given waypoint being non-interruptible. The duration is taken from the actual window and by default 10 minutes are subtracted for the execution time. The waypoint for charging on the docking station should be \"ChargingPoint\", but we could manually charge somewhere else.   maintenance_task : pretty much the same as  charging_task , but interruptible  bellbot  Start a bellbot task to guide a person to a destination. The Waypoint given is the one where the robot is to pick up the person (usually \"Rezeption\").  walking_group_slow : Start the  slow  walking group. The Waypoint given here is pretty useless, as the tour is hard coded.  walking_group_fast : Start the  fast  walking group. The Waypoint given here is pretty useless, as the tour is hard coded.  store_logs : runs the message_store replication, configured  here  /topological_prediction/build_temporal_model  should be run every day in the evening in a ~5 minute window.", 
            "title": "Tasks that can be scheduled"
        }, 
        {
            "location": "/aaf_deployment/wiki/Home/#general-structure", 
            "text": "The deployment shall be based on released packages on indigo amd64  This  aaf_deployment  repository is meant solely for  AAF-specific packages , anything that has a wider use outside the AAF deployment shall go in dedicated packages, e.g. in  strands_apps .  Contributions are via pull requests only and rigorous code review will take place, in particular after the feature freeze milestone.", 
            "title": "General structure"
        }, 
        {
            "location": "/aaf_deployment/wiki/Home/#milestones", 
            "text": "AAF 2015 Feature Freeze   2/4/15 :  All features implemented for the different task  A \"global\" scheduling approach working  Basic GUIs in place with functionality  The system will be ran continuously at UoL and Vienna from this point onwards, with bug fixes being integrated as soon as they arrive.   AAF 2015 Pre-Deployment   13/4/15 :  Henry will perform 3 days at AAF, based on the released packages  The pre-deployment will comprise a staff training session, to empower AAF members of staff to operate the robot and fill confident about using it  The interfaces and tasks need to be completed by this milestone, as staff training will be based on these interfaces. Minor design tweaks allowed based on feedback from staff. No structural changes in program logic or code structure beyond this point. No new features to be added either.  AAF 2015 Deployment   11/5/15 :  Start of the actual deployment for 30 days (including weekends)", 
            "title": "Milestones"
        }, 
        {
            "location": "/aaf_deployment/wiki/Home/#henryaaf", 
            "text": "For the deployment, Henry will be remotely administrated by the STRANDS team (check https://github.com/strands-project/strands_management/wiki/Y2-Integration-Scenarios for details to log in)  On site, two laptops are provided for staff interactions and robot monitoring, one in Tobias' and Denise's AAF office, and one at the reception desk  The control interface (web service) shall be running on this:   scheduling bellbot tasks  displaying the robots current screen  seeing the robot on a map  scheduling other tasks  monitoring the robot's health (battery, disk space)    The laptop in the AAF office is also the control-PC, running mongodb replication and serving the websites for the control interfaces (see above)  The docking station will be in the reception for Henry to approach whenever needed autonomously  An additional charging opportunity is in the AAF office by cable, an non-interruptable \"maintenance\" task shall be implemented to call the robot into the office for maintenance and charging, where it will be explicitly released again for other jobs by clicking in the GUI  (Video-)Recording will be disabled in the therapy wing (AAF to indicate the topological nodes that should be excluded)  An additional web cam shall be place above the screen for recording interactions. This shall be continuously recording data (using data_compression node) whenever permitted at low framerate.  Data will be uploaded during charging period to the control-PC", 
            "title": "Henry@AAF"
        }, 
        {
            "location": "/aaf_deployment/wiki/Home/#setup-henry", 
            "text": "Networking:  when WIFI network connection drops we shall try to reconnect automatically by pasting  nm-connect.sh  into root's crontab ( sudo crontab -e )  Voice:  script  added that sets Henry's voice", 
            "title": "Setup Henry"
        }, 
        {
            "location": "/aaf_deployment/wiki/Home/#deployment-feedback", 
            "text": "", 
            "title": "Deployment Feedback"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/", 
            "text": "Topics to log:\n\n\nRos message printouts (above warning priority):\n\n\n\n\n/rosout\n\n\n\n\nRobot status:\n\n\n\n\n/tf\n - The tf description\n\n\n/robot_pose\n - Also in tf but might as well\n\n\n/cmd_vel\n - Velocity sent to motors\n\n\n/goal\n - Might be the move base goal\n\n\n/mileage\n - Distance traveled\n\n\n\n\n/motor_status\n - Is motor stop on? Free run etc.\n\n\n\n\n\n\n/head/actual_state\n - Positions of eyes\n\n\n\n\n/head/cmd_light_state\n - Eye lights\n\n\n\n\n/head/commanded_state\n - Eye position commands?\n\n\n\n\n\n\n/barrier_status\n - Magnetic strip?\n\n\n\n\n/battery_state\n - Is charging, battery percentage etc.\n\n\n/bumper\n - Is bumper pressed?\n\n\n/charger_status\n - Charging status\n\n\n\n\n/rfid\n - Magnetic strip etc.\n\n\n\n\n\n\n/diagnostics\n - Network status, joystick driver status, PTU status etc.\n\n\n\n\n\n\n/SetPTUState/goal\n - PTU position command\n\n\n\n\n\n\n/ResetPtu/goal\n - PTU reset command\n\n\n\n\n\n\n/EBC/parameter_updates\n - cfg changes of ebc\n\n\n\n\n/Charger/parameter_updates\n - cfg changes of charger\n\n\n\n\nTopological nav:\n\n\n\n\n/topological_navigation/Route\n - current route through nodes\n\n\n\n\n/topological_navigation/Statistics\n - statistics on top nav (already logged but might as well)\n\n\n\n\n\n\n/current_node\n - current node in topo nav\n\n\n\n\n/current_edge\n - current edge in topo nav\n\n\n/closest_node\n - closest node in topo nav\n\n\n\n\nMonitored nav:\n\n\n\n\n/do_backtrack/goal\n - Command to do the backtrack recovery\n\n\n\n\nCommunication:\n\n\n\n\n/speak/goal\n - mary tts speak command\n\n\n\n\n/mary_tts/speak\n - same?\n\n\n\n\n\n\n/strands_emails/goal\n - send email command\n\n\n\n\n\n\n/strands_image_tweets/goal\n - send image tweet command\n\n\n\n\n\n\n/pygame_player_negotiation\n - highest audio priority + node?\n\n\n\n\n\n\nDocking servers:\n\n\n\n\n/chargingServer/goal\n\n\n/chargingServer/result\n\n\n\n\n/chargingServer/cancel\n\n\n\n\n\n\n/docking/goal\n\n\n\n\n/docking/result\n\n\n\n\n/docking/cancel\n\n\n\n\n\n\n/undocking/goal\n\n\n\n\n/undocking/result\n\n\n/undocking/cancel\n\n\n\n\nMove base:\n\n\n\n\n\n\n/map_updates\n\n\n\n\n\n\n/move_base/NavfnROS/plan\n - local plan?\n\n\n\n\n/move_base/current_goal\n - current goal\n\n\n/move_base/DWAPlannerROS/global_plan\n - global plan\n\n\n/move_base/DWAPlannerROS/local_plan\n - local plan\n\n\n/move_base/goal\n - current goal\n\n\n\n\nRoutine related:\n\n\n\n\n/wait_node/goal\n - Action goal for wait action\n\n\n/wait_node/result\n\n\n/wait_node/cancel\n\n\n\n\nAlready logged (won't be logged by this):\n\n\nTask executor:\n\n\n\n\n/execute_policy_mode/Statistics\n - Scheduling stats, times etc?\n\n\n/task_executor/events\n - ?\n\n\n/current_schedule\n - Current schedule\n\n\n\n\nMonitored nav:\n\n\n\n\n/monitored_navigation/monitored_nav_event\n - (Already logged by Bruno?)\n\n\n/monitored_navigation/srv_pause_requested\n - ?\n\n\n/monitored_navigation/stuck_on_carpet\n - Stuck on carpet state\n\n\n/monitored_navigation/pause_requested\n - Nav pause", 
            "title": "Topics to log during deployments"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#topics-to-log", 
            "text": "", 
            "title": "Topics to log:"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#ros-message-printouts-above-warning-priority", 
            "text": "/rosout", 
            "title": "Ros message printouts (above warning priority):"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#robot-status", 
            "text": "/tf  - The tf description  /robot_pose  - Also in tf but might as well  /cmd_vel  - Velocity sent to motors  /goal  - Might be the move base goal  /mileage  - Distance traveled   /motor_status  - Is motor stop on? Free run etc.    /head/actual_state  - Positions of eyes   /head/cmd_light_state  - Eye lights   /head/commanded_state  - Eye position commands?    /barrier_status  - Magnetic strip?   /battery_state  - Is charging, battery percentage etc.  /bumper  - Is bumper pressed?  /charger_status  - Charging status   /rfid  - Magnetic strip etc.    /diagnostics  - Network status, joystick driver status, PTU status etc.    /SetPTUState/goal  - PTU position command    /ResetPtu/goal  - PTU reset command    /EBC/parameter_updates  - cfg changes of ebc   /Charger/parameter_updates  - cfg changes of charger", 
            "title": "Robot status:"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#topological-nav", 
            "text": "/topological_navigation/Route  - current route through nodes   /topological_navigation/Statistics  - statistics on top nav (already logged but might as well)    /current_node  - current node in topo nav   /current_edge  - current edge in topo nav  /closest_node  - closest node in topo nav", 
            "title": "Topological nav:"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#monitored-nav", 
            "text": "/do_backtrack/goal  - Command to do the backtrack recovery", 
            "title": "Monitored nav:"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#communication", 
            "text": "/speak/goal  - mary tts speak command   /mary_tts/speak  - same?    /strands_emails/goal  - send email command    /strands_image_tweets/goal  - send image tweet command    /pygame_player_negotiation  - highest audio priority + node?", 
            "title": "Communication:"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#docking-servers", 
            "text": "/chargingServer/goal  /chargingServer/result   /chargingServer/cancel    /docking/goal   /docking/result   /docking/cancel    /undocking/goal   /undocking/result  /undocking/cancel", 
            "title": "Docking servers:"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#move-base", 
            "text": "/map_updates    /move_base/NavfnROS/plan  - local plan?   /move_base/current_goal  - current goal  /move_base/DWAPlannerROS/global_plan  - global plan  /move_base/DWAPlannerROS/local_plan  - local plan  /move_base/goal  - current goal", 
            "title": "Move base:"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#routine-related", 
            "text": "/wait_node/goal  - Action goal for wait action  /wait_node/result  /wait_node/cancel", 
            "title": "Routine related:"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#already-logged-wont-be-logged-by-this", 
            "text": "", 
            "title": "Already logged (won't be logged by this):"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#task-executor", 
            "text": "/execute_policy_mode/Statistics  - Scheduling stats, times etc?  /task_executor/events  - ?  /current_schedule  - Current schedule", 
            "title": "Task executor:"
        }, 
        {
            "location": "/aaf_deployment/wiki/Topics-to-log-during-deployments/#monitored-nav_1", 
            "text": "/monitored_navigation/monitored_nav_event  - (Already logged by Bruno?)  /monitored_navigation/srv_pause_requested  - ?  /monitored_navigation/stuck_on_carpet  - Stuck on carpet state  /monitored_navigation/pause_requested  - Nav pause", 
            "title": "Monitored nav:"
        }, 
        {
            "location": "/activity_analysis/", 
            "text": "activity_analysis\n\n\nSTRANDS activity analysis code", 
            "title": "Home"
        }, 
        {
            "location": "/activity_analysis/#activity_analysis", 
            "text": "STRANDS activity analysis code", 
            "title": "activity_analysis"
        }, 
        {
            "location": "/annotation_tool_kth/annotation-tool/", 
            "text": "3D ANNOTATION TOOL\n\n\nAuthor: Adri\u00e0 Gallart del Burgo\n\n\nCo-author: Balasubramanian Rajasekaran, Akshaya Thippur\n\n\nEmails: bara@kth.se, akshaya@kth.se\n\n\n3D Annotation Tool is an application designed to annotate objects in a point\ncloud scene. Initially, it was developed with the aim to annotate the objects of\na table.\n\n\nContents\n\n\n\n\ndata: folder that contains some examples of .pcd files and annotations (.xml file).\n\n\ndocumentation: folder that contains the user guide.\n\n\nicons: folder with all the icons used.\n\n\nsrc: this folder contains the source code.\n\n\nannotation_tool.pro: QtCreator project file.\n\n\nCMakeLists.txt: instructions for the installation.\n\n\nREADME.txt: readme file.\n\n\ninfo_app.xml: .xml file with information used for the application.\n\n\ninfo_objects.xml: .xml file with the list of objects annotated by the user.\n\n\n\n\nSystem requirements\n\n\n3D Annotation Tool requires installation of the ROS Indigo and catkin as prerequisites.\n\n\nIf (ros-indigo-desktop-full) is installed in your machine, you can proceed \nwith the installation of the 3D Annotation Tool.\n\n\nElse, make sure the following packages are installed\n\n\n\n\n\n\nInstall the catkin library:\n\nsudo apt-get install ros-indigo-catkin\n\n\n\n\n\n\nMake sure that the pcl package is installed:\n\nsudo apt-get install libpcl-1.7-all-dev\n \n\n\n\n\n\n\nInstall the vtk-qt library:\n\nsudo apt-get install libvtk5.8-qt4\n\n\n\n\n\n\nInstallation\n\n\nTo install the annotation tool:\n\n\nWith catkin:\n\n\n\n\n\n\nGit clone or extract the files into your catkin workspace (\n/catkin_workspace_path/src/3d_annotation_tool/\n)\n\n\n\n\n\n\ncd /catkin_workspace_path/\n\n\n\n\n\n\ncatkin_make\n\n\n\n\n\n\nUsing the tool\n\n\nTo use the annotation tool:\n\n\nWith catkin:\n\n\n\n\n\n\ncd /catkin_workspace_path/devel/lib/3d_annotation_tool/\n\n\n\n\n\n\ncd annotation-tool\n\n\n\n\n\n\n./Annotation_tool", 
            "title": "Annotation tool"
        }, 
        {
            "location": "/annotation_tool_kth/annotation-tool/#3d-annotation-tool", 
            "text": "Author: Adri\u00e0 Gallart del Burgo  Co-author: Balasubramanian Rajasekaran, Akshaya Thippur  Emails: bara@kth.se, akshaya@kth.se  3D Annotation Tool is an application designed to annotate objects in a point\ncloud scene. Initially, it was developed with the aim to annotate the objects of\na table.", 
            "title": "3D ANNOTATION TOOL"
        }, 
        {
            "location": "/annotation_tool_kth/annotation-tool/#contents", 
            "text": "data: folder that contains some examples of .pcd files and annotations (.xml file).  documentation: folder that contains the user guide.  icons: folder with all the icons used.  src: this folder contains the source code.  annotation_tool.pro: QtCreator project file.  CMakeLists.txt: instructions for the installation.  README.txt: readme file.  info_app.xml: .xml file with information used for the application.  info_objects.xml: .xml file with the list of objects annotated by the user.", 
            "title": "Contents"
        }, 
        {
            "location": "/annotation_tool_kth/annotation-tool/#system-requirements", 
            "text": "3D Annotation Tool requires installation of the ROS Indigo and catkin as prerequisites.  If (ros-indigo-desktop-full) is installed in your machine, you can proceed \nwith the installation of the 3D Annotation Tool.  Else, make sure the following packages are installed    Install the catkin library: sudo apt-get install ros-indigo-catkin    Make sure that the pcl package is installed: sudo apt-get install libpcl-1.7-all-dev      Install the vtk-qt library: sudo apt-get install libvtk5.8-qt4", 
            "title": "System requirements"
        }, 
        {
            "location": "/annotation_tool_kth/annotation-tool/#installation", 
            "text": "To install the annotation tool:  With catkin:    Git clone or extract the files into your catkin workspace ( /catkin_workspace_path/src/3d_annotation_tool/ )    cd /catkin_workspace_path/    catkin_make", 
            "title": "Installation"
        }, 
        {
            "location": "/annotation_tool_kth/annotation-tool/#using-the-tool", 
            "text": "To use the annotation tool:  With catkin:    cd /catkin_workspace_path/devel/lib/3d_annotation_tool/    cd annotation-tool    ./Annotation_tool", 
            "title": "Using the tool"
        }, 
        {
            "location": "/annotation_tool_kth/", 
            "text": "The 3D Annotation Tool is an application designed to manually annotate objects in a point\ncloud 3D image. Initially, it was developed with the aim to annotate the objects on a desktop. \nHowever, the tool can be used to annotate any objects in 3D standing on a supporting plane.\n\n\nThis is the tool that was used to annotate the KTH-3D-Total dataset - https://strands.pdc.kth.se/public/kth-3d-total/readme.html\n\n\nKTH-3D-TOTAL: A 3D dataset for discovering spatial structures for long-term autonomous learning\n\n\nThippur, Akshaya and Ambrus, Rares and Agrawal, Gaurav and Del Burgo, Adria Gallart and Ramesh, \nJanardhan Haryadi and Jha, Mayank Kumar and Akhil, Malepati Bala Siva Sai and Shetty, \nNishan Bhavanishankar and Folkesson, John and Jensfelt, Patric\n13th International Conference of Control Automation Robotics \n Vision (ICARCV), 2014 \n\n\nThe format of the point cloud supported is .pcd. \n\n\n3D Annotation Tool has been developed by Adria Gallart del Burgo at KTH in 2013 and modified and maintained by \nAkshaya Thippur since then.\n\n\nThe tool can perform the following:\n- Plane detection \n- Plane segmentation\n- Object annotation\n- Data file generation (XML)\n\n\nThe objects annotated are stored together per scene in their XML file. They record the the following for each annotated object:\n- position\n- pose\n- minimum oriented bounding box\n- comprising pixel indices in 3D\n- length, breadth, height \n\n\nThe tool also allows for loading a previous annotation and modifying it.", 
            "title": "Home"
        }, 
        {
            "location": "/annotation_tool_kth/rgbd_grabber/", 
            "text": "rgbd_grabber\n\n\nThis node grabbs images published by the openni ros node (RGB and depth) and saves them on the disk. By default the files are saved in a folder named based on the timestamp generated at the start of the program, however the user can force the creation of new folders where subsequent files will be saved.\n\n\nOptions supported (at runtime, on the console):\n- s : save one file.\n- a : start saving a sequence of files.\n- z : stop saving a sequence of files.\n- n : create a new folder.\n- i : increase the frame skip by 1 (default is 0, i.e. save all frames).\n- u : decrease the frame skip by 1.\n\n\nThe node saves both RGB and depth streams as png files. The naming convention is RGB_SEQ_.png and Depth_SEQ_.png, where \nSEQ\n is the sequence number and takes values between 0001 and 9999 (thus a maximum of 10000 images can be saved per folder). In addition, each folder contains an index.txt file where each line contains one file name and the timestamp at which it was saved (taken from the approriapte ros messages). The depth and RGB streams are synchronized based on the timestamps of the original ROS messages.\n\n\nNote that when in the save sequence mode, if the lens of the device is covered, the node will automatically skip the incomming frames, as well as create a new folder where the new frames will be put, once the lens has been uncovered.\n\n\nrgbd_grabber", 
            "title": "Rgbd grabber"
        }, 
        {
            "location": "/annotation_tool_kth/rgbd_grabber/#rgbd_grabber", 
            "text": "This node grabbs images published by the openni ros node (RGB and depth) and saves them on the disk. By default the files are saved in a folder named based on the timestamp generated at the start of the program, however the user can force the creation of new folders where subsequent files will be saved.  Options supported (at runtime, on the console):\n- s : save one file.\n- a : start saving a sequence of files.\n- z : stop saving a sequence of files.\n- n : create a new folder.\n- i : increase the frame skip by 1 (default is 0, i.e. save all frames).\n- u : decrease the frame skip by 1.  The node saves both RGB and depth streams as png files. The naming convention is RGB_SEQ_.png and Depth_SEQ_.png, where  SEQ  is the sequence number and takes values between 0001 and 9999 (thus a maximum of 10000 images can be saved per folder). In addition, each folder contains an index.txt file where each line contains one file name and the timestamp at which it was saved (taken from the approriapte ros messages). The depth and RGB streams are synchronized based on the timestamps of the original ROS messages.  Note that when in the save sequence mode, if the lens of the device is covered, the node will automatically skip the incomming frames, as well as create a new folder where the new frames will be put, once the lens has been uncovered.  rgbd_grabber", 
            "title": "rgbd_grabber"
        }, 
        {
            "location": "/datasets/auto_benchmark/", 
            "text": "Automated benchmarking\n\n\nSome of the datasets provide an easy way of use through our automated benchmarking system for robotics experiments. The system is based on open source, freely-available tools commonly used in software development. While it allows for a seamless and fair comparison of a newly developed method with the original one, it does not require disclosure of neither the original codes nor evaluation datasets. Apart from the description of the system, we provide two use cases, where the researchers were able to compare their methods to the original ones in a matter of minutes without running the method on their own hardware.\n\n\n\\\n \\\n \\\n\n\n\n\n\n  Automated benchmarking system workflow.\n\n\n\n\n\\\n \\\n\n\n\n\nRead more in our \npaper\n. We attached a \nbibtex\n record for your convenience.\n\n\n\n\nIn the future, the system will extend to all datasets in the \nLCAS-STRANDS long-term dataset collection\n.", 
            "title": "Auto benchmark"
        }, 
        {
            "location": "/datasets/auto_benchmark/#automated-benchmarking", 
            "text": "Some of the datasets provide an easy way of use through our automated benchmarking system for robotics experiments. The system is based on open source, freely-available tools commonly used in software development. While it allows for a seamless and fair comparison of a newly developed method with the original one, it does not require disclosure of neither the original codes nor evaluation datasets. Apart from the description of the system, we provide two use cases, where the researchers were able to compare their methods to the original ones in a matter of minutes without running the method on their own hardware.  \\\n \\\n \\   \n  Automated benchmarking system workflow.   \\\n \\   Read more in our  paper . We attached a  bibtex  record for your convenience.   In the future, the system will extend to all datasets in the  LCAS-STRANDS long-term dataset collection .", 
            "title": "Automated benchmarking"
        }, 
        {
            "location": "/datasets/care_home/", 
            "text": "Long-term indoor dataset\n\n\nThis dataset contains laser scans, odometry, and AMCL results of a SCITOS-G5 robot which was roaming an indoor environment for more than 100 days. The robot served as an info-terminal, assistant-therapist and bellboy in a care home in Vienna for from November 2016 to April 2017, covering autonomously over 100km.\n\n\n\\\n \\\n \\\n\n\n\n\n\n  Map of the AAF environment\n\n\n\n\n\\\n \\\n\n\n\n\nDataset structure and download\n\n\nThe data, which are organised in archive files of one-day length are available \nhere\n. Moreover, the events, when the robot got mislocalised, are \nhere\n. The overall report on the robot navigation activity at the AAF is available in as a google doc \nspreadsheet\n\n\n\n\nCondition of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n, for which we originally collected the data. We attached a \nbibtex\n record for your convenience.\n\n\n\n\nThis dataset is part of the larger \nLCAS-STRANDS long-term dataset collection\n.", 
            "title": "Care home"
        }, 
        {
            "location": "/datasets/care_home/#long-term-indoor-dataset", 
            "text": "This dataset contains laser scans, odometry, and AMCL results of a SCITOS-G5 robot which was roaming an indoor environment for more than 100 days. The robot served as an info-terminal, assistant-therapist and bellboy in a care home in Vienna for from November 2016 to April 2017, covering autonomously over 100km.  \\\n \\\n \\   \n  Map of the AAF environment   \\\n \\", 
            "title": "Long-term indoor dataset"
        }, 
        {
            "location": "/datasets/care_home/#dataset-structure-and-download", 
            "text": "The data, which are organised in archive files of one-day length are available  here . Moreover, the events, when the robot got mislocalised, are  here . The overall report on the robot navigation activity at the AAF is available in as a google doc  spreadsheet", 
            "title": "Dataset structure and download"
        }, 
        {
            "location": "/datasets/care_home/#condition-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper , for which we originally collected the data. We attached a  bibtex  record for your convenience.   This dataset is part of the larger  LCAS-STRANDS long-term dataset collection .", 
            "title": "Condition of use"
        }, 
        {
            "location": "/datasets/", 
            "text": "STRANDS\n data sets are provided for testing and benchmarking robotics and computer visions algorithms.\n\n\nAutomated benchmarking data sets\n: Automatic benchmarkning system at University of Lincoln (tkrajnik@lincoln.ac.uk)\n\n\nObject Recognition RGB-D Data Sets:\n\n\nTUW data sets\n: Several Ground truth and annotated data sets from TUW. This data is targeted towards object instance recognition.\n\n\n3DNet Dataset\n: The 3DNet dataset is a free resource for object class recognition and 6DOF pose estimation from point cloud data. Alternative link: \n3DNet Dataset\n\n\nLong Term Mapping Data Sets:\n\n\nMHT RGB-D\n: RGB-D image data of three locations in a MHT lab collect by a robot every 5 min over 16 days by the University of Lincoln. This data is described in Krajnik et al., Spectral analysis for long-term robotic mapping, ICRA 2014. (tkrajnik@lincoln.ac.uk)\n\n\nWitham Wharf\n: RGB-D snapshots of eight locations collected by a mobile robot over the period of one year by the University of Lincoln. Data is described in: Krajnik et al., Long-Term Topological Localization for Service Robots in in Dynamic Environments using spectral maps, IROS 2014. (tkrajnik@lincoln.ac.uk) Alternative link: \nWitham Wharf Dataset\n\n\nSmall office data sets\n: Kinect depth images every 5 seconds between April 2014 and May 2016. Data is provided by the University of Lincoln. This data is described in Krajnik et al., FROctomap: an efficient spatio-temporal environment representation, Advances in Autonomous Robotics Systems, 2014. (tkrajnik@lincoln.ac.uk)\n\n\nMeta rooms\n: RGB-D data with registration over several days and places. (raambrus@kth.se)\n\n\nG4S meta rooms\n: RGB-D data 150 sweeps with 18 images per sweep. This data is not public. (c.j.c.burbridge@cs.bham.ac.uk)\n\n\nActivity Data Sets:\n\n\nLAD\n: The Leeds Activity Dataset--Breakfast (LAD--Breakfast) is currently composed of 15 annotated videos, representing five different people having breakfast or other simple meal; it is recorded within a lab setting, but is as realistic as possible (real food, eating and drinking). The videos were recorded using an ASUS Xtion PRO LIVE - RGB and Depth Sensor. The objects in the video were tracked using an OpenNI based tool created by our RACE project partner, the University of Aveiro (this data set has be co-created by RACE and STRANDS) and the ground truth provided makes reference to the objects tracked. The videos have been annotated with activities at various levels of complexity and abstraction to be selected, in order to build a hierarchy of activities. The videos were not scripted in detail: each person could choose what to eat or drink and when, and the objects he needed were on the table. The videos are quite varied in terms of duration, type and sequence of activities, number and type of visible objects. High level activities such as \"preparing coffee\" or \"consuming meal\" are composed of low level ones (e.g. \"pickup kettle\", \"pour kettle\", \"putdown kettle\"). (strands@comp.leeds.ac.uk) Alternative link: \nLAD Dataset\n\n\nLong-term people activity data set\n: Several weeks of person activity in two different environments. Described in Coppola et al.: Learning temporal context for activity recognition, In ECAI, 2015.(tkrajnik@lincoln.ac.uk)\n\n\nMobile robot observing kitchen activities data set\n:\n\n\nExtended Train Robots data set\n:\n\n\nLeeds Robotic Commands data set\n:\n\n\nAutonomous Long-term Learning Data Sets:\n\n\nLongterm\n: 8 locations over more than one month; contains 51 stiched point clouds, with camera poses and camera parameters as well as registered and corrected poses and parameters. (raambrus@kth.se)\n\n\nLongterm Labeled\n: This dataset contains a subset of the observations from the longterm dataset (longterm dataset above). In addition to the raw data, this dataset also stores for each observation object annotations (masks and labels). (raambrus@kth.se)\n\n\nMoving Labeled\n: This dataset extends the longterm datatset with more locations within the same office environment at KTH. The dataset contains a subset of the labels and these objects are consistently located in different positions in multiple rooms. (nbore@kth.se)\n\n\nKTH-3D-TOTAL\n: RGB-D Data with objects on desktops annotated. 20 Desks, 3 times per day, over 19 days. Data provided by KTH and described in, Thippur et al., \"KTH-3D-TOTAL: A 3D Dataset for Discovering Spatial Structures for Long-Term Autonomous Learning\", International Conference on Control, Automation, Robotics and Vision, ICARCV 2014. (akshaya@kth.se)\n\n\nMarathon\n: Strands Marathon 2014 data, from all universities; contains metric sweeps and mongodb databases with other system logs: (raambrus@kth.se)\n\n\nPeople Data Sets:\n\n\nPeople tracks\n: The data was collected at University of Birmingham library. It is composed of human tracks which can be used for human motion analysis. Contact: ferdian.jovan@gmail.com\n\n\nLong-term object people presence data set\n: Several weeks of person/object presence in three different environments. Used for robot search in Krajnik et al.: Where's waldo at time t ? using spatio-temporal models for mobile robot search. In ICRA 2014. Used for life-long exploration in Santos et al.: Spatio-temporal exploration strategies for long-term autonomy of mobile robots. Robotics and Autonomous Systems, 2016.(tkrajnik@lincoln.ac.uk)\n\n\nLong-term robot navigation data set\n: Data from a mobile robot that served as an info-terminal, assistant-therapist and bellboy in a care home in Vienna for more than 120 days, covering over 100km. (tkrajnik@lincoln.ac.uk)", 
            "title": "Home"
        }, 
        {
            "location": "/datasets/#object-recognition-rgb-d-data-sets", 
            "text": "TUW data sets : Several Ground truth and annotated data sets from TUW. This data is targeted towards object instance recognition.  3DNet Dataset : The 3DNet dataset is a free resource for object class recognition and 6DOF pose estimation from point cloud data. Alternative link:  3DNet Dataset", 
            "title": "Object Recognition RGB-D Data Sets:"
        }, 
        {
            "location": "/datasets/#long-term-mapping-data-sets", 
            "text": "MHT RGB-D : RGB-D image data of three locations in a MHT lab collect by a robot every 5 min over 16 days by the University of Lincoln. This data is described in Krajnik et al., Spectral analysis for long-term robotic mapping, ICRA 2014. (tkrajnik@lincoln.ac.uk)  Witham Wharf : RGB-D snapshots of eight locations collected by a mobile robot over the period of one year by the University of Lincoln. Data is described in: Krajnik et al., Long-Term Topological Localization for Service Robots in in Dynamic Environments using spectral maps, IROS 2014. (tkrajnik@lincoln.ac.uk) Alternative link:  Witham Wharf Dataset  Small office data sets : Kinect depth images every 5 seconds between April 2014 and May 2016. Data is provided by the University of Lincoln. This data is described in Krajnik et al., FROctomap: an efficient spatio-temporal environment representation, Advances in Autonomous Robotics Systems, 2014. (tkrajnik@lincoln.ac.uk)  Meta rooms : RGB-D data with registration over several days and places. (raambrus@kth.se)  G4S meta rooms : RGB-D data 150 sweeps with 18 images per sweep. This data is not public. (c.j.c.burbridge@cs.bham.ac.uk)", 
            "title": "Long Term Mapping Data Sets:"
        }, 
        {
            "location": "/datasets/#activity-data-sets", 
            "text": "LAD : The Leeds Activity Dataset--Breakfast (LAD--Breakfast) is currently composed of 15 annotated videos, representing five different people having breakfast or other simple meal; it is recorded within a lab setting, but is as realistic as possible (real food, eating and drinking). The videos were recorded using an ASUS Xtion PRO LIVE - RGB and Depth Sensor. The objects in the video were tracked using an OpenNI based tool created by our RACE project partner, the University of Aveiro (this data set has be co-created by RACE and STRANDS) and the ground truth provided makes reference to the objects tracked. The videos have been annotated with activities at various levels of complexity and abstraction to be selected, in order to build a hierarchy of activities. The videos were not scripted in detail: each person could choose what to eat or drink and when, and the objects he needed were on the table. The videos are quite varied in terms of duration, type and sequence of activities, number and type of visible objects. High level activities such as \"preparing coffee\" or \"consuming meal\" are composed of low level ones (e.g. \"pickup kettle\", \"pour kettle\", \"putdown kettle\"). (strands@comp.leeds.ac.uk) Alternative link:  LAD Dataset  Long-term people activity data set : Several weeks of person activity in two different environments. Described in Coppola et al.: Learning temporal context for activity recognition, In ECAI, 2015.(tkrajnik@lincoln.ac.uk)  Mobile robot observing kitchen activities data set :  Extended Train Robots data set :  Leeds Robotic Commands data set :", 
            "title": "Activity Data Sets:"
        }, 
        {
            "location": "/datasets/#autonomous-long-term-learning-data-sets", 
            "text": "Longterm : 8 locations over more than one month; contains 51 stiched point clouds, with camera poses and camera parameters as well as registered and corrected poses and parameters. (raambrus@kth.se)  Longterm Labeled : This dataset contains a subset of the observations from the longterm dataset (longterm dataset above). In addition to the raw data, this dataset also stores for each observation object annotations (masks and labels). (raambrus@kth.se)  Moving Labeled : This dataset extends the longterm datatset with more locations within the same office environment at KTH. The dataset contains a subset of the labels and these objects are consistently located in different positions in multiple rooms. (nbore@kth.se)  KTH-3D-TOTAL : RGB-D Data with objects on desktops annotated. 20 Desks, 3 times per day, over 19 days. Data provided by KTH and described in, Thippur et al., \"KTH-3D-TOTAL: A 3D Dataset for Discovering Spatial Structures for Long-Term Autonomous Learning\", International Conference on Control, Automation, Robotics and Vision, ICARCV 2014. (akshaya@kth.se)  Marathon : Strands Marathon 2014 data, from all universities; contains metric sweeps and mongodb databases with other system logs: (raambrus@kth.se)", 
            "title": "Autonomous Long-term Learning Data Sets:"
        }, 
        {
            "location": "/datasets/#people-data-sets", 
            "text": "People tracks : The data was collected at University of Birmingham library. It is composed of human tracks which can be used for human motion analysis. Contact: ferdian.jovan@gmail.com  Long-term object people presence data set : Several weeks of person/object presence in three different environments. Used for robot search in Krajnik et al.: Where's waldo at time t ? using spatio-temporal models for mobile robot search. In ICRA 2014. Used for life-long exploration in Santos et al.: Spatio-temporal exploration strategies for long-term autonomy of mobile robots. Robotics and Autonomous Systems, 2016.(tkrajnik@lincoln.ac.uk)  Long-term robot navigation data set : Data from a mobile robot that served as an info-terminal, assistant-therapist and bellboy in a care home in Vienna for more than 120 days, covering over 100km. (tkrajnik@lincoln.ac.uk)", 
            "title": "People Data Sets:"
        }, 
        {
            "location": "/datasets/kth_3d/", 
            "text": "KTH-3D-TOTAL\n\n\n\n\n   \n   \n\n\n\n\nSample table tops recorded in the dataset\n\n\nThis dataset constains RGB-D data with annotated objects on desktops. In total 20 desks have been recorded, 3 times per day over 19 days.\n\n\nDownload\n\n\nThis dataset is available for download in a single archive \nfile\n (\\~ 177 GB). As an alternative, the individual folders and files can be obtained from \nhere\n, and would have to be downloaded manually.\n\n\n\n\nCondition of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n that describes it:\n\n\n    KTH-3D-TOTAL: A 3D dataset for discovering spatial structures for long-term autonomous learning\n    Thippur, Akshaya and Ambrus, Rares and Agrawal, Gaurav and Del Burgo, Adria Gallart and Ramesh, \n    Janardhan Haryadi and Jha, Mayank Kumar and Akhil, Malepati Bala Siva Sai and Shetty, \n    Nishan Bhavanishankar and Folkesson, John and Jensfelt, Patric\n    Control Automation Robotics \n Vision (ICARCV), 2014 13th International Conference on\n\n\n\nWe attached a \nbibtex\n record for your convenience.", 
            "title": "Kth 3d"
        }, 
        {
            "location": "/datasets/kth_3d/#kth-3d-total", 
            "text": "Sample table tops recorded in the dataset  This dataset constains RGB-D data with annotated objects on desktops. In total 20 desks have been recorded, 3 times per day over 19 days.", 
            "title": "KTH-3D-TOTAL"
        }, 
        {
            "location": "/datasets/kth_3d/#download", 
            "text": "This dataset is available for download in a single archive  file  (\\~ 177 GB). As an alternative, the individual folders and files can be obtained from  here , and would have to be downloaded manually.", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/kth_3d/#condition-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper  that describes it:      KTH-3D-TOTAL: A 3D dataset for discovering spatial structures for long-term autonomous learning\n    Thippur, Akshaya and Ambrus, Rares and Agrawal, Gaurav and Del Burgo, Adria Gallart and Ramesh, \n    Janardhan Haryadi and Jha, Mayank Kumar and Akhil, Malepati Bala Siva Sai and Shetty, \n    Nishan Bhavanishankar and Folkesson, John and Jensfelt, Patric\n    Control Automation Robotics   Vision (ICARCV), 2014 13th International Conference on  We attached a  bibtex  record for your convenience.", 
            "title": "Condition of use"
        }, 
        {
            "location": "/datasets/kth_lt/", 
            "text": "KTH Longterm Dataset\n\n\n\n\nKTH Scitos G5 robot - Rosie\n\n\nThe data was collected autonomously by a Scitos G5 robot with an RGB-D camera on a pan-tilt, navigating through the KTH office environment over a period of approximately 30 days. Each observation consists of a set of 51 RGB-D images obtained by moving the pan-tilt in a pattern, in increments of 20 degrees horizontally and 25 vertically. Waypoints are visited between 80 and 100 times and a total of approximately 720 observations are collected at the eight waypoints that can be seen in the figure below. The data is a part of the \nStrands\n EU FP7 project.\n\n\n\n\n\n\nWayPoint positions on the map\n\n\nObservations overlayed on the 2D map\n\n\nDataset structure\n\n\nThe \ndata\n is structured in folders as follows: \nYYYYMMDD/patrol_run_YYY/room_ZZZ\n, where:\n\n\n\n\nYYYYMMDD\n represents the year, month \n day when those particular observations were acquired. Each such folder contains the patrol runs the robot collected on that specific date.\n\n\npatrol_run_YYY\n represents one of the patrol runs collected by the robot.\n\n\nroom_ZZZ\n represents a particular observation collected during a patrol run.\n\n\n\n\nEach folder of the type \nYYYMMDD/patrol_run_YYY/room_ZZZ\n contains the following files:\n\n\n\n\nroom.xml\n - contains information relevant for the observation (described in the next section)\n\n\ncomplete_cloud.pcd\n - the point cloud of the observation (obtained by merging the individual point clouds together)\n\n\nintermediate_cloud*.pcd\n - ordered point clouds, each corresponding to an RGB and depth image acquired by the camera while conducting the sweep (51 such point clouds for each observation)\n\n\n\n\nThe \nroom.xml\n file accompanying an observation contains the following (relevant) fields:\n\n\nRoomLogName\n - identifier which associates the observation with the folder structure\n\n\nRoomRunNumber\n - identifier which denotes when the observation was acquired during the patrol run (i.e. 0 - first, 1 - second, etc.)\n\n\nRoomStringId\n - identifier which corresponds to the waypoint at which the observation was acquired.\n\n\nRoomLogStartTime / RoomLogEndTime\n - acquisition time\n\n\nCentroid\n - observation centroid in the map frame\n\n\nRoomCompleteCloud\n - complete cloud filename\n\n\nRoomIntermediateClouds\n\n\nRoomIntermediateCloud\n - intermediate cloud filename\n\n\n\n\nRoomIntermediateCloudTransform\n - transform from the RGB-D sensor frame to the map frame, as given by the robot odometry\n\n\nRoomIntermediateCloudTransformRegistered\n - transform which corrects the pose of the intermediate clouds so that they are well registered with respect to each other\n\n\n\n\nParsing\n\n\nA parser is provided \nhere\n (can be installed with \nsudo apt-get install ros-indigo-metaroom-xml-parser\n) which reads in the data and returns C++ data structures encapsulating the low-level data from the disk. Form more information please refer to \nthe parser README\n ( or \nhere\n for a list of supported methods). Information about how to use the Strands package repository can be found \nhere\n.\n\n\nDownload\n\n\nThis dataset is available for download in a single archive \nfile\n (\\~ 300 GB). As an alternative, the individual folders and files can be obtained from \nhere\n, and would have to be downloaded manually.\n\n\n\n\nCondition of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n that describes it:\n\n\n    Unsupervised learning of spatial-temporal models of objects in a long-term autonomy scenario \n    Ambrus, Rares and Ekekrantz, Johan and Folkesson, John and Jensfelt, Patric\n    Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on\n\n\n\nWe attached a \nbibtex\n record for your convenience.", 
            "title": "Kth lt"
        }, 
        {
            "location": "/datasets/kth_lt/#kth-longterm-dataset", 
            "text": "KTH Scitos G5 robot - Rosie  The data was collected autonomously by a Scitos G5 robot with an RGB-D camera on a pan-tilt, navigating through the KTH office environment over a period of approximately 30 days. Each observation consists of a set of 51 RGB-D images obtained by moving the pan-tilt in a pattern, in increments of 20 degrees horizontally and 25 vertically. Waypoints are visited between 80 and 100 times and a total of approximately 720 observations are collected at the eight waypoints that can be seen in the figure below. The data is a part of the  Strands  EU FP7 project.    WayPoint positions on the map  Observations overlayed on the 2D map", 
            "title": "KTH Longterm Dataset"
        }, 
        {
            "location": "/datasets/kth_lt/#dataset-structure", 
            "text": "The  data  is structured in folders as follows:  YYYYMMDD/patrol_run_YYY/room_ZZZ , where:   YYYYMMDD  represents the year, month   day when those particular observations were acquired. Each such folder contains the patrol runs the robot collected on that specific date.  patrol_run_YYY  represents one of the patrol runs collected by the robot.  room_ZZZ  represents a particular observation collected during a patrol run.   Each folder of the type  YYYMMDD/patrol_run_YYY/room_ZZZ  contains the following files:   room.xml  - contains information relevant for the observation (described in the next section)  complete_cloud.pcd  - the point cloud of the observation (obtained by merging the individual point clouds together)  intermediate_cloud*.pcd  - ordered point clouds, each corresponding to an RGB and depth image acquired by the camera while conducting the sweep (51 such point clouds for each observation)   The  room.xml  file accompanying an observation contains the following (relevant) fields:  RoomLogName  - identifier which associates the observation with the folder structure  RoomRunNumber  - identifier which denotes when the observation was acquired during the patrol run (i.e. 0 - first, 1 - second, etc.)  RoomStringId  - identifier which corresponds to the waypoint at which the observation was acquired.  RoomLogStartTime / RoomLogEndTime  - acquisition time  Centroid  - observation centroid in the map frame  RoomCompleteCloud  - complete cloud filename  RoomIntermediateClouds  RoomIntermediateCloud  - intermediate cloud filename   RoomIntermediateCloudTransform  - transform from the RGB-D sensor frame to the map frame, as given by the robot odometry  RoomIntermediateCloudTransformRegistered  - transform which corrects the pose of the intermediate clouds so that they are well registered with respect to each other", 
            "title": "Dataset structure"
        }, 
        {
            "location": "/datasets/kth_lt/#parsing", 
            "text": "A parser is provided  here  (can be installed with  sudo apt-get install ros-indigo-metaroom-xml-parser ) which reads in the data and returns C++ data structures encapsulating the low-level data from the disk. Form more information please refer to  the parser README  ( or  here  for a list of supported methods). Information about how to use the Strands package repository can be found  here .", 
            "title": "Parsing"
        }, 
        {
            "location": "/datasets/kth_lt/#download", 
            "text": "This dataset is available for download in a single archive  file  (\\~ 300 GB). As an alternative, the individual folders and files can be obtained from  here , and would have to be downloaded manually.", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/kth_lt/#condition-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper  that describes it:      Unsupervised learning of spatial-temporal models of objects in a long-term autonomy scenario \n    Ambrus, Rares and Ekekrantz, Johan and Folkesson, John and Jensfelt, Patric\n    Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on  We attached a  bibtex  record for your convenience.", 
            "title": "Condition of use"
        }, 
        {
            "location": "/datasets/kth_lt_labels/", 
            "text": "KTH Longterm Dataset Labels\n\n\n\n\nKTH Scitos G5 robot - Rosie\n\n\nThis dataset contains a subset of observations from \nthis dataset\n - 88 observations acquired at \nWayPoint16\n. Each observation consists of a set of 17 RGB-D images (\noriginally\n 51, however this dataset only contains the RGB-D clouds corresponding to a single height value of the PTU) obtained by moving the pan-tilt in a horizontal pattern, in increments of 20 degrees. In addition to the raw sensor data, each observation contains object annotations (masks and labels). The data is a part of the \nStrands\n EU FP7 project.\n\n\nDataset structure\n\n\nThe \ndata\n is structured in folders as follows: \nYYYYMMDD/patrol_run_YYY/room_ZZZ\n, where:\n\n\n\n\nYYYYMMDD\n represents the year, month \n day when those particular observations were acquired. Each such folder contains the patrol runs the robot collected on that specific date.\n\n\npatrol_run_YYY\n represents one of the patrol runs collected by the robot.\n\n\nroom_ZZZ\n represents a particular observation collected during a patrol run.\n\n\n\n\nEach folder of the type \nYYYMMDD/patrol_run_YYY/room_ZZZ\n contains the following files:\n\n\n\n\nroom.xml\n - contains information relevant for the observation (described in the next section)\n\n\ncomplete_cloud.pcd\n - the point cloud of the observation (obtained by merging the individual point clouds together)\n\n\nintermediate_cloudXXXX.pcd\n - ordered point clouds, each corresponding to an RGB and depth image acquired by the camera while conducting the sweep (17 such point clouds for each observation, with \nXXXX\n going from 0000 to 0016)\n\n\nrgb_XXXX.jpg\n - RGB image generated from the corresponding \nintermediate_cloudXXXX.pcd\n\n\ndepth_XXXX.png\n - depth image generated from the corresponding \nintermediate_cloudXXXX.pcd\n\n\nrgb_XXXX_label_#.jpg\n - binary mask corresponding to one of the objects annotated in the image \nrgb_XXXX.jpg\n\n\nrgb_XXXX_label_#.pcd\n - point cloud generated from object mask \nrgb_XXXX_label_#.jpg\n\n\nrgb_XXXX_label_#.txt\n - object label corresponding to annotation \nrgb_XXXX_label_#.jpg\n\n\nrgb_XXXX_label_#.xml\n - xml file storing the labelled object data (created with data from \nrgb_XXXX_label_#.pcd\n, \nrgb_XXXX_label_#.txt\n)\n\n\nrgb_XXXX_object_#.jpg\n - rgb mask corresponding to one of the objects annotated in the image \nrgb_XXXX.jpg\n\n\n\n\nThe description of the \nroom.xml\n file accompanying an observation can be found \nhere\n. \\\n\\\n Each object xml file (\nrgb_XXXX_label_#.xml\n) contains the following data:\n\n\n\n\nlabel\n - object label\n\n\nfilename\n - object pcd file\n\n\nCentroid\n - object centroid, in the map frame\n\n\nLogTime\n - time when the object was observed\n\n\n\n\nParsing\n\n\nA parser is provided \nhere\n (can be installed with \nsudo apt-get install ros-indigo-metaroom-xml-parser\n) which reads in the data and returns C++ data structures encapsulating the low-level data from the disk. Form more information please refer to \nthe parser README\n ( or \nhere\n for a list of supported methods). Information about how to use the Strands package repository can be found \nhere\n. \\\n\\\n Assuming \nthe parser\n has been installed, the labelled data can be visualized with the following sample application: \\\n\\\n \nrosrun metaroom_xml_parser load_labelled_data /path/to/data WayPoint16\n \\\n\\\n This loads all the observations along with the labelled data from the path specified and displays each observation along with the corresponding labelled objects in a visualizer window.\n\n\n\n\n   \n   \n\n\n\n\nObservation (red) with labelled objects (RGB)\n\n\nLabelling\n\n\nFor more information on the labelling tool used, please refer to \nthis page\n.\n\n\nDownload\n\n\nThis dataset is available for download in a single archive \nfile\n (\\~ 15 GB). As an alternative, the individual folders and files can be obtained from \nhere\n, and would have to be downloaded manually.\n\n\n\n\nCondition of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n that describes it:\n\n\n    Unsupervised learning of spatial-temporal models of objects in a long-term autonomy scenario \n    Ambrus, Rares and Ekekrantz, Johan and Folkesson, John and Jensfelt, Patric\n    Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on\n\n\n\nWe attached a \nbibtex\n record for your convenience.", 
            "title": "Kth lt labels"
        }, 
        {
            "location": "/datasets/kth_lt_labels/#kth-longterm-dataset-labels", 
            "text": "KTH Scitos G5 robot - Rosie  This dataset contains a subset of observations from  this dataset  - 88 observations acquired at  WayPoint16 . Each observation consists of a set of 17 RGB-D images ( originally  51, however this dataset only contains the RGB-D clouds corresponding to a single height value of the PTU) obtained by moving the pan-tilt in a horizontal pattern, in increments of 20 degrees. In addition to the raw sensor data, each observation contains object annotations (masks and labels). The data is a part of the  Strands  EU FP7 project.", 
            "title": "KTH Longterm Dataset Labels"
        }, 
        {
            "location": "/datasets/kth_lt_labels/#dataset-structure", 
            "text": "The  data  is structured in folders as follows:  YYYYMMDD/patrol_run_YYY/room_ZZZ , where:   YYYYMMDD  represents the year, month   day when those particular observations were acquired. Each such folder contains the patrol runs the robot collected on that specific date.  patrol_run_YYY  represents one of the patrol runs collected by the robot.  room_ZZZ  represents a particular observation collected during a patrol run.   Each folder of the type  YYYMMDD/patrol_run_YYY/room_ZZZ  contains the following files:   room.xml  - contains information relevant for the observation (described in the next section)  complete_cloud.pcd  - the point cloud of the observation (obtained by merging the individual point clouds together)  intermediate_cloudXXXX.pcd  - ordered point clouds, each corresponding to an RGB and depth image acquired by the camera while conducting the sweep (17 such point clouds for each observation, with  XXXX  going from 0000 to 0016)  rgb_XXXX.jpg  - RGB image generated from the corresponding  intermediate_cloudXXXX.pcd  depth_XXXX.png  - depth image generated from the corresponding  intermediate_cloudXXXX.pcd  rgb_XXXX_label_#.jpg  - binary mask corresponding to one of the objects annotated in the image  rgb_XXXX.jpg  rgb_XXXX_label_#.pcd  - point cloud generated from object mask  rgb_XXXX_label_#.jpg  rgb_XXXX_label_#.txt  - object label corresponding to annotation  rgb_XXXX_label_#.jpg  rgb_XXXX_label_#.xml  - xml file storing the labelled object data (created with data from  rgb_XXXX_label_#.pcd ,  rgb_XXXX_label_#.txt )  rgb_XXXX_object_#.jpg  - rgb mask corresponding to one of the objects annotated in the image  rgb_XXXX.jpg   The description of the  room.xml  file accompanying an observation can be found  here . \\\n\\\n Each object xml file ( rgb_XXXX_label_#.xml ) contains the following data:   label  - object label  filename  - object pcd file  Centroid  - object centroid, in the map frame  LogTime  - time when the object was observed", 
            "title": "Dataset structure"
        }, 
        {
            "location": "/datasets/kth_lt_labels/#parsing", 
            "text": "A parser is provided  here  (can be installed with  sudo apt-get install ros-indigo-metaroom-xml-parser ) which reads in the data and returns C++ data structures encapsulating the low-level data from the disk. Form more information please refer to  the parser README  ( or  here  for a list of supported methods). Information about how to use the Strands package repository can be found  here . \\\n\\\n Assuming  the parser  has been installed, the labelled data can be visualized with the following sample application: \\\n\\\n  rosrun metaroom_xml_parser load_labelled_data /path/to/data WayPoint16  \\\n\\\n This loads all the observations along with the labelled data from the path specified and displays each observation along with the corresponding labelled objects in a visualizer window.             Observation (red) with labelled objects (RGB)", 
            "title": "Parsing"
        }, 
        {
            "location": "/datasets/kth_lt_labels/#labelling", 
            "text": "For more information on the labelling tool used, please refer to  this page .", 
            "title": "Labelling"
        }, 
        {
            "location": "/datasets/kth_lt_labels/#download", 
            "text": "This dataset is available for download in a single archive  file  (\\~ 15 GB). As an alternative, the individual folders and files can be obtained from  here , and would have to be downloaded manually.", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/kth_lt_labels/#condition-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper  that describes it:      Unsupervised learning of spatial-temporal models of objects in a long-term autonomy scenario \n    Ambrus, Rares and Ekekrantz, Johan and Folkesson, John and Jensfelt, Patric\n    Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on  We attached a  bibtex  record for your convenience.", 
            "title": "Condition of use"
        }, 
        {
            "location": "/datasets/kth_lt_moving/", 
            "text": "KTH Moving Objects Dataset\n\n\n\n\nKTH Scitos G5 robot - Rosie\n\n\nThis dataset extends \nKTH Longterm Dataset Labels\n with more locations within the same office environment at KTH. The dataset contains a subset of the labels and these objects are consistently located in different positions in multiple rooms. The label subset annotated in this dataset is {\nchair6\n, \nchair1\n, \nwater_boiler\n, \nbackpack1\n, \npillow\n, \ntrash_bin\n, \nbackpack3\n, \nchair2\n, \nhanger_jacket\n, \nbackpack2\n}. Each observation consists of a set of 17 RGB-D images obtained by moving the pan-tilt in a horizontal pattern, in increments of 20 degrees. In addition to the raw sensor data, each observation contains object annotations (masks and labels). The data is a part of the \nStrands\n EU FP7 project.\n\n\nDataset structure\n\n\nThe \ndata\n is structured in folders as follows: \nYYYYMMDD/patrol_run_YYY/room_ZZZ\n, where:\n\n\n\n\nYYYYMMDD\n represents the year, month \n day when those particular observations were acquired. Each such folder contains the patrol runs the robot collected on that specific date.\n\n\npatrol_run_YYY\n represents one of the patrol runs collected by the robot.\n\n\nroom_ZZZ\n represents a particular observation collected during a patrol run.\n\n\n\n\nEach folder of the type \nYYYMMDD/patrol_run_YYY/room_ZZZ\n contains the following files:\n\n\n\n\nroom.xml\n - contains information relevant for the observation (described in the next section)\n\n\nsurfel_map.pcd\n - the surfel map constructed by using the data fusion component of \nElastic Fusion\n\n\ncomplete_cloud.pcd\n - the point cloud of the observation (obtained by thresholding the confidence of surfel_map.pcd at 0.3)\n\n\nintermediate_cloudXXXX.pcd\n - ordered point clouds, each corresponding to an RGB and depth image acquired by the camera while conducting the sweep (17 such point clouds for each observation, with \nXXXX\n going from 0000 to 0016)\n\n\nrgb_XXXX.jpg\n - RGB image generated from the corresponding \nintermediate_cloudXXXX.pcd\n\n\ndepth_XXXX.png\n - depth image generated from the corresponding \nintermediate_cloudXXXX.pcd\n\n\nrgb_XXXX_label_#.jpg\n - binary mask corresponding to one of the objects annotated in the image \nrgb_XXXX.jpg\n\n\nrgb_XXXX_label_#.pcd\n - point cloud generated from object mask \nrgb_XXXX_label_#.jpg\n\n\nrgb_XXXX_label_#.txt\n - object label corresponding to annotation \nrgb_XXXX_label_#.jpg\n\n\nrgb_XXXX_label_#.xml\n - xml file storing the labelled object data (created with data from \nrgb_XXXX_label_#.pcd\n, \nrgb_XXXX_label_#.txt\n)\n\n\nrgb_XXXX_object_#.jpg\n - rgb mask corresponding to one of the objects annotated in the image \nrgb_XXXX.jpg\n\n\n\n\nThe description of the \nroom.xml\n file accompanying an observation can be found \nhere\n. \\\n\\\n Each object xml file (\nrgb_XXXX_label_#.xml\n) contains the following data:\n\n\n\n\nlabel\n - object label\n\n\nfilename\n - object pcd file\n\n\nCentroid\n - object centroid, in the map frame\n\n\nLogTime\n - time when the object was observed\n\n\n\n\nParsing\n\n\nA parser is provided \nhere\n (can be installed with \nsudo apt-get install ros-indigo-metaroom-xml-parser\n) which reads in the data and returns C++ data structures encapsulating the low-level data from the disk. Form more information please refer to \nthe parser README\n ( or \nhere\n for a list of supported methods). Information about how to use the Strands package repository can be found \nhere\n. \\\n\\\n Assuming \nthe parser\n has been installed, the labelled data can be visualized with the following sample application: \\\n\\\n \nrosrun metaroom_xml_parser load_labelled_data /path/to/data WayPoint10\n \\\n\\\n This loads all the observations along with the labelled data from the path specified and displays each observation along with the corresponding labelled objects in a visualizer window.\n\n\n\n\n   \n   \n\n\n\n\nObservation (red) with labelled objects (RGB)\n\n\nLabelling\n\n\nFor more information on the labelling tool used, please refer to \nthis page\n.\n\n\nDownload\n\n\nThis dataset is available for download in a single archive \nfile\n (\\~ 6 GB). As an alternative, the individual folders and files can be obtained from \nhere\n, and would have to be downloaded manually.\n\n\n\n\nContact\n\n\nNils Bore\n\\\n \nhttp://www.csc.kth.se/\\~nbore/\n\\\n CSC/CVAP\\\n KTH\\\n SE-100 44 Stockholm\\\n Sweden", 
            "title": "Kth lt moving"
        }, 
        {
            "location": "/datasets/kth_lt_moving/#kth-moving-objects-dataset", 
            "text": "KTH Scitos G5 robot - Rosie  This dataset extends  KTH Longterm Dataset Labels  with more locations within the same office environment at KTH. The dataset contains a subset of the labels and these objects are consistently located in different positions in multiple rooms. The label subset annotated in this dataset is { chair6 ,  chair1 ,  water_boiler ,  backpack1 ,  pillow ,  trash_bin ,  backpack3 ,  chair2 ,  hanger_jacket ,  backpack2 }. Each observation consists of a set of 17 RGB-D images obtained by moving the pan-tilt in a horizontal pattern, in increments of 20 degrees. In addition to the raw sensor data, each observation contains object annotations (masks and labels). The data is a part of the  Strands  EU FP7 project.", 
            "title": "KTH Moving Objects Dataset"
        }, 
        {
            "location": "/datasets/kth_lt_moving/#dataset-structure", 
            "text": "The  data  is structured in folders as follows:  YYYYMMDD/patrol_run_YYY/room_ZZZ , where:   YYYYMMDD  represents the year, month   day when those particular observations were acquired. Each such folder contains the patrol runs the robot collected on that specific date.  patrol_run_YYY  represents one of the patrol runs collected by the robot.  room_ZZZ  represents a particular observation collected during a patrol run.   Each folder of the type  YYYMMDD/patrol_run_YYY/room_ZZZ  contains the following files:   room.xml  - contains information relevant for the observation (described in the next section)  surfel_map.pcd  - the surfel map constructed by using the data fusion component of  Elastic Fusion  complete_cloud.pcd  - the point cloud of the observation (obtained by thresholding the confidence of surfel_map.pcd at 0.3)  intermediate_cloudXXXX.pcd  - ordered point clouds, each corresponding to an RGB and depth image acquired by the camera while conducting the sweep (17 such point clouds for each observation, with  XXXX  going from 0000 to 0016)  rgb_XXXX.jpg  - RGB image generated from the corresponding  intermediate_cloudXXXX.pcd  depth_XXXX.png  - depth image generated from the corresponding  intermediate_cloudXXXX.pcd  rgb_XXXX_label_#.jpg  - binary mask corresponding to one of the objects annotated in the image  rgb_XXXX.jpg  rgb_XXXX_label_#.pcd  - point cloud generated from object mask  rgb_XXXX_label_#.jpg  rgb_XXXX_label_#.txt  - object label corresponding to annotation  rgb_XXXX_label_#.jpg  rgb_XXXX_label_#.xml  - xml file storing the labelled object data (created with data from  rgb_XXXX_label_#.pcd ,  rgb_XXXX_label_#.txt )  rgb_XXXX_object_#.jpg  - rgb mask corresponding to one of the objects annotated in the image  rgb_XXXX.jpg   The description of the  room.xml  file accompanying an observation can be found  here . \\\n\\\n Each object xml file ( rgb_XXXX_label_#.xml ) contains the following data:   label  - object label  filename  - object pcd file  Centroid  - object centroid, in the map frame  LogTime  - time when the object was observed", 
            "title": "Dataset structure"
        }, 
        {
            "location": "/datasets/kth_lt_moving/#parsing", 
            "text": "A parser is provided  here  (can be installed with  sudo apt-get install ros-indigo-metaroom-xml-parser ) which reads in the data and returns C++ data structures encapsulating the low-level data from the disk. Form more information please refer to  the parser README  ( or  here  for a list of supported methods). Information about how to use the Strands package repository can be found  here . \\\n\\\n Assuming  the parser  has been installed, the labelled data can be visualized with the following sample application: \\\n\\\n  rosrun metaroom_xml_parser load_labelled_data /path/to/data WayPoint10  \\\n\\\n This loads all the observations along with the labelled data from the path specified and displays each observation along with the corresponding labelled objects in a visualizer window.             Observation (red) with labelled objects (RGB)", 
            "title": "Parsing"
        }, 
        {
            "location": "/datasets/kth_lt_moving/#labelling", 
            "text": "For more information on the labelling tool used, please refer to  this page .", 
            "title": "Labelling"
        }, 
        {
            "location": "/datasets/kth_lt_moving/#download", 
            "text": "This dataset is available for download in a single archive  file  (\\~ 6 GB). As an alternative, the individual folders and files can be obtained from  here , and would have to be downloaded manually.", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/kth_lt_moving/#contact", 
            "text": "Nils Bore \\\n  http://www.csc.kth.se/\\~nbore/ \\\n CSC/CVAP\\\n KTH\\\n SE-100 44 Stockholm\\\n Sweden", 
            "title": "Contact"
        }, 
        {
            "location": "/datasets/marathon/", 
            "text": "Strands Robot Marathon 2014\n\n\n\n\n\n  The Strands project\n\n\n\n\n\\\n\\\n This dataset was acquired by the Strands robots participating in the \nStrands Robot Marathon 2014\n. The data is a part of the \nStrands\n EU FP7 project.\n\n\nDataset structure\n\n\nThe dataset contains all the \ndata\n logged by the robots during operation, exported from the \nmongodb\n database. For information on the relevant collections please refer to the \nStrands\n project. \\\n\\\n Separately, data corresponding to observations acquired by the robot with the Pan-Tilt unit and the RGB-D sensor have been exported and can be found \nhere\n. For a description of the format as well as how to parse the data please refer to this \nreadme file\n.\n\n\nDownload\n\n\nThis dataset is available for download in a single archive \nfile\n (\\~ 136 GB). As an alternative, the individual folders and files can be obtained from \nhere\n, and would have to be downloaded manually.", 
            "title": "Marathon"
        }, 
        {
            "location": "/datasets/marathon/#strands-robot-marathon-2014", 
            "text": "The Strands project   \\\n\\\n This dataset was acquired by the Strands robots participating in the  Strands Robot Marathon 2014 . The data is a part of the  Strands  EU FP7 project.", 
            "title": "Strands Robot Marathon 2014"
        }, 
        {
            "location": "/datasets/marathon/#dataset-structure", 
            "text": "The dataset contains all the  data  logged by the robots during operation, exported from the  mongodb  database. For information on the relevant collections please refer to the  Strands  project. \\\n\\\n Separately, data corresponding to observations acquired by the robot with the Pan-Tilt unit and the RGB-D sensor have been exported and can be found  here . For a description of the format as well as how to parse the data please refer to this  readme file .", 
            "title": "Dataset structure"
        }, 
        {
            "location": "/datasets/marathon/#download", 
            "text": "This dataset is available for download in a single archive  file  (\\~ 136 GB). As an alternative, the individual folders and files can be obtained from  here , and would have to be downloaded manually.", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/meta_rooms/", 
            "text": "KTH Meta-rooms dataset\n\n\n\n\n\n\nKTH Scitos G5 robot - Rosie\n\n\n2D Map with waypoints\n\n\nThe data was collected autonomously by a Scitos G5 robot with an RGB-D camera on a pan-tilt, navigating through the KTH office environment over a period of 7 days. Three waypoints have been defined on the 2D map, and each one is visited once per day. Each observation consists of a set of 28 RGB-D images obtained by moving the pan-tilt in a pattern, in increments of 60 degrees horizontally and 30 degrees vertically. The data is a part of the \nStrands\n EU FP7 project.\n\n\n\n\n\n\n\n\n\n\nObservations acquired by the robot\n\n\nDataset structure\n\n\nThe \ndata\n is structured in folders as follows: \nYYYYMMDD/patrol_run_YYY/room_ZZZ\n, where:\n\n\n\n\nYYYYMMDD\n represents the year, month \n day when those particular observations were acquired. Each such folder contains the patrol runs the robot collected on that specific date.\n\n\npatrol_run_YYY\n represents one of the patrol runs collected by the robot.\n\n\nroom_ZZZ\n represents a particular observation collected during a patrol run.\n\n\n\n\nEach folder of the type \nYYYMMDD/patrol_run_YYY/room_ZZZ\n contains the following files:\n\n\n\n\nroom.xml\n - contains information relevant for the observation (described in the next section)\n\n\ncomplete_cloud.pcd\n - the point cloud of the observation (obtained by merging the individual point clouds together)\n\n\nintermediate_cloud*.pcd\n - ordered point clouds, each corresponding to an RGB and depth image acquired by the camera while conducting the sweep (28 such point clouds for each observation)\n\n\n\n\nThe \nroom.xml\n file accompanying an observation contains the following (relevant) fields:\n\n\nRoomLogName\n - identifier which associates the observation with the folder structure\n\n\nRoomRunNumber\n - identifier which denotes when the observation was acquired during the patrol run (i.e. 0 - first, 1 - second, etc.)\n\n\nRoomStringId\n - identifier which corresponds to the waypoint at which the observation was acquired.\n\n\nRoomLogStartTime / RoomLogEndTime\n - acquisition time\n\n\nCentroid\n - observation centroid in the map frame\n\n\nRoomCompleteCloud\n - complete cloud filename\n\n\nRoomIntermediateClouds\n\n\nRoomIntermediateCloud\n - intermediate cloud filename\n\n\n\n\nRoomIntermediateCloudTransform\n - transform from the RGB-D sensor frame to the map frame, as given by the robot odometry\n\n\n\n\nParsing\n\n\nA parser is provided \nhere\n (can be installed with \nsudo apt-get install ros-indigo-metaroom-xml-parser\n) which reads in the data and returns C++ data structures encapsulating the low-level data from the disk. Form more information please refer to \nthe parser README\n ( or \nhere\n for a list of supported methods). Information about how to use the Strands package repository can be found \nhere\n.\n\n\nDownload\n\n\nThis dataset is available for download in a single archive \nfile\n (\\~ 12 GB). As an alternative, the individual folders and files can be obtained from \nhere\n, and would have to be downloaded manually.\n\n\n\n\nCondition of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n that describes it:\n\n\n    Meta-rooms: Building and maintaining long term spatial models in a dynamic world\n    Ambrus, Rares and Bore, Nils and Folkesson, John and Jensfelt, Patric\n    Intelligent Robots and Systems (IROS), 2014 IEEE/RSJ International Conference on\n\n\n\nWe attached a \nbibtex\n record for your convenience.", 
            "title": "Meta rooms"
        }, 
        {
            "location": "/datasets/meta_rooms/#kth-meta-rooms-dataset", 
            "text": "KTH Scitos G5 robot - Rosie  2D Map with waypoints  The data was collected autonomously by a Scitos G5 robot with an RGB-D camera on a pan-tilt, navigating through the KTH office environment over a period of 7 days. Three waypoints have been defined on the 2D map, and each one is visited once per day. Each observation consists of a set of 28 RGB-D images obtained by moving the pan-tilt in a pattern, in increments of 60 degrees horizontally and 30 degrees vertically. The data is a part of the  Strands  EU FP7 project.      Observations acquired by the robot", 
            "title": "KTH Meta-rooms dataset"
        }, 
        {
            "location": "/datasets/meta_rooms/#dataset-structure", 
            "text": "The  data  is structured in folders as follows:  YYYYMMDD/patrol_run_YYY/room_ZZZ , where:   YYYYMMDD  represents the year, month   day when those particular observations were acquired. Each such folder contains the patrol runs the robot collected on that specific date.  patrol_run_YYY  represents one of the patrol runs collected by the robot.  room_ZZZ  represents a particular observation collected during a patrol run.   Each folder of the type  YYYMMDD/patrol_run_YYY/room_ZZZ  contains the following files:   room.xml  - contains information relevant for the observation (described in the next section)  complete_cloud.pcd  - the point cloud of the observation (obtained by merging the individual point clouds together)  intermediate_cloud*.pcd  - ordered point clouds, each corresponding to an RGB and depth image acquired by the camera while conducting the sweep (28 such point clouds for each observation)   The  room.xml  file accompanying an observation contains the following (relevant) fields:  RoomLogName  - identifier which associates the observation with the folder structure  RoomRunNumber  - identifier which denotes when the observation was acquired during the patrol run (i.e. 0 - first, 1 - second, etc.)  RoomStringId  - identifier which corresponds to the waypoint at which the observation was acquired.  RoomLogStartTime / RoomLogEndTime  - acquisition time  Centroid  - observation centroid in the map frame  RoomCompleteCloud  - complete cloud filename  RoomIntermediateClouds  RoomIntermediateCloud  - intermediate cloud filename   RoomIntermediateCloudTransform  - transform from the RGB-D sensor frame to the map frame, as given by the robot odometry", 
            "title": "Dataset structure"
        }, 
        {
            "location": "/datasets/meta_rooms/#parsing", 
            "text": "A parser is provided  here  (can be installed with  sudo apt-get install ros-indigo-metaroom-xml-parser ) which reads in the data and returns C++ data structures encapsulating the low-level data from the disk. Form more information please refer to  the parser README  ( or  here  for a list of supported methods). Information about how to use the Strands package repository can be found  here .", 
            "title": "Parsing"
        }, 
        {
            "location": "/datasets/meta_rooms/#download", 
            "text": "This dataset is available for download in a single archive  file  (\\~ 12 GB). As an alternative, the individual folders and files can be obtained from  here , and would have to be downloaded manually.", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/meta_rooms/#condition-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper  that describes it:      Meta-rooms: Building and maintaining long term spatial models in a dynamic world\n    Ambrus, Rares and Bore, Nils and Folkesson, John and Jensfelt, Patric\n    Intelligent Robots and Systems (IROS), 2014 IEEE/RSJ International Conference on  We attached a  bibtex  record for your convenience.", 
            "title": "Condition of use"
        }, 
        {
            "location": "/datasets/mht_rgbd/", 
            "text": "MHT Building RGB-D and laser dataset\n\n\nThe MHT building dataset was collected for purposes of testing RGB-D mapping of changing environments. The dataset consists of 2D (laser scanner) and 3D (Asus Xtion) data collected by a SCITOS-G5 mobile robot. We provide the gathered data in form of rosbags.\n\n\n\n\nDataset purpose\n\n\nThe intended use was to create a dataset to benchmark spatio-temporal representations of changing environments. The robot was patrolling a small office (see the video below) every 5 minutes. Each patrol started and ended at a charging station. During each patrol, the robot continuously collected its laser scans and odometric data. Moreover, it stopped at three different locations, took snapshots using its RGB-D sensor and attempted to detect people presence.\n\n\n\\\n\n\n\n\n\n  MHT office night collections with 3D sweeps\n\n\n\n\n\n\nDataset structure\n\n\nThe provided archives contain rosbags, which are zipped into separate files according to the day of the data collection and data type. Each rosbag with a 3D prefix contains a depth/color image, camera information, robot position, tf data, laser scan and person detection gathered by the robot at a location and time that is encoded in the rosbag name, which contains day, month, year, hour, minute and location id. For example, \n3D_23-08-13-15-20_place_2.bag\n contains data gathered at location 2 on August 23 2013 at 15:20 o'clock. Each rosbag with a 2D prefix contains AMCL position estimates, robot odometry, tf data and laser scans. Day, month, year, hour and minute are part of the bag file name.\n\n\n\n\nDownload\n\n\n\n\n\n\n\n\n\n\n\n\n\n**August 2013**\n\n\n\n\n\n\n\n\n\n[2D data Aug](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08.zip)\n\n\n\n\n\n[3D data Aug](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-08-24](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-24.zip)\n\n\n\n\n\n[3D\\_2013-08-24](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-24.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-08-25](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-25.zip)\n\n\n\n\n\n[3D\\_2013-08-25](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-25.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-08-26](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-26.zip)\n\n\n\n\n\n[3D\\_2013-08-26](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-26.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-08-27](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-27.zip)\n\n\n\n\n\n[3D\\_2013-08-27](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-27.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-08-28](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-28.zip)\n\n\n\n\n\n[3D\\_2013-08-28](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-28.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-08-29](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-29.zip)\n\n\n\n\n\n[3D\\_2013-08-29](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-29.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-08-30](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-30.zip)\n\n\n\n\n\n[3D\\_2013-08-30](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-30.zip)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n**September 2013**\n\n\n\n\n\n\n\n\n\n[2D data Sep](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09.zip)\n\n\n\n\n\n[3D data Sep](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-09-01](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)\n\n\n\n\n\n[3D\\_2013-09-01](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-09-02](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)\n\n\n\n\n\n[3D\\_2013-09-02](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-02.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-09-03](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)\n\n\n\n\n\n[3D\\_2013-09-03](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-03.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-09-04](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)\n\n\n\n\n\n[3D\\_2013-09-04](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-04.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-09-05](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)\n\n\n\n\n\n[3D\\_2013-09-05](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-05.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-09-06](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)\n\n\n\n\n\n[3D\\_2013-09-06](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-06.zip)\n\n\n\n\n\n\n\n\n\n[2D\\_2013-09-07](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)\n\n\n\n\n\n[3D\\_2013-09-07](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-07.zip)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConditions of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n that describes the data collection in detail. We attached a \nbibtex\n record for your convenience.\n\n\n\n\nThis dataset is part of the larger \nLCAS-STRANDS long-term dataset collection\n.", 
            "title": "Mht rgbd"
        }, 
        {
            "location": "/datasets/mht_rgbd/#mht-building-rgb-d-and-laser-dataset", 
            "text": "The MHT building dataset was collected for purposes of testing RGB-D mapping of changing environments. The dataset consists of 2D (laser scanner) and 3D (Asus Xtion) data collected by a SCITOS-G5 mobile robot. We provide the gathered data in form of rosbags.", 
            "title": "MHT Building RGB-D and laser dataset"
        }, 
        {
            "location": "/datasets/mht_rgbd/#dataset-purpose", 
            "text": "The intended use was to create a dataset to benchmark spatio-temporal representations of changing environments. The robot was patrolling a small office (see the video below) every 5 minutes. Each patrol started and ended at a charging station. During each patrol, the robot continuously collected its laser scans and odometric data. Moreover, it stopped at three different locations, took snapshots using its RGB-D sensor and attempted to detect people presence.  \\   \n  MHT office night collections with 3D sweeps", 
            "title": "Dataset purpose"
        }, 
        {
            "location": "/datasets/mht_rgbd/#dataset-structure", 
            "text": "The provided archives contain rosbags, which are zipped into separate files according to the day of the data collection and data type. Each rosbag with a 3D prefix contains a depth/color image, camera information, robot position, tf data, laser scan and person detection gathered by the robot at a location and time that is encoded in the rosbag name, which contains day, month, year, hour, minute and location id. For example,  3D_23-08-13-15-20_place_2.bag  contains data gathered at location 2 on August 23 2013 at 15:20 o'clock. Each rosbag with a 2D prefix contains AMCL position estimates, robot odometry, tf data and laser scans. Day, month, year, hour and minute are part of the bag file name.", 
            "title": "Dataset structure"
        }, 
        {
            "location": "/datasets/mht_rgbd/#download", 
            "text": "**August 2013**    \n[2D data Aug](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08.zip)  \n[3D data Aug](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08.zip)    \n[2D\\_2013-08-24](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-24.zip)  \n[3D\\_2013-08-24](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-24.zip)    \n[2D\\_2013-08-25](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-25.zip)  \n[3D\\_2013-08-25](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-25.zip)    \n[2D\\_2013-08-26](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-26.zip)  \n[3D\\_2013-08-26](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-26.zip)    \n[2D\\_2013-08-27](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-27.zip)  \n[3D\\_2013-08-27](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-27.zip)    \n[2D\\_2013-08-28](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-28.zip)  \n[3D\\_2013-08-28](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-28.zip)    \n[2D\\_2013-08-29](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-29.zip)  \n[3D\\_2013-08-29](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-29.zip)    \n[2D\\_2013-08-30](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/2D_2013-08-30.zip)  \n[3D\\_2013-08-30](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-08-30.zip)          \n**September 2013**    \n[2D data Sep](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09.zip)  \n[3D data Sep](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09.zip)    \n[2D\\_2013-09-01](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)  \n[3D\\_2013-09-01](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)    \n[2D\\_2013-09-02](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)  \n[3D\\_2013-09-02](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-02.zip)    \n[2D\\_2013-09-03](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)  \n[3D\\_2013-09-03](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-03.zip)    \n[2D\\_2013-09-04](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)  \n[3D\\_2013-09-04](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-04.zip)    \n[2D\\_2013-09-05](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)  \n[3D\\_2013-09-05](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-05.zip)    \n[2D\\_2013-09-06](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)  \n[3D\\_2013-09-06](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-06.zip)    \n[2D\\_2013-09-07](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-01.zip)  \n[3D\\_2013-09-07](https://lcas.lincoln.ac.uk/owncloud/shared/datasets/MHT/3D_2013-09-07.zip)", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/mht_rgbd/#conditions-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper  that describes the data collection in detail. We attached a  bibtex  record for your convenience.   This dataset is part of the larger  LCAS-STRANDS long-term dataset collection .", 
            "title": "Conditions of use"
        }, 
        {
            "location": "/datasets/object_presence/", 
            "text": "Object/people presence aka where is Waldo? datasets\n\n\nThis dataset contains information about people and object presence gathered in three different environments. The dataset is divided in three smaller ones according to the environment these were gathered in:\n\n\n\n\nAruba\n dataset of person presence collected at a smart apartment by the Center for Advanced Studies in Adaptive Systems, \nCASAS\n,\n\n\nBrayford\n dataset of person presence created at the premised of the Lincoln Centre for Autonomous System, \nLCAS\n),\n\n\nKTH\n dataset of object presence created by the Centre of Autonomous Systems of the Royal Institute of Technology in Stockholm, \nCAS/KTH\n.\n\n\n\n\n\n\nDataset structure\n\n\nAruba\n\n\n\n\n\n\nAruba apartment visualisation\n\n\nAruba sensor layout (see \nCASAS\n)\n\n\nThe \nAruba\n folder contains \nlocations.min\n, which contains the location (room) of a person in a small apartment every minute for 16 weeks. The \nlocations.names\n indicate which rooms corresponds to which number. Example: number 0 on line 10 of the locations.min means that in 10th minute after midnight of the first day, the person was in the \nMaster bedroom\n. \nAruba\n was extracted from the \nCASAS\n datasets.\n\n\nBrayford\n\n\n\n\nBrayford office locations and topological structure\n\n\nThe \nBrayford\n directory contains presence of people every 10 minutes on eight different areas in an open-plan office. Again, \nlocations.names\n file describes the locations. Example: Having '0' on line 6 of file 'learning_06.txt' means that there was no-one at 1:00AM in the kitchenette.\n\n\nKTH\n\n\n\n\nKTH dataset: object retrieval from point cloud data\n\n\nThe \nKTH\n data containe presence of objects observed by a mobile robot at different rooms of the \nCAS/KTH\n. The observations are stored in *.occ files that have 'epoch time' + presence of a given object at that time. Description of the individual objects is also provided in the \nclusters_meaning\n file. Example: line 50 of the \ntest/31.occ\n file indicates that 'Johan's laptop' was present at time 1411317123 (which is Sunday September 21, 2014 18:32:03).\n\n\n\n\nDownload\n\n\nAll of these datasets are available for download in a single archive \nfile\n. After you unzip the file, you get three folders which correspond to the individual datasets.\n\n\n\n\nCondition of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n that describes it. We attached a \nbibtex\n record for your convenience. If you use the \nAruba\n subset, you must also acknowledge the original \nCASAS\n paper.\n\n\n\n\nThis dataset is part of the larger \nLCAS-STRANDS long-term dataset collection\n.", 
            "title": "Object presence"
        }, 
        {
            "location": "/datasets/object_presence/#objectpeople-presence-aka-where-is-waldo-datasets", 
            "text": "This dataset contains information about people and object presence gathered in three different environments. The dataset is divided in three smaller ones according to the environment these were gathered in:   Aruba  dataset of person presence collected at a smart apartment by the Center for Advanced Studies in Adaptive Systems,  CASAS ,  Brayford  dataset of person presence created at the premised of the Lincoln Centre for Autonomous System,  LCAS ),  KTH  dataset of object presence created by the Centre of Autonomous Systems of the Royal Institute of Technology in Stockholm,  CAS/KTH .", 
            "title": "Object/people presence aka where is Waldo? datasets"
        }, 
        {
            "location": "/datasets/object_presence/#dataset-structure", 
            "text": "", 
            "title": "Dataset structure"
        }, 
        {
            "location": "/datasets/object_presence/#aruba", 
            "text": "Aruba apartment visualisation  Aruba sensor layout (see  CASAS )  The  Aruba  folder contains  locations.min , which contains the location (room) of a person in a small apartment every minute for 16 weeks. The  locations.names  indicate which rooms corresponds to which number. Example: number 0 on line 10 of the locations.min means that in 10th minute after midnight of the first day, the person was in the  Master bedroom .  Aruba  was extracted from the  CASAS  datasets.", 
            "title": "Aruba"
        }, 
        {
            "location": "/datasets/object_presence/#brayford", 
            "text": "Brayford office locations and topological structure  The  Brayford  directory contains presence of people every 10 minutes on eight different areas in an open-plan office. Again,  locations.names  file describes the locations. Example: Having '0' on line 6 of file 'learning_06.txt' means that there was no-one at 1:00AM in the kitchenette.", 
            "title": "Brayford"
        }, 
        {
            "location": "/datasets/object_presence/#kth", 
            "text": "KTH dataset: object retrieval from point cloud data  The  KTH  data containe presence of objects observed by a mobile robot at different rooms of the  CAS/KTH . The observations are stored in *.occ files that have 'epoch time' + presence of a given object at that time. Description of the individual objects is also provided in the  clusters_meaning  file. Example: line 50 of the  test/31.occ  file indicates that 'Johan's laptop' was present at time 1411317123 (which is Sunday September 21, 2014 18:32:03).", 
            "title": "KTH"
        }, 
        {
            "location": "/datasets/object_presence/#download", 
            "text": "All of these datasets are available for download in a single archive  file . After you unzip the file, you get three folders which correspond to the individual datasets.", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/object_presence/#condition-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper  that describes it. We attached a  bibtex  record for your convenience. If you use the  Aruba  subset, you must also acknowledge the original  CASAS  paper.   This dataset is part of the larger  LCAS-STRANDS long-term dataset collection .", 
            "title": "Condition of use"
        }, 
        {
            "location": "/datasets/people_tracks/", 
            "text": "KTH Track Dataset\n\n\n\n\nUoB Scitos G5 robot - Bob\n\n\nThe track data were collected autonomously by a Scitos G5 robot with an RGB-D camera on a pan-tilt and a mounted laser scanner on the base of the robot, navigating through University of Birmingham library for approximately ten hours. The data contain 6251 human tracks. Each track is regarded as a sequence of human positions sticthed together based on the chronology of the positions. This track is obtained by detecting and tracking a person passing in front of the robot. All collected tracks overlayed over UoB library map can be seen in the figure below. The chronology of each track is shown by the transition of the colour from blue to green. The data are a part of the \nStrands\n EU FP7 project.\n\n\n\n\nHuman tracks overlayed on the 2D map\n\n\n\\\n\n\nDataset structure\n\n\nData contain 6251 human tracks. Each track is stored in a cell \nC~n~\n and each cell is composed of \n4 x N\n matrix.\n\n\nFirst and second rows correspond to \n(x, y)\n coordinates in meters for each captured position.\n\n\nThird and fourth rows correspond to the second and nanosecond of the epoch/unix timestamp information of each position. The timestamp has the format \nssssssssss.nnnnnnnnn\n where \ns\n is the second and \nn\n is the nanosecond of the timestamp.\n\n\nThe unix timestamp and corresponding date of a column j in a cell Cn is calculated using the formula:\n\n\n\n\nTSn(j) = Cn(3,j) + Cn(4,j)*1e-09\n\n\ntimestamp = str2num(num2str(TSn(j), '%.10g'))\n\n\ndate = datestr(timestamp/86400 + datenum(1970,1,1))\n\n\n\n\nDownload\n\n\nThis dataset is available for download as a .mat \nfile\n (\\~8.5 MB).\n\n\n\n\nCondition of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n that describes it:\n\n\n    Real-Time Multisensor People Tracking for Human-Robot Spatial Interaction \n    Dondrup, Christian and Bellotto, Nicola and Jovan, Ferdian and Hanheide, Marc\n    Robotics and Automation (ICRA), 2015 IEEE International Conference on\n\n\n\nWe attached a \nbibtex\n record for your convenience.\n\n\nFor further questions regarding the dataset, you can contact Ferdian Jovan (fxj345[at]cs.bham.ac.uk).", 
            "title": "People tracks"
        }, 
        {
            "location": "/datasets/people_tracks/#kth-track-dataset", 
            "text": "UoB Scitos G5 robot - Bob  The track data were collected autonomously by a Scitos G5 robot with an RGB-D camera on a pan-tilt and a mounted laser scanner on the base of the robot, navigating through University of Birmingham library for approximately ten hours. The data contain 6251 human tracks. Each track is regarded as a sequence of human positions sticthed together based on the chronology of the positions. This track is obtained by detecting and tracking a person passing in front of the robot. All collected tracks overlayed over UoB library map can be seen in the figure below. The chronology of each track is shown by the transition of the colour from blue to green. The data are a part of the  Strands  EU FP7 project.   Human tracks overlayed on the 2D map  \\", 
            "title": "KTH Track Dataset"
        }, 
        {
            "location": "/datasets/people_tracks/#dataset-structure", 
            "text": "Data contain 6251 human tracks. Each track is stored in a cell  C~n~  and each cell is composed of  4 x N  matrix.  First and second rows correspond to  (x, y)  coordinates in meters for each captured position.  Third and fourth rows correspond to the second and nanosecond of the epoch/unix timestamp information of each position. The timestamp has the format  ssssssssss.nnnnnnnnn  where  s  is the second and  n  is the nanosecond of the timestamp.  The unix timestamp and corresponding date of a column j in a cell Cn is calculated using the formula:   TSn(j) = Cn(3,j) + Cn(4,j)*1e-09  timestamp = str2num(num2str(TSn(j), '%.10g'))  date = datestr(timestamp/86400 + datenum(1970,1,1))", 
            "title": "Dataset structure"
        }, 
        {
            "location": "/datasets/people_tracks/#download", 
            "text": "This dataset is available for download as a .mat  file  (\\~8.5 MB).", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/people_tracks/#condition-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper  that describes it:      Real-Time Multisensor People Tracking for Human-Robot Spatial Interaction \n    Dondrup, Christian and Bellotto, Nicola and Jovan, Ferdian and Hanheide, Marc\n    Robotics and Automation (ICRA), 2015 IEEE International Conference on  We attached a  bibtex  record for your convenience.  For further questions regarding the dataset, you can contact Ferdian Jovan (fxj345[at]cs.bham.ac.uk).", 
            "title": "Condition of use"
        }, 
        {
            "location": "/datasets/person_activity/", 
            "text": "Long-term person activity datasets\n\n\nThis dataset contains information about a given person activity over several weeks. It was used to evaluate which type of spatio-temporal models improve the accuracy of activity classification over time. The dataset contains information about human activity in two different environments:\n\n\n\n\nAruba\n dataset contains person activity collected at a smart apartment by the Center for Advanced Studies in Adaptive Systems, \nCASAS\n,\n\n\nWitham Wharf\n dataset shows activity at the Lincoln Centre for Autonomous System, \nLCAS\n.\n\n\n\n\n\n\nDataset structure\n\n\nAruba\n\n\n\n\n\n\nAruba apartment visualisation\n\n\nAruba sensor layout (see \nCASAS\n)\n\n\nThe \nAruba\n folder contains \nactivity.min\n, which indicates the activity performed by a home-bound person in a small apartment every minute for 16 weeks. In addition \nlocations.min\n contains the person location (room) minute-by-minute as well. The \nlocation.names\n and \nactivity.names\n indicate which rooms and activities correspond to which number in the \nactivity.min\n and \nlocation.min\n files. Example: number 0 on line 10 of the \nlocation.min\n and number 2 on 10th line of \nactivity.min\n indicate that in the 10th minute after midnight of the first day, the person was \nEating\n in the \nMaster bedroom\n. The \nAruba\n dataset was extracted from the \nCASAS\n datasets.\n\n\nWitham Wharf\n\n\n\n\n\n\nWitham office overview\n\n\nWitham office topological layout\n\n\nThe \nWitham Wharf\n directory contains activity of a particular student in an open-plan office for three weeks. Again, \nlocations.names\n and \nactivity.names\n files describe the locations and activities, which are stored in \nlocation.min\n and \nactivity.min\n files.\n\n\nThese contain the student's activity and location on a minute-by-minute basis.\n\n\nDownload\n\n\nAll of these datasets are available for download in a single archive \nfile\n. After you unzip the file, you get two folders which correspond to the individual datasets.\n\n\n\n\nCondition of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n that describes it. We attached a \nbibtex\n record for your convenience. If you use the \nAruba\n subset, you must also acknowledge the original \nCASAS\n paper.\n\n\n\n\nThis dataset is part of the larger \nLCAS-STRANDS long-term dataset collection\n.", 
            "title": "Person activity"
        }, 
        {
            "location": "/datasets/person_activity/#long-term-person-activity-datasets", 
            "text": "This dataset contains information about a given person activity over several weeks. It was used to evaluate which type of spatio-temporal models improve the accuracy of activity classification over time. The dataset contains information about human activity in two different environments:   Aruba  dataset contains person activity collected at a smart apartment by the Center for Advanced Studies in Adaptive Systems,  CASAS ,  Witham Wharf  dataset shows activity at the Lincoln Centre for Autonomous System,  LCAS .", 
            "title": "Long-term person activity datasets"
        }, 
        {
            "location": "/datasets/person_activity/#dataset-structure", 
            "text": "", 
            "title": "Dataset structure"
        }, 
        {
            "location": "/datasets/person_activity/#aruba", 
            "text": "Aruba apartment visualisation  Aruba sensor layout (see  CASAS )  The  Aruba  folder contains  activity.min , which indicates the activity performed by a home-bound person in a small apartment every minute for 16 weeks. In addition  locations.min  contains the person location (room) minute-by-minute as well. The  location.names  and  activity.names  indicate which rooms and activities correspond to which number in the  activity.min  and  location.min  files. Example: number 0 on line 10 of the  location.min  and number 2 on 10th line of  activity.min  indicate that in the 10th minute after midnight of the first day, the person was  Eating  in the  Master bedroom . The  Aruba  dataset was extracted from the  CASAS  datasets.", 
            "title": "Aruba"
        }, 
        {
            "location": "/datasets/person_activity/#witham-wharf", 
            "text": "Witham office overview  Witham office topological layout  The  Witham Wharf  directory contains activity of a particular student in an open-plan office for three weeks. Again,  locations.names  and  activity.names  files describe the locations and activities, which are stored in  location.min  and  activity.min  files.  These contain the student's activity and location on a minute-by-minute basis.", 
            "title": "Witham Wharf"
        }, 
        {
            "location": "/datasets/person_activity/#download", 
            "text": "All of these datasets are available for download in a single archive  file . After you unzip the file, you get two folders which correspond to the individual datasets.", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/person_activity/#condition-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper  that describes it. We attached a  bibtex  record for your convenience. If you use the  Aruba  subset, you must also acknowledge the original  CASAS  paper.   This dataset is part of the larger  LCAS-STRANDS long-term dataset collection .", 
            "title": "Condition of use"
        }, 
        {
            "location": "/datasets/small_office/", 
            "text": "MHT building lecturer office\n\n\nThis dataset contains depth images captured by a stationary Asus Xtion sensor located in a small lecturer office in the University of Lincoln. The sensor captures a depth image every 5 seconds since April 2014 and the depth data are compressed and uploaded automatically every day. The data is recorded simply as a stream of 320x240x16b images. To convert them to ROS-compatible \nimage_raw\n format, you will need to use the \nconvert_raw\n utility, which is part of the \nfroctomap\n package.\n\n\n\\\n \\\n \\\n\n\n\n\n             \n\n  Typical afternoon depth map.   Typical midnight depth map.\n\n\n\n\n\\\n \\\n\n\n\n\nDataset structure and download\n\n\nThe depth images, organised in chunks of one-day length are \nhere\n. Each zip archive contains a \nxx.3D\n file with the depth image taked every 5 seconds and a \nxx.1D\n file that contains timestamps and distance of central pixel of the depth image taken at 30Hz. The date and time, encoded in the file name, denote the moment when the data collection started. Note that since a frame is captured every 5 seconds, the first frame of \n2016-02-25_23:59:56.3D\n file is actually taken at midnight Feb 26 2016. The dataset collection terminated on May 2016, when the room started to be occupied by another researcher.\n\n\nTo obtain ROS-compatible depth data stream, unzip the downloaded file, and use the \nconvert_raw\n utility, which is part of the \nfroctomap\n package. Calling \nrosrun froctomap convert_raw xxx.3D\n will publish the depth images on the \n/camera/depth/image_raw\n topic with the timestamp from the data collection time. To modify the timestamp to the current time, simply do the changes aroung line 97 of \nconvert_raw.cpp\n.\n\n\n\n\nConditions of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n that describes it. We attached a \nbibtex\n record for your convenience.\n\n\n\n\nThis dataset is part of the larger \nLCAS-STRANDS long-term dataset collection\n.", 
            "title": "Small office"
        }, 
        {
            "location": "/datasets/small_office/#mht-building-lecturer-office", 
            "text": "This dataset contains depth images captured by a stationary Asus Xtion sensor located in a small lecturer office in the University of Lincoln. The sensor captures a depth image every 5 seconds since April 2014 and the depth data are compressed and uploaded automatically every day. The data is recorded simply as a stream of 320x240x16b images. To convert them to ROS-compatible  image_raw  format, you will need to use the  convert_raw  utility, which is part of the  froctomap  package.  \\\n \\\n \\                 \n  Typical afternoon depth map.   Typical midnight depth map.   \\\n \\", 
            "title": "MHT building lecturer office"
        }, 
        {
            "location": "/datasets/small_office/#dataset-structure-and-download", 
            "text": "The depth images, organised in chunks of one-day length are  here . Each zip archive contains a  xx.3D  file with the depth image taked every 5 seconds and a  xx.1D  file that contains timestamps and distance of central pixel of the depth image taken at 30Hz. The date and time, encoded in the file name, denote the moment when the data collection started. Note that since a frame is captured every 5 seconds, the first frame of  2016-02-25_23:59:56.3D  file is actually taken at midnight Feb 26 2016. The dataset collection terminated on May 2016, when the room started to be occupied by another researcher.  To obtain ROS-compatible depth data stream, unzip the downloaded file, and use the  convert_raw  utility, which is part of the  froctomap  package. Calling  rosrun froctomap convert_raw xxx.3D  will publish the depth images on the  /camera/depth/image_raw  topic with the timestamp from the data collection time. To modify the timestamp to the current time, simply do the changes aroung line 97 of  convert_raw.cpp .", 
            "title": "Dataset structure and download"
        }, 
        {
            "location": "/datasets/small_office/#conditions-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper  that describes it. We attached a  bibtex  record for your convenience.   This dataset is part of the larger  LCAS-STRANDS long-term dataset collection .", 
            "title": "Conditions of use"
        }, 
        {
            "location": "/datasets/three_d_net/", 
            "text": "3DNet Dataset\n\n\nThe 3DNet dataset is a free resource for object class recognition and 6DOF pose estimation from point cloud data. 3DNet provides a large-scale hierarchical CAD-model databases with increasing numbers of classes and difficulty with 10, 60 and 200 object classes together with evaluation datasets that contain thousands of scenes captured with an RGB-D sensor.\n\n\nWhen using this database, please cite: \\\n Walter Wohlkinger, Aitor Aldoma Buchaca, Radu Rusu, Markus Vincze. \n\"3DNet: Large-Scale Object Class Recognition from CAD Models\"\n. In IEEE International Conference on Robotics and Automation (ICRA), 2012. (\nPDF\n) (\nbibtex\n)\n\n\n\n\n\n\n\n\n\n\nCat10: Basic object classes\n\n\nThe basic dataset consists of common, simple, geometrically distinguishable but partially similar objects.\n\n\nCat10_ModelDatabase.zip (42 MB)\n\n\nCat10_TestDatabase.zip (3.6 GB)\n\n\n\n\n\n\nCat60: Subclasses\n\n\nThe Cat50 model database consists of the Cat10 database with 50 additional classes. The classes in this database are still distinguishable by shape only, but also include subcategories (chair, office-chair, armchair and car, convertible, pickup, formula-car).\n\n\nCat60_ModelDatabase.zip (222 MB)\n\n\nCat60_TestDatabase.zip (3.7 GB)\n\n\n\n\n\n\nCat200: Color and size\n\n\nThis database adds objects which are similar in shape but can be uniquely distinguished when using color as an additional cue. One more important aspect of objects and object classes was not used and not needed in the previous category databases: size. To successfully distinguish among our 200 categories database, the real world size of the objects becomes important.\n\n\nCat200_ModelDatabase.zip (569 MB)", 
            "title": "Three d net"
        }, 
        {
            "location": "/datasets/three_d_net/#3dnet-dataset", 
            "text": "The 3DNet dataset is a free resource for object class recognition and 6DOF pose estimation from point cloud data. 3DNet provides a large-scale hierarchical CAD-model databases with increasing numbers of classes and difficulty with 10, 60 and 200 object classes together with evaluation datasets that contain thousands of scenes captured with an RGB-D sensor.  When using this database, please cite: \\\n Walter Wohlkinger, Aitor Aldoma Buchaca, Radu Rusu, Markus Vincze.  \"3DNet: Large-Scale Object Class Recognition from CAD Models\" . In IEEE International Conference on Robotics and Automation (ICRA), 2012. ( PDF ) ( bibtex )", 
            "title": "3DNet Dataset"
        }, 
        {
            "location": "/datasets/three_d_net/#cat10-basic-object-classes", 
            "text": "The basic dataset consists of common, simple, geometrically distinguishable but partially similar objects.  Cat10_ModelDatabase.zip (42 MB)  Cat10_TestDatabase.zip (3.6 GB)", 
            "title": "Cat10: Basic object classes"
        }, 
        {
            "location": "/datasets/three_d_net/#cat60-subclasses", 
            "text": "The Cat50 model database consists of the Cat10 database with 50 additional classes. The classes in this database are still distinguishable by shape only, but also include subcategories (chair, office-chair, armchair and car, convertible, pickup, formula-car).  Cat60_ModelDatabase.zip (222 MB)  Cat60_TestDatabase.zip (3.7 GB)", 
            "title": "Cat60: Subclasses"
        }, 
        {
            "location": "/datasets/three_d_net/#cat200-color-and-size", 
            "text": "This database adds objects which are similar in shape but can be uniquely distinguished when using color as an additional cue. One more important aspect of objects and object classes was not used and not needed in the previous category databases: size. To successfully distinguish among our 200 categories database, the real world size of the objects becomes important.  Cat200_ModelDatabase.zip (569 MB)", 
            "title": "Cat200: Color and size"
        }, 
        {
            "location": "/datasets/tuw/", 
            "text": "HOME\n\n\n\n\nTUW Object Instance Recognition Dataset\n\n\nThis website provides annotated RGB-D point clouds of indoor environments. The TUW dataset contains sequences of point clouds in 15 static and 3 partly dynamic environments. Each view of a scene presents multiple objects; some object instances occur multiple times and are highly occluded in certain views. The model database consists of 17 models with a maximum extent of 30 cm, which are partly symmetric and/or lack distinctive surface texture. The dataset consists of the reconstructed 3D object models, the individual key frames of the models, test scenes and the 6DOF pose of each object present in the respective view. Each point cloud is represented by RGB color, depth and normal information.\n\n\nFurthermore, we provide annotation for the Willow and Challenge dataset .\n\n\nTUW\n\n\n\n\n\n\nThis dataset is composed of 15 multi-view sequences of static indoor scenes totalling 163 RGB-D frames ( and 3 dynamic scenes with 61 views in total). The number of objects in the different sequences amounts to 162, resulting in 1911 object instances (some of them totally occluded in some frames).\n\n\nDownload:\n\n\n\n\ntraining set (object models)\n (2.0GB)\n\n\n[test set (multi-view sequences of static [1] and dynamic [2] scenes)](https://repo.acin.tuwien.ac.at/tmp/permanent/data/TUW_test_set.tar.gz) (0.43GB)\n\n\nground-truth annotations\n\n\n\n\nGround-truth annotation (created by [1] with manual verification)\n\n\n\n\n[TUW static [1] + dynamic[2]](show_dataset.php?dir_gt=iros2014/annotated_images/gt\ndir_scenes=iros2014/annotated_images/scenes\nmodel_dir=iros2014/training_data/models\ndataset=0\nframe=0\nocclusion_dir=iros2014/semi_automatic_ground_truth\nnum_shown_dataset_letters=9)\n\n\n\n\nResults:\n\n\n\n\nObtained by [1]\n\n\nObtained by [2]\n\n\nObtained by [3]\n\n\n\n\nWillow and Challenge Dataset\n\n\n\n\n\n\n\n\n\n\nThe Willow dataset is composed of 24 multi-view sequences totalling 353 RGB-D frames. The number of objects in the different sequences amounts to 110, resulting in 1628 object instances (some of them totally occluded in some frames). The Challenge dataset is composed of 39 multi-view sequences totalling 176 RGB-D frames. The number of objects in the different sequences amounts to 97, resulting in 434 object instances. \\\n\n\nDownload\n\n\n\n\ntraining set (object models)\n (3.3GB)\n\n\ntest set \nWillow\n(0.8GB)\u00a0\u00a0\nChallenge\n(0.34GB)\n\n\nground-truth annotations\n\n\n\n\nGround-truth annotation (created by [1] with manual verification):\n\n\n\n\nWillow\n\n\nChallenge\n\n\n\n\nCode is available at \ngithub.com/strands-project/v4r\n (for ROS wrappers and tutorials see \ngithub.com/strands-project/v4r_ros_wrappers\n). \\\n For object learning data checkout \nstrands.pdc.kth.se/public/ICRA-2016-Data/\n.\\\n The research leading to these results has received funding from the European Community's Seventh Framework Programme FP7/2007-2013 under grant agreement No. 600623, STRANDS and No. 610532, SQUIRREL.\n\n\n[1] Aitor Aldoma, Thomas F\u00e4ulhammer, Markus Vincze, \n\"Automation of Ground-Truth Annotation for Multi-View RGB-D Object Instance Recognition Datasets\"\n, IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2014 (\nbibtex\n) \\\n [2] Thomas F\u00e4ulhammer, Aitor Aldoma, Michael Zillich, Markus Vincze, \n\"Temporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered And Dynamic Environments\"\n, IEEE Int. Conf. on Robotics and Automation (ICRA), 2015 (\nPDF\n) (\nbibtex\n) \\\n [3] Thomas F\u00e4ulhammer, Michael Zillich, Markus Vincze, \n\"Multi-View Hypotheses Transfer for Enhanced Object Recognition in Clutter\"\n, IAPR Conference on Machine Vision Applications (MVA), 2015 (\nPDF\n) (\nbibtex\n)", 
            "title": "Tuw"
        }, 
        {
            "location": "/datasets/tuw/#tuw-object-instance-recognition-dataset", 
            "text": "This website provides annotated RGB-D point clouds of indoor environments. The TUW dataset contains sequences of point clouds in 15 static and 3 partly dynamic environments. Each view of a scene presents multiple objects; some object instances occur multiple times and are highly occluded in certain views. The model database consists of 17 models with a maximum extent of 30 cm, which are partly symmetric and/or lack distinctive surface texture. The dataset consists of the reconstructed 3D object models, the individual key frames of the models, test scenes and the 6DOF pose of each object present in the respective view. Each point cloud is represented by RGB color, depth and normal information.  Furthermore, we provide annotation for the Willow and Challenge dataset .", 
            "title": "TUW Object Instance Recognition Dataset"
        }, 
        {
            "location": "/datasets/tuw/#tuw", 
            "text": "This dataset is composed of 15 multi-view sequences of static indoor scenes totalling 163 RGB-D frames ( and 3 dynamic scenes with 61 views in total). The number of objects in the different sequences amounts to 162, resulting in 1911 object instances (some of them totally occluded in some frames).  Download:   training set (object models)  (2.0GB)  [test set (multi-view sequences of static [1] and dynamic [2] scenes)](https://repo.acin.tuwien.ac.at/tmp/permanent/data/TUW_test_set.tar.gz) (0.43GB)  ground-truth annotations   Ground-truth annotation (created by [1] with manual verification)   [TUW static [1] + dynamic[2]](show_dataset.php?dir_gt=iros2014/annotated_images/gt dir_scenes=iros2014/annotated_images/scenes model_dir=iros2014/training_data/models dataset=0 frame=0 occlusion_dir=iros2014/semi_automatic_ground_truth num_shown_dataset_letters=9)   Results:   Obtained by [1]  Obtained by [2]  Obtained by [3]", 
            "title": "TUW"
        }, 
        {
            "location": "/datasets/tuw/#willow-and-challenge-dataset", 
            "text": "The Willow dataset is composed of 24 multi-view sequences totalling 353 RGB-D frames. The number of objects in the different sequences amounts to 110, resulting in 1628 object instances (some of them totally occluded in some frames). The Challenge dataset is composed of 39 multi-view sequences totalling 176 RGB-D frames. The number of objects in the different sequences amounts to 97, resulting in 434 object instances. \\  Download   training set (object models)  (3.3GB)  test set  Willow (0.8GB)\u00a0\u00a0 Challenge (0.34GB)  ground-truth annotations   Ground-truth annotation (created by [1] with manual verification):   Willow  Challenge   Code is available at  github.com/strands-project/v4r  (for ROS wrappers and tutorials see  github.com/strands-project/v4r_ros_wrappers ). \\\n For object learning data checkout  strands.pdc.kth.se/public/ICRA-2016-Data/ .\\\n The research leading to these results has received funding from the European Community's Seventh Framework Programme FP7/2007-2013 under grant agreement No. 600623, STRANDS and No. 610532, SQUIRREL.  [1] Aitor Aldoma, Thomas F\u00e4ulhammer, Markus Vincze,  \"Automation of Ground-Truth Annotation for Multi-View RGB-D Object Instance Recognition Datasets\" , IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2014 ( bibtex ) \\\n [2] Thomas F\u00e4ulhammer, Aitor Aldoma, Michael Zillich, Markus Vincze,  \"Temporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered And Dynamic Environments\" , IEEE Int. Conf. on Robotics and Automation (ICRA), 2015 ( PDF ) ( bibtex ) \\\n [3] Thomas F\u00e4ulhammer, Michael Zillich, Markus Vincze,  \"Multi-View Hypotheses Transfer for Enhanced Object Recognition in Clutter\" , IAPR Conference on Machine Vision Applications (MVA), 2015 ( PDF ) ( bibtex )", 
            "title": "Willow and Challenge Dataset"
        }, 
        {
            "location": "/datasets/witham_wharf/", 
            "text": "Witham Wharf RGB-D dataset\n\n\nThe Witham Wharf was collected for purposes of testing RGB-D localization in changing environments. The dataset consists of one training set and three testing sets that cover changes of eight locations in an open-plan office for one year. We provide the gathered data in form of rosbags and color images.\n\n\n\n\nDataset purpose\n\n\nThe intended use was to create a dataset to benchmark visual and RGB-D localization in changing environments. See and hear the 2 minute video below. If you are interrested only in how the dataset was collected, simply skip the first minute of the video or click \nhere\n.\n\n\n\\\n\n\n\n\n\n  LCAS Witham Wharf dataset collection process (time 1:13-1:32)\n\n\n\n\n\n\nDataset structure\n\n\nRosbags\n\n\nThe provided archives contain rosbags, which are zipped into separate files according to the day of the data collection. Each rosbag contains a depth/color image, camera information, robot position, tf data and laser scan captured by the robot at a location and time that is encoded in the rosbag name, which contains day, month, year, hour, minute and location id. For example, \n3D_16-11-13-15-20_place_3.bag\n contains data gathered at location 3 on Nov 16 2013 at 15:20 o'clock.\n\n\nColor images\n\n\nColor images of the dataset are arranged into eight folders according to the location they were captured at. Images were captured on a regular basis every 10 minutes and their filename corresponds to the time they were captured at. For example \n00000.bmp\n was captured at midnight the first day, \n00012.bmp\n at 2:10 am on the first day and \n00145.bmp\n at 0:10am of the second day of the data collection. Please note that sometimes the robot got stuck and failed to capture an image at a given location. For our method, we needed to substitute these missing images by taking the last previously captured image.\n\n\n\n\nDownload\n\n\n\n\nName                     Length    Dates              Images                                         Rosbags\n  November 2013 training   7 days    10-16/Nov/2013     \nNov 2013 training\n   \nNov 2013 training\n\n  November 2013 testing    1 day     17/Nov/2013        \nNov 2013 testing\n     \nNov 2013 testing\n\n  February 2014 testing    1 day     02/Feb/2014        \nFeb 2014 testing\n     \nFeb 2014 testing\n\n  December 2014 testing    1 day     14/Dec/2014        \nDec 2014 testing\n     \nDec 2014 testing\n\n  Complete dataset         10 days   all of the above   \nComplete dataset\n       \nComplete dataset\n\n\n\n\n\n\nAdditional information\n\n\nIf you need to transform the point clouds to a global coordinate frame, you can use the \nww_transforms\n package, which provides a map of the environment and additional transforms that complement the \ntf\n information that is contained in the rosbags. The robot also performed 360\u00b0\ntimes90\u00b0 3D \nsweeps\n on an hourly basis at three different locations of the Witham Wharf.\n\n\n\n\nConditions of use\n\n\nIf you use the dataset for your research, please cite our \npaper\n that describes the data collection in detail. We attached a \nbibtex\n record for your convenience.\n\n\n\n\nThis dataset is part of the larger \nLCAS-STRANDS long-term dataset collection\n.", 
            "title": "Witham wharf"
        }, 
        {
            "location": "/datasets/witham_wharf/#witham-wharf-rgb-d-dataset", 
            "text": "The Witham Wharf was collected for purposes of testing RGB-D localization in changing environments. The dataset consists of one training set and three testing sets that cover changes of eight locations in an open-plan office for one year. We provide the gathered data in form of rosbags and color images.", 
            "title": "Witham Wharf RGB-D dataset"
        }, 
        {
            "location": "/datasets/witham_wharf/#dataset-purpose", 
            "text": "The intended use was to create a dataset to benchmark visual and RGB-D localization in changing environments. See and hear the 2 minute video below. If you are interrested only in how the dataset was collected, simply skip the first minute of the video or click  here .  \\   \n  LCAS Witham Wharf dataset collection process (time 1:13-1:32)", 
            "title": "Dataset purpose"
        }, 
        {
            "location": "/datasets/witham_wharf/#dataset-structure", 
            "text": "", 
            "title": "Dataset structure"
        }, 
        {
            "location": "/datasets/witham_wharf/#rosbags", 
            "text": "The provided archives contain rosbags, which are zipped into separate files according to the day of the data collection. Each rosbag contains a depth/color image, camera information, robot position, tf data and laser scan captured by the robot at a location and time that is encoded in the rosbag name, which contains day, month, year, hour, minute and location id. For example,  3D_16-11-13-15-20_place_3.bag  contains data gathered at location 3 on Nov 16 2013 at 15:20 o'clock.", 
            "title": "Rosbags"
        }, 
        {
            "location": "/datasets/witham_wharf/#color-images", 
            "text": "Color images of the dataset are arranged into eight folders according to the location they were captured at. Images were captured on a regular basis every 10 minutes and their filename corresponds to the time they were captured at. For example  00000.bmp  was captured at midnight the first day,  00012.bmp  at 2:10 am on the first day and  00145.bmp  at 0:10am of the second day of the data collection. Please note that sometimes the robot got stuck and failed to capture an image at a given location. For our method, we needed to substitute these missing images by taking the last previously captured image.", 
            "title": "Color images"
        }, 
        {
            "location": "/datasets/witham_wharf/#download", 
            "text": "Name                     Length    Dates              Images                                         Rosbags\n  November 2013 training   7 days    10-16/Nov/2013      Nov 2013 training     Nov 2013 training \n  November 2013 testing    1 day     17/Nov/2013         Nov 2013 testing       Nov 2013 testing \n  February 2014 testing    1 day     02/Feb/2014         Feb 2014 testing       Feb 2014 testing \n  December 2014 testing    1 day     14/Dec/2014         Dec 2014 testing       Dec 2014 testing \n  Complete dataset         10 days   all of the above    Complete dataset         Complete dataset", 
            "title": "Download"
        }, 
        {
            "location": "/datasets/witham_wharf/#additional-information", 
            "text": "If you need to transform the point clouds to a global coordinate frame, you can use the  ww_transforms  package, which provides a map of the environment and additional transforms that complement the  tf  information that is contained in the rosbags. The robot also performed 360\u00b0 times90\u00b0 3D  sweeps  on an hourly basis at three different locations of the Witham Wharf.", 
            "title": "Additional information"
        }, 
        {
            "location": "/datasets/witham_wharf/#conditions-of-use", 
            "text": "If you use the dataset for your research, please cite our  paper  that describes the data collection in detail. We attached a  bibtex  record for your convenience.   This dataset is part of the larger  LCAS-STRANDS long-term dataset collection .", 
            "title": "Conditions of use"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/", 
            "text": "Overview\n\n\nThe FremenArray is a tool for prediction of binary states based on non-uniform, incremental Fourier Transform.\nIn contrast to the FremenServer, the FremenArray is meant to interface occupancy grids used in the ROS navigation stack.\nIt uses frequency spectrum analysis to identify reoccuring patterns in temporal sequences of binary states and represents the probability of these states in an analythical form as a combination of harmonic functions. \nThis allows to learn long-term dynamics of the environment and use the learned experience to predict states of the world models used in mobile robotics.\nThis ability is beneficial for tasks like self-localization, object search, path planning, anomaly detection and exploration.\n\n\nPractical\n\n\nThe FremenArray is an action server that maintains a set of state models held in an single-dimensional array which conforms to the ROS occupancy grid format, i.e. -1 means unknown cell and occupancy probability is scaled from 0 to 100. \nThree different operations can be performed with the array of states.\nThese correspond to three types of goals specified in the \noperation\n string.\n\n\nThe 'add' operation\n\n\nThis allows to add a the measured map \nstates\n along with the \ntime\n of its observations.\nOn the first call of the \nadd\n operation, the size of the \nstates\n array is remembered and used as a number of cells of the map. \nThis number cannot be changed later on. \nThe \nadd\n operation remembers the last timestamp that was provided and adds only newer observation to the model.\n\n\nInputs\n\n\n\n\nstates\n a sequence of numbers from -1 to 100, where -1 is an unknown cell observed at\n\n\ntime\n which is in seconds. The length of the fields \nstates\n in all calls has to be the same.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains a number of cells updated in the model. Note that this might be lower than the length of the \nstates\n in case that some were unobserved (valued -1). \nsuccess\n equals to -2 if \nstates\n has a different length from before. \n\n\nmessage\n contains more detailed report or error message.\n\n\n\n\nThe 'predict' operation\n\n\nThis operation calculates the (scaled by 100, so it conforms to the ROS map format) \nprobabilities\n of the states for the given \ntime\n.\n\n\nInputs\n\n\n\n\ntime\n at which the \nprobabilities\n of the state should be estimated (in seconds).\n\n\norder\n of the model used for the estimation. The \norder\n equals to the number of periodic processes to be modeled. Setting the order to 0 results in a static model with \nprobabilities\n constant in time.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the number of predicted states or -1 if the \norder\n is out of bounds.\n\n\nmessage\n contains a detailed report or error message.\n\n\nprobabilities\n is an array of predicted probabilities (scaled by 100) of the states t given \ntime\n.\n\n\n\n\nThe 'entropy' operation\n\n\nThis operation calculates the \nentropies\n of the states for the given \ntime\n.\n\n\nInputs\n\n\n\n\ntime\n is the time instant at which the \nentropies\n of the states should be estimated.\n\n\norder\n equals to the number of periodic processes to be modeled, i.e. model order. Setting \norder\n to 0 results in a static model, so that the \nentropies\n will be constant in time.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the number of predicted states or -1 if the \norder\n is out of bounds.\n\n\nmessage\n contains a detailed report or an error message.\n\n\nentropies\n is an array of state's predicted entropies at given \ntime\n.\n\n\n\n\nThe 'evaluate' operation\n\n\nThe \nevaluate\n operation is meant to support decisions what model order to use for probability and entropy predictions.\nIt allows to calculate the prediction error of the various model orders of a given (by \nid\n) state.\nThe user provides a sequence of observed \nstates\n and \ntimes\n of observations and a maximal \norder\n to be evaluated.\nThe server performs predictions for the given \ntimes\n and \norders\n, compares those with the provided \nstates\n and calculates the percentage of unsuccessful predictions for model orders from 0 to \norder\n.\n\n\nInputs\n\n\n\n\nid\n identification of the state that is concerned. If the \nid\n did not exist, an error is reported.\n\n\nstates\n is a sequence of zeros and ones indicating the observed states at  \n\n\ntimes\n which are in seconds. The length of the fields \ntimes\n and \nstates\n has to be the same.\n\n\norder\n is the maximal model order to be evaluated. \n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the best performing model order, -1 if the state \nid\n does not exist and -2 if \ntimes\n and \nstates\n have different lengths.\n\n\nmessage\n contains a detailed report or an error message.\n\n\nerrors\n is an array of prediction errors for model orders from 0 to \norder\n.\n\n\n\n\nThe 'print' operation\n\n\nMakes the fremenarray server to print data about the states on the screen.\n\n\nInputs\n\n\n\n\norder\n number of periodic components to print for each state.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the number of states in the collection before the operation was performed. \n\n\nmessage\n contains a detailed report or an error message.", 
            "title": "Fremen2DGrid"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#overview", 
            "text": "The FremenArray is a tool for prediction of binary states based on non-uniform, incremental Fourier Transform.\nIn contrast to the FremenServer, the FremenArray is meant to interface occupancy grids used in the ROS navigation stack.\nIt uses frequency spectrum analysis to identify reoccuring patterns in temporal sequences of binary states and represents the probability of these states in an analythical form as a combination of harmonic functions. \nThis allows to learn long-term dynamics of the environment and use the learned experience to predict states of the world models used in mobile robotics.\nThis ability is beneficial for tasks like self-localization, object search, path planning, anomaly detection and exploration.", 
            "title": "Overview"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#practical", 
            "text": "The FremenArray is an action server that maintains a set of state models held in an single-dimensional array which conforms to the ROS occupancy grid format, i.e. -1 means unknown cell and occupancy probability is scaled from 0 to 100. \nThree different operations can be performed with the array of states.\nThese correspond to three types of goals specified in the  operation  string.", 
            "title": "Practical"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#the-add-operation", 
            "text": "This allows to add a the measured map  states  along with the  time  of its observations.\nOn the first call of the  add  operation, the size of the  states  array is remembered and used as a number of cells of the map. \nThis number cannot be changed later on. \nThe  add  operation remembers the last timestamp that was provided and adds only newer observation to the model.", 
            "title": "The 'add' operation"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#inputs", 
            "text": "states  a sequence of numbers from -1 to 100, where -1 is an unknown cell observed at  time  which is in seconds. The length of the fields  states  in all calls has to be the same.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#outputs", 
            "text": "success  contains a number of cells updated in the model. Note that this might be lower than the length of the  states  in case that some were unobserved (valued -1).  success  equals to -2 if  states  has a different length from before.   message  contains more detailed report or error message.", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#the-predict-operation", 
            "text": "This operation calculates the (scaled by 100, so it conforms to the ROS map format)  probabilities  of the states for the given  time .", 
            "title": "The 'predict' operation"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#inputs_1", 
            "text": "time  at which the  probabilities  of the state should be estimated (in seconds).  order  of the model used for the estimation. The  order  equals to the number of periodic processes to be modeled. Setting the order to 0 results in a static model with  probabilities  constant in time.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#outputs_1", 
            "text": "success  contains the number of predicted states or -1 if the  order  is out of bounds.  message  contains a detailed report or error message.  probabilities  is an array of predicted probabilities (scaled by 100) of the states t given  time .", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#the-entropy-operation", 
            "text": "This operation calculates the  entropies  of the states for the given  time .", 
            "title": "The 'entropy' operation"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#inputs_2", 
            "text": "time  is the time instant at which the  entropies  of the states should be estimated.  order  equals to the number of periodic processes to be modeled, i.e. model order. Setting  order  to 0 results in a static model, so that the  entropies  will be constant in time.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#outputs_2", 
            "text": "success  contains the number of predicted states or -1 if the  order  is out of bounds.  message  contains a detailed report or an error message.  entropies  is an array of state's predicted entropies at given  time .", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#the-evaluate-operation", 
            "text": "The  evaluate  operation is meant to support decisions what model order to use for probability and entropy predictions.\nIt allows to calculate the prediction error of the various model orders of a given (by  id ) state.\nThe user provides a sequence of observed  states  and  times  of observations and a maximal  order  to be evaluated.\nThe server performs predictions for the given  times  and  orders , compares those with the provided  states  and calculates the percentage of unsuccessful predictions for model orders from 0 to  order .", 
            "title": "The 'evaluate' operation"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#inputs_3", 
            "text": "id  identification of the state that is concerned. If the  id  did not exist, an error is reported.  states  is a sequence of zeros and ones indicating the observed states at    times  which are in seconds. The length of the fields  times  and  states  has to be the same.  order  is the maximal model order to be evaluated.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#outputs_3", 
            "text": "success  contains the best performing model order, -1 if the state  id  does not exist and -2 if  times  and  states  have different lengths.  message  contains a detailed report or an error message.  errors  is an array of prediction errors for model orders from 0 to  order .", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#the-print-operation", 
            "text": "Makes the fremenarray server to print data about the states on the screen.", 
            "title": "The 'print' operation"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#inputs_4", 
            "text": "order  number of periodic components to print for each state.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/Fremen2DGrid/#outputs_4", 
            "text": "success  contains the number of states in the collection before the operation was performed.   message  contains a detailed report or an error message.", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenArray/", 
            "text": "Overview\n\n\nThe FremenArray is a tool for prediction of binary states based on non-uniform, incremental Fourier Transform.\nIn contrast to the FremenServer, the FremenArray is meant to interface occupancy grids used in the ROS navigation stack.\nIt uses frequency spectrum analysis to identify reoccuring patterns in temporal sequences of binary states and represents the probability of these states in an analythical form as a combination of harmonic functions. \nThis allows to learn long-term dynamics of the environment and use the learned experience to predict states of the world models used in mobile robotics.\nThis ability is beneficial for tasks like self-localization, object search, path planning, anomaly detection and exploration.\n\n\nPractical\n\n\nThe FremenArray is an action server that maintains a set of state models held in an single-dimensional array which conforms to the ROS occupancy grid format, i.e. -1 means unknown cell and occupancy probability is scaled from 0 to 100. \nThree different operations can be performed with the array of states.\nThese correspond to three types of goals specified in the \noperation\n string.\n\n\nThe 'add' operation\n\n\nThis allows to add a the measured map \nstates\n along with the \ntime\n of its observations.\nOn the first call of the \nadd\n operation, the size of the \nstates\n array is remembered and used as a number of cells of the map. \nThis number cannot be changed later on. \nThe \nadd\n operation remembers the last timestamp that was provided and adds only newer observation to the model.\n\n\nInputs\n\n\n\n\nstates\n a sequence of numbers from -1 to 100, where -1 is an unknown cell observed at\n\n\ntime\n which is in seconds. The length of the fields \nstates\n in all calls has to be the same.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains a number of cells updated in the model. Note that this might be lower than the length of the \nstates\n in case that some were unobserved (valued -1). \nsuccess\n equals to -2 if \nstates\n has a different length from before. \n\n\nmessage\n contains more detailed report or error message.\n\n\n\n\nThe 'predict' operation\n\n\nThis operation calculates the (scaled by 100, so it conforms to the ROS map format) \nprobabilities\n of the states for the given \ntime\n.\n\n\nInputs\n\n\n\n\ntime\n at which the \nprobabilities\n of the state should be estimated (in seconds).\n\n\norder\n of the model used for the estimation. The \norder\n equals to the number of periodic processes to be modeled. Setting the order to 0 results in a static model with \nprobabilities\n constant in time.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the number of predicted states or -1 if the \norder\n is out of bounds.\n\n\nmessage\n contains a detailed report or error message.\n\n\nprobabilities\n is an array of predicted probabilities (scaled by 100) of the states t given \ntime\n.\n\n\n\n\nThe 'entropy' operation\n\n\nThis operation calculates the \nentropies\n of the states for the given \ntime\n.\n\n\nInputs\n\n\n\n\ntime\n is the time instant at which the \nentropies\n of the states should be estimated.\n\n\norder\n equals to the number of periodic processes to be modeled, i.e. model order. Setting \norder\n to 0 results in a static model, so that the \nentropies\n will be constant in time.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the number of predicted states or -1 if the \norder\n is out of bounds.\n\n\nmessage\n contains a detailed report or an error message.\n\n\nentropies\n is an array of state's predicted entropies at given \ntime\n.\n\n\n\n\nThe 'evaluate' operation\n\n\nThe \nevaluate\n operation is meant to support decisions what model order to use for probability and entropy predictions.\nIt allows to calculate the prediction error of the various model orders of a given (by \nid\n) state.\nThe user provides a sequence of observed \nstates\n and \ntimes\n of observations and a maximal \norder\n to be evaluated.\nThe server performs predictions for the given \ntimes\n and \norders\n, compares those with the provided \nstates\n and calculates the percentage of unsuccessful predictions for model orders from 0 to \norder\n.\n\n\nInputs\n\n\n\n\nid\n identification of the state that is concerned. If the \nid\n did not exist, an error is reported.\n\n\nstates\n is a sequence of zeros and ones indicating the observed states at  \n\n\ntimes\n which are in seconds. The length of the fields \ntimes\n and \nstates\n has to be the same.\n\n\norder\n is the maximal model order to be evaluated. \n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the best performing model order, -1 if the state \nid\n does not exist and -2 if \ntimes\n and \nstates\n have different lengths.\n\n\nmessage\n contains a detailed report or an error message.\n\n\nerrors\n is an array of prediction errors for model orders from 0 to \norder\n.\n\n\n\n\nThe 'print' operation\n\n\nMakes the fremenarray server to print data about the states on the screen.\n\n\nInputs\n\n\n\n\norder\n number of periodic components to print for each state.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the number of states in the collection before the operation was performed. \n\n\nmessage\n contains a detailed report or an error message.", 
            "title": "FremenArray"
        }, 
        {
            "location": "/fremen/FremenArray/#overview", 
            "text": "The FremenArray is a tool for prediction of binary states based on non-uniform, incremental Fourier Transform.\nIn contrast to the FremenServer, the FremenArray is meant to interface occupancy grids used in the ROS navigation stack.\nIt uses frequency spectrum analysis to identify reoccuring patterns in temporal sequences of binary states and represents the probability of these states in an analythical form as a combination of harmonic functions. \nThis allows to learn long-term dynamics of the environment and use the learned experience to predict states of the world models used in mobile robotics.\nThis ability is beneficial for tasks like self-localization, object search, path planning, anomaly detection and exploration.", 
            "title": "Overview"
        }, 
        {
            "location": "/fremen/FremenArray/#practical", 
            "text": "The FremenArray is an action server that maintains a set of state models held in an single-dimensional array which conforms to the ROS occupancy grid format, i.e. -1 means unknown cell and occupancy probability is scaled from 0 to 100. \nThree different operations can be performed with the array of states.\nThese correspond to three types of goals specified in the  operation  string.", 
            "title": "Practical"
        }, 
        {
            "location": "/fremen/FremenArray/#the-add-operation", 
            "text": "This allows to add a the measured map  states  along with the  time  of its observations.\nOn the first call of the  add  operation, the size of the  states  array is remembered and used as a number of cells of the map. \nThis number cannot be changed later on. \nThe  add  operation remembers the last timestamp that was provided and adds only newer observation to the model.", 
            "title": "The 'add' operation"
        }, 
        {
            "location": "/fremen/FremenArray/#inputs", 
            "text": "states  a sequence of numbers from -1 to 100, where -1 is an unknown cell observed at  time  which is in seconds. The length of the fields  states  in all calls has to be the same.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenArray/#outputs", 
            "text": "success  contains a number of cells updated in the model. Note that this might be lower than the length of the  states  in case that some were unobserved (valued -1).  success  equals to -2 if  states  has a different length from before.   message  contains more detailed report or error message.", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenArray/#the-predict-operation", 
            "text": "This operation calculates the (scaled by 100, so it conforms to the ROS map format)  probabilities  of the states for the given  time .", 
            "title": "The 'predict' operation"
        }, 
        {
            "location": "/fremen/FremenArray/#inputs_1", 
            "text": "time  at which the  probabilities  of the state should be estimated (in seconds).  order  of the model used for the estimation. The  order  equals to the number of periodic processes to be modeled. Setting the order to 0 results in a static model with  probabilities  constant in time.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenArray/#outputs_1", 
            "text": "success  contains the number of predicted states or -1 if the  order  is out of bounds.  message  contains a detailed report or error message.  probabilities  is an array of predicted probabilities (scaled by 100) of the states t given  time .", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenArray/#the-entropy-operation", 
            "text": "This operation calculates the  entropies  of the states for the given  time .", 
            "title": "The 'entropy' operation"
        }, 
        {
            "location": "/fremen/FremenArray/#inputs_2", 
            "text": "time  is the time instant at which the  entropies  of the states should be estimated.  order  equals to the number of periodic processes to be modeled, i.e. model order. Setting  order  to 0 results in a static model, so that the  entropies  will be constant in time.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenArray/#outputs_2", 
            "text": "success  contains the number of predicted states or -1 if the  order  is out of bounds.  message  contains a detailed report or an error message.  entropies  is an array of state's predicted entropies at given  time .", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenArray/#the-evaluate-operation", 
            "text": "The  evaluate  operation is meant to support decisions what model order to use for probability and entropy predictions.\nIt allows to calculate the prediction error of the various model orders of a given (by  id ) state.\nThe user provides a sequence of observed  states  and  times  of observations and a maximal  order  to be evaluated.\nThe server performs predictions for the given  times  and  orders , compares those with the provided  states  and calculates the percentage of unsuccessful predictions for model orders from 0 to  order .", 
            "title": "The 'evaluate' operation"
        }, 
        {
            "location": "/fremen/FremenArray/#inputs_3", 
            "text": "id  identification of the state that is concerned. If the  id  did not exist, an error is reported.  states  is a sequence of zeros and ones indicating the observed states at    times  which are in seconds. The length of the fields  times  and  states  has to be the same.  order  is the maximal model order to be evaluated.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenArray/#outputs_3", 
            "text": "success  contains the best performing model order, -1 if the state  id  does not exist and -2 if  times  and  states  have different lengths.  message  contains a detailed report or an error message.  errors  is an array of prediction errors for model orders from 0 to  order .", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenArray/#the-print-operation", 
            "text": "Makes the fremenarray server to print data about the states on the screen.", 
            "title": "The 'print' operation"
        }, 
        {
            "location": "/fremen/FremenArray/#inputs_4", 
            "text": "order  number of periodic components to print for each state.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenArray/#outputs_4", 
            "text": "success  contains the number of states in the collection before the operation was performed.   message  contains a detailed report or an error message.", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenServer/", 
            "text": "Overview\n\n\nThe FremenServer is a tool for prediction of binary states based on non-uniform, incremental Fourier Transform.\nIt uses frequency spectrum analysis to identify reoccuring patterns in temporal sequences of binary states and represents the probability of these states in an analythical form as a combination of harmonic functions. \nThis allows to learn long-term dynamics of the environment and use the learned experience to predict states of the world models used in mobile robotics.\nThis ability is beneficial for tasks like self-localization, object search, path planning, anomaly detection and exploration.\n\n\nPractical\n\n\nThe FremenServer is an action server that maintains a collection of state models, each with a different \nid\n.\nSix different operations can be performed with each state held in the collection.\nThese correspond to six types of goals specified in the \noperation\n string that are related to a state given by its \nid\n.\n\n\nThe 'add' operation\n\n\nThis allows to add a sequence of observed \nstates\n along with the \ntimes\n of observations to the model of the state \nid\n.\nIf the \nid\n is given for the first time, a new state is created and filled with the values.\nThe \nadd\n operation remembers the last timestamp that was provided and adds only newer observation to the model.\n\n\nInputs\n\n\n\n\nid\n identification of the state that is concerned. If the \nid\n did not exist, a new state will be created. \n\n\nstates\n a sequence of zeroes and ones observed at\n\n\ntimes\n which are in seconds. The length of the fields \ntimes\n and \nstates\n has to be the same.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains a number of observation added to the model. Note that this might be lower than the length of the \nstates\n in case that some of the \ntimes\n are older that the \ntimes\n of the previous step. Equals to -2 if \ntimes\n and \nstates\n have different lengths. \n\n\nmessage\n contains more detailed report or error message.\n\n\n\n\nThe 'predict' operation\n\n\nThis operation calculates the \nprobabilities\n of the state \nid\n  for the given \ntimes\n.\nCan predict the probability of one state for several time instants.\n\n\nInputs\n\n\n\n\nid\n is identification of the state that is concerned. If the \nid\n does not exist, an error is reported.\n\n\ntimes\n at which the \nprobabilities\n of the state should be estimated (in seconds).\n\n\norder\n of the model used for the estimation. The \norder\n equals to the number of periodic processes to be modeled. Setting the order to 0 results in a static model with \nprobabilities\n constant in time.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the number of predicted states or -1 if the state \nid\n does not exist.\n\n\nmessage\n contains a detailed report or error message.\n\n\nprobabilities\n is an array of predicted probabilities of the state \nid\n at given \ntimes\n.\n\n\n\n\nThe 'forecast' operation\n\n\nThis operation calculates the \nprobabilities\n of the states in the \nids\n array for the  \ntimes\n.\nCan predict the probability of several states for one time instant.\n\n\nInputs\n\n\n\n\nids\n is identification of the states concerned. If the \nids\n is not filled, an error is reported.\n\n\ntimes\n at which the \nprobabilities\n of the states should be estimated (in seconds). Must have length 1, otherwise an error is reported. \n\n\norders\n of the model used for the estimation. Each \norders\n element equals to the number of periodic processes to be modeled for a given state. Setting the order to 0 results in a static model with \nprobabilities\n constant in time.\n\n\norder\n alternatively, the same \norder\n can be used for all states. The \norder\n is used instead of \norders\n if the number of elements in the \nids\n and \norders\n differ.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the number of predicted states, or -1 then error.\n\n\nmessage\n contains a detailed report or error message.\n\n\nprobabilities\n is an array of predicted probabilities of the states \nids\n at given \ntimes\n.\n\n\n\n\nThe 'entropy' operation\n\n\nThis operation calculates the \nentropies\n of the state \nid\n  for the given \ntimes\n.\n\n\nInputs\n\n\n\n\nid\n is an identification of the state that is concerned. If the \nid\n did not exist, an error is reported.\n\n\ntimes\n are time instants at which the \nentropies\n of the state should be estimated.\n\n\norder\n equals to the number of periodic processes to be modeled, i.e. model order. Setting \norder\n to 0 results in a static model, so that the \nentropies\n will be constant in time.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n is the number of predicted states, -1 if \nid\n does not exist.\n\n\nmessage\n contains a detailed report or an error message.\n\n\nentropies\n is an array of state's \nid\n predicted entropies at given \ntimes\n.\n\n\n\n\nThe 'evaluate' operation\n\n\nThe \nevaluate\n operation is meant to support decisions what model order to use for probability and entropy predictions.\nIt allows to calculate the prediction error of the various model orders of a given (by \nid\n) state.\nThe user provides a sequence of observed \nstates\n and \ntimes\n of observations and a maximal \norder\n to be evaluated.\nThe server performs predictions for the given \ntimes\n and \norders\n, compares those with the provided \nstates\n and calculates the percentage of unsuccessful predictions for model orders from 0 to \norder\n.\n\n\nInputs\n\n\n\n\nid\n identification of the state that is concerned. If the \nid\n did not exist, an error is reported.\n\n\nstates\n is a sequence of zeros and ones indicating the observed states at  \n\n\ntimes\n which are in seconds. The length of the fields \ntimes\n and \nstates\n has to be the same.\n\n\norder\n is the maximal model order to be evaluated. \n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the best performing model order, -1 if the state \nid\n does not exist and -2 if \ntimes\n and \nstates\n have different lengths.\n\n\nmessage\n contains a detailed report or an error message.\n\n\nerrors\n is an array of prediction errors for model orders from 0 to \norder\n.\n\n\n\n\nThe 'delete' operation\n\n\nDeletes a state with a given \nid\n from the state collection held in the server.\n\n\nInputs\n\n\n\n\nid\n identification of the state that is concerned. If the \nid\n did not exist, an error is reported.\n\n\n\n\nOutputs\n\n\n\n\nsuccess\n contains the number of states in the collection before the operation was performed. \n\n\nmessage\n contains a detailed report or an error message.\n\n\n\n\nThe 'update' operation\n\n\nReserved for future use when FreMEn is fused with Gaussian Mixture Models.", 
            "title": "FremenServer"
        }, 
        {
            "location": "/fremen/FremenServer/#overview", 
            "text": "The FremenServer is a tool for prediction of binary states based on non-uniform, incremental Fourier Transform.\nIt uses frequency spectrum analysis to identify reoccuring patterns in temporal sequences of binary states and represents the probability of these states in an analythical form as a combination of harmonic functions. \nThis allows to learn long-term dynamics of the environment and use the learned experience to predict states of the world models used in mobile robotics.\nThis ability is beneficial for tasks like self-localization, object search, path planning, anomaly detection and exploration.", 
            "title": "Overview"
        }, 
        {
            "location": "/fremen/FremenServer/#practical", 
            "text": "The FremenServer is an action server that maintains a collection of state models, each with a different  id .\nSix different operations can be performed with each state held in the collection.\nThese correspond to six types of goals specified in the  operation  string that are related to a state given by its  id .", 
            "title": "Practical"
        }, 
        {
            "location": "/fremen/FremenServer/#the-add-operation", 
            "text": "This allows to add a sequence of observed  states  along with the  times  of observations to the model of the state  id .\nIf the  id  is given for the first time, a new state is created and filled with the values.\nThe  add  operation remembers the last timestamp that was provided and adds only newer observation to the model.", 
            "title": "The 'add' operation"
        }, 
        {
            "location": "/fremen/FremenServer/#inputs", 
            "text": "id  identification of the state that is concerned. If the  id  did not exist, a new state will be created.   states  a sequence of zeroes and ones observed at  times  which are in seconds. The length of the fields  times  and  states  has to be the same.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenServer/#outputs", 
            "text": "success  contains a number of observation added to the model. Note that this might be lower than the length of the  states  in case that some of the  times  are older that the  times  of the previous step. Equals to -2 if  times  and  states  have different lengths.   message  contains more detailed report or error message.", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenServer/#the-predict-operation", 
            "text": "This operation calculates the  probabilities  of the state  id   for the given  times .\nCan predict the probability of one state for several time instants.", 
            "title": "The 'predict' operation"
        }, 
        {
            "location": "/fremen/FremenServer/#inputs_1", 
            "text": "id  is identification of the state that is concerned. If the  id  does not exist, an error is reported.  times  at which the  probabilities  of the state should be estimated (in seconds).  order  of the model used for the estimation. The  order  equals to the number of periodic processes to be modeled. Setting the order to 0 results in a static model with  probabilities  constant in time.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenServer/#outputs_1", 
            "text": "success  contains the number of predicted states or -1 if the state  id  does not exist.  message  contains a detailed report or error message.  probabilities  is an array of predicted probabilities of the state  id  at given  times .", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenServer/#the-forecast-operation", 
            "text": "This operation calculates the  probabilities  of the states in the  ids  array for the   times .\nCan predict the probability of several states for one time instant.", 
            "title": "The 'forecast' operation"
        }, 
        {
            "location": "/fremen/FremenServer/#inputs_2", 
            "text": "ids  is identification of the states concerned. If the  ids  is not filled, an error is reported.  times  at which the  probabilities  of the states should be estimated (in seconds). Must have length 1, otherwise an error is reported.   orders  of the model used for the estimation. Each  orders  element equals to the number of periodic processes to be modeled for a given state. Setting the order to 0 results in a static model with  probabilities  constant in time.  order  alternatively, the same  order  can be used for all states. The  order  is used instead of  orders  if the number of elements in the  ids  and  orders  differ.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenServer/#outputs_2", 
            "text": "success  contains the number of predicted states, or -1 then error.  message  contains a detailed report or error message.  probabilities  is an array of predicted probabilities of the states  ids  at given  times .", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenServer/#the-entropy-operation", 
            "text": "This operation calculates the  entropies  of the state  id   for the given  times .", 
            "title": "The 'entropy' operation"
        }, 
        {
            "location": "/fremen/FremenServer/#inputs_3", 
            "text": "id  is an identification of the state that is concerned. If the  id  did not exist, an error is reported.  times  are time instants at which the  entropies  of the state should be estimated.  order  equals to the number of periodic processes to be modeled, i.e. model order. Setting  order  to 0 results in a static model, so that the  entropies  will be constant in time.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenServer/#outputs_3", 
            "text": "success  is the number of predicted states, -1 if  id  does not exist.  message  contains a detailed report or an error message.  entropies  is an array of state's  id  predicted entropies at given  times .", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenServer/#the-evaluate-operation", 
            "text": "The  evaluate  operation is meant to support decisions what model order to use for probability and entropy predictions.\nIt allows to calculate the prediction error of the various model orders of a given (by  id ) state.\nThe user provides a sequence of observed  states  and  times  of observations and a maximal  order  to be evaluated.\nThe server performs predictions for the given  times  and  orders , compares those with the provided  states  and calculates the percentage of unsuccessful predictions for model orders from 0 to  order .", 
            "title": "The 'evaluate' operation"
        }, 
        {
            "location": "/fremen/FremenServer/#inputs_4", 
            "text": "id  identification of the state that is concerned. If the  id  did not exist, an error is reported.  states  is a sequence of zeros and ones indicating the observed states at    times  which are in seconds. The length of the fields  times  and  states  has to be the same.  order  is the maximal model order to be evaluated.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenServer/#outputs_4", 
            "text": "success  contains the best performing model order, -1 if the state  id  does not exist and -2 if  times  and  states  have different lengths.  message  contains a detailed report or an error message.  errors  is an array of prediction errors for model orders from 0 to  order .", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenServer/#the-delete-operation", 
            "text": "Deletes a state with a given  id  from the state collection held in the server.", 
            "title": "The 'delete' operation"
        }, 
        {
            "location": "/fremen/FremenServer/#inputs_5", 
            "text": "id  identification of the state that is concerned. If the  id  did not exist, an error is reported.", 
            "title": "Inputs"
        }, 
        {
            "location": "/fremen/FremenServer/#outputs_5", 
            "text": "success  contains the number of states in the collection before the operation was performed.   message  contains a detailed report or an error message.", 
            "title": "Outputs"
        }, 
        {
            "location": "/fremen/FremenServer/#the-update-operation", 
            "text": "Reserved for future use when FreMEn is fused with Gaussian Mixture Models.", 
            "title": "The 'update' operation"
        }, 
        {
            "location": "/fremen/fremengrid/", 
            "text": "FremenGrid", 
            "title": "Fremengrid"
        }, 
        {
            "location": "/fremen/fremengrid/#fremengrid", 
            "text": "", 
            "title": "FremenGrid"
        }, 
        {
            "location": "/fremen/frenap/", 
            "text": "Overview\n\n\nThe FreNaP package contains FreMEn-based prediction framework intended for topological navigation.\nIt takes the navigation statistics collected in the \nmongo_store\n and creates a dynamic FreMEn-based model of the navigation result and duration.\nThese dynamic models can be used to predict if the robot is likely to succeed in reaching its intended destination. \n\n\nPractical\n\n\nThe FreNap package contains an action server that receives four types of goals specified in the action string.\n\n\nThe \nbuild\n action\n\n\nretrieves the data from the Mongo and builds the FreMEn models:\n\n\ninput:\n\n\n\n\nmapName\n with the topological map name that you want to process,\n\n\nresultOrder\n which is FreMEn order for the result prediction - I recommend the values 2 or 1, but you can also let FreNaP to choose the model by entering the value of -1.\n\n\ntimeOrder\n which is FreMEn order for the duration prediction -  I recommend a static model for that, but again, you can enter -1 and FreNaP will choose the model order with the best estimation (not prediction!) accuracy.\n\n\n\n\noutput:\n\n\n\n\nstatus\n - string containing the number of edges,\n\n\nedgeName\n - list of edges,\n\n\nprobability\n - list of the model reconstruction errors (for the individual edges) as defined in the FreMEn \npaper\n,\n\n\nduration\n - average duration prediction error (for the individual edges). \n\n\n\n\nThe \npredict\n action\n\n\nperforms the prediction for a particular time given in epoch seconds,\n\n\ninput:\n\n\n\n\npredictionTime\n: in seconds since epoch,\n\n\n\n\noutput:\n\n\n\n\nedgeName\n:   list of edges,\n\n\nprobabilities\n: list of predicted probabilities that the edge will be traversed successfully,\n\n\ndurations\n: list of predicted durations.\n\n\n\n\nThe \ntimeline\n action\n\n\nallows to recover several predictions for a particular edge\n\n\ninput:\n\n\n\n\nmapName\n:  name of the edge to get the prediction time-line,\n\n\nstartTime\n:  the starting time of the predictions,\n\n\nendTime\n:  the final time of the predictions,\n\n\npredictionTime\n:  the time step between the two previous times.\n\n\n\n\noutput:\n\n\n\n\nprobabilities\n: list of predicted probabilities that the edge will be traversed successfully,\n\n\ndurations\n: list of predicted durations.\n\n\n\n\nThe \ndebug\n action\n\n\nThe debug action prints detailed info about the edge given in\n-\nmapName\n with the topological map name that you want to process,", 
            "title": "Frenap"
        }, 
        {
            "location": "/fremen/frenap/#overview", 
            "text": "The FreNaP package contains FreMEn-based prediction framework intended for topological navigation.\nIt takes the navigation statistics collected in the  mongo_store  and creates a dynamic FreMEn-based model of the navigation result and duration.\nThese dynamic models can be used to predict if the robot is likely to succeed in reaching its intended destination.", 
            "title": "Overview"
        }, 
        {
            "location": "/fremen/frenap/#practical", 
            "text": "The FreNap package contains an action server that receives four types of goals specified in the action string.", 
            "title": "Practical"
        }, 
        {
            "location": "/fremen/frenap/#the-build-action", 
            "text": "retrieves the data from the Mongo and builds the FreMEn models:", 
            "title": "The build action"
        }, 
        {
            "location": "/fremen/frenap/#input", 
            "text": "mapName  with the topological map name that you want to process,  resultOrder  which is FreMEn order for the result prediction - I recommend the values 2 or 1, but you can also let FreNaP to choose the model by entering the value of -1.  timeOrder  which is FreMEn order for the duration prediction -  I recommend a static model for that, but again, you can enter -1 and FreNaP will choose the model order with the best estimation (not prediction!) accuracy.", 
            "title": "input:"
        }, 
        {
            "location": "/fremen/frenap/#output", 
            "text": "status  - string containing the number of edges,  edgeName  - list of edges,  probability  - list of the model reconstruction errors (for the individual edges) as defined in the FreMEn  paper ,  duration  - average duration prediction error (for the individual edges).", 
            "title": "output:"
        }, 
        {
            "location": "/fremen/frenap/#the-predict-action", 
            "text": "performs the prediction for a particular time given in epoch seconds,", 
            "title": "The predict action"
        }, 
        {
            "location": "/fremen/frenap/#input_1", 
            "text": "predictionTime : in seconds since epoch,", 
            "title": "input:"
        }, 
        {
            "location": "/fremen/frenap/#output_1", 
            "text": "edgeName :   list of edges,  probabilities : list of predicted probabilities that the edge will be traversed successfully,  durations : list of predicted durations.", 
            "title": "output:"
        }, 
        {
            "location": "/fremen/frenap/#the-timeline-action", 
            "text": "allows to recover several predictions for a particular edge", 
            "title": "The timeline action"
        }, 
        {
            "location": "/fremen/frenap/#input_2", 
            "text": "mapName :  name of the edge to get the prediction time-line,  startTime :  the starting time of the predictions,  endTime :  the final time of the predictions,  predictionTime :  the time step between the two previous times.", 
            "title": "input:"
        }, 
        {
            "location": "/fremen/frenap/#output_2", 
            "text": "probabilities : list of predicted probabilities that the edge will be traversed successfully,  durations : list of predicted durations.", 
            "title": "output:"
        }, 
        {
            "location": "/fremen/frenap/#the-debug-action", 
            "text": "The debug action prints detailed info about the edge given in\n- mapName  with the topological map name that you want to process,", 
            "title": "The debug action"
        }, 
        {
            "location": "/fremen/froctomap/", 
            "text": "FrOctoMap\n\n\nThe FrOctoMap [1] method allows for efficient volumetric representation of dynamic three-dimensional environments over long periods of time.\nIt is based on combination of a well-established 3D mapping framework called Octomaps [2] and an idea to model environment dynamics by its frequency spectrum [3].\nThe proposed method allows not only for efficient representation, but also reliable prediction of the future states of dynamic three-dimensional environments.\nThis repository contains the spatio-temporal mapping framework intended for ROS.\n\n\n\n\nT.Krajnik, J.M.Santos, B.Seemann, T.Duckett: \nFROctomap: An Efficient Spatio-Temporal Environment Representation.\n TAROS 2014.\n\n\nA. Hornung,. K.M. Wurm, M. Bennewitz, C. Stachniss, and W. Burgard, \nOctoMap: An Efficient Probabilistic 3D Mapping Framework Based on Octrees\n in Autonomous Robots, 2013; DOI: 10.1007/s10514-012-9321-0.\n\n\nT.Krajnik, J.P.Fentanes, G.Cielniak, C.Dondrup, T.Duckett: \nSpectral Analysis for Long-Term Robotic Mapping.\n ICRA 2014.\n\n\n\n\nHow to use the software:\n\n\n\n\nGet a rosbag with octomaps. You can get one here: http://purl.org/robotics/octomaps\n\n\nLaunch the FRoctomap framework: roslaunch fremen froctomap.launch \n\n\nNow, you can save and update the fremen model via services '/save_grid' and  '/update_grid'. For parameters, check our paper.\n\n\nYou can also predict or reconstruct the Octomap by providing relevant timestamp (currently a simple sequence number) via service './generate_octomap'.\n\n\n\n\nThis is a demo intended for review purposes of TAROS 2014 to demonstrate its ability to compress observations made over long periods of time.\nThe main problem of this implementation of the FrOctoMap is that it requires the environment measurements to be performed on a regular basis.\nThis issue can be solved by using non-uniform Fourier transform, that does not have this requirement, but is more computationally expensive.", 
            "title": "Froctomap"
        }, 
        {
            "location": "/fremen/froctomap/#froctomap", 
            "text": "The FrOctoMap [1] method allows for efficient volumetric representation of dynamic three-dimensional environments over long periods of time.\nIt is based on combination of a well-established 3D mapping framework called Octomaps [2] and an idea to model environment dynamics by its frequency spectrum [3].\nThe proposed method allows not only for efficient representation, but also reliable prediction of the future states of dynamic three-dimensional environments.\nThis repository contains the spatio-temporal mapping framework intended for ROS.   T.Krajnik, J.M.Santos, B.Seemann, T.Duckett:  FROctomap: An Efficient Spatio-Temporal Environment Representation.  TAROS 2014.  A. Hornung,. K.M. Wurm, M. Bennewitz, C. Stachniss, and W. Burgard,  OctoMap: An Efficient Probabilistic 3D Mapping Framework Based on Octrees  in Autonomous Robots, 2013; DOI: 10.1007/s10514-012-9321-0.  T.Krajnik, J.P.Fentanes, G.Cielniak, C.Dondrup, T.Duckett:  Spectral Analysis for Long-Term Robotic Mapping.  ICRA 2014.   How to use the software:   Get a rosbag with octomaps. You can get one here: http://purl.org/robotics/octomaps  Launch the FRoctomap framework: roslaunch fremen froctomap.launch   Now, you can save and update the fremen model via services '/save_grid' and  '/update_grid'. For parameters, check our paper.  You can also predict or reconstruct the Octomap by providing relevant timestamp (currently a simple sequence number) via service './generate_octomap'.   This is a demo intended for review purposes of TAROS 2014 to demonstrate its ability to compress observations made over long periods of time.\nThe main problem of this implementation of the FrOctoMap is that it requires the environment measurements to be performed on a regular basis.\nThis issue can be solved by using non-uniform Fourier transform, that does not have this requirement, but is more computationally expensive.", 
            "title": "FrOctoMap"
        }, 
        {
            "location": "/fremen/frongo/", 
            "text": "FRONGO\n\n\nTHE FREMEN-MONGODB LINK YOU'VE BEEN LOOKING FOR\n\n\nWhat is Frongo?\n\n\nFrongo is a Fremen-based temporal extension for MongoDB, in other words it is a tool for converting mongodb queries into temporal models using Frequency Map Enhancement (FreMEn).\n\n\nWhy would I need such a thing?\n\n\nThe Frequency Map Enhancement (FreMEn) models the uncertainty of elementary states by a combination of periodic functions rather than by a constant probability,this allows the integration of observations into memory-efficient temporal models. These models can predict future states with a given level of confidence. \n\n\nCombining the predictive power of the FreMEn models with MongoDB's support for advanced queries,\ncan be extremely useful to create state predictions based on the information stored in the DB.\n\n\nHow does it work?\n\n\nFrongo is a \nROS\n node that can be executed running:\n\n\nrosrun frongo fremeniser.py [-h] [-yaml_defs YAML_DEFS]\n\n\nWhere YAML_DEFS is a file defining the Fremen model to be created, defining the query and data type that is to be modeled, an example file could be like this:\n\n\n- model:\n    name: \nPresence\n\n    db: \nopenhab\n\n    collection: \nopenhab\n\n    query: '{\nitem\n:\nPresence\n}'\n    timestamp_field: \ntimestamp\n\n    data_field: \nvalue\n\n    data_type: \nfloat\n\n- model:\n    name: \nEMC\n\n    db: \nopenhab\n\n    collection: \nopenhab\n\n    query: '{\nitem\n:\nEntry_Multi_Contact\n}'\n    timestamp_field: \ntimestamp\n\n    data_field: \nvalue\n\n    data_type: \nboolean\n\n    data_conf: '{\nTrue\n:[\nOPEN\n], \nFalse\n:[\nCLOSED\n]}'\n\n\n\n\nYou can also create your own models in runtime sending the \nYAML\n configuration as a string to the node via the  \n/frongo/add_model_defs\n service. Alternatively you can the \nadd_models_from_yaml\n script, which can be executed as follows:\n\n\nrosrun frongo add_models_from_yaml.py definitions.yaml\n\n\nWhere \ndefinitions.yaml\n is a \nYAML\n file with the definitions of the models to be added.\n\n\nWhat do all these fields mean?\n\n\nLets look at them again with the default values for each of them:\n\n\n- model:                                    # Mandatory: This is the root definition of a new model\n    name: \nmodel_1\n                         # Mandatory: This is the name of the model you *MUST* define this value\n    db: \nmessage_store\n                     # Optional: This is the the db name for the query\n    collection: \nmessage_store\n             # Optional: This is the the collection name for the query\n    query: '{}'                             # Optional: This is the query to be made\n    timestamp_field: \n_meta.inserted_at\n    # Optional: This is the field where the timestamp is within the entry\n    timestamp_type: 'datetime'              # Optional: This is the timestamp type it can either be 'datetime' for datetime objects or 'int' for epoch values\n    data_field: \ndata\n                      # Optional: This is the field where the value to be modeled is found\n    data_type: \nboolean\n                    # Optional: This is the type of the data to be modeled you can have 'boolean' values for true or false or 'float' for values between 0 and 1\n    data_conf: '{*}'                        # Optional: This is a JSON string where you can define some preprocessing for the values * See next section for understanding its set-up\n\n\n\n\nBut, how do I define the \ndata_conf\n field?\n\n\nThis a JSON string where you can define some preprocessing for the values.\n\n\nBoolean models\n\n\nFor \nboolean\n models such definition looks like this:\n\n\n{\n    \nTrue\n:[\nOPEN\n, \nTRUE\n, 1], \n    \nFalse\n:[\nCLOSED\n, \nFALSE\n, 0]\n}\n\n\n\n\nWhere \nTrue\n is a list of the values that should be considered as \n1\n in your fremen model, and \nFalse\n is a list of the values that should be considered as \n0\n\n\nFloat type models\n\n\nFor \nfloat\n models you can define a linear normalisation to fit the values between \n0\n and \n1\n such definition looks like this:\n\n\n{\n    \nmim\n:0, \n    \nmax\n:1\n}\n\n\n\n\nWhere \nmin\n is the value that should be correspond to \n0\n and  \nmax\n the value that corresponds to \n1\n\n\nOk, I can create my own models, but, what do I do with them?\n\n\nOnce you have created the models you can get predictions using the service \n/frongo/predict_models\n which is as follows:\n\n\nstring model_name       # SEND: The name of the model to be predicted\nuint32[] epochs         # SEND: The list of epochs you want the prediction for\n---\nuint32[] epochs         # GET: The list of epochs the prediction was made for\nfloat64[] predictions   # GET: The list of state probabilities for each epoch on the top list\n\n\n\n\nThis service uses the best order evaluated for this model.\n\n\nIf you want to create predictions with an specific fremen order you can use the \n/frongo/predict_models_with_order\n wich looks like this:\n\n\nstring model_name\nint32    order\nuint32[] epochs\n---\nuint32[] epochs\nfloat64[] predictions\n\n\n\n\nThe order field allows you to request predictions using any order.\n\n\nHow do I setup my MongoDB address and port?\n\n\nThe MongoDB address and port can be set via rosparams:\n\n\nrosparam set /mongodb_host \nyour mongodb server address\n\n\nrosparam set /mongodb_port \nyour mongodb server port\n\n\nHow do I know which models are already loaded?\n\n\nYou can use the \n/frongo/get_models\n service which look like this:\n\n\n---\nstring[] names\nstring[] info\n\n\n\n\nWhere \nnames\n is the list of all the model names and \ninfo\n is a yaml string with the internal variables of each model.\n\n\nIs there anything else I should know?\n\n\nYES, this is so awesome you can also get the model entropy over time using the services:\n\n\n\n\n\n\n/frongo/get_entropies\n For the best model order, which looks like this:\n\n\n```\nstring model_name\nuint32[] epochs\n\n\n\n\nuint32[] epochs\nfloat64[] predictions\n```\n\n\n\n\n\n\n/frongo/get_entropies_with_order\n to specify the model order, like this:\n\n\n```\nstring model_name\nint32    order\nuint32[] epochs\n\n\n\n\nuint32[] epochs\nfloat64[] predictions\n```", 
            "title": "Frongo"
        }, 
        {
            "location": "/fremen/frongo/#frongo", 
            "text": "", 
            "title": "FRONGO"
        }, 
        {
            "location": "/fremen/frongo/#the-fremen-mongodb-link-youve-been-looking-for", 
            "text": "", 
            "title": "THE FREMEN-MONGODB LINK YOU'VE BEEN LOOKING FOR"
        }, 
        {
            "location": "/fremen/frongo/#what-is-frongo", 
            "text": "Frongo is a Fremen-based temporal extension for MongoDB, in other words it is a tool for converting mongodb queries into temporal models using Frequency Map Enhancement (FreMEn).", 
            "title": "What is Frongo?"
        }, 
        {
            "location": "/fremen/frongo/#why-would-i-need-such-a-thing", 
            "text": "The Frequency Map Enhancement (FreMEn) models the uncertainty of elementary states by a combination of periodic functions rather than by a constant probability,this allows the integration of observations into memory-efficient temporal models. These models can predict future states with a given level of confidence.   Combining the predictive power of the FreMEn models with MongoDB's support for advanced queries,\ncan be extremely useful to create state predictions based on the information stored in the DB.", 
            "title": "Why would I need such a thing?"
        }, 
        {
            "location": "/fremen/frongo/#how-does-it-work", 
            "text": "Frongo is a  ROS  node that can be executed running:  rosrun frongo fremeniser.py [-h] [-yaml_defs YAML_DEFS]  Where YAML_DEFS is a file defining the Fremen model to be created, defining the query and data type that is to be modeled, an example file could be like this:  - model:\n    name:  Presence \n    db:  openhab \n    collection:  openhab \n    query: '{ item : Presence }'\n    timestamp_field:  timestamp \n    data_field:  value \n    data_type:  float \n- model:\n    name:  EMC \n    db:  openhab \n    collection:  openhab \n    query: '{ item : Entry_Multi_Contact }'\n    timestamp_field:  timestamp \n    data_field:  value \n    data_type:  boolean \n    data_conf: '{ True :[ OPEN ],  False :[ CLOSED ]}'  You can also create your own models in runtime sending the  YAML  configuration as a string to the node via the   /frongo/add_model_defs  service. Alternatively you can the  add_models_from_yaml  script, which can be executed as follows:  rosrun frongo add_models_from_yaml.py definitions.yaml  Where  definitions.yaml  is a  YAML  file with the definitions of the models to be added.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/fremen/frongo/#what-do-all-these-fields-mean", 
            "text": "Lets look at them again with the default values for each of them:  - model:                                    # Mandatory: This is the root definition of a new model\n    name:  model_1                          # Mandatory: This is the name of the model you *MUST* define this value\n    db:  message_store                      # Optional: This is the the db name for the query\n    collection:  message_store              # Optional: This is the the collection name for the query\n    query: '{}'                             # Optional: This is the query to be made\n    timestamp_field:  _meta.inserted_at     # Optional: This is the field where the timestamp is within the entry\n    timestamp_type: 'datetime'              # Optional: This is the timestamp type it can either be 'datetime' for datetime objects or 'int' for epoch values\n    data_field:  data                       # Optional: This is the field where the value to be modeled is found\n    data_type:  boolean                     # Optional: This is the type of the data to be modeled you can have 'boolean' values for true or false or 'float' for values between 0 and 1\n    data_conf: '{*}'                        # Optional: This is a JSON string where you can define some preprocessing for the values * See next section for understanding its set-up", 
            "title": "What do all these fields mean?"
        }, 
        {
            "location": "/fremen/frongo/#but-how-do-i-define-the-data_conf-field", 
            "text": "This a JSON string where you can define some preprocessing for the values.", 
            "title": "But, how do I define the data_conf field?"
        }, 
        {
            "location": "/fremen/frongo/#boolean-models", 
            "text": "For  boolean  models such definition looks like this:  {\n     True :[ OPEN ,  TRUE , 1], \n     False :[ CLOSED ,  FALSE , 0]\n}  Where  True  is a list of the values that should be considered as  1  in your fremen model, and  False  is a list of the values that should be considered as  0", 
            "title": "Boolean models"
        }, 
        {
            "location": "/fremen/frongo/#float-type-models", 
            "text": "For  float  models you can define a linear normalisation to fit the values between  0  and  1  such definition looks like this:  {\n     mim :0, \n     max :1\n}  Where  min  is the value that should be correspond to  0  and   max  the value that corresponds to  1", 
            "title": "Float type models"
        }, 
        {
            "location": "/fremen/frongo/#ok-i-can-create-my-own-models-but-what-do-i-do-with-them", 
            "text": "Once you have created the models you can get predictions using the service  /frongo/predict_models  which is as follows:  string model_name       # SEND: The name of the model to be predicted\nuint32[] epochs         # SEND: The list of epochs you want the prediction for\n---\nuint32[] epochs         # GET: The list of epochs the prediction was made for\nfloat64[] predictions   # GET: The list of state probabilities for each epoch on the top list  This service uses the best order evaluated for this model.  If you want to create predictions with an specific fremen order you can use the  /frongo/predict_models_with_order  wich looks like this:  string model_name\nint32    order\nuint32[] epochs\n---\nuint32[] epochs\nfloat64[] predictions  The order field allows you to request predictions using any order.", 
            "title": "Ok, I can create my own models, but, what do I do with them?"
        }, 
        {
            "location": "/fremen/frongo/#how-do-i-setup-my-mongodb-address-and-port", 
            "text": "The MongoDB address and port can be set via rosparams:  rosparam set /mongodb_host  your mongodb server address  rosparam set /mongodb_port  your mongodb server port", 
            "title": "How do I setup my MongoDB address and port?"
        }, 
        {
            "location": "/fremen/frongo/#how-do-i-know-which-models-are-already-loaded", 
            "text": "You can use the  /frongo/get_models  service which look like this:  ---\nstring[] names\nstring[] info  Where  names  is the list of all the model names and  info  is a yaml string with the internal variables of each model.", 
            "title": "How do I know which models are already loaded?"
        }, 
        {
            "location": "/fremen/frongo/#is-there-anything-else-i-should-know", 
            "text": "YES, this is so awesome you can also get the model entropy over time using the services:    /frongo/get_entropies  For the best model order, which looks like this:  ```\nstring model_name\nuint32[] epochs   uint32[] epochs\nfloat64[] predictions\n```    /frongo/get_entropies_with_order  to specify the model order, like this:  ```\nstring model_name\nint32    order\nuint32[] epochs   uint32[] epochs\nfloat64[] predictions\n```", 
            "title": "Is there anything else I should know?"
        }, 
        {
            "location": "/fremen/", 
            "text": "FreMEn\n\n\nFrequency Map Enhancement (FreMEn) is a method that allows to introduce dynamics into spatial models used in the mobile robotics domain.\nMany of these models describe the environment by a set of discrete components with binary states.\nFor example, cells of an occupancy grid are occupied or free, edges of a topological map are traversable or not, doors are opened or closed, rooms are vacant or occupied, landmarks are visible or occluded, etc.\nTypically, the state of every model component is uncertain, because it is measured indirectly by means of sensors which are affected by noise.\nThe uncertainty is typically represented by means of a probability, that is normally considered a static variable.\nThus, the probability of a particular component being in a particular state remains constant unless the state is being measured by the robot.\n\n\nFrequency Map Enhancement considers the probability of each environment state as a function of time and represents it by a combination of harmonic components.\nThe idea assumes that in populated environments, many of the observed changes are caused by humans performing their daily activities.\nTherefore, the environment's dynamics is naturally periodic and can be modelled by its frequency spectrum that represent a combination of harmonic functions that correspond to periodic processes influencing the environment.\nSuch a model not only allows representation of environment dynamics over arbitrary timescales with constant memory requirements, but also prediction of future environment states.\nThe proposed approach can be applied to many of the state-of-the-art environment models.\n\n\nIn particular, we have shown that occupancy grids, topological or landmark maps can be easily extended by FreMEn.\n- We have shown that the FreMEn allows to represent millions of observations by a few spectral parameters, can reliably predict environment states and detect anomalies [1].\n- Applying FreMEn to visibility of visual features improves mobile robot localisation in \nchanging environments\n [2].\n- Combining FreMEn with Octomaps results in an efficient spatio-temporal environment model called FrOctoMAp [3] that achieves compression rates up to 1:100000 for timescales over three months.\n\n\n======\n1. T.Krajnik, J.P.Fentanes, G.Cielniak, C.Dondrup, T.Duckett: \nSpectral Analysis for Long-Term Robotic Mapping.\n ICRA 2014.\n2. T.Krajnik, J.P.Fentanes, O.M.Mozos, T.Duckett, J.Ekekrantz, M.Hanheide: \nLong-term topological localisation for service robots in dynamic environments using spectral maps.\n IROS 2014.\n3. T.Krajnik, J.M.Santos, B.Seemann, T.Duckett: \nFROctomap: An Efficient Spatio-Temporal Environment Representation.\n TAROS 2014.", 
            "title": "Home"
        }, 
        {
            "location": "/fremen/#fremen", 
            "text": "Frequency Map Enhancement (FreMEn) is a method that allows to introduce dynamics into spatial models used in the mobile robotics domain.\nMany of these models describe the environment by a set of discrete components with binary states.\nFor example, cells of an occupancy grid are occupied or free, edges of a topological map are traversable or not, doors are opened or closed, rooms are vacant or occupied, landmarks are visible or occluded, etc.\nTypically, the state of every model component is uncertain, because it is measured indirectly by means of sensors which are affected by noise.\nThe uncertainty is typically represented by means of a probability, that is normally considered a static variable.\nThus, the probability of a particular component being in a particular state remains constant unless the state is being measured by the robot.  Frequency Map Enhancement considers the probability of each environment state as a function of time and represents it by a combination of harmonic components.\nThe idea assumes that in populated environments, many of the observed changes are caused by humans performing their daily activities.\nTherefore, the environment's dynamics is naturally periodic and can be modelled by its frequency spectrum that represent a combination of harmonic functions that correspond to periodic processes influencing the environment.\nSuch a model not only allows representation of environment dynamics over arbitrary timescales with constant memory requirements, but also prediction of future environment states.\nThe proposed approach can be applied to many of the state-of-the-art environment models.  In particular, we have shown that occupancy grids, topological or landmark maps can be easily extended by FreMEn.\n- We have shown that the FreMEn allows to represent millions of observations by a few spectral parameters, can reliably predict environment states and detect anomalies [1].\n- Applying FreMEn to visibility of visual features improves mobile robot localisation in  changing environments  [2].\n- Combining FreMEn with Octomaps results in an efficient spatio-temporal environment model called FrOctoMAp [3] that achieves compression rates up to 1:100000 for timescales over three months.  ======\n1. T.Krajnik, J.P.Fentanes, G.Cielniak, C.Dondrup, T.Duckett:  Spectral Analysis for Long-Term Robotic Mapping.  ICRA 2014.\n2. T.Krajnik, J.P.Fentanes, O.M.Mozos, T.Duckett, J.Ekekrantz, M.Hanheide:  Long-term topological localisation for service robots in dynamic environments using spectral maps.  IROS 2014.\n3. T.Krajnik, J.M.Santos, B.Seemann, T.Duckett:  FROctomap: An Efficient Spatio-Temporal Environment Representation.  TAROS 2014.", 
            "title": "FreMEn"
        }, 
        {
            "location": "/mongodb_store/", 
            "text": "mongodb_store\n\n\nThis package wraps up MongoDB database server in ROS, allowing it to be used to store configuration parameters.\n\n\nSee \nmongodb_store/README.md\n for further details", 
            "title": "Home"
        }, 
        {
            "location": "/mongodb_store/#mongodb_store", 
            "text": "This package wraps up MongoDB database server in ROS, allowing it to be used to store configuration parameters.  See  mongodb_store/README.md  for further details", 
            "title": "mongodb_store"
        }, 
        {
            "location": "/mongodb_store/mongodb_log/", 
            "text": "Generic MongoDB Logging for ROS\n\n\nAutonomous mobile robots produce an astonishing amount of run-time\ndata during their operation. Data is acquired from sensors and\nactuator feedback, processed to extract information, and further\nrefined as the basis for decision making or parameter estimation. In\ntoday's robot systems, this data is typically volatile. It is\ngenerated, used, and disposed right away. However, some of this data\nmight be useful later, for example to analyze faults or evaluate the\nrobot's performance. A system is required to store this data as well\nas enable efficient and flexible querying mechanisms.\n\n\nThis package contains the relevant pieces to setup this generic\nlogging for ROS. It features a generic logger written in Python\ncapable of logging any topic, and loggers written in C++ written for\nspecific message types which are known to be large in size or to be\ntransmitted at a high frequency. This is to cope with Python inherent\noverhead and processing limits. You might find that for certain\nadditional topics you need to write custom fast loggers following a\ntemplate of the existing loggers. If you do this for publicly\navailable message types we'd like to hear about it and maybe include\nit in a later release. We are also investigating the possibility to\nhave generic message introspection in C++ for ROS, which would remedy\nthis problem.\nAdditionally, this package includes tools to monitor the logging\nprocess and MongoDB and generate graphs using RRDtool.\n\n\nFor the conceptual background please refer to the paper \"A Generic\nRobot Database and its Application in Fault Analysis and Performance\nEvaluation\" by Niemueller et al. [1].\nFor technical information please refer to the project's website [2].\nThe ROS wiki page gives more details on how to get started on ROS [3].\n\n\n[1] http://www.fawkesrobotics.org/publications/2012/robodb-iros2012/\n[2] http://www.fawkesrobotics.org/projects/mongodb-log/\n[3] http://www.ros.org/wiki/mongodb_log", 
            "title": "Mongodb log"
        }, 
        {
            "location": "/mongodb_store/mongodb_log/#generic-mongodb-logging-for-ros", 
            "text": "Autonomous mobile robots produce an astonishing amount of run-time\ndata during their operation. Data is acquired from sensors and\nactuator feedback, processed to extract information, and further\nrefined as the basis for decision making or parameter estimation. In\ntoday's robot systems, this data is typically volatile. It is\ngenerated, used, and disposed right away. However, some of this data\nmight be useful later, for example to analyze faults or evaluate the\nrobot's performance. A system is required to store this data as well\nas enable efficient and flexible querying mechanisms.  This package contains the relevant pieces to setup this generic\nlogging for ROS. It features a generic logger written in Python\ncapable of logging any topic, and loggers written in C++ written for\nspecific message types which are known to be large in size or to be\ntransmitted at a high frequency. This is to cope with Python inherent\noverhead and processing limits. You might find that for certain\nadditional topics you need to write custom fast loggers following a\ntemplate of the existing loggers. If you do this for publicly\navailable message types we'd like to hear about it and maybe include\nit in a later release. We are also investigating the possibility to\nhave generic message introspection in C++ for ROS, which would remedy\nthis problem.\nAdditionally, this package includes tools to monitor the logging\nprocess and MongoDB and generate graphs using RRDtool.  For the conceptual background please refer to the paper \"A Generic\nRobot Database and its Application in Fault Analysis and Performance\nEvaluation\" by Niemueller et al. [1].\nFor technical information please refer to the project's website [2].\nThe ROS wiki page gives more details on how to get started on ROS [3].  [1] http://www.fawkesrobotics.org/publications/2012/robodb-iros2012/\n[2] http://www.fawkesrobotics.org/projects/mongodb-log/\n[3] http://www.ros.org/wiki/mongodb_log", 
            "title": "Generic MongoDB Logging for ROS"
        }, 
        {
            "location": "/mongodb_store/mongodb_store/", 
            "text": "mongodb_store\n\n\nThis package wraps up MongoDB database server in ROS, allowing it to be used to store configuration parameters.\n\n\nTwo nodes are provided:\n- mongodb_server.py\n- config_manager.py\n\n\nThese node depends on MongoDB and the Python client libraries (\n=2.3). Install by:\n\n\nsudo apt-get install python-pymongo mongodb\n\n\n\n\nIf this does not give the required version, you can use:\n\n\nsudo pip install pymongo\n\n\n\n\nRunning the mongodb_server\n\n\nThe start the datacentre:\n\n\nrosparam set mongodb_port 62345\nrosparam set mongodb_host bob # note that if using multiple machines, 'localhost' is no good\n\nrosrun mongodb_store mongodb_server.py\n\n\n\n\nBy default, the mongod database will be stored in \n/opt/strands/mongodb_store\n. This can be overridden by setting the private parameter ~database_path for the node. If it is the first time that the database is used, be sure to first run\n\n\nmkdir  /opt/strands/mongodb_store\n \n\n\nIf you prefer to use different mongodb instance, set the mongodb_* parameters accordingly.\n\n\nOr if you'd like to use existing mongod (e.g. mongod launched as Linux service)\n\n\nrosparam set mongodb_use_daemon true\nrosparam set mongodb_port 62345\nrosparam set mongodb_host localhost\n\nroslaunch mongodb_store mongodb_store use_daemon:=true\n\n\n\n\nConfig Manager Overview\n\n\nThe config manager provides a centralised way to store robot application parameters, with optional site-specific overrides. All configurations are stored inside the mongodb_store mongodb, within a database named \"configs\". \n\n\nTwo levels of parameters are considered:\n\n\n1) Global default parameters. \nThese should be \"working defaults\" - so all essential parameters at least have a default value. For example, if a robot application requires some calibration data then default values should be provided.\nDefault parameters can be shared among sites and stored inside a shared ROS package. When the config manager is started, all .yaml files stored in a 'defaults' folder will be examined. Any new default parameters will automatically be inserted into the \"defaults\" collection within the configs database. The defaults folder should be supplied as a private parameter: \n~defaults_path\n either set to a system path or in the form \npkg://ros_package_name/inside/package\n.\n\n\n2) Local parameters.\nThese parameters override the same named  global default parameters, allowing site-specific parameter setting. They are stored within the database inside the \"local\" collection.\n\n\nAt start up, the config manager places all parameters onto the ros parameter server to allow interoperability with existing software. Parameters can also be queried using the /config_manager/get_param service, or by directly connection to and querying the mongo database server.\n\n\nLikewise, local parameter overrides can be set using the /config_manager/set_param service or by directly editing the \"local\" collection in the configs database.\n\n\nRunning config manager\n\n\nTo start the config manager, make sure that you have the mongo db running then:\n\n\nrosrun mongodb_store config_manager.py _defaults_path:=pkg://my_package/defaults\n\n\n\n\nThis will load all parameters onto the ros parameter server, which can be checked with:\n\n\nrosparam list\n\n\n\n\nReading parameters\n\n\nThere are three methods to access parameter values:\n1) Use the copy on the ros parameter server:\n\n\nrosparam get /my/parameter\n\n\n\n\nand likewise with the rospy and roscpp libraries.\n2) Use the config_manager service:\n\n\nrosservice call /config_manager/get_param \nparam_name: '/my/parameter'\n \n\n\n\n\n3) Using the database server directly\n\n\nSetting parameters\n\n\nDefault parameters are set by placing them in the yaml files in the defaults directory in mongodb_store. This way, default parameters are added to the github repo and shared between all users.\n\n\nThe local parameter overrides can be set in 2 ways:\n1) Using the config_manager service:\n\n\nrosservice call /config_manager/set_param \nparam: '{\\\npath\\\n:\\\n/chris\\\n,\\\nvalue\\\n:43}'\n \n\n\n\n\nNote the syntax of the parameter: it is a json representation of a dictionary with path and value keys.\n\n\n2) Using the config_manager service:\n\n\nrosservice call /config_manager/save_param name_of_the_parameter_to_be_saved\n\n\n\n\nNote: This will save the current value of the parameter into the locals database\n\n\n3) Using the database server directly\n\n\nLaunch files\n\n\nBoth mongodb and config_manager can be started together using the datacentre.launch file:\n\n\nHOSTNAME=yourhost roslaunch mongodb_store mongodb_store.launch db_path:=/path/to/db db_port:=62345\n\n\n\n\nThe HOSTNAME env variable is required; db_path will default to /opt/strands/mongodb_store and db_port will default to 62345. \n\n\nReplication\n\n\nIf the constructor arcgument to the message store node \nreplicate_on_write\n is set to true, replication of the message store parts of the datacentre is done manually to allow different content to appear on different hosts. A list of hosts and ports where replications should be made can be set via the \nmongodb_store_extras\n parameter:\n\n\nmongodb_store_extras: [[\nlocalhost\n, 62344], [\nlocalhost\n, 62333]]\n\n\n\n\nInserts and updates are performed acorss the main and replicant datacentres.\n\n\nIf \nmongodb_store_extras\n is set (regardless of \nreplicate_on_write\n), queries are performed on the main first, and if nothing found, the replicants are tried.\n\n\nYou can launch additional datacentres as follows, e.g.\n\n\nrosrun mongodb_store mongodb_server.py _master:=false _database_path:=/opt/strands/strands_mongodb_62344 _host:=localhost _port:=62344\nrosrun mongodb_store mongodb_server.py _master:=false _database_path:=/opt/strands/strands_mongodb_62333 _host:=localhost _port:=62333\n\n\n\n\nYou can test if this works by adding some things to the message store, deleting them from the master using RoboMongo (not the message store as the deletes are replicated), then running queries.\n\n\nAction Server for Replication\n\n\nThe \nMoveEntries\n action and the corresponding action server:\n\n\nrosrun mongodb_store replicator_node.py \n\n\n\n\n(which is included in \ndatacentre.launch\n)\n\n\nallows you to bulk copy or move entries from message store collections to the mongod instances defined under \nmongodb_store_extras\n. The client accepts a list of collection names and uses the \nmeta[\"inserted_at\"]\n field of the message store entries to replicate or move all entries that were inserted before a particular time. If no time is provided then the default is 24 hours ago. There is an example client that does this for a list of collections specified on the command line. This \nmoves\n entries inserted 24 hours ago or earlier.\n\n\nrosrun mongodb_store replicator_client.py message_store robblog scheduling_problems\n\n\n\n\nNOTE THAT this all makes \nupdate\n operations a bit uncertain, so please do not use this type of replication on collections you plan to use update on.", 
            "title": "Mongodb store"
        }, 
        {
            "location": "/mongodb_store/mongodb_store/#mongodb_store", 
            "text": "This package wraps up MongoDB database server in ROS, allowing it to be used to store configuration parameters.  Two nodes are provided:\n- mongodb_server.py\n- config_manager.py  These node depends on MongoDB and the Python client libraries ( =2.3). Install by:  sudo apt-get install python-pymongo mongodb  If this does not give the required version, you can use:  sudo pip install pymongo", 
            "title": "mongodb_store"
        }, 
        {
            "location": "/mongodb_store/mongodb_store/#running-the-mongodb_server", 
            "text": "The start the datacentre:  rosparam set mongodb_port 62345\nrosparam set mongodb_host bob # note that if using multiple machines, 'localhost' is no good\n\nrosrun mongodb_store mongodb_server.py  By default, the mongod database will be stored in  /opt/strands/mongodb_store . This can be overridden by setting the private parameter ~database_path for the node. If it is the first time that the database is used, be sure to first run  mkdir  /opt/strands/mongodb_store    If you prefer to use different mongodb instance, set the mongodb_* parameters accordingly.  Or if you'd like to use existing mongod (e.g. mongod launched as Linux service)  rosparam set mongodb_use_daemon true\nrosparam set mongodb_port 62345\nrosparam set mongodb_host localhost\n\nroslaunch mongodb_store mongodb_store use_daemon:=true", 
            "title": "Running the mongodb_server"
        }, 
        {
            "location": "/mongodb_store/mongodb_store/#config-manager-overview", 
            "text": "The config manager provides a centralised way to store robot application parameters, with optional site-specific overrides. All configurations are stored inside the mongodb_store mongodb, within a database named \"configs\".   Two levels of parameters are considered:  1) Global default parameters. \nThese should be \"working defaults\" - so all essential parameters at least have a default value. For example, if a robot application requires some calibration data then default values should be provided.\nDefault parameters can be shared among sites and stored inside a shared ROS package. When the config manager is started, all .yaml files stored in a 'defaults' folder will be examined. Any new default parameters will automatically be inserted into the \"defaults\" collection within the configs database. The defaults folder should be supplied as a private parameter:  ~defaults_path  either set to a system path or in the form  pkg://ros_package_name/inside/package .  2) Local parameters.\nThese parameters override the same named  global default parameters, allowing site-specific parameter setting. They are stored within the database inside the \"local\" collection.  At start up, the config manager places all parameters onto the ros parameter server to allow interoperability with existing software. Parameters can also be queried using the /config_manager/get_param service, or by directly connection to and querying the mongo database server.  Likewise, local parameter overrides can be set using the /config_manager/set_param service or by directly editing the \"local\" collection in the configs database.", 
            "title": "Config Manager Overview"
        }, 
        {
            "location": "/mongodb_store/mongodb_store/#running-config-manager", 
            "text": "To start the config manager, make sure that you have the mongo db running then:  rosrun mongodb_store config_manager.py _defaults_path:=pkg://my_package/defaults  This will load all parameters onto the ros parameter server, which can be checked with:  rosparam list", 
            "title": "Running config manager"
        }, 
        {
            "location": "/mongodb_store/mongodb_store/#reading-parameters", 
            "text": "There are three methods to access parameter values:\n1) Use the copy on the ros parameter server:  rosparam get /my/parameter  and likewise with the rospy and roscpp libraries.\n2) Use the config_manager service:  rosservice call /config_manager/get_param  param_name: '/my/parameter'    3) Using the database server directly", 
            "title": "Reading parameters"
        }, 
        {
            "location": "/mongodb_store/mongodb_store/#setting-parameters", 
            "text": "Default parameters are set by placing them in the yaml files in the defaults directory in mongodb_store. This way, default parameters are added to the github repo and shared between all users.  The local parameter overrides can be set in 2 ways:\n1) Using the config_manager service:  rosservice call /config_manager/set_param  param: '{\\ path\\ :\\ /chris\\ ,\\ value\\ :43}'    Note the syntax of the parameter: it is a json representation of a dictionary with path and value keys.  2) Using the config_manager service:  rosservice call /config_manager/save_param name_of_the_parameter_to_be_saved  Note: This will save the current value of the parameter into the locals database  3) Using the database server directly", 
            "title": "Setting parameters"
        }, 
        {
            "location": "/mongodb_store/mongodb_store/#launch-files", 
            "text": "Both mongodb and config_manager can be started together using the datacentre.launch file:  HOSTNAME=yourhost roslaunch mongodb_store mongodb_store.launch db_path:=/path/to/db db_port:=62345  The HOSTNAME env variable is required; db_path will default to /opt/strands/mongodb_store and db_port will default to 62345.", 
            "title": "Launch files"
        }, 
        {
            "location": "/mongodb_store/mongodb_store/#replication", 
            "text": "If the constructor arcgument to the message store node  replicate_on_write  is set to true, replication of the message store parts of the datacentre is done manually to allow different content to appear on different hosts. A list of hosts and ports where replications should be made can be set via the  mongodb_store_extras  parameter:  mongodb_store_extras: [[ localhost , 62344], [ localhost , 62333]]  Inserts and updates are performed acorss the main and replicant datacentres.  If  mongodb_store_extras  is set (regardless of  replicate_on_write ), queries are performed on the main first, and if nothing found, the replicants are tried.  You can launch additional datacentres as follows, e.g.  rosrun mongodb_store mongodb_server.py _master:=false _database_path:=/opt/strands/strands_mongodb_62344 _host:=localhost _port:=62344\nrosrun mongodb_store mongodb_server.py _master:=false _database_path:=/opt/strands/strands_mongodb_62333 _host:=localhost _port:=62333  You can test if this works by adding some things to the message store, deleting them from the master using RoboMongo (not the message store as the deletes are replicated), then running queries.", 
            "title": "Replication"
        }, 
        {
            "location": "/mongodb_store/mongodb_store/#action-server-for-replication", 
            "text": "The  MoveEntries  action and the corresponding action server:  rosrun mongodb_store replicator_node.py   (which is included in  datacentre.launch )  allows you to bulk copy or move entries from message store collections to the mongod instances defined under  mongodb_store_extras . The client accepts a list of collection names and uses the  meta[\"inserted_at\"]  field of the message store entries to replicate or move all entries that were inserted before a particular time. If no time is provided then the default is 24 hours ago. There is an example client that does this for a list of collections specified on the command line. This  moves  entries inserted 24 hours ago or earlier.  rosrun mongodb_store replicator_client.py message_store robblog scheduling_problems  NOTE THAT this all makes  update  operations a bit uncertain, so please do not use this type of replication on collections you plan to use update on.", 
            "title": "Action Server for Replication"
        }, 
        {
            "location": "/planning_tutorial/", 
            "text": "planning_tutorial\n\n\n\nMaterial for the ICAPS '16 Summer School tutorial task scheduling and execution for long-term autonomy", 
            "title": "Home"
        }, 
        {
            "location": "/planning_tutorial/#planning_tutorial", 
            "text": "Material for the ICAPS '16 Summer School tutorial task scheduling and execution for long-term autonomy", 
            "title": "planning_tutorial"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_1/", 
            "text": "Exercise 1\n\n\nIn this first exercise you will use our MDP/LTL planning framework to make the robot drive around the map, and also play with the edges of the underlying map to see how different long-term robot experiences influence its behaviour when creating navigation policies.\n\n\nBackground\n\n\nYou must first run some basic elements from the STRANDS system. You should ideally run each of these in a separate terminal where you have sourced both the ROS and your local workspace \nsetup.bash\n files, as described in \ntutorial_prep.md\n. \n\n\nMongoDB Store\n\n\nFirst, check the \ndb\n directory exists (which you should've created following \ntutorial_prep.md\n). The following should not list any files or report an error:\n\n\nls `rospack find planning_tutorial`/db\n\n\n\n\nIf that is ok, then launch \nmongodb_store\n using that directory as its data path:\n\n\nroslaunch mongodb_store mongodb_store.launch db_path:=`rospack find planning_tutorial`/db\n\n\n\n\nMORSE Simulation\n\n\nIn another terminal, launch our simplified simulation of the \nTransport Systems Catapult\n (TSC). \n\n\nroslaunch strands_morse tsc_morse.launch \n\n\n\n\nIf you press the 'h' key in MORSE you can see a list of available keyboard commands.\n\n\n2D and Topological Navigation\n\n\nWe have predefined a simple topological map for you to use in this tutorial. The first time (and only the first time!) you want to use topological navigation in the TSC simulation, you must add this map to the mongodb store. Do it with the following command:\n\n\nrosrun topological_utils load_yaml_map.py `rospack find planning_tutorial`/maps/plan_tut_top_map.yaml\n\n\n\n\nYou can check the result of this with the following command which should print a description of the topological map \nplan_tut\n.\n\n\nrosrun topological_utils list_maps \n\n\n\n\nIf this was successful you can launch the 2D (amcl and move_base) and topological localisation and navigation for your simulated robot.\n\n\nroslaunch planning_tutorial tsc_navigation.launch\n\n\n\n\nTo see everything running, launch the ROS visualisation tool \nrviz\n with the provided config file:\n\n\nrviz -d `rospack find planning_tutorial`/plan_tut.rviz\n\n\n\n\nIf you click on a green arrow in a topological node, the robot should start working its way there. Feel free to add whatever extra visualisation parts you want to this (or ask us what the various bits are if you're new to robotics).\n\n\nEdge Prediction and MDP Planning\n\n\nFor this tutorial we're going to interfere with the expected duration and success probabilities for navigating edges in the topological map. Usually these are computed from data gathered as navigation happens, but for now we're going to fix them. To launch a node which reports fixed predictions for map edges, do the following in a new terminal (the argument is the file which contains the values to be reported):\n\n\nrosrun topological_navigation manual_edge_predictions.py `rospack find planning_tutorial`/maps/plan_tut_edges.yaml\n\n\n\n\nOnce this is running you can launch the MDP-based task executive system in (yet another!) new terminal:\n\n\nroslaunch mdp_plan_exec mdp_plan_exec_extended.launch\n\n\n\n\nExercise 1a\n\n\nWith all this up and running, you're now ready to give the robot some tasks. For now we're only going to worry about navigation tasks, i.e. reaching a node in the topological graph. If you open up the file \n$WS_ROOT_DIR/src/planning_tutorial/scripts/ltl_nav.py\n you will see some code which creates an MDP task to achieve an LTL goal. You can execute this file as follows:\n\n\nrosrun planning_tutorial ltl_nav.py\n\n\n\n\nAs this runs you should see the robot move around, as is appropriate for the task (i.e. following the optimal policy given the edge predictions). Parts of the policy are visualised as large arrows in rviz under the \nMarkerArray\n topic \ntopological_edges_policies\n (this is part of the preconfigured rviz file you launched above). \n\n\nThe important part of this file is the goal specification, i.e.:\n\n\ngoal_formula = '(F \nWayPoint2\n)'\n\n\n\n\nIn this case \nF\n means \"eventually\" and \n\"WayPoint2\"\n stands for the robot being at \nWayPoint2\n. Try editing this formula to try more complex goals involving other waypoints, LTL operators or Boolean connectives. For inspiration, take a look at the list below:\n\n\n\n\n\n\ngoal_formula = '(F \"WayPoint2\") \n (F \"WayPoint7\") '\n\n  Eventually reach \nWayPoint2\n and eventually reach \nWayPoint7\n. Choose best ordering to do so.\n\n\n\n\n\n\ngoal_formula = '(F \"WayPoint2\") | (F \"WayPoint7\") '\n\n  Eventually reach \nWayPoint2\n or eventually reach \nWayPoint7\n. Choose best  one to visit considering your current position.\n\n\n\n\n\n\ngoal_formula = '(F (\"WayPoint2\" \n (F \"WayPoint7\"))) '\n \n  Eventually reach \nWayPoint2\n and eventually reach \nWayPoint7\n. Choose best  one to visit considering your current position.\n\n\n\n\n\n\ngoal_formula = '((!\"WayPoint7\") U \"WayPoint5\") '\n \n  Avoid \nWayPoint7\n until you reach \nWayPoint5\n. Compare this policy with  the one obtained for \n'(F \"WayPoint5\") '\n.\n\n\n\n\n\n\ngoal_formula = '(F (\"WayPoint1\" \n (X \"WayPoint2\"))) '\n \n  Eventually reach \nWayPoint1\n and immediately after (\nX\n) go to \nWayPoint2\n. This allows us to \"tell\" the robot to navigate through some edges, e.g., for environmental exploration.\n\n\n\n\n\n\nThese specifications can also be connected using boolean connectives, e.g.,  \n'((!\"WayPoint7\") U \"WayPoint5\") ' \n  (F (\"WayPoint1\" \n (X \"WayPoint2\")))\n.\n\n\nExercise 1b\n\n\nSo far the robot is using the default, static edge durations and probabilities which we provided earlier. Now we'll play with these probabilities to observe the changes in the robot's behaviour. If you kill the \nmanual_edge_predictions.py\n node (\nCTRL-c\n in it's terminal) then edit the yaml file  \n$WS_ROOT_DIR/src/planning_tutorial/maps/plan_tut_edges.yaml\n you can alter the expected duration (in seconds) and the success probability of each edge in the map. After you've made your edits, restart the node as before and check if the robot creates policies which respect the new edge information you provided.", 
            "title": "Exercise 1"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_1/#exercise-1", 
            "text": "In this first exercise you will use our MDP/LTL planning framework to make the robot drive around the map, and also play with the edges of the underlying map to see how different long-term robot experiences influence its behaviour when creating navigation policies.", 
            "title": "Exercise 1"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_1/#background", 
            "text": "You must first run some basic elements from the STRANDS system. You should ideally run each of these in a separate terminal where you have sourced both the ROS and your local workspace  setup.bash  files, as described in  tutorial_prep.md .", 
            "title": "Background"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_1/#mongodb-store", 
            "text": "First, check the  db  directory exists (which you should've created following  tutorial_prep.md ). The following should not list any files or report an error:  ls `rospack find planning_tutorial`/db  If that is ok, then launch  mongodb_store  using that directory as its data path:  roslaunch mongodb_store mongodb_store.launch db_path:=`rospack find planning_tutorial`/db", 
            "title": "MongoDB Store"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_1/#morse-simulation", 
            "text": "In another terminal, launch our simplified simulation of the  Transport Systems Catapult  (TSC).   roslaunch strands_morse tsc_morse.launch   If you press the 'h' key in MORSE you can see a list of available keyboard commands.", 
            "title": "MORSE Simulation"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_1/#2d-and-topological-navigation", 
            "text": "We have predefined a simple topological map for you to use in this tutorial. The first time (and only the first time!) you want to use topological navigation in the TSC simulation, you must add this map to the mongodb store. Do it with the following command:  rosrun topological_utils load_yaml_map.py `rospack find planning_tutorial`/maps/plan_tut_top_map.yaml  You can check the result of this with the following command which should print a description of the topological map  plan_tut .  rosrun topological_utils list_maps   If this was successful you can launch the 2D (amcl and move_base) and topological localisation and navigation for your simulated robot.  roslaunch planning_tutorial tsc_navigation.launch  To see everything running, launch the ROS visualisation tool  rviz  with the provided config file:  rviz -d `rospack find planning_tutorial`/plan_tut.rviz  If you click on a green arrow in a topological node, the robot should start working its way there. Feel free to add whatever extra visualisation parts you want to this (or ask us what the various bits are if you're new to robotics).", 
            "title": "2D and Topological Navigation"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_1/#edge-prediction-and-mdp-planning", 
            "text": "For this tutorial we're going to interfere with the expected duration and success probabilities for navigating edges in the topological map. Usually these are computed from data gathered as navigation happens, but for now we're going to fix them. To launch a node which reports fixed predictions for map edges, do the following in a new terminal (the argument is the file which contains the values to be reported):  rosrun topological_navigation manual_edge_predictions.py `rospack find planning_tutorial`/maps/plan_tut_edges.yaml  Once this is running you can launch the MDP-based task executive system in (yet another!) new terminal:  roslaunch mdp_plan_exec mdp_plan_exec_extended.launch", 
            "title": "Edge Prediction and MDP Planning"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_1/#exercise-1a", 
            "text": "With all this up and running, you're now ready to give the robot some tasks. For now we're only going to worry about navigation tasks, i.e. reaching a node in the topological graph. If you open up the file  $WS_ROOT_DIR/src/planning_tutorial/scripts/ltl_nav.py  you will see some code which creates an MDP task to achieve an LTL goal. You can execute this file as follows:  rosrun planning_tutorial ltl_nav.py  As this runs you should see the robot move around, as is appropriate for the task (i.e. following the optimal policy given the edge predictions). Parts of the policy are visualised as large arrows in rviz under the  MarkerArray  topic  topological_edges_policies  (this is part of the preconfigured rviz file you launched above).   The important part of this file is the goal specification, i.e.:  goal_formula = '(F  WayPoint2 )'  In this case  F  means \"eventually\" and  \"WayPoint2\"  stands for the robot being at  WayPoint2 . Try editing this formula to try more complex goals involving other waypoints, LTL operators or Boolean connectives. For inspiration, take a look at the list below:    goal_formula = '(F \"WayPoint2\")   (F \"WayPoint7\") ' \n  Eventually reach  WayPoint2  and eventually reach  WayPoint7 . Choose best ordering to do so.    goal_formula = '(F \"WayPoint2\") | (F \"WayPoint7\") ' \n  Eventually reach  WayPoint2  or eventually reach  WayPoint7 . Choose best  one to visit considering your current position.    goal_formula = '(F (\"WayPoint2\"   (F \"WayPoint7\"))) '  \n  Eventually reach  WayPoint2  and eventually reach  WayPoint7 . Choose best  one to visit considering your current position.    goal_formula = '((!\"WayPoint7\") U \"WayPoint5\") '  \n  Avoid  WayPoint7  until you reach  WayPoint5 . Compare this policy with  the one obtained for  '(F \"WayPoint5\") ' .    goal_formula = '(F (\"WayPoint1\"   (X \"WayPoint2\"))) '  \n  Eventually reach  WayPoint1  and immediately after ( X ) go to  WayPoint2 . This allows us to \"tell\" the robot to navigate through some edges, e.g., for environmental exploration.    These specifications can also be connected using boolean connectives, e.g.,   '((!\"WayPoint7\") U \"WayPoint5\") '    (F (\"WayPoint1\"   (X \"WayPoint2\"))) .", 
            "title": "Exercise 1a"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_1/#exercise-1b", 
            "text": "So far the robot is using the default, static edge durations and probabilities which we provided earlier. Now we'll play with these probabilities to observe the changes in the robot's behaviour. If you kill the  manual_edge_predictions.py  node ( CTRL-c  in it's terminal) then edit the yaml file   $WS_ROOT_DIR/src/planning_tutorial/maps/plan_tut_edges.yaml  you can alter the expected duration (in seconds) and the success probability of each edge in the map. After you've made your edits, restart the node as before and check if the robot creates policies which respect the new edge information you provided.", 
            "title": "Exercise 1b"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/", 
            "text": "Exercise 2\n\n\nIn this exercise you will use our MDP/LTL planning framework to get the robot to perform 3D mapping sweeps of a simple. This example connects many of the different elements of our robot system, and may seem complex at first. If something is unclear, or you want more information, please just ask.\n\n\nBackground\n\n\nYou must first run some basic elements from the STRANDS system. You should ideally run each of these in a separate terminal where you have sourced both the ROS and your local workspace \nsetup.bash\n files, as described in \ntutorial_prep.md\n. \n\n\nMongoDB Store\n\n\n(If you still have the database running from \nExercise 1\n you can skip this step)\n\n\nFirst, check the \ndb\n directory exists (which you should've created following \ntutorial_prep.md\n). The following should not list any files or report an error:\n\n\nls `rospack find planning_tutorial`/db\n\n\n\n\nIf that is ok, then launch \nmongodb_store\n using that directory as its data path:\n\n\nroslaunch mongodb_store mongodb_store.launch db_path:=`rospack find planning_tutorial`/db\n\n\n\n\nMORSE Simulation\n\n\nIn another terminal, launch our object simulation taken from the \nALOOF Project\n. \n\n\nroslaunch strands_morse aloof_morse.launch \n\n\n\n\nIf you press the 'h' key in MORSE you can see a list of available keyboard commands.\n\n\n2D and Topological Navigation\n\n\nWe have predefined a simple topological map for you to use in this tutorial. The first time (and only the first time!) you want to use topological navigation in the ALOOF simulation, you must add this map to the mongodb store. Do it with the following command:\n\n\nrosrun topological_utils load_yaml_map.py `rospack find strands_morse`/aloof/maps/aloof_top_map.yaml\n\n\n\n\nYou can check the result of this with the following command which should print a description of the topological map \naloof\n.\n\n\nrosrun topological_utils list_maps \n\n\n\n\nIf this was successful you can launch the 2D (amcl and move_base) and topological localisation and navigation for your simulated robot. Note that in this configuration, the statistics for edge transitions in the topological map will be learnt from experience (as opposed to manually specified as in \nExercise 1\n).\n\n\nroslaunch strands_morse aloof_navigation.launch\n\n\n\n\nTo see everything running, launch the ROS visualisation tool \nrviz\n with the provided config file:\n\n\nrviz -d `rospack find planning_tutorial`/plan_tut.rviz\n\n\n\n\nYou'll find that the robot is not localised at the correct place compared to the simulation. Therefore use the \n2D Pose Estimate\n button and click (then hold and rotate to set angle) on the correct part of the map.\n\n\nIf you click on a green arrow in a topological node, the robot should start working its way there. Feel free to add whatever extra visualisation parts you want to this (or ask us what the various bits are if you're new to robotics).\n\n\nMDP Planning\n\n\nNext launch the MDP-based task executive system in (yet another!) new terminal:\n\n\nroslaunch mdp_plan_exec mdp_plan_exec_extended.launch\n\n\n\n\n3D Mapping Nodes\n\n\nOur 3D mapping framework makes use of an approach called \nmeta-rooms\n which builds 3D maps at waypoints in the environment. Before you run this for the first time you need to create somewhere for the system to store maps. Do this with the following command:\n\n\nmkdir ~/.semanticMap\n\n\n\n\nIf this was successful, you can launch the meta-room nodes with the following command:\n\n\nroslaunch planning_tutorial meta_rooms.launch\n\n\n\n\nExercise 2a\n\n\nIn \nExercise 1\n you exploited the fact that the execution framework automatically creates an MDP for navigation across the topological map. In this exercise we will extend this MDP with additional actions which connect ROS \nactionlib servers\n to actions in the MDP. \n\n\nIn this part we will walk through an example where the outcome of the invocation of an action server is tied to a non-deterministic outcome of an action in an MDP. After showing you how to do this, the next step will be for you to edit the file to change how the action is encoded in the MDP.\n\n\nAll of the code for this part of the exercise is in \nplanning_tutorial/script/sweep_at_waypoints.py\n so only the important fragments will be included here.\n\n\nThe task we will undertake is to trigger the \n/do_sweep\n action server with the argument \nmedium\n (signifying the number of views to take in the sweep) at a set of waypoints in the topological map. The key idea is that we associate the action server with an MDP action which we add to the navigation MDP. To make sure we only complete the action once, we connect the successful completion of this action to a state variable in our MDP specification, which can be used in pre- and post-conditions for our action. \n\n\nWe start by creating a name for our action. As we care about the success of an action at a waypoint, we need a different action for each waypoint. This is captured in the naming of the action, which should be different for each waypoint, i.e.\n\n\n    action_name = 'do_sweep_at_' + waypoint  \n\n\n\n\nNext we create a the MDP state variable for tracking the success of the action. This state variable will form part of our goal statement, e.g. if we have the state variable \nexecuted_do_sweep_at_WayPoint1\n the goal to eventually make this true would be \n(F executed_do_sweep_at_WayPoint1=1)\n.\n\n\n    state_var_name = 'executed_' + action_name\n    # Create the state var object, initialise to 0 (false)\n    state_var = MdpStateVar(name = state_var_name, init_val = 0, min_range = 0, max_range = 1) \n\n\n\n\nFollowing this we need to encode the possible outcomes of the action and the way they can change the state in the MDP. Although there is no restriction on the number of outcomes an action can have (over one), we will use two: succeeded or not succeeded. The \n/do_sweep\n action reports its outcomes when it completes, so we will use this return signal to tell the MDP which outcome occurred (allowing it to update its internal state correctly). Each outcome has a probability of occurring (which is used when creating a policy to solve the MDP), and a probability distribution over how long it might take to reach the outcome. In this example we make up these values, but in our full system we can learn them from experience at execution time.\n\n\n    successful_outcome = MdpActionOutcome(\n                # the probability of this outcome occurring\n                probability = 0.99,\n                # the effects of this action on the MDP state, in this case setting the state variable to 1\n                post_conds = [StringIntPair(string_data = state_var_name, int_data = 1)],\n                # how long it will take to reach this outcome, and with what probability\n                duration_probs = [1.0],\n                durations = [240.0], \n                # And the action server outcome which will tell us if this outcome occurred. In this case if the action server returns with SUCCEEDED \n                status = [GoalStatus(status = GoalStatus.SUCCEEDED)])\n\n\n\n\nBelow is a similar encoding for the unsuccessful outcome, i.e. the cases where the action server reports that it has aborted or was preempted.\n\n\n    unsuccessful_outcome = MdpActionOutcome(probability = (1 - successful_outcome.probability),\n                post_conds = [StringIntPair(string_data = state_var_name, int_data = 0)],\n                duration_probs = [1.0],\n                durations = [120.0],               \n                status = [GoalStatus(status = GoalStatus.ABORTED), GoalStatus(status = GoalStatus.PREEMPTED)])\n\n\n\n\nThe final step is to link the MDP action to the actionlib server. This is done in the code below. The \naction_server\n argument is used to configure the action server client, and the \nadd_string_argument\n call is used to embed the parameters for the call into the action itself (see \nstrands_executive_msgs.mdp_action_utils\n for more options).\n\n\n    # Finally we tie all this together to an actionlib server\n    action = MdpAction(name=action_name,\n            # This is the actual action server topic to be used \n            action_server='/do_sweep', \n            # The action will only be attemped if the preconditions are satisfied. In this case we can't have succeeded in the action before \n            pre_conds=[StringIntPair(string_data=state_var_name, int_data=0)],\n            # These are the possible outcomes we defined above\n            outcomes=[successful_outcome, unsuccessful_outcome],\n            # And this is where we can execute the action. \n            waypoints = [waypoint])\n    add_string_argument(action, 'medium')\n\n\n\n\n\nFor the purposes of the tutorial you should understand the overall approach we are taking for this, and be able to map this understanding into the code. To see it working you can run:\n\n\nrosrun planning_tutorial sweep_at_waypoints.py\n\n\n\n\nFeel free to edit the code as you wish and play around with the file. You could add more waypoints to the list, or add additional LTL navigation goals (as in \nExercise 1\n) to the string \nmdp_spec.ltl_task\n to change the robot's behaviour (e.g. get it to do a sweep without every visiting a given waypoint).\n\n\nExercise 2b\n\n\nTo successfully complete the following you need to be competent with both Python and ROS. If you're not comfortable with these, please pair up with someone who is.\n\n\nTo test your understanding, create a copy of \nplanning_tutorial sweep_at_waypoints.py\n and re-write it such that instead of completing a sweep at every waypoint, the problem is to simply complete a single sweep at any point at the map. However you should be able to specify that the chance of the action succeeding is different at different waypoints (with you inventing the numbers), and this should result in the robot choosing the waypoint with the highest chance of success for the sweep. \n\n\nExercise 2c\n\n\nWhat happens when you combine your answer to 2b with the ability to change the probabilities of success on the edges of the topological map? Talk to one of the lecturers for help with getting this running if you want to try it.", 
            "title": "Exercise 2"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/#exercise-2", 
            "text": "In this exercise you will use our MDP/LTL planning framework to get the robot to perform 3D mapping sweeps of a simple. This example connects many of the different elements of our robot system, and may seem complex at first. If something is unclear, or you want more information, please just ask.", 
            "title": "Exercise 2"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/#background", 
            "text": "You must first run some basic elements from the STRANDS system. You should ideally run each of these in a separate terminal where you have sourced both the ROS and your local workspace  setup.bash  files, as described in  tutorial_prep.md .", 
            "title": "Background"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/#mongodb-store", 
            "text": "(If you still have the database running from  Exercise 1  you can skip this step)  First, check the  db  directory exists (which you should've created following  tutorial_prep.md ). The following should not list any files or report an error:  ls `rospack find planning_tutorial`/db  If that is ok, then launch  mongodb_store  using that directory as its data path:  roslaunch mongodb_store mongodb_store.launch db_path:=`rospack find planning_tutorial`/db", 
            "title": "MongoDB Store"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/#morse-simulation", 
            "text": "In another terminal, launch our object simulation taken from the  ALOOF Project .   roslaunch strands_morse aloof_morse.launch   If you press the 'h' key in MORSE you can see a list of available keyboard commands.", 
            "title": "MORSE Simulation"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/#2d-and-topological-navigation", 
            "text": "We have predefined a simple topological map for you to use in this tutorial. The first time (and only the first time!) you want to use topological navigation in the ALOOF simulation, you must add this map to the mongodb store. Do it with the following command:  rosrun topological_utils load_yaml_map.py `rospack find strands_morse`/aloof/maps/aloof_top_map.yaml  You can check the result of this with the following command which should print a description of the topological map  aloof .  rosrun topological_utils list_maps   If this was successful you can launch the 2D (amcl and move_base) and topological localisation and navigation for your simulated robot. Note that in this configuration, the statistics for edge transitions in the topological map will be learnt from experience (as opposed to manually specified as in  Exercise 1 ).  roslaunch strands_morse aloof_navigation.launch  To see everything running, launch the ROS visualisation tool  rviz  with the provided config file:  rviz -d `rospack find planning_tutorial`/plan_tut.rviz  You'll find that the robot is not localised at the correct place compared to the simulation. Therefore use the  2D Pose Estimate  button and click (then hold and rotate to set angle) on the correct part of the map.  If you click on a green arrow in a topological node, the robot should start working its way there. Feel free to add whatever extra visualisation parts you want to this (or ask us what the various bits are if you're new to robotics).", 
            "title": "2D and Topological Navigation"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/#mdp-planning", 
            "text": "Next launch the MDP-based task executive system in (yet another!) new terminal:  roslaunch mdp_plan_exec mdp_plan_exec_extended.launch", 
            "title": "MDP Planning"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/#3d-mapping-nodes", 
            "text": "Our 3D mapping framework makes use of an approach called  meta-rooms  which builds 3D maps at waypoints in the environment. Before you run this for the first time you need to create somewhere for the system to store maps. Do this with the following command:  mkdir ~/.semanticMap  If this was successful, you can launch the meta-room nodes with the following command:  roslaunch planning_tutorial meta_rooms.launch", 
            "title": "3D Mapping Nodes"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/#exercise-2a", 
            "text": "In  Exercise 1  you exploited the fact that the execution framework automatically creates an MDP for navigation across the topological map. In this exercise we will extend this MDP with additional actions which connect ROS  actionlib servers  to actions in the MDP.   In this part we will walk through an example where the outcome of the invocation of an action server is tied to a non-deterministic outcome of an action in an MDP. After showing you how to do this, the next step will be for you to edit the file to change how the action is encoded in the MDP.  All of the code for this part of the exercise is in  planning_tutorial/script/sweep_at_waypoints.py  so only the important fragments will be included here.  The task we will undertake is to trigger the  /do_sweep  action server with the argument  medium  (signifying the number of views to take in the sweep) at a set of waypoints in the topological map. The key idea is that we associate the action server with an MDP action which we add to the navigation MDP. To make sure we only complete the action once, we connect the successful completion of this action to a state variable in our MDP specification, which can be used in pre- and post-conditions for our action.   We start by creating a name for our action. As we care about the success of an action at a waypoint, we need a different action for each waypoint. This is captured in the naming of the action, which should be different for each waypoint, i.e.      action_name = 'do_sweep_at_' + waypoint    Next we create a the MDP state variable for tracking the success of the action. This state variable will form part of our goal statement, e.g. if we have the state variable  executed_do_sweep_at_WayPoint1  the goal to eventually make this true would be  (F executed_do_sweep_at_WayPoint1=1) .      state_var_name = 'executed_' + action_name\n    # Create the state var object, initialise to 0 (false)\n    state_var = MdpStateVar(name = state_var_name, init_val = 0, min_range = 0, max_range = 1)   Following this we need to encode the possible outcomes of the action and the way they can change the state in the MDP. Although there is no restriction on the number of outcomes an action can have (over one), we will use two: succeeded or not succeeded. The  /do_sweep  action reports its outcomes when it completes, so we will use this return signal to tell the MDP which outcome occurred (allowing it to update its internal state correctly). Each outcome has a probability of occurring (which is used when creating a policy to solve the MDP), and a probability distribution over how long it might take to reach the outcome. In this example we make up these values, but in our full system we can learn them from experience at execution time.      successful_outcome = MdpActionOutcome(\n                # the probability of this outcome occurring\n                probability = 0.99,\n                # the effects of this action on the MDP state, in this case setting the state variable to 1\n                post_conds = [StringIntPair(string_data = state_var_name, int_data = 1)],\n                # how long it will take to reach this outcome, and with what probability\n                duration_probs = [1.0],\n                durations = [240.0], \n                # And the action server outcome which will tell us if this outcome occurred. In this case if the action server returns with SUCCEEDED \n                status = [GoalStatus(status = GoalStatus.SUCCEEDED)])  Below is a similar encoding for the unsuccessful outcome, i.e. the cases where the action server reports that it has aborted or was preempted.      unsuccessful_outcome = MdpActionOutcome(probability = (1 - successful_outcome.probability),\n                post_conds = [StringIntPair(string_data = state_var_name, int_data = 0)],\n                duration_probs = [1.0],\n                durations = [120.0],               \n                status = [GoalStatus(status = GoalStatus.ABORTED), GoalStatus(status = GoalStatus.PREEMPTED)])  The final step is to link the MDP action to the actionlib server. This is done in the code below. The  action_server  argument is used to configure the action server client, and the  add_string_argument  call is used to embed the parameters for the call into the action itself (see  strands_executive_msgs.mdp_action_utils  for more options).      # Finally we tie all this together to an actionlib server\n    action = MdpAction(name=action_name,\n            # This is the actual action server topic to be used \n            action_server='/do_sweep', \n            # The action will only be attemped if the preconditions are satisfied. In this case we can't have succeeded in the action before \n            pre_conds=[StringIntPair(string_data=state_var_name, int_data=0)],\n            # These are the possible outcomes we defined above\n            outcomes=[successful_outcome, unsuccessful_outcome],\n            # And this is where we can execute the action. \n            waypoints = [waypoint])\n    add_string_argument(action, 'medium')  For the purposes of the tutorial you should understand the overall approach we are taking for this, and be able to map this understanding into the code. To see it working you can run:  rosrun planning_tutorial sweep_at_waypoints.py  Feel free to edit the code as you wish and play around with the file. You could add more waypoints to the list, or add additional LTL navigation goals (as in  Exercise 1 ) to the string  mdp_spec.ltl_task  to change the robot's behaviour (e.g. get it to do a sweep without every visiting a given waypoint).", 
            "title": "Exercise 2a"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/#exercise-2b", 
            "text": "To successfully complete the following you need to be competent with both Python and ROS. If you're not comfortable with these, please pair up with someone who is.  To test your understanding, create a copy of  planning_tutorial sweep_at_waypoints.py  and re-write it such that instead of completing a sweep at every waypoint, the problem is to simply complete a single sweep at any point at the map. However you should be able to specify that the chance of the action succeeding is different at different waypoints (with you inventing the numbers), and this should result in the robot choosing the waypoint with the highest chance of success for the sweep.", 
            "title": "Exercise 2b"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_2/#exercise-2c", 
            "text": "What happens when you combine your answer to 2b with the ability to change the probabilities of success on the edges of the topological map? Talk to one of the lecturers for help with getting this running if you want to try it.", 
            "title": "Exercise 2c"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_3/", 
            "text": "Exercise 3 - UNFINISHED\n\n\nIn this exercise you will use our MDP/LTL planning framework to encode an object search task in a simple environment. This example connects many of the different elements of our robot system, and may seem complex at first. If something is unclear, or you want more information, please just ask.\n\n\nBackground\n\n\nYou must first run some basic elements from the STRANDS system. You should ideally run each of these in a separate terminal where you have sourced both the ROS and your local workspace \nsetup.bash\n files, as described in \ntutorial_prep.md\n. \n\n\nMongoDB Store\n\n\n(If you still have the database running from \nExercise 1\n you can skip this step)\n\n\nFirst, check the \ndb\n directory exists (which you should've created following \ntutorial_prep.md\n). The following should not list any files or report an error:\n\n\nls `rospack find planning_tutorial`/db\n\n\n\n\nIf that is ok, then launch \nmongodb_store\n using that directory as its data path:\n\n\nroslaunch mongodb_store mongodb_store.launch db_path:=`rospack find planning_tutorial`/db\n\n\n\n\nMORSE Simulation\n\n\nIn another terminal, launch our object simulation taken from the \nALOOF Project\n. \n\n\nroslaunch strands_morse aloof_morse.launch \n\n\n\n\nIf you press the 'h' key in MORSE you can see a list of available keyboard commands.\n\n\n2D and Topological Navigation\n\n\nWe have predefined a simple topological map for you to use in this tutorial. The first time (and only the first time!) you want to use topological navigation in the ALOOF simulation, you must add this map to the mongodb store. Do it with the following command:\n\n\nrosrun topological_utils load_yaml_map.py `rospack find strands_morse`/aloof/maps/aloof_top_map.yaml\n\n\n\n\nYou can check the result of this with the following command which should print a description of the topological map \naloof\n.\n\n\nrosrun topological_utils list_maps \n\n\n\n\nIf this was successful you can launch the 2D (amcl and move_base) and topological localisation and navigation for your simulated robot. Note that in this configuration, the statistics for edge transitions in the topological map will be learnt from experience (as opposed to manually specified as in \nExercise 1\n).\n\n\nroslaunch strands_morse aloof_navigation.launch\n\n\n\n\nTo see everything running, launch the ROS visualisation tool \nrviz\n with the provided config file:\n\n\nrviz -d `rospack find planning_tutorial`/plan_tut.rviz\n\n\n\n\nYou'll find that the robot is not localised at the correct place compared to the simulation. Therefore use the \n2D Pose Estimate\n button and click (then hold and rotate to set angle) on the correct part of the map.\n\n\nIf you click on a green arrow in a topological node, the robot should start working its way there. Feel free to add whatever extra visualisation parts you want to this (or ask us what the various bits are if you're new to robotics).\n\n\nMDP Planning\n\n\nNext launch the MDP-based task executive system in (yet another!) new terminal:\n\n\nroslaunch mdp_plan_exec mdp_plan_exec_extended.launch\n\n\n\n\nSemantic Map\n\n\nOur object search framework makes use of a semantic map of the environment  to know where to look for objects. There is a predefined map in this repository. Before you run the semantic mapping launch file for the first time, load the predefined map into mongodb with the following command.\n\n\nmkdir ~/.semanticMap\nmongorestore --port 62345 `rospack find planning_tutorial`/maps/soma_dump\n\n\n\n\nIf this was successful, you can launch the semantic map nodes with the following command:\n\n\nroslaunch planning_tutorial aloof_semantic_map.launch\n\n\n\n\nAfter you've done this you should see some blue and yellow regions appear in RViz.\n\n\nExercise 2a\n\n\nIn \nExercise 1\n you exploited the fact that the execution framework automatically creates an MDP for navigation across the topological map. In this exercise we will extend this MDP with additional actions which connect ROS \nactionlib servers\n to actions in the MDP. \n\n\nIn order for the robot to search for objects, it first needs to execute a \nmeta-room sweep\n in each room where it may need to look. This allows it to build a 3D map of each room for reasoning about supporting surfaces and views.\n\n\nWe connect the", 
            "title": "Exercise 3"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_3/#exercise-3-unfinished", 
            "text": "In this exercise you will use our MDP/LTL planning framework to encode an object search task in a simple environment. This example connects many of the different elements of our robot system, and may seem complex at first. If something is unclear, or you want more information, please just ask.", 
            "title": "Exercise 3 - UNFINISHED"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_3/#background", 
            "text": "You must first run some basic elements from the STRANDS system. You should ideally run each of these in a separate terminal where you have sourced both the ROS and your local workspace  setup.bash  files, as described in  tutorial_prep.md .", 
            "title": "Background"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_3/#mongodb-store", 
            "text": "(If you still have the database running from  Exercise 1  you can skip this step)  First, check the  db  directory exists (which you should've created following  tutorial_prep.md ). The following should not list any files or report an error:  ls `rospack find planning_tutorial`/db  If that is ok, then launch  mongodb_store  using that directory as its data path:  roslaunch mongodb_store mongodb_store.launch db_path:=`rospack find planning_tutorial`/db", 
            "title": "MongoDB Store"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_3/#morse-simulation", 
            "text": "In another terminal, launch our object simulation taken from the  ALOOF Project .   roslaunch strands_morse aloof_morse.launch   If you press the 'h' key in MORSE you can see a list of available keyboard commands.", 
            "title": "MORSE Simulation"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_3/#2d-and-topological-navigation", 
            "text": "We have predefined a simple topological map for you to use in this tutorial. The first time (and only the first time!) you want to use topological navigation in the ALOOF simulation, you must add this map to the mongodb store. Do it with the following command:  rosrun topological_utils load_yaml_map.py `rospack find strands_morse`/aloof/maps/aloof_top_map.yaml  You can check the result of this with the following command which should print a description of the topological map  aloof .  rosrun topological_utils list_maps   If this was successful you can launch the 2D (amcl and move_base) and topological localisation and navigation for your simulated robot. Note that in this configuration, the statistics for edge transitions in the topological map will be learnt from experience (as opposed to manually specified as in  Exercise 1 ).  roslaunch strands_morse aloof_navigation.launch  To see everything running, launch the ROS visualisation tool  rviz  with the provided config file:  rviz -d `rospack find planning_tutorial`/plan_tut.rviz  You'll find that the robot is not localised at the correct place compared to the simulation. Therefore use the  2D Pose Estimate  button and click (then hold and rotate to set angle) on the correct part of the map.  If you click on a green arrow in a topological node, the robot should start working its way there. Feel free to add whatever extra visualisation parts you want to this (or ask us what the various bits are if you're new to robotics).", 
            "title": "2D and Topological Navigation"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_3/#mdp-planning", 
            "text": "Next launch the MDP-based task executive system in (yet another!) new terminal:  roslaunch mdp_plan_exec mdp_plan_exec_extended.launch", 
            "title": "MDP Planning"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_3/#semantic-map", 
            "text": "Our object search framework makes use of a semantic map of the environment  to know where to look for objects. There is a predefined map in this repository. Before you run the semantic mapping launch file for the first time, load the predefined map into mongodb with the following command.  mkdir ~/.semanticMap\nmongorestore --port 62345 `rospack find planning_tutorial`/maps/soma_dump  If this was successful, you can launch the semantic map nodes with the following command:  roslaunch planning_tutorial aloof_semantic_map.launch  After you've done this you should see some blue and yellow regions appear in RViz.", 
            "title": "Semantic Map"
        }, 
        {
            "location": "/planning_tutorial/doc/exercise_3/#exercise-2a", 
            "text": "In  Exercise 1  you exploited the fact that the execution framework automatically creates an MDP for navigation across the topological map. In this exercise we will extend this MDP with additional actions which connect ROS  actionlib servers  to actions in the MDP.   In order for the robot to search for objects, it first needs to execute a  meta-room sweep  in each room where it may need to look. This allows it to build a 3D map of each room for reasoning about supporting surfaces and views.  We connect the", 
            "title": "Exercise 2a"
        }, 
        {
            "location": "/planning_tutorial/doc/tutorial_prep/", 
            "text": "Support\n\n\nIf you have difficulty following the steps in this document, please either contact Nick Hawes (n.a.hawes@cs.bham.ac.uk) or use the gitter chat at https://gitter.im/strands-project/planning_tutorial\n\n\nComputer Setup\n\n\nTo take part in the practical session of the tutorial, you will need to have your own laptop configured with the following software.\n\n\n\n\n\n\nInstall Ubuntu Linux 14.04LTS 64bit on your computer. Please make sure that you have exactly this version installed: 14.04 for 64bit. Download the image from here: http://releases.ubuntu.com/14.04.4/ (the image download is ubuntu-14.04.4-desktop-amd64.iso). Note, that you can perfectly install Ubuntu 14.04 alongside an existing operating system (even on a MacBook), or you could run this in a virtual machine.\n\n\n\n\n\n\nWithin Ubuntu, follow the instructions at https://github.com/strands-project-releases/strands-releases/wiki#using-the-strands-repository to install both ROS and the STRANDS software packages. We assume a basic understanding of Unix-like operating systems and shell usage here. If you need help using the command line, this might be a good start: https://help.ubuntu.com/community/UsingTheTerminal. \nThe relevant installation steps are copied below for your convenience:\n\n\n\n\nEnable the ROS repositories: Follow steps 1.1-1.3 in http://wiki.ros.org/indigo/Installation/Ubuntu#Installation. There is no need to do the steps after 1.3. In short, this is:\n\n\nConfigure your Ubuntu repositories to allow \"restricted,\" \"universe,\" and \"multiverse.\"\n\n\nsudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" \n /etc/apt/sources.list.d/ros-latest.list'\n\n\nsudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net --recv-key 0xB01FA116\n\n\n\n\n\n\nEnable the STRANDS repositories:\n\n\nAdd the STRANDS public key to verify packages:\n   \ncurl -s http://lcas.lincoln.ac.uk/repos/public.key | sudo apt-key add -\n\n\nAdd the STRANDS repository: \nsudo apt-add-repository http://lcas.lincoln.ac.uk/repos/release\n\n\n\n\n\n\nupdate your index: \nsudo apt-get update\n\n\ninstall required packages: \nsudo apt-get install ros-indigo-desktop-full ros-indigo-strands-desktop python-catkin-tools ros-indigo-semantic-map-launcher ros-indigo-door-pass\n\n\n\n\n\n\n\n\nTry out the \u201cMORSE\u201d simulator (run all the following in your terminal): \n\n\n\n\nconfigure ROS: \nsource /opt/ros/indigo/setup.bash\n\n\nlaunch the simulator: \nroslaunch strands_morse tsc_morse.launch\n\nYou should see the Morse simulator popping up with our robot platform being configured. \nIf your laptop uses an NVidia graphics card it might be worth looking at: https://wiki.ubuntu.com/Bumblebee to use it to its full potential.\nYou should be all set now!\n\n\n\n\n\n\n\n\nAdd the line \nsource /opt/ros/indigo/setup.bash\n to your \n.bashrc\n if you want ROS to be be loaded every time you open a terminal. You will be opening a few terminals, \nso this is advisable\n.\n\n\n\n\n\n\nConfiguration\n\n\nNext, create the local directory where you will do your work for this tutorial. This will contain a database and a \nROS catkin workspace\n. The following steps will be basic steps we will assume you execute:\n\n\nexport WS_ROOT_DIR=~/rob_plan_ws\nmkdir -p $WS_ROOT_DIR/src\n\n\n\n\nNext clone this repository into the newly created src dir and create a directory within it for our database later on:\n\n\ncd $WS_ROOT_DIR/src\ngit clone https://github.com/strands-project/planning_tutorial.git\nmkdir planning_tutorial/db\n\n\n\n\nFinally, if that was all successful, you should be able to build your workspace using \ncatkin tools\n (you can also use \ncatkin_make\n if you prefer that approach).\n\n\ncd $WS_ROOT_DIR\ncatkin init\ncatkin build\n\n\n\n\nAssuming that was all successful you now need to source your workspace so you have access to the packages there. For this you can do \n\n\nsource $WS_ROOT_DIR/devel/setup.bash\n\n\n\n\n(e.g. \nsource ~/rob_plan_ws/devel/setup.bash\n if you used the default from above). As with the other \nsetup.bash\n file mentioned above, you need to source this file in every terminal you open, therefore it is \nhighly advisable\n to add this command to the end of your \n.bashrc\n file so that it is done automatically. You can test if the file was sourced by running \nroscd\n (with no arguments). It everything was sourced correctly, this should take you to the directory \n$WS_ROOT_DIR/devel/\n.\n\n\nTutorial Packages\n\n\nFor the tutorial itself you will be asked to write some simple Python scripts to control a robot in ROS via our planning and execution packages. You will be taught about these packages in the tutorial itself, but  we will not have time to cover the basics of Python. If you are not familiar with Python, it would certainly help to run through a quick Python tutorial (there are many online) to get comfortable with the language. That said, we will try our hardest to ensure that you can access as much of the tutorial material as possible without knowing Python.", 
            "title": "Tutorial prep"
        }, 
        {
            "location": "/planning_tutorial/doc/tutorial_prep/#support", 
            "text": "If you have difficulty following the steps in this document, please either contact Nick Hawes (n.a.hawes@cs.bham.ac.uk) or use the gitter chat at https://gitter.im/strands-project/planning_tutorial", 
            "title": "Support"
        }, 
        {
            "location": "/planning_tutorial/doc/tutorial_prep/#computer-setup", 
            "text": "To take part in the practical session of the tutorial, you will need to have your own laptop configured with the following software.    Install Ubuntu Linux 14.04LTS 64bit on your computer. Please make sure that you have exactly this version installed: 14.04 for 64bit. Download the image from here: http://releases.ubuntu.com/14.04.4/ (the image download is ubuntu-14.04.4-desktop-amd64.iso). Note, that you can perfectly install Ubuntu 14.04 alongside an existing operating system (even on a MacBook), or you could run this in a virtual machine.    Within Ubuntu, follow the instructions at https://github.com/strands-project-releases/strands-releases/wiki#using-the-strands-repository to install both ROS and the STRANDS software packages. We assume a basic understanding of Unix-like operating systems and shell usage here. If you need help using the command line, this might be a good start: https://help.ubuntu.com/community/UsingTheTerminal. \nThe relevant installation steps are copied below for your convenience:   Enable the ROS repositories: Follow steps 1.1-1.3 in http://wiki.ros.org/indigo/Installation/Ubuntu#Installation. There is no need to do the steps after 1.3. In short, this is:  Configure your Ubuntu repositories to allow \"restricted,\" \"universe,\" and \"multiverse.\"  sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\"   /etc/apt/sources.list.d/ros-latest.list'  sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net --recv-key 0xB01FA116    Enable the STRANDS repositories:  Add the STRANDS public key to verify packages:\n    curl -s http://lcas.lincoln.ac.uk/repos/public.key | sudo apt-key add -  Add the STRANDS repository:  sudo apt-add-repository http://lcas.lincoln.ac.uk/repos/release    update your index:  sudo apt-get update  install required packages:  sudo apt-get install ros-indigo-desktop-full ros-indigo-strands-desktop python-catkin-tools ros-indigo-semantic-map-launcher ros-indigo-door-pass     Try out the \u201cMORSE\u201d simulator (run all the following in your terminal):    configure ROS:  source /opt/ros/indigo/setup.bash  launch the simulator:  roslaunch strands_morse tsc_morse.launch \nYou should see the Morse simulator popping up with our robot platform being configured. \nIf your laptop uses an NVidia graphics card it might be worth looking at: https://wiki.ubuntu.com/Bumblebee to use it to its full potential.\nYou should be all set now!     Add the line  source /opt/ros/indigo/setup.bash  to your  .bashrc  if you want ROS to be be loaded every time you open a terminal. You will be opening a few terminals,  so this is advisable .", 
            "title": "Computer Setup"
        }, 
        {
            "location": "/planning_tutorial/doc/tutorial_prep/#configuration", 
            "text": "Next, create the local directory where you will do your work for this tutorial. This will contain a database and a  ROS catkin workspace . The following steps will be basic steps we will assume you execute:  export WS_ROOT_DIR=~/rob_plan_ws\nmkdir -p $WS_ROOT_DIR/src  Next clone this repository into the newly created src dir and create a directory within it for our database later on:  cd $WS_ROOT_DIR/src\ngit clone https://github.com/strands-project/planning_tutorial.git\nmkdir planning_tutorial/db  Finally, if that was all successful, you should be able to build your workspace using  catkin tools  (you can also use  catkin_make  if you prefer that approach).  cd $WS_ROOT_DIR\ncatkin init\ncatkin build  Assuming that was all successful you now need to source your workspace so you have access to the packages there. For this you can do   source $WS_ROOT_DIR/devel/setup.bash  (e.g.  source ~/rob_plan_ws/devel/setup.bash  if you used the default from above). As with the other  setup.bash  file mentioned above, you need to source this file in every terminal you open, therefore it is  highly advisable  to add this command to the end of your  .bashrc  file so that it is done automatically. You can test if the file was sourced by running  roscd  (with no arguments). It everything was sourced correctly, this should take you to the directory  $WS_ROOT_DIR/devel/ .", 
            "title": "Configuration"
        }, 
        {
            "location": "/planning_tutorial/doc/tutorial_prep/#tutorial-packages", 
            "text": "For the tutorial itself you will be asked to write some simple Python scripts to control a robot in ROS via our planning and execution packages. You will be taught about these packages in the tutorial itself, but  we will not have time to cover the basics of Python. If you are not familiar with Python, it would certainly help to run through a quick Python tutorial (there are many online) to get comfortable with the language. That said, we will try our hardest to ensure that you can access as much of the tutorial material as possible without knowing Python.", 
            "title": "Tutorial Packages"
        }, 
        {
            "location": "/robblog/", 
            "text": "Robblog -- A robot blogging tool\n\n\nRobblog is a tool which converts entries from the \nmongodb_store\n into blog posts for \nJekyll\n. It runs as a ros node and provides its own web server, binding to a host and port of your choice.\n\n\nInstallation\n\n\nRobblog currently uses \nJekyll\n (version 2) to generate pages. To install the dependencies under Ubuntu 14.04 do:\n\n\nsudo apt-get ruby1-dev nodejs\nsudo -E gem install jekyll -v 2.5.3\n\n\n\n\nOn OS X do:\n\n\nbrew install ruby nodejs\ngem install jekyll -v 2.5.3\n\n\n\n\nUsage\n\n\nRobblog has two parts: clients which add \nrobblog/RobblogEntry\n entries to mongodb_store, and the blog engine which creates blog posts from these entries. \n\n\nRunning the Robblog engine\n\n\nThe configuration information that you need to run Robblog is as follows: mongodb_store collection to monitor for new entries; the path on the system where Jekyll will create your blog; and the hostname and port to server the webblog at. The code snippet below shows how to create and configure your blog. In this example mongodb_store collection is provided as an argument \nEntryConverter\n (\n'robblog'\n), the system path is the \nblog_path\n variable which is defined relative to the \ny1_interfaces\n package (\nroslib.packages.get_pkg_dir('y1_interfaces') + '/content'\n), and the host details are obtained from local parameters if provided. \n\n\n#!/usr/bin/env python\n\nimport rospy\nimport roslib\nimport robblog\nimport robblog.utils\n\nfrom datetime import *\n\nif __name__ == '__main__':\n    rospy.init_node(\nrobblog\n)\n\n    server_host = rospy.get_param(\n~host_ip\n, \n127.0.0.1\n)\n    server_port = rospy.get_param(\n~port\n, \n4000\n)\n\n    # where are the blog files going to be put\n    blog_path = roslib.packages.get_pkg_dir('y1_interfaces') + '/content'\n\n    # initialise blog\n    robblog.utils.init_blog(blog_path)\n\n    # and start serving content at this place\n    proc = robblog.utils.serve(blog_path, server_host, server_port)\n\n    try: \n        converter = robblog.utils.EntryConverter(blog_path=blog_path, collection='robblog')\n\n        while not rospy.is_shutdown():\n            converter.convert()\n            rospy.sleep(1)\n\n    except Exception, e:\n                rospy.logfatal(e)\n    finally:\n        proc.terminate()\n\n\n\n\n\nAdding entries\n\n\nTo add entries to robblog, clients should add instances of the \nrobblog/RobblogEntry\n message type to mongodb_store (usually via the message store proxy). The message type is simply:\n\n\nstring title\nstring body\n\n\n\n\nThe title is translated into the title of the blog post and the name of the markdown file containing the post in the Jekyll install. The body should be \nMarkdown\n. \n\n\nRules for creating entries\n\n\nTitle choice\n\n\nTo march Jekyll's structuring rules, each entry is converted into a file which is named using the date (YYYY-MM-DD) plus the title (with spaces converted to hyphens). If you create two entries on the same day with the same title this will only create one blog entry in Jekyll, despite their being two different entries in mongodb_store. Until Robblog is updated to fix this, if you do plan to create muliple entries with the same title on the same day, it might be worth adding a counter or some other unique characters to the title.\n\n\nIllegal characters\n\n\nAs titles are turned into filenames, you need to avoid illegal characters. Currently no sanitisation is done. As far as seen, characters within the body of the entry are fine.\n\n\nImages\n\n\nImages can be included in entries. To include one, you must add the image to mongodb_store, retaining the object ID you receive in response. You can then use standard Markdown image inclusion, but replace the image URL with the object ID wrapped in \nObjectID()\n, e.g. \n\n\n[My helpful screenshot](ObjectID(5367e93d54a6f7f69297335e))\n\n\n\n\nThis ID is used to automatically create a jpeg image to include in the blog post.\n\n\nExample\n\n\nThe following example (also available \nin full here\n) adds a few entries to mongodb_store then serves them. Once you've run this go to \nhttp://localhost:4040\n and you should see the blog entries.\n\n\n#!/usr/bin/env python\n\nimport rospy\nimport roslib\nfrom mongodb_store.message_store import MessageStoreProxy\nfrom robblog.msg import RobblogEntry\nimport robblog.utils\nimport cv2\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nfrom datetime import *\n\nif __name__ == '__main__':\n    rospy.init_node(\nrobblog_example\n)\n\n    blog_collection = 'example_blog'\n\n    # Create some blog entries\n    msg_store = MessageStoreProxy(collection=blog_collection)\n\n    create_entries = True\n\n    robblog_path =  roslib.packages.get_pkg_dir('robblog') \n\n    if create_entries:\n        e1 = RobblogEntry(title='Test Title 1', body='blah blah')\n        e2 = RobblogEntry(title='Test Title 2', body='blah blah')\n        e3 = RobblogEntry(title='Test Title 3', body='blah blah')\n        msg_store.insert(e1)\n        msg_store.insert(e2)\n        msg_store.insert(e3)\n\n        # add a complex markdown example\n        with open(robblog_path + '/data/example.md' , 'r') as f:\n            e4 = RobblogEntry(title='Markdown Example', body=f.read())\n            msg_store.insert(e4)\n            # print e4\n\n        # add an example with an image\n        cv_image = cv2.imread(robblog_path + '/data/rur.jpg')\n        bridge = CvBridge()\n        img_msg = bridge.cv2_to_imgmsg(cv_image)\n        img_id = msg_store.insert(img_msg)\n        e5 = RobblogEntry(title='Image Test', body='This is what a robot looks like.\\n\\n![My helpful screenshot](ObjectID(%s))' % img_id)\n        msg_store.insert(e5)\n\n\n    serve = True\n\n    if serve:\n        # where are the blog files going to be put\n        blog_path = robblog_path + '/content'\n\n        # initialise blog\n        robblog.utils.init_blog(blog_path)\n        proc = robblog.utils.serve(blog_path, 'localhost', '4040')\n\n        try: \n            converter = robblog.utils.EntryConverter(blog_path=blog_path, collection=blog_collection)\n\n\n            while not rospy.is_shutdown():\n                # supply True convert to force all pages to be regenerated\n                converter.convert()\n                rospy.sleep(1)\n\n        except Exception, e:\n                    rospy.logfatal(e)\n        finally:\n            proc.terminate()", 
            "title": "Home"
        }, 
        {
            "location": "/robblog/#robblog-a-robot-blogging-tool", 
            "text": "Robblog is a tool which converts entries from the  mongodb_store  into blog posts for  Jekyll . It runs as a ros node and provides its own web server, binding to a host and port of your choice.", 
            "title": "Robblog -- A robot blogging tool"
        }, 
        {
            "location": "/robblog/#installation", 
            "text": "Robblog currently uses  Jekyll  (version 2) to generate pages. To install the dependencies under Ubuntu 14.04 do:  sudo apt-get ruby1-dev nodejs\nsudo -E gem install jekyll -v 2.5.3  On OS X do:  brew install ruby nodejs\ngem install jekyll -v 2.5.3", 
            "title": "Installation"
        }, 
        {
            "location": "/robblog/#usage", 
            "text": "Robblog has two parts: clients which add  robblog/RobblogEntry  entries to mongodb_store, and the blog engine which creates blog posts from these entries.", 
            "title": "Usage"
        }, 
        {
            "location": "/robblog/#running-the-robblog-engine", 
            "text": "The configuration information that you need to run Robblog is as follows: mongodb_store collection to monitor for new entries; the path on the system where Jekyll will create your blog; and the hostname and port to server the webblog at. The code snippet below shows how to create and configure your blog. In this example mongodb_store collection is provided as an argument  EntryConverter  ( 'robblog' ), the system path is the  blog_path  variable which is defined relative to the  y1_interfaces  package ( roslib.packages.get_pkg_dir('y1_interfaces') + '/content' ), and the host details are obtained from local parameters if provided.   #!/usr/bin/env python\n\nimport rospy\nimport roslib\nimport robblog\nimport robblog.utils\n\nfrom datetime import *\n\nif __name__ == '__main__':\n    rospy.init_node( robblog )\n\n    server_host = rospy.get_param( ~host_ip ,  127.0.0.1 )\n    server_port = rospy.get_param( ~port ,  4000 )\n\n    # where are the blog files going to be put\n    blog_path = roslib.packages.get_pkg_dir('y1_interfaces') + '/content'\n\n    # initialise blog\n    robblog.utils.init_blog(blog_path)\n\n    # and start serving content at this place\n    proc = robblog.utils.serve(blog_path, server_host, server_port)\n\n    try: \n        converter = robblog.utils.EntryConverter(blog_path=blog_path, collection='robblog')\n\n        while not rospy.is_shutdown():\n            converter.convert()\n            rospy.sleep(1)\n\n    except Exception, e:\n                rospy.logfatal(e)\n    finally:\n        proc.terminate()", 
            "title": "Running the Robblog engine"
        }, 
        {
            "location": "/robblog/#adding-entries", 
            "text": "To add entries to robblog, clients should add instances of the  robblog/RobblogEntry  message type to mongodb_store (usually via the message store proxy). The message type is simply:  string title\nstring body  The title is translated into the title of the blog post and the name of the markdown file containing the post in the Jekyll install. The body should be  Markdown .", 
            "title": "Adding entries"
        }, 
        {
            "location": "/robblog/#rules-for-creating-entries", 
            "text": "", 
            "title": "Rules for creating entries"
        }, 
        {
            "location": "/robblog/#title-choice", 
            "text": "To march Jekyll's structuring rules, each entry is converted into a file which is named using the date (YYYY-MM-DD) plus the title (with spaces converted to hyphens). If you create two entries on the same day with the same title this will only create one blog entry in Jekyll, despite their being two different entries in mongodb_store. Until Robblog is updated to fix this, if you do plan to create muliple entries with the same title on the same day, it might be worth adding a counter or some other unique characters to the title.", 
            "title": "Title choice"
        }, 
        {
            "location": "/robblog/#illegal-characters", 
            "text": "As titles are turned into filenames, you need to avoid illegal characters. Currently no sanitisation is done. As far as seen, characters within the body of the entry are fine.", 
            "title": "Illegal characters"
        }, 
        {
            "location": "/robblog/#images", 
            "text": "Images can be included in entries. To include one, you must add the image to mongodb_store, retaining the object ID you receive in response. You can then use standard Markdown image inclusion, but replace the image URL with the object ID wrapped in  ObjectID() , e.g.   [My helpful screenshot](ObjectID(5367e93d54a6f7f69297335e))  This ID is used to automatically create a jpeg image to include in the blog post.", 
            "title": "Images"
        }, 
        {
            "location": "/robblog/#example", 
            "text": "The following example (also available  in full here ) adds a few entries to mongodb_store then serves them. Once you've run this go to  http://localhost:4040  and you should see the blog entries.  #!/usr/bin/env python\n\nimport rospy\nimport roslib\nfrom mongodb_store.message_store import MessageStoreProxy\nfrom robblog.msg import RobblogEntry\nimport robblog.utils\nimport cv2\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nfrom datetime import *\n\nif __name__ == '__main__':\n    rospy.init_node( robblog_example )\n\n    blog_collection = 'example_blog'\n\n    # Create some blog entries\n    msg_store = MessageStoreProxy(collection=blog_collection)\n\n    create_entries = True\n\n    robblog_path =  roslib.packages.get_pkg_dir('robblog') \n\n    if create_entries:\n        e1 = RobblogEntry(title='Test Title 1', body='blah blah')\n        e2 = RobblogEntry(title='Test Title 2', body='blah blah')\n        e3 = RobblogEntry(title='Test Title 3', body='blah blah')\n        msg_store.insert(e1)\n        msg_store.insert(e2)\n        msg_store.insert(e3)\n\n        # add a complex markdown example\n        with open(robblog_path + '/data/example.md' , 'r') as f:\n            e4 = RobblogEntry(title='Markdown Example', body=f.read())\n            msg_store.insert(e4)\n            # print e4\n\n        # add an example with an image\n        cv_image = cv2.imread(robblog_path + '/data/rur.jpg')\n        bridge = CvBridge()\n        img_msg = bridge.cv2_to_imgmsg(cv_image)\n        img_id = msg_store.insert(img_msg)\n        e5 = RobblogEntry(title='Image Test', body='This is what a robot looks like.\\n\\n![My helpful screenshot](ObjectID(%s))' % img_id)\n        msg_store.insert(e5)\n\n\n    serve = True\n\n    if serve:\n        # where are the blog files going to be put\n        blog_path = robblog_path + '/content'\n\n        # initialise blog\n        robblog.utils.init_blog(blog_path)\n        proc = robblog.utils.serve(blog_path, 'localhost', '4040')\n\n        try: \n            converter = robblog.utils.EntryConverter(blog_path=blog_path, collection=blog_collection)\n\n\n            while not rospy.is_shutdown():\n                # supply True convert to force all pages to be regenerated\n                converter.convert()\n                rospy.sleep(1)\n\n        except Exception, e:\n                    rospy.logfatal(e)\n        finally:\n            proc.terminate()", 
            "title": "Example"
        }, 
        {
            "location": "/robblog/data/2014-05-05-strands-test-post/", 
            "text": "This is a \ntest\n.\n\n\n![My helpful screenshot]({{ site.url }}/assets/screenshot.jpg)", 
            "title": "2014 05 05 strands test post"
        }, 
        {
            "location": "/robblog/data/example/", 
            "text": "Cristae Peleus\n\n\nNon ut littera debuit\n\n\nLorem markdownum e solis parcere Paeonas. Nec \nvix habuisse\ncognoscere\n intra potiuntur pectora, corpora\nqui miracula saepe caelum petunt rivus neque adire tutus laetaris videntur!\nGraves similes tenvit onus, enim manat, coegi Olenos tum. Flammas resistere\nfuerunt de titulum caelum.\n\n\nSed vultus fraudem mansura tremor est, \nsui me\n Athamanta\nille: nec ego laurus pullo, et ipse. Sues quos, quem, pede tot verba undae\npatitur sero modo saepe ut viderit accipe gaudia auditaque.\n\n\nEst notat timorque Siculaeque quorum incaluere nostri\n\n\nNegetur igne, fretumque imagine repperit, et e iuventus tenet inpar \nfore\nacuta\n. Cadentem desierat, coactis regis\npampineis ille teloque, \ncum Tityos\n, mihi non non. Cum dixit. Ibi corpora ore,\nPhoci fixus cadavera quoque?\n\n\nif (guid.multiplatform(1)) {\n    shortcutBridge.cloud(word_non, 365386 + room_install);\n    newsgroup_zone_card(backsideCircuitLinux - hard_resolution,\n            microphonePhreakingPda(driveSector, target_adc, wi),\n            fileDefault);\n} else {\n    telnet.access_extranet.leaderboard(dvd, circuit + marketTwain, -3);\n}\nbootAjax = type_internet_smishing + -5;\nif (7 - e_illegal \n= 1 - 4 + 5) {\n    osiMemoryFlops.gatewayCode += intranetSequence(dns_c_bittorrent,\n            pebibyte_circuit_snmp - web_mainframe_wais, 3);\n    newsgroup(boot);\n    hover(guiMicrophoneMultithreading, bridge_ebook, uatCifsMms);\n}\nvar drop = 5 * motion_url(aclPipelineCisc,\n        multiprocessing_printer_internet);\n\n\n\nMala artem\n\n\nTimoris robora et o consorte umeris. Coniuge pias inque pontus, ne adest\nrefugerit \namoris\n. Eurus hunc, hic fudit veteres, sequor caeleste te quemque\nfumabat; armos! Angue illa \nquondam\n. Sine et est inquit natorum parat.\n\n\nsrgb_dvd = page_vrml_media * 1;\nvar docking_power = nodeResponsiveMeta;\nif (windows) {\n    plugSubnet /= passive_motherboard;\n    network(udpHub + nocFunction, -3);\n    resources(broadband.protector_graphic.boot(pixelVectorMulti, wave));\n} else {\n    bmp.boot(natVirtualWebmail(web, smtp, latency_disk_iteration));\n    denial_sector_access = thread_direct * cifs_install;\n}\nformat_pcb(linkedin_pop_flash(lampFile.quad(1), unix_disk_pmu(-5,\n        widget_uat, template)), statusCompressionBus(1, controlExcelDrive -\n        ecc_prom_interpreter, -4 + -1), -5 / -1 + webmail_load_post);\nvar module_development = browser;\n\n\n\nNe Procrin natus fui \npassis\n\nAmpycides somnus nullo: manu ultima si totoque! Nec dat consedit illo collo\nripis laevaque omnia, sonat.", 
            "title": "Example"
        }, 
        {
            "location": "/robblog/data/example/#cristae-peleus", 
            "text": "", 
            "title": "Cristae Peleus"
        }, 
        {
            "location": "/robblog/data/example/#non-ut-littera-debuit", 
            "text": "Lorem markdownum e solis parcere Paeonas. Nec  vix habuisse\ncognoscere  intra potiuntur pectora, corpora\nqui miracula saepe caelum petunt rivus neque adire tutus laetaris videntur!\nGraves similes tenvit onus, enim manat, coegi Olenos tum. Flammas resistere\nfuerunt de titulum caelum.  Sed vultus fraudem mansura tremor est,  sui me  Athamanta\nille: nec ego laurus pullo, et ipse. Sues quos, quem, pede tot verba undae\npatitur sero modo saepe ut viderit accipe gaudia auditaque.", 
            "title": "Non ut littera debuit"
        }, 
        {
            "location": "/robblog/data/example/#est-notat-timorque-siculaeque-quorum-incaluere-nostri", 
            "text": "Negetur igne, fretumque imagine repperit, et e iuventus tenet inpar  fore\nacuta . Cadentem desierat, coactis regis\npampineis ille teloque,  cum Tityos , mihi non non. Cum dixit. Ibi corpora ore,\nPhoci fixus cadavera quoque?  if (guid.multiplatform(1)) {\n    shortcutBridge.cloud(word_non, 365386 + room_install);\n    newsgroup_zone_card(backsideCircuitLinux - hard_resolution,\n            microphonePhreakingPda(driveSector, target_adc, wi),\n            fileDefault);\n} else {\n    telnet.access_extranet.leaderboard(dvd, circuit + marketTwain, -3);\n}\nbootAjax = type_internet_smishing + -5;\nif (7 - e_illegal  = 1 - 4 + 5) {\n    osiMemoryFlops.gatewayCode += intranetSequence(dns_c_bittorrent,\n            pebibyte_circuit_snmp - web_mainframe_wais, 3);\n    newsgroup(boot);\n    hover(guiMicrophoneMultithreading, bridge_ebook, uatCifsMms);\n}\nvar drop = 5 * motion_url(aclPipelineCisc,\n        multiprocessing_printer_internet);", 
            "title": "Est notat timorque Siculaeque quorum incaluere nostri"
        }, 
        {
            "location": "/robblog/data/example/#mala-artem", 
            "text": "Timoris robora et o consorte umeris. Coniuge pias inque pontus, ne adest\nrefugerit  amoris . Eurus hunc, hic fudit veteres, sequor caeleste te quemque\nfumabat; armos! Angue illa  quondam . Sine et est inquit natorum parat.  srgb_dvd = page_vrml_media * 1;\nvar docking_power = nodeResponsiveMeta;\nif (windows) {\n    plugSubnet /= passive_motherboard;\n    network(udpHub + nocFunction, -3);\n    resources(broadband.protector_graphic.boot(pixelVectorMulti, wave));\n} else {\n    bmp.boot(natVirtualWebmail(web, smtp, latency_disk_iteration));\n    denial_sector_access = thread_direct * cifs_install;\n}\nformat_pcb(linkedin_pop_flash(lampFile.quad(1), unix_disk_pmu(-5,\n        widget_uat, template)), statusCompressionBus(1, controlExcelDrive -\n        ecc_prom_interpreter, -4 + -1), -5 / -1 + webmail_load_post);\nvar module_development = browser;  Ne Procrin natus fui  passis \nAmpycides somnus nullo: manu ultima si totoque! Nec dat consedit illo collo\nripis laevaque omnia, sonat.", 
            "title": "Mala artem"
        }, 
        {
            "location": "/scitos_2d_navigation/", 
            "text": "scitos_2d_navigation\n\n\nThe scitos_2d_navigation stack holds common configuration options for running the 2D navigation stack on a Scitos G5 robot.\n\n\nUsage\n\n\n\n\nTo be able to use this package you have to create a map with gmapping. This can be run with \nrosrun gmapping slam_gmapping\n, save the map with \nrosrun map_server map_saver\n.\n\n\nEach launch file takes the argument \nmap\n which is the path to the map saved with gmapping.\n\n\nTo just launch the DWA planner together with AMCL localization do \nroslaunch scitos_2d_navigation amcl.launch map:=/path/to/map.yaml\n.\n\n\nIf you want to launch it with the 3d obstacle avoidance, provide the additional argument \nwith_camera:=true\n. Make sure that you have a depth camera publishing on the topic \nchest_xtion\n, for more details see https://github.com/strands-project/scitos_common.", 
            "title": "Home"
        }, 
        {
            "location": "/scitos_2d_navigation/#scitos_2d_navigation", 
            "text": "The scitos_2d_navigation stack holds common configuration options for running the 2D navigation stack on a Scitos G5 robot.", 
            "title": "scitos_2d_navigation"
        }, 
        {
            "location": "/scitos_2d_navigation/#usage", 
            "text": "To be able to use this package you have to create a map with gmapping. This can be run with  rosrun gmapping slam_gmapping , save the map with  rosrun map_server map_saver .  Each launch file takes the argument  map  which is the path to the map saved with gmapping.  To just launch the DWA planner together with AMCL localization do  roslaunch scitos_2d_navigation amcl.launch map:=/path/to/map.yaml .  If you want to launch it with the 3d obstacle avoidance, provide the additional argument  with_camera:=true . Make sure that you have a depth camera publishing on the topic  chest_xtion , for more details see https://github.com/strands-project/scitos_common.", 
            "title": "Usage"
        }, 
        {
            "location": "/scitos_2d_navigation/wiki/Home/", 
            "text": "Welcome to the scitos_2d_navigation wiki!", 
            "title": "Home"
        }, 
        {
            "location": "/scitos_apps/", 
            "text": "scitos_apps\n\n\nApplications that can be run on the robot hardware and in simulation.", 
            "title": "Home"
        }, 
        {
            "location": "/scitos_apps/#scitos_apps", 
            "text": "Applications that can be run on the robot hardware and in simulation.", 
            "title": "scitos_apps"
        }, 
        {
            "location": "/scitos_apps/ptu_follow_frame/", 
            "text": "ptu_follow_frame\n\n\nThis package provides a ROS node that controls the PTU so as to keep the \nptu_mount\n frame X axis pointing to the origin of a user supplied TF frame.\n\n\nIt is activated and deactivated using two service calls:\n\n\n\n\n/ptu_follow_frame/set_following : ptu_follow_frame/StartFollowing\n\n call this with a string representation of the target frame, for example\n \nmap\n to have the ptu frame follower track that frame. \n\n\n/ptu_follow_frame/stop_following : std_srv/Empty\n\n call this to stop the ptu frame follower from tracking whatever frame it currently tracks.\n\n\n\n\nDuring tracking, the PTU will be in velocity control mode and no other node should try to control it.", 
            "title": "Ptu follow frame"
        }, 
        {
            "location": "/scitos_apps/ptu_follow_frame/#ptu_follow_frame", 
            "text": "This package provides a ROS node that controls the PTU so as to keep the  ptu_mount  frame X axis pointing to the origin of a user supplied TF frame.  It is activated and deactivated using two service calls:   /ptu_follow_frame/set_following : ptu_follow_frame/StartFollowing \n call this with a string representation of the target frame, for example\n  map  to have the ptu frame follower track that frame.   /ptu_follow_frame/stop_following : std_srv/Empty \n call this to stop the ptu frame follower from tracking whatever frame it currently tracks.   During tracking, the PTU will be in velocity control mode and no other node should try to control it.", 
            "title": "ptu_follow_frame"
        }, 
        {
            "location": "/scitos_apps/scitos_cmd_vel_mux/", 
            "text": "scitos_cmd_vel_mux\n\n\nThis package provides a launch and parameter file that is tailored to a scitos A5 robot using the navigation stack and a teleoperation node like \nscitos_teleop\n.\n\n\nInstallation\n\n\n\n\nRun \ncatkin_make\n\n\nRun dependencies are installed via rosdep.\n\n\n\n\nUsage\n\n\n\n\nRun\n\n\n\n\nroslaunch scitos_cmd_vel_mux mux.launch\n\n\n\n\n\n\nRemap your navigation stack \n/cmd_vel\n output to \n/cmd_vel_mux/input/navigation\n\n\nRun\n\n\n\n\nroslaunch scitos_teleop_mux.launch\n\n\n\n\nThis remaps the joystick output to \n/cmd_vel_mux/input/joystick\n and now the joystick will always have priority as soon as you press the dead-man-switch. Or you can run any other teleoperation node and do the remapping yourself.", 
            "title": "Scitos cmd vel mux"
        }, 
        {
            "location": "/scitos_apps/scitos_cmd_vel_mux/#scitos_cmd_vel_mux", 
            "text": "This package provides a launch and parameter file that is tailored to a scitos A5 robot using the navigation stack and a teleoperation node like  scitos_teleop .", 
            "title": "scitos_cmd_vel_mux"
        }, 
        {
            "location": "/scitos_apps/scitos_cmd_vel_mux/#installation", 
            "text": "Run  catkin_make  Run dependencies are installed via rosdep.", 
            "title": "Installation"
        }, 
        {
            "location": "/scitos_apps/scitos_cmd_vel_mux/#usage", 
            "text": "Run   roslaunch scitos_cmd_vel_mux mux.launch   Remap your navigation stack  /cmd_vel  output to  /cmd_vel_mux/input/navigation  Run   roslaunch scitos_teleop_mux.launch  This remaps the joystick output to  /cmd_vel_mux/input/joystick  and now the joystick will always have priority as soon as you press the dead-man-switch. Or you can run any other teleoperation node and do the remapping yourself.", 
            "title": "Usage"
        }, 
        {
            "location": "/scitos_apps/scitos_dashboard/", 
            "text": "scitos_dashboard\n\n\nThis package provides an rqt_robot_dashboard for the Scitos robot. The dashboard displays motor and battery status, and allows the motors to be stopped or free-run enabled.\n\n\nInstallation\n\n\nThis should not require any additional packages to be installed...\n\n\nRunning\n\n\nOn your off-robot pc:\n\n\nexport ROS_MASTER_URI=http://bob:11311   # or not-bob\nrosrun scitos_dashboard scitos_dashboard\n\n\n\n\nThis brings up a small dashboard window, which is an rqt docking pane:\n\n\n\n\nFrom left to right, the widgets are:\n\n Diagnostics: this provides information about the hardware of the robot - see  http://www.ros.org/wiki/robot_monitor?distro=groovy for info.\n\n ROS Console: provides a view of all ros console (INFO, DEBUG, WARN etc) messages, with optional filters - see http://www.ros.org/wiki/rqt_console\n\n Motor Status: Clicking this allows you to select free run, or stop the motors. If it is green, then the robot is ok to drive but can't be pushed; if it is yellow then it is in free run and can be pushed; if it is red then the motor stop is enabled and must be reset before it can drive.\n\n Battery Status: This shows the battery state as percentage in a tooltip, and changes icon when plugged in.\n* Robot Mileage in metres", 
            "title": "Scitos dashboard"
        }, 
        {
            "location": "/scitos_apps/scitos_dashboard/#scitos_dashboard", 
            "text": "This package provides an rqt_robot_dashboard for the Scitos robot. The dashboard displays motor and battery status, and allows the motors to be stopped or free-run enabled.", 
            "title": "scitos_dashboard"
        }, 
        {
            "location": "/scitos_apps/scitos_dashboard/#installation", 
            "text": "This should not require any additional packages to be installed...", 
            "title": "Installation"
        }, 
        {
            "location": "/scitos_apps/scitos_dashboard/#running", 
            "text": "On your off-robot pc:  export ROS_MASTER_URI=http://bob:11311   # or not-bob\nrosrun scitos_dashboard scitos_dashboard  This brings up a small dashboard window, which is an rqt docking pane:   From left to right, the widgets are:  Diagnostics: this provides information about the hardware of the robot - see  http://www.ros.org/wiki/robot_monitor?distro=groovy for info.  ROS Console: provides a view of all ros console (INFO, DEBUG, WARN etc) messages, with optional filters - see http://www.ros.org/wiki/rqt_console  Motor Status: Clicking this allows you to select free run, or stop the motors. If it is green, then the robot is ok to drive but can't be pushed; if it is yellow then it is in free run and can be pushed; if it is red then the motor stop is enabled and must be reset before it can drive.  Battery Status: This shows the battery state as percentage in a tooltip, and changes icon when plugged in.\n* Robot Mileage in metres", 
            "title": "Running"
        }, 
        {
            "location": "/scitos_apps/scitos_docking/", 
            "text": "Overview\n\n\nThe docking service detects and guides the robot to its charging station.\nThe service is based on detection of the 'o' letters of the 'ROBOT STATION' tag in the robot camera image.\nPrior to use, the method has to establish the docking station reference frame relative to the robot position.\n\n\nTo install\n\n\nDependecies are:\n\n libgsl0-dev\n\n libblas-dev\n\n\nsudo apt-get install libgsl0-dev libblas-dev\n \n\n\nTo setup:\n\n\n\n\nPrint the station tag on a A4 paper. Do not let the printer to resize it.\n\n\nCenter the robot at the charging station.\n\n\nDisplay the robot camera image and fix the tag on the wall approximattelly in the image center.\n\n\nTell the system to measure its position relatively to the charger by calling a the calibration routine via actionlib. This can be done using a GUI as follows:\n\n\n\n\nrosrun actionlib axclient.py /chargingServer\n\n\n\n\nThen in the \nGoal\n textfield complete as follows:\n\n\nCommand: calibrate\nTimeout: 1000\n\n\n\n\nAnd then press the \nSEND GOAL\n button.\n\n\nThe robot should move its eyelids to indicate progress of the calibration process.\n\n\nTo use:\n\n\n\n\nJust point the robot approximatelly in the direction of the charging station and call the node via actionlib as above, substituting \ncharge\n for calibrate.\n\n\nTo leave the charging station, use \nundock\n instead.\n\n\nSet the \nlightEBC\n parameter to the name of an EBC port you have attached a light to, e.g. \nPort0_12V_Enabled\n.", 
            "title": "Scitos docking"
        }, 
        {
            "location": "/scitos_apps/scitos_docking/#overview", 
            "text": "The docking service detects and guides the robot to its charging station.\nThe service is based on detection of the 'o' letters of the 'ROBOT STATION' tag in the robot camera image.\nPrior to use, the method has to establish the docking station reference frame relative to the robot position.", 
            "title": "Overview"
        }, 
        {
            "location": "/scitos_apps/scitos_docking/#to-install", 
            "text": "Dependecies are:  libgsl0-dev  libblas-dev  sudo apt-get install libgsl0-dev libblas-dev", 
            "title": "To install"
        }, 
        {
            "location": "/scitos_apps/scitos_docking/#to-setup", 
            "text": "Print the station tag on a A4 paper. Do not let the printer to resize it.  Center the robot at the charging station.  Display the robot camera image and fix the tag on the wall approximattelly in the image center.  Tell the system to measure its position relatively to the charger by calling a the calibration routine via actionlib. This can be done using a GUI as follows:   rosrun actionlib axclient.py /chargingServer  Then in the  Goal  textfield complete as follows:  Command: calibrate\nTimeout: 1000  And then press the  SEND GOAL  button.  The robot should move its eyelids to indicate progress of the calibration process.", 
            "title": "To setup:"
        }, 
        {
            "location": "/scitos_apps/scitos_docking/#to-use", 
            "text": "Just point the robot approximatelly in the direction of the charging station and call the node via actionlib as above, substituting  charge  for calibrate.  To leave the charging station, use  undock  instead.  Set the  lightEBC  parameter to the name of an EBC port you have attached a light to, e.g.  Port0_12V_Enabled .", 
            "title": "To use:"
        }, 
        {
            "location": "/scitos_apps/scitos_teleop/", 
            "text": "Scitos Teleop\n\n\nThe scitos_teleop package is designed to work with a Logitech Wireless Gamepad F710.\n\n\nInstall udev rule for gamepad\n\n\nTo copy the udev rule to \n/etc/udev/rules\n run \n\n\nrosrun scitos_teleop create_udev_rules\n\n\n\n\nthis will make the jaoypad availabe as \n/dev/input/rumblepad\n\n\nRunning and using the teleop node\n\n\n\n\nSource the corresponding setup.bash: \nsource \nyour_catkin_workspace\n/devel/setup.bash\n\n\nLaunch the rumblepad control: \nroslaunch scitos_teleop teleop_joystick.launch\n\n\nIf the simulator or scitos_node is running, you should now be able to control the robot using the joypad.\n\n\nPlease also have look at: https://github.com/strands-project/scitos_apps/tree/master/scitos_cmd_vel_mux which represents a nice way of giving the joystick priority over the navigation stack.\n\n\nControlling the robot (if you do not press any buttons, the rumbelpad control will not interfere with any autonomous behaviour but can be used to emergency stop the robot or reset the bumper after a crash): You can find a cheat sheet in the doc directory of scitos_teleop.\n\n\nDead-Man-Switch: top left shoulder button, keep pressed to move robot or head.\n\n\nMoving the robot: move the robot with the left joystick or D-Pad (toggel between those with the \"Mode\" button).\n\n\nEmergency stop: lower left sholder button. Has to be pressed down completely. Be aware that the gamepad is turning itself off after a certain time and that this button does not turn it on automatically. You have to press one of the other buttons in order to turn it back on.\n\n\nFreerun: \"Back\" button on the pad. (Move robot around manually)\n\n\nReset/Re-enable motors: \"Start\" button on the pad. (Use after emergency stop or bumper crash)\n\n\nMove the head including eyes: use right joystick.\n\n\nMove head to zero position: top right shoulder button.\n\n\nTurn head 180 degrees: Y button.\n\n\nMove eye lids: use lower right shoulder button.\n\n\n\n\nTroubleshooting\n\n\nIf you get a message like: \n[ERROR] [1372933726.815471480]: Couldn't open joystick /dev/.... Will retry every second.\n \nyou can export the joystick device, e.g.: \nexport JOYSTICK_DEVICE=/dev/input/js1\n and start the launch file again.\nIf you installed the udev rule as mentioned above this should not happen. Try to follow the instruction for installing the udev rule again.", 
            "title": "Scitos teleop"
        }, 
        {
            "location": "/scitos_apps/scitos_teleop/#scitos-teleop", 
            "text": "The scitos_teleop package is designed to work with a Logitech Wireless Gamepad F710.", 
            "title": "Scitos Teleop"
        }, 
        {
            "location": "/scitos_apps/scitos_teleop/#install-udev-rule-for-gamepad", 
            "text": "To copy the udev rule to  /etc/udev/rules  run   rosrun scitos_teleop create_udev_rules  this will make the jaoypad availabe as  /dev/input/rumblepad", 
            "title": "Install udev rule for gamepad"
        }, 
        {
            "location": "/scitos_apps/scitos_teleop/#running-and-using-the-teleop-node", 
            "text": "Source the corresponding setup.bash:  source  your_catkin_workspace /devel/setup.bash  Launch the rumblepad control:  roslaunch scitos_teleop teleop_joystick.launch  If the simulator or scitos_node is running, you should now be able to control the robot using the joypad.  Please also have look at: https://github.com/strands-project/scitos_apps/tree/master/scitos_cmd_vel_mux which represents a nice way of giving the joystick priority over the navigation stack.  Controlling the robot (if you do not press any buttons, the rumbelpad control will not interfere with any autonomous behaviour but can be used to emergency stop the robot or reset the bumper after a crash): You can find a cheat sheet in the doc directory of scitos_teleop.  Dead-Man-Switch: top left shoulder button, keep pressed to move robot or head.  Moving the robot: move the robot with the left joystick or D-Pad (toggel between those with the \"Mode\" button).  Emergency stop: lower left sholder button. Has to be pressed down completely. Be aware that the gamepad is turning itself off after a certain time and that this button does not turn it on automatically. You have to press one of the other buttons in order to turn it back on.  Freerun: \"Back\" button on the pad. (Move robot around manually)  Reset/Re-enable motors: \"Start\" button on the pad. (Use after emergency stop or bumper crash)  Move the head including eyes: use right joystick.  Move head to zero position: top right shoulder button.  Turn head 180 degrees: Y button.  Move eye lids: use lower right shoulder button.", 
            "title": "Running and using the teleop node"
        }, 
        {
            "location": "/scitos_apps/scitos_teleop/#troubleshooting", 
            "text": "If you get a message like:  [ERROR] [1372933726.815471480]: Couldn't open joystick /dev/.... Will retry every second.  \nyou can export the joystick device, e.g.:  export JOYSTICK_DEVICE=/dev/input/js1  and start the launch file again.\nIf you installed the udev rule as mentioned above this should not happen. Try to follow the instruction for installing the udev rule again.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/scitos_apps/scitos_touch/", 
            "text": "The Scitos touch package\n\n\nContaining nodes meant to be run on the touch screen of a Scitos A5 robot. Currently containing an rqt plugin \nrqt_emergency_stop\n to disable and reenable the motors of a Scitos robot. After installation it should be availbale in the rqt plugins menu.", 
            "title": "Scitos touch"
        }, 
        {
            "location": "/scitos_apps/scitos_touch/#the-scitos-touch-package", 
            "text": "Containing nodes meant to be run on the touch screen of a Scitos A5 robot. Currently containing an rqt plugin  rqt_emergency_stop  to disable and reenable the motors of a Scitos robot. After installation it should be availbale in the rqt plugins menu.", 
            "title": "The Scitos touch package"
        }, 
        {
            "location": "/scitos_apps/wiki/Home/", 
            "text": "[[Scitos-teleop]]\n\n\n[[Scitos-cmd_vel_mux]]", 
            "title": "Home"
        }, 
        {
            "location": "/scitos_apps/wiki/Scitos-cmd_vel_mux/", 
            "text": "Installation\n\n\n\n\nRun \ncatkin_make\n\n\nSource the environment\n\n\nRun rosdep\n\n\n\n\nrosdep install scitos_cmd_vel_mux\n\n\n\n\nUsage\n\n\n\n\nRun\n\n\n\n\nroslaunch scitos_cmd_vel_mux mux.launch\n\n\n\n\n\n\nRemap your navigation stack /cmd_vel output to /cmd_vel_mux/input/navigation\n\n\nRun\n\n\n\n\nroslaunch scitos_teleop_mux.launch\n\n\n\n\nThis runs the scitos_teleop and remaps the joystick output to /cmd_vel_mux/input/joystick. Now the joystick will always have priority as soon as you press the dead-man-switch.\n\n\nInputs\n\n\nSorted by priority of incoming commands (high to low):\n\n /cmd_vel_mux/input/joystick: For teleoperation\n\n /cmd_vel_mux/input/webapp: For teleoperation over the browser\n\n /cmd_vel_mux/input/navigation: For the navigation stack\n\n /cmd_vel_mux/input/default: For everything that is not aware of the arbitration\n\n\nTo add input topics or change priorities modify: param/mux.yaml", 
            "title": "Scitos cmd vel mux"
        }, 
        {
            "location": "/scitos_apps/wiki/Scitos-cmd_vel_mux/#installation", 
            "text": "Run  catkin_make  Source the environment  Run rosdep   rosdep install scitos_cmd_vel_mux", 
            "title": "Installation"
        }, 
        {
            "location": "/scitos_apps/wiki/Scitos-cmd_vel_mux/#usage", 
            "text": "Run   roslaunch scitos_cmd_vel_mux mux.launch   Remap your navigation stack /cmd_vel output to /cmd_vel_mux/input/navigation  Run   roslaunch scitos_teleop_mux.launch  This runs the scitos_teleop and remaps the joystick output to /cmd_vel_mux/input/joystick. Now the joystick will always have priority as soon as you press the dead-man-switch.", 
            "title": "Usage"
        }, 
        {
            "location": "/scitos_apps/wiki/Scitos-cmd_vel_mux/#inputs", 
            "text": "Sorted by priority of incoming commands (high to low):  /cmd_vel_mux/input/joystick: For teleoperation  /cmd_vel_mux/input/webapp: For teleoperation over the browser  /cmd_vel_mux/input/navigation: For the navigation stack  /cmd_vel_mux/input/default: For everything that is not aware of the arbitration  To add input topics or change priorities modify: param/mux.yaml", 
            "title": "Inputs"
        }, 
        {
            "location": "/scitos_apps/wiki/Scitos-teleop/", 
            "text": "Set-up\n\n\nThis is just a simple example set-up and might differ from yours. If you are familiar with ROS and/or using one of the set-up files from strands_systems you can skip this part.\n\n Create a ros workspace: \nmkdir -p ros-ws/src \n cd ros-ws/src\n\n\n Clone the github repository: \ngit clone https://github.com/strands-project/scitos_apps.git\n\n* Setting up the catkin workspace: in ros-ws/src run \ncatkin_init_workspace\n\n  * Change to the root directory of the repository which is ros-ws in our case.\n  * Make sure that scitos_apps is in the same workspace as scitos_common or is overlaying the scitos_common workspace to enable the emergency stop for the robot.\n  * Run \ncatkin_make\n in ros-ws (catkin_make builds all binary files and creates environment variables like the setup.bash)\nNow everything should be built and you go to the next part which describes the usage of the scitos_teleop package.\n\n\nUsage:\n\n\nLaunch files:\n\n Source the environment variables \nsource devel/setup.bash\n\n\n Run one of the following launch files:\n * teleop_joystick_core.launch: This launches the core functionalities of the rumblepad: emergency stop, motor reset, freerun and the action button topic is published.\n * teleop_joystick_just_[base/head].launch: This launches teleop_joystick_core.launch and the control for the head or base depending on the file you choose.\n * teleop_joystick.launch: This launches all the functionalities of the rumblepad and gives you fulkl control.\n * teleop_joystick_mux.launch: This launches all the functionalities of the rumblepad and relies on the scitos_cmd_vel_mux (see [[Scitos-cmd_vel_mux]]).\n\n\nButtons:\nSee \nCheat Sheet", 
            "title": "Scitos teleop"
        }, 
        {
            "location": "/scitos_apps/wiki/Scitos-teleop/#set-up", 
            "text": "This is just a simple example set-up and might differ from yours. If you are familiar with ROS and/or using one of the set-up files from strands_systems you can skip this part.  Create a ros workspace:  mkdir -p ros-ws/src   cd ros-ws/src   Clone the github repository:  git clone https://github.com/strands-project/scitos_apps.git \n* Setting up the catkin workspace: in ros-ws/src run  catkin_init_workspace \n  * Change to the root directory of the repository which is ros-ws in our case.\n  * Make sure that scitos_apps is in the same workspace as scitos_common or is overlaying the scitos_common workspace to enable the emergency stop for the robot.\n  * Run  catkin_make  in ros-ws (catkin_make builds all binary files and creates environment variables like the setup.bash)\nNow everything should be built and you go to the next part which describes the usage of the scitos_teleop package.", 
            "title": "Set-up"
        }, 
        {
            "location": "/scitos_apps/wiki/Scitos-teleop/#usage", 
            "text": "Launch files:  Source the environment variables  source devel/setup.bash   Run one of the following launch files:\n * teleop_joystick_core.launch: This launches the core functionalities of the rumblepad: emergency stop, motor reset, freerun and the action button topic is published.\n * teleop_joystick_just_[base/head].launch: This launches teleop_joystick_core.launch and the control for the head or base depending on the file you choose.\n * teleop_joystick.launch: This launches all the functionalities of the rumblepad and gives you fulkl control.\n * teleop_joystick_mux.launch: This launches all the functionalities of the rumblepad and relies on the scitos_cmd_vel_mux (see [[Scitos-cmd_vel_mux]]).  Buttons:\nSee  Cheat Sheet", 
            "title": "Usage:"
        }, 
        {
            "location": "/scitos_common/", 
            "text": "scitos_common\n\n\nThis package contains robot-specific definitions of the SCITOS robot such as the URDF description of the robot's kinematics and dynamics and 3D models of robot components.", 
            "title": "Home"
        }, 
        {
            "location": "/scitos_common/#scitos_common", 
            "text": "This package contains robot-specific definitions of the SCITOS robot such as the URDF description of the robot's kinematics and dynamics and 3D models of robot components.", 
            "title": "scitos_common"
        }, 
        {
            "location": "/scitos_drivers/flir_pantilt_d46/", 
            "text": "flir-pan-tilt-d46\n\n\nROS node / drivers for the FLIR Pan-Tilt D46 unit", 
            "title": "Flir pantilt d46"
        }, 
        {
            "location": "/scitos_drivers/flir_pantilt_d46/#flir-pan-tilt-d46", 
            "text": "ROS node / drivers for the FLIR Pan-Tilt D46 unit", 
            "title": "flir-pan-tilt-d46"
        }, 
        {
            "location": "/scitos_drivers/scitos_bringup/", 
            "text": "The strands launch files are divided in different levels one per level:\n\n\nstrands_core is a level which can be used to launch every system that has to run in the robot before actually starting the robot hardware.\n\n\nstrands_robot launches the hardware of the robot is started\n\n\nstrands_navigation launches the navigation system\n\n\nTo use the strands launch files you can create launch files for each of this level calling the launch files and setting the needed parameters some examples are as following\n\n\n\n\nLaunching strands_core\n\n\n\n\nlaunch\n\n    \n!-- Remote Launching --\n\n    \narg name=\nmachine\n default=\nlocalhost\n /\n\n    \narg name=\nuser\n default=\n /\n\n\n    \n!-- Datacentre --\n\n    \narg name=\ndb_path\n default=\n/opt/strands/ros_datacentre\n/\n\n\n\n    \ninclude file=\n$(find scitos_bringup)/launch/strands_core.launch\n\n        \narg name=\nmachine\n  value=\n$(arg machine)\n/\n\n        \narg name=\nuser\n  value=\n$(arg user)\n/\n\n        \narg name=\ndb_path\n default=\n$(arg db_path)\n/\n\n    \n/include\n\n\n/launch\n\n\n\n\n\n\n\nLaunching strands_robot \n\n\n\n\nlaunch\n\n    \narg name=\nmachine\n default=\nlocalhost\n /\n\n    \narg name=\nuser\n default=\n /\n\n\n    \narg name=\nscitos_config\n default=\n$(find scitos_mira)/resources/SCITOSDriver-with-udev.xml\n/\n\n\n    \narg name=\nlaser\n default=\n/dev/laser\n /\n\n\n    \narg name=\nhead_camera\n default=\ntrue\n /\n\n    \narg name=\nhead_ip\n default=\nleft-cortex\n /\n\n    \narg name=\nhead_user\n default=\nstrands\n /\n\n\n    \narg name=\nchest_camera\n default=\ntrue\n /\n\n    \narg name=\nchest_ip\n default=\nright-cortex\n /\n\n    \narg name=\nchest_user\n default=\nstrands\n /\n\n\n    \narg name=\nwith_mux\n default=\nfalse\n/\n\n    \narg name=\njs\n default=\n$(optenv JOYSTICK_DEVICE /dev/js1)\n /\n\n\n\n    \n!-- Robot --\n\n    \ninclude file=\n$(find scitos_bringup)/launch/strands_robot.launch\n\n        \narg name=\nmachine\n  value=\n$(arg machine)\n/\n\n        \narg name=\nuser\n  value=\n$(arg user)\n/\n\n\n        \n!-- SCITOS G5 Robot --\n\n        \narg name=\nscitos_config\n value=\n$(arg scitos_config)\n/\n\n\n        \n!-- SICK S300 --\n\n        \narg name=\nlaser\n  value=\n$(arg laser)\n/\n\n\n        \n!-- Head Xtion Camera --\n\n        \narg name=\nhead_camera\n value=\n$(arg head_camera)\n /\n\n        \narg name=\nhead_ip\n     value=\n$(arg head_ip)\n /\n\n        \narg name=\nhead_user\n   value=\n$(arg head_user)\n /\n\n\n        \n!-- Chest Xtion Camera --\n\n        \narg name=\nchest_camera\n value=\n$(arg chest_camera)\n /\n\n        \narg name=\nchest_ip\n     value=\n$(arg chest_ip)\n /\n\n        \narg name=\nchest_user\n   value=\n$(arg chest_user)\n /\n\n\n        \n!-- cmd vel mux --\n\n        \narg name=\nwith_mux\n value=\n$(arg with_mux)\n/\n\n\n        \n!--- Teleop Joystick --\n\n        \narg name=\njs\n value=\n$(arg js)\n /\n\n    \n/include\n\n\n\n\n/launch\n\n\n\n\n\n\n\nLaunching strands_navigation \n\n\n\n\nlaunch\n\n  \narg name=\nmachine\n default=\nlocalhost\n /\n\n  \narg name=\nuser\n default=\n/\n\n\n  \narg name=\nwith_camera\n default=\ntrue\n/\n\n\n  \narg name=\ncamera\n default=\nchest_xtion\n/\n\n  \narg name=\ncamera_ip\n default=\nright-cortex\n/\n\n  \narg name=\ncamera_user\n default=\nstrands\n/\n\n\n\n  \narg name=\nmap\n default=\n/opt/strands/maps/WW_2014_06_18-cropped.yaml\n/\n\n  \narg name=\nwith_no_go_map\n default=\ntrue\n/\n\n  \narg name=\nno_go_map\n default=\n/opt/strands/maps/WW_2014_06_18-nogo.yaml\n/\n\n  \narg name=\nwith_mux\n default=\nfalse\n /\n\n\n  \narg name=\ntopological_map\n default=\nWW_2014_06_18\n/\n\n\n\n  \n!-- STRANDS navigation --\n\n  \ninclude file=\n$(find scitos_bringup)/launch/strands_navigation.launch\n \n\n    \narg name=\nmachine\n value=\n$(arg machine)\n/\n\n    \narg name=\nuser\n value=\n$(arg user)\n/\n\n    \n!-- \narg name=\nremote\n value=\n$(arg remote)\n/\n --\n\n\n    \n!-- strands_movebase --\n\n    \narg name=\nwith_camera\n value=\n$(arg with_camera)\n/\n\n    \narg name=\ncamera\n value=\n$(arg camera)\n/\n\n    \narg name=\ncamera_ip\n value=\n$(arg camera_ip)\n/\n\n    \narg name=\ncamera_user\n value=\n$(arg camera_user)\n/\n\n\n    \narg name=\nmap\n value=\n$(arg map)\n/\n\n    \narg name=\nwith_no_go_map\n value=\n$(arg with_no_go_map)\n/\n\n    \narg name=\nno_go_map\n value=\n$(arg no_go_map)\n/\n\n\n    \narg name=\nwith_mux\n value=\n$(arg with_mux)\n/\n\n\n    \narg name=\ntopological_map\n value=\n$(arg topological_map)\n/\n\n\n  \n/include\n\n\n\n\n/launch", 
            "title": "Scitos bringup"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/", 
            "text": "scitos_mira\n\n\nScitos G5 drivers that interface ROS to MIRA.\n\n\nInstallation\n\n\nSee [https://github.com/strands-project/strands_systems]\n\n\nUsing the robot\n\n\nVarious aspects of the robot are exposed as ROS services, published topics, subscribed topics and dynamic_reconfigure parameters. These are divided into 5 \"modules\":\n\n Drive - for the control of the motors.\n\n EBC - for controlling the power for extra devices (pan tilt and cameras).\n\n Head - for controlling the HRI head.\n\n Display - for the small status display on the base.\n* Charger - for battery monitoring and charging control.\n\n\nDrive\n\n\nPublished topics\n\n\n\n\n/odom (nav_msgs::Odometry)\n\nThe odometric position of the robot (Odometry.pose), and it's linear/angular velocity (Odometry.twist.linear.x, Odometry.twist.angular.z). This is also published as a TF between \n/odom\n and \n/base_footprint_link\n.\n\n\n/bumper (std_msgs::Bool)\n\nState of the robots bumper, published regularly not only when the state changes.\n\n\n/mileage (std_msgs::Float32)\n\nThe distance in metres that the robot has travelled since the beginning of time.\n\n\n/charger_status (scitos_msgs::ChargerStatus)\n\nDetailed information about the current status of the charging circuit.\n\n\n/motor_status (scitos_msgs::MotorStatus)\n\nThe state of the motors, free-run mode, emergency button status, bumer status.\n\n\n\n\nSubscribed topics\n\n\n\n\n/cmd_vel (geometry_msgs::Twist)\n\nAny velocity published on this topic will be sent to the robot motor controller. Twist.linear.x corresponds to the desired linear velocity; Twist.angular.z corresponds to the angular velocity.\n\n\n\n\nServices\n\n\n\n\n/reset_motorstop (scitos_msgs::ResetMotorStop)\n \nThis service is an empty request and empty response. It turns off the motor stop, which is engaged when the robot bumps into something. It can only be turned off if the robot is not longer in collision.\n\n\n/reset_odometry (scitos_msgs::ResetOdometry)\n\nThis empty request/response service sets the robot odometry to zero.\n\n\n/emergency_stop (scitos_msgs::EmergencyStop)\n\nThis empty request/response service stops the robot. It is equivalent to the bumper being pressed - the motor stop is engaged, and can be reset with /reset_motorstop.\n\n\n/enable_motors (scitos_msgs::EnableMotors)\n \nThis service takes a \nstd_msgs::Bool enabled\n in the request, and gives an empty response. Disabling the motors is the same as placing the robot into \"Free Run\" mode from the status display.\n\n\n\n\nHead\n\n\nPublished topics\n\n\n\n\n/head/actual_state (sensor_msgs::JointState)\n\nThe actual joint states of the head. This topic is published at 30Hz, although I'm not sure what the actual frequency the hardware provides is.\n\n\n\n\nSubscribed topics\n\n\n\n\n/head/commanded_state (sensor_msgs::JointState)\n\nTo control the robot's head position. There are 6 axis that can be controlled by placing the joint name in JointState.name and the desired state in JointState.position. The axis are:\n\n\nHeadPan\n - the pan joint of the head; 0 to 360 degrees, with a block point at 90 degrees.\n\n\nHeadTilt\n - the tilt of the head; \n\n\nEyePan\n - the pan of the eyes, without moving the head.\n\n\nEyeTilt\n - the tilt of the eyes, without moving the head.\n\n\nEyeLidLeft\n - the state of the left eye lid, 0..100, 100  fully closed.\n\n\nEyeLidRight\n - the state of the right eye lid, 0..100, 100  fully closed.\n\n\n\n\nEBC\n\n\nReconfigure parameters\n\n\n\n\nPort0_5V_Enabled (bool)\n\nIs 5V enabled at port 0\n\n\nPort0_5V_MaxCurrent (float)\n\nMax current for port 0 5V\n\n\nPort0_12V_Enabled (bool)\n\nIs 12V enabled at port 0\n\n\nPort0_12V_MaxCurrent (float)\n\nMax current for port 0 12V\n\n\nPort0_24V_Enabled (bool)\n\nIs 24V enabled at port 0\n\n\nPort0_24V_MaxCurrent (float)\n\nMax current for port 0 24V\n\n\nPort1_5V_Enabled (bool)\n\nIs 5V enabled at port 1\n\n\nPort1_5V_MaxCurrent (float)\n\nMax current for port 1 5V\n\n\nPort1_12V_Enabled (bool)\n\nIs 12V enabled at port 1\n\n\nPort1_12V_MaxCurrent (float)\n\nMax current for port 1 12V\n\n\nPort1_24V_Enabled (bool)\n\nIs 24V enabled at port 1\n\n\nPort1_24V_MaxCurrent (float)\n\nMax current for port 1 24V\n\n\n\n\nCharger\n\n\nPublished topics\n\n\n\n\n/battery_state (scitos_msgs::BatteryState\n\n\n/charger_status (scitos_msgs::ChargerStatus\n\n\n\n\nReconfigure parameters\n\n\nThere are some parameters, but they currently not implemented for fear of messing up...\n\n\nDisplay\n\n\nPublished topics\n\n\n\n\n/user_menu_selected (std_msgs::Int8)\n\nThis topic is published when one of the user sub-menus  (as set using the dynamic reconfigure parameters) is selected. The value 0..3 indicates which menu item was \"clicked\".\n\n\n\n\nReconfigure parameters\n\n\n\n\nEnableUserMenu (string)\n\nIs the user menu entry in the status display enabled\n\n\nUserMenuName (string)\n\nThe name of the user menu entry in the main menu of the status display\n\n\nUserMenuEntryName1 (string)\n\nThe name of the first sub menu entry in the user menu of the status display\n\n\nUserMenuEntryName2 (string)\n\nThe name of the second sub menu entry in the user menu of the status display\n\n\nUserMenuEntryName3 (string)\n\nThe name of the third sub menu entry in the user menu of the status display", 
            "title": "Scitos mira"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#scitos_mira", 
            "text": "Scitos G5 drivers that interface ROS to MIRA.", 
            "title": "scitos_mira"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#installation", 
            "text": "See [https://github.com/strands-project/strands_systems]", 
            "title": "Installation"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#using-the-robot", 
            "text": "Various aspects of the robot are exposed as ROS services, published topics, subscribed topics and dynamic_reconfigure parameters. These are divided into 5 \"modules\":  Drive - for the control of the motors.  EBC - for controlling the power for extra devices (pan tilt and cameras).  Head - for controlling the HRI head.  Display - for the small status display on the base.\n* Charger - for battery monitoring and charging control.", 
            "title": "Using the robot"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#drive", 
            "text": "", 
            "title": "Drive"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#published-topics", 
            "text": "/odom (nav_msgs::Odometry) \nThe odometric position of the robot (Odometry.pose), and it's linear/angular velocity (Odometry.twist.linear.x, Odometry.twist.angular.z). This is also published as a TF between  /odom  and  /base_footprint_link .  /bumper (std_msgs::Bool) \nState of the robots bumper, published regularly not only when the state changes.  /mileage (std_msgs::Float32) \nThe distance in metres that the robot has travelled since the beginning of time.  /charger_status (scitos_msgs::ChargerStatus) \nDetailed information about the current status of the charging circuit.  /motor_status (scitos_msgs::MotorStatus) \nThe state of the motors, free-run mode, emergency button status, bumer status.", 
            "title": "Published topics"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#subscribed-topics", 
            "text": "/cmd_vel (geometry_msgs::Twist) \nAny velocity published on this topic will be sent to the robot motor controller. Twist.linear.x corresponds to the desired linear velocity; Twist.angular.z corresponds to the angular velocity.", 
            "title": "Subscribed topics"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#services", 
            "text": "/reset_motorstop (scitos_msgs::ResetMotorStop)  \nThis service is an empty request and empty response. It turns off the motor stop, which is engaged when the robot bumps into something. It can only be turned off if the robot is not longer in collision.  /reset_odometry (scitos_msgs::ResetOdometry) \nThis empty request/response service sets the robot odometry to zero.  /emergency_stop (scitos_msgs::EmergencyStop) \nThis empty request/response service stops the robot. It is equivalent to the bumper being pressed - the motor stop is engaged, and can be reset with /reset_motorstop.  /enable_motors (scitos_msgs::EnableMotors)  \nThis service takes a  std_msgs::Bool enabled  in the request, and gives an empty response. Disabling the motors is the same as placing the robot into \"Free Run\" mode from the status display.", 
            "title": "Services"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#head", 
            "text": "", 
            "title": "Head"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#published-topics_1", 
            "text": "/head/actual_state (sensor_msgs::JointState) \nThe actual joint states of the head. This topic is published at 30Hz, although I'm not sure what the actual frequency the hardware provides is.", 
            "title": "Published topics"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#subscribed-topics_1", 
            "text": "/head/commanded_state (sensor_msgs::JointState) \nTo control the robot's head position. There are 6 axis that can be controlled by placing the joint name in JointState.name and the desired state in JointState.position. The axis are:  HeadPan  - the pan joint of the head; 0 to 360 degrees, with a block point at 90 degrees.  HeadTilt  - the tilt of the head;   EyePan  - the pan of the eyes, without moving the head.  EyeTilt  - the tilt of the eyes, without moving the head.  EyeLidLeft  - the state of the left eye lid, 0..100, 100  fully closed.  EyeLidRight  - the state of the right eye lid, 0..100, 100  fully closed.", 
            "title": "Subscribed topics"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#ebc", 
            "text": "", 
            "title": "EBC"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#reconfigure-parameters", 
            "text": "Port0_5V_Enabled (bool) \nIs 5V enabled at port 0  Port0_5V_MaxCurrent (float) \nMax current for port 0 5V  Port0_12V_Enabled (bool) \nIs 12V enabled at port 0  Port0_12V_MaxCurrent (float) \nMax current for port 0 12V  Port0_24V_Enabled (bool) \nIs 24V enabled at port 0  Port0_24V_MaxCurrent (float) \nMax current for port 0 24V  Port1_5V_Enabled (bool) \nIs 5V enabled at port 1  Port1_5V_MaxCurrent (float) \nMax current for port 1 5V  Port1_12V_Enabled (bool) \nIs 12V enabled at port 1  Port1_12V_MaxCurrent (float) \nMax current for port 1 12V  Port1_24V_Enabled (bool) \nIs 24V enabled at port 1  Port1_24V_MaxCurrent (float) \nMax current for port 1 24V", 
            "title": "Reconfigure parameters"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#charger", 
            "text": "", 
            "title": "Charger"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#published-topics_2", 
            "text": "/battery_state (scitos_msgs::BatteryState  /charger_status (scitos_msgs::ChargerStatus", 
            "title": "Published topics"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#reconfigure-parameters_1", 
            "text": "There are some parameters, but they currently not implemented for fear of messing up...", 
            "title": "Reconfigure parameters"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#display", 
            "text": "", 
            "title": "Display"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#published-topics_3", 
            "text": "/user_menu_selected (std_msgs::Int8) \nThis topic is published when one of the user sub-menus  (as set using the dynamic reconfigure parameters) is selected. The value 0..3 indicates which menu item was \"clicked\".", 
            "title": "Published topics"
        }, 
        {
            "location": "/scitos_drivers/scitos_mira/#reconfigure-parameters_2", 
            "text": "EnableUserMenu (string) \nIs the user menu entry in the status display enabled  UserMenuName (string) \nThe name of the user menu entry in the main menu of the status display  UserMenuEntryName1 (string) \nThe name of the first sub menu entry in the user menu of the status display  UserMenuEntryName2 (string) \nThe name of the second sub menu entry in the user menu of the status display  UserMenuEntryName3 (string) \nThe name of the third sub menu entry in the user menu of the status display", 
            "title": "Reconfigure parameters"
        }, 
        {
            "location": "/scitos_drivers/scitos_pc_monitor/", 
            "text": "scitos_pc_monitor\n\n\nThis package provides a node that monitors the health of the embedded scitos pc. Status updates are published as diagnostics messages on /diagnostics. These messages are collated by a diagnostics aggregator, which then allows them to be viewed on the scitos_dashboard.\n\n\nRunning\n\n\nThe monitor is started automatically when you use scitos_bringup. To start independently (assuming a roscore is existing somewhere, ie from scitos bringup):\n\n\nrosrun scitos_pc_monitor pc_monitor.py\n\n\n\n\nThis then sends publishes on /diagnostics. To make use of the information, launch the diagnostics aggregator:\n\n\nroslaunch scitos_bringup diagnostic_agg.launch\n\n\n\n\nand view the message on the dashboard on your of-board pc:\n\n\nrosrun scitos_dashboard scitos_dashboard.py\n\n\n\n\nSee https://github.com/strands-project/scitos_apps/tree/master/scitos_dashboard for dashboard documentation.", 
            "title": "Scitos pc monitor"
        }, 
        {
            "location": "/scitos_drivers/scitos_pc_monitor/#scitos_pc_monitor", 
            "text": "This package provides a node that monitors the health of the embedded scitos pc. Status updates are published as diagnostics messages on /diagnostics. These messages are collated by a diagnostics aggregator, which then allows them to be viewed on the scitos_dashboard.", 
            "title": "scitos_pc_monitor"
        }, 
        {
            "location": "/scitos_drivers/scitos_pc_monitor/#running", 
            "text": "The monitor is started automatically when you use scitos_bringup. To start independently (assuming a roscore is existing somewhere, ie from scitos bringup):  rosrun scitos_pc_monitor pc_monitor.py  This then sends publishes on /diagnostics. To make use of the information, launch the diagnostics aggregator:  roslaunch scitos_bringup diagnostic_agg.launch  and view the message on the dashboard on your of-board pc:  rosrun scitos_dashboard scitos_dashboard.py  See https://github.com/strands-project/scitos_apps/tree/master/scitos_dashboard for dashboard documentation.", 
            "title": "Running"
        }, 
        {
            "location": "/scitos_robot/wiki/Bob's-Network-Configuration/", 
            "text": "Network configuration\n\n\nBob (hostname \nbob\n) has the following network interfaces:\n - \neth0\n connects to the internal bob network \n[192.168.0.*]\n, statically assigned IP \n192.168.0.100\n\n - \neth1\n is unused, but can be connected to the robotics lab wired network for maintenance. Dynamic IP.\n - \nwlan0\n is connected to what ever wireless network is in use, usually \nccarobot\n.\n\n\nThe additional onboard PC (hostname \nbobl\n) has a single wired network connection \neth0\n to the internal bob network. Network manager is removed and setup is in \n/etc/network/interfaces\n.\n\n\nInternal \"bob network\"\n\n\nThe bob network \n[192.168.0.*]\n is supplied with an outside connection by NAT through \nbob\n. \nbob\n is the main machine on this network providing routing to other networks and DNS services. \nbobl\n is able to access any machine accessible from \nbob\n over the \nwlan0\n interface by default. However, to access \nbobl\n from a machine that has access to \nbob\n (\nwoody\n for example) you need to fist add \nbob\n as a route in the machines table:\n\n\nsudo route add -net 192.168.0.0/24 dev eth0 gw bob\n\n\n\n\nHostnames and IP addresses\n\n\nTo enable ROS communication between your machine and bob network hosts, add an entry in \n/etc/hosts\n on \nbob\n and then update the internal network dns server:\n\n\nsudo /etc/init.d/dnsmasq restart\n\n\n\n\nThen both \nbobl\n and \nbob\n will be able to find your IP, editing \n/etc/hosts\n on \nbobl\n is not required.\nYou might also want to add \nbob\n as an addition DNS server on your machine so that you do not have to \nmaintain your own hosts file.\n\n\nTime\n\n\nThe time on \nbob\n and \nbobl\n are synchronised using \nchrony\n. \nbob\n is the master, and can be used by offboard machines to synchonise with:\n\n\nsudo ntpdate bob\n\n\n\n\nROS communication\n\n\nbobl\n looks to \nbob\n for the ros core. On any off board machine, set the ROS_MASTER_URI to bob and make sure you add your hostname as above. In order to have full access to topics published by \nbobl\n you must first set the route as above.\n\n\nPing of life\n\n\nFixed machines sometimes loose communication with \nbob\n due to some unknown problem with the \nccarobot\n network routers. This has always been fixable by pinging the lost machine from Bob, which then allows the machine to ping/ssh/etc Bob.\nAs a work-around and to avoid regular manual pinging, the script \n/usr/bin/ping-hosts\n can be used to ping all hosts in Bob's hosts file. This is also ran automatically every 2 minutes, logging to \n/var/log/ping-hosts\n. \n\n\nstrands@bob:~$ cat /var/log/ping-hosts\n----------------------------------------------------------------------\nWed, 14 Jan 2015 11:14:01 +0000\nlocalhost                    live\nbob                          live\nbobl                         live\nbobr                         down\ncca-1128                     live\nheatwave                     live\nwoody                        live\n\n\n\n\nConfiguration files\n\n\n\n\nThe NAT is setup on \nbob\n in the file \n/etc/robotnet.sh\n, which is invoked by \n/etc/rc.local\n. robotnet  script: https://gist.github.com/cburbridge/3ee13fb45a4f05ea7e1e\n\n\nThe DNS services are provided using \ndnsmasq\n, configured in \n/etc/dnsmasq.conf\n: https://gist.github.com/cburbridge/56c1a4d94bfb48c1be46. The user of dns-maq by network manager needs to be removed by commenting out in \n/etc/NetworkManager/NetworkManager.conf\n.\n\n\nThe network on \nbob\n is configured in Network Manager.\n\n\nThe network connection on \nbobl\n is configured in \n/etc/network/interfaces\n: https://gist.github.com/cburbridge/2b7bc5e104ac95848501\n\n\nThe chrony configuration is in \n/etc/chrony/chrony.conf\n. \nbobl\n gets from \nbob\n, \nbob\n from \ntimehost.cs.bham.ac.uk\n when available. Bob configuration: https://gist.github.com/cburbridge/27502dc2ad7f74f0b48e; Bob-L configuration: https://gist.github.com/cburbridge/12eccdeb3dc2bdb63eb2\n\n\nping-hosts\n script: https://gist.github.com/cburbridge/b185c60efc5efbc83bc5", 
            "title": "Bob's Network Configuration"
        }, 
        {
            "location": "/scitos_robot/wiki/Bob's-Network-Configuration/#network-configuration", 
            "text": "Bob (hostname  bob ) has the following network interfaces:\n -  eth0  connects to the internal bob network  [192.168.0.*] , statically assigned IP  192.168.0.100 \n -  eth1  is unused, but can be connected to the robotics lab wired network for maintenance. Dynamic IP.\n -  wlan0  is connected to what ever wireless network is in use, usually  ccarobot .  The additional onboard PC (hostname  bobl ) has a single wired network connection  eth0  to the internal bob network. Network manager is removed and setup is in  /etc/network/interfaces .", 
            "title": "Network configuration"
        }, 
        {
            "location": "/scitos_robot/wiki/Bob's-Network-Configuration/#internal-bob-network", 
            "text": "The bob network  [192.168.0.*]  is supplied with an outside connection by NAT through  bob .  bob  is the main machine on this network providing routing to other networks and DNS services.  bobl  is able to access any machine accessible from  bob  over the  wlan0  interface by default. However, to access  bobl  from a machine that has access to  bob  ( woody  for example) you need to fist add  bob  as a route in the machines table:  sudo route add -net 192.168.0.0/24 dev eth0 gw bob", 
            "title": "Internal \"bob network\""
        }, 
        {
            "location": "/scitos_robot/wiki/Bob's-Network-Configuration/#hostnames-and-ip-addresses", 
            "text": "To enable ROS communication between your machine and bob network hosts, add an entry in  /etc/hosts  on  bob  and then update the internal network dns server:  sudo /etc/init.d/dnsmasq restart  Then both  bobl  and  bob  will be able to find your IP, editing  /etc/hosts  on  bobl  is not required.\nYou might also want to add  bob  as an addition DNS server on your machine so that you do not have to \nmaintain your own hosts file.", 
            "title": "Hostnames and IP addresses"
        }, 
        {
            "location": "/scitos_robot/wiki/Bob's-Network-Configuration/#time", 
            "text": "The time on  bob  and  bobl  are synchronised using  chrony .  bob  is the master, and can be used by offboard machines to synchonise with:  sudo ntpdate bob", 
            "title": "Time"
        }, 
        {
            "location": "/scitos_robot/wiki/Bob's-Network-Configuration/#ros-communication", 
            "text": "bobl  looks to  bob  for the ros core. On any off board machine, set the ROS_MASTER_URI to bob and make sure you add your hostname as above. In order to have full access to topics published by  bobl  you must first set the route as above.", 
            "title": "ROS communication"
        }, 
        {
            "location": "/scitos_robot/wiki/Bob's-Network-Configuration/#ping-of-life", 
            "text": "Fixed machines sometimes loose communication with  bob  due to some unknown problem with the  ccarobot  network routers. This has always been fixable by pinging the lost machine from Bob, which then allows the machine to ping/ssh/etc Bob.\nAs a work-around and to avoid regular manual pinging, the script  /usr/bin/ping-hosts  can be used to ping all hosts in Bob's hosts file. This is also ran automatically every 2 minutes, logging to  /var/log/ping-hosts .   strands@bob:~$ cat /var/log/ping-hosts\n----------------------------------------------------------------------\nWed, 14 Jan 2015 11:14:01 +0000\nlocalhost                    live\nbob                          live\nbobl                         live\nbobr                         down\ncca-1128                     live\nheatwave                     live\nwoody                        live", 
            "title": "Ping of life"
        }, 
        {
            "location": "/scitos_robot/wiki/Bob's-Network-Configuration/#configuration-files", 
            "text": "The NAT is setup on  bob  in the file  /etc/robotnet.sh , which is invoked by  /etc/rc.local . robotnet  script: https://gist.github.com/cburbridge/3ee13fb45a4f05ea7e1e  The DNS services are provided using  dnsmasq , configured in  /etc/dnsmasq.conf : https://gist.github.com/cburbridge/56c1a4d94bfb48c1be46. The user of dns-maq by network manager needs to be removed by commenting out in  /etc/NetworkManager/NetworkManager.conf .  The network on  bob  is configured in Network Manager.  The network connection on  bobl  is configured in  /etc/network/interfaces : https://gist.github.com/cburbridge/2b7bc5e104ac95848501  The chrony configuration is in  /etc/chrony/chrony.conf .  bobl  gets from  bob ,  bob  from  timehost.cs.bham.ac.uk  when available. Bob configuration: https://gist.github.com/cburbridge/27502dc2ad7f74f0b48e; Bob-L configuration: https://gist.github.com/cburbridge/12eccdeb3dc2bdb63eb2  ping-hosts  script: https://gist.github.com/cburbridge/b185c60efc5efbc83bc5", 
            "title": "Configuration files"
        }, 
        {
            "location": "/scitos_robot/wiki/Home/", 
            "text": "", 
            "title": "Home"
        }, 
        {
            "location": "/semantic_segmentation/", 
            "text": "many segment :D\n\n\nFor the semantic segmentation service:\n\nroslaunch semantic_segmentation semantic_segmentation_integrate.launch\n\n\nA dummy client to test it:\n\nrosrun semantic_segmentation dummy_client \nwaypoint_name\n\n\nTODO:\n\n\nASAP:\n\n Update service to take waypoint\n\n Query center from Rares\n\n Fix flipping of normals\n\n Play around with rotations and flipping\n\n Try neighborhood features! (Normals, psl, color,)\n\n Store labeled pointclouds and make a latched topic for publishing them.\n\n\nPOST REV:\n\n Merge the two cmakelists files!\n\n Think about a cleaner config file handling\n* Pull back other data again?\n\n\nDONE?:\n* Fix up label set.", 
            "title": "Home"
        }, 
        {
            "location": "/semantic_segmentation/#many-segment-d", 
            "text": "For the semantic segmentation service: roslaunch semantic_segmentation semantic_segmentation_integrate.launch  A dummy client to test it: rosrun semantic_segmentation dummy_client  waypoint_name", 
            "title": "many segment :D"
        }, 
        {
            "location": "/semantic_segmentation/#todo", 
            "text": "ASAP:  Update service to take waypoint  Query center from Rares  Fix flipping of normals  Play around with rotations and flipping  Try neighborhood features! (Normals, psl, color,)  Store labeled pointclouds and make a latched topic for publishing them.  POST REV:  Merge the two cmakelists files!  Think about a cleaner config file handling\n* Pull back other data again?  DONE?:\n* Fix up label set.", 
            "title": "TODO:"
        }, 
        {
            "location": "/semantic_segmentation/src/densecrf/", 
            "text": "DenseCRF - Code\n\n\nhttp://graphics.stanford.edu/projects/drf/\n\n\nThis software pertains to the research described in the ICML 2013 paper:\nParameter Learning and Convergent Inference for Dense Random Fields, by\nPhilipp Kr\u00e4henb\u00fchl and Vladlen Koltun\nand the NIPS 2011 paper:\nEfficient Inference in Fully Connected CRFs with Gaussian Edge Potentials, by\nPhilipp Kr\u00e4henb\u00fchl and Vladlen Koltun\n\n\nIf you're using this code in a publication, please cite our papers.\n\n\nThis software is provided for research purposes, with absolutely no warranty\nor suggested support, and use of it most follow the BSD license agreement, at\nthe top of each source file. \nPlease do not contact the authors for assistance\nwith installing, understanding or running the code.\n However if you think you\nhave found an interesting bug, the authors would be grateful if you could pass\non the information.\n\n\nChanges to the original code\n\n\nThe only major difference in this released version of the code is, that I directly\ncompute the gradient of the permutohedral lattice, instead of the general Gauss\nTransform (3 line formula in p.6 in ICML 2013 paper). The gradient of the\npermutohedral lattice evaluated the exact gradient of the approximate filter.\nIn higher dimensions (\n3) the filter can be non continuous, which can complicate\nthe optimization. The kernel gradient is also scaled lower than other parameters,\nwhich complicates the optimization. \n\n\nHow to compile the code\n\n\nDependencies:\n * cmake  http://www.cmake.org/\n * Eigen (included)\n * liblbfgs (included)\n\n\nLinux, Mac OS X and Windows (cygwin):\n mkdir build\n cd build\n cmake -D CMAKE_BUILD_TYPE=Release ..\n make\n cd ..\n\n\nWindows\n You're probably better off just copying all files into a Visual Studio\n project\n\n\nHow to run the example\n\n\nAn example on how to use the DenseCRF can be found in\nexamples/dense_inference.cpp. The example loads an image and some annotations.\nIt then uses a very simple classifier to compute a unary term based on those\nannotations. A dense CRF with both color dependent and color independent terms\nfind the final accurate labeling.\n\n\nLinux, Mac OS X and Windows (cygwin):\n build/examples/dense_inference input_image.ppm annotations.ppm output.ppm\n\n\nFor example:\n build/examples/dense_inference examples/im1.ppm examples/anno1.ppm output1.ppm\n\n\nAn example on how to unse the learning code can be found in \nexamples/dense_learning.cpp. The example loads a color image and ground truth\nannotation. It then learn a CRF model with a logistic regression, a label comp\nand Gaussian kernel.\n\n\nLinux, Mac OS X and Windows (cygwin):\n build/examples/dense_learning input_image.ppm annotations.ppm output.ppm\n\n\nFor example:\n build/examples/dense_learning examples/im1.ppm examples/anno1.ppm output1.ppm\n\n\nPlease note that this implementation is slightly slower than the one used to\nin our NIPS 2011 paper. Mainly because I tried to keep the code clean and easy\nto understand.", 
            "title": "Densecrf"
        }, 
        {
            "location": "/semantic_segmentation/src/densecrf/#densecrf-code", 
            "text": "http://graphics.stanford.edu/projects/drf/  This software pertains to the research described in the ICML 2013 paper:\nParameter Learning and Convergent Inference for Dense Random Fields, by\nPhilipp Kr\u00e4henb\u00fchl and Vladlen Koltun\nand the NIPS 2011 paper:\nEfficient Inference in Fully Connected CRFs with Gaussian Edge Potentials, by\nPhilipp Kr\u00e4henb\u00fchl and Vladlen Koltun  If you're using this code in a publication, please cite our papers.  This software is provided for research purposes, with absolutely no warranty\nor suggested support, and use of it most follow the BSD license agreement, at\nthe top of each source file.  Please do not contact the authors for assistance\nwith installing, understanding or running the code.  However if you think you\nhave found an interesting bug, the authors would be grateful if you could pass\non the information.", 
            "title": "DenseCRF - Code"
        }, 
        {
            "location": "/semantic_segmentation/src/densecrf/#changes-to-the-original-code", 
            "text": "The only major difference in this released version of the code is, that I directly\ncompute the gradient of the permutohedral lattice, instead of the general Gauss\nTransform (3 line formula in p.6 in ICML 2013 paper). The gradient of the\npermutohedral lattice evaluated the exact gradient of the approximate filter.\nIn higher dimensions ( 3) the filter can be non continuous, which can complicate\nthe optimization. The kernel gradient is also scaled lower than other parameters,\nwhich complicates the optimization.", 
            "title": "Changes to the original code"
        }, 
        {
            "location": "/semantic_segmentation/src/densecrf/#how-to-compile-the-code", 
            "text": "Dependencies:\n * cmake  http://www.cmake.org/\n * Eigen (included)\n * liblbfgs (included)  Linux, Mac OS X and Windows (cygwin):\n mkdir build\n cd build\n cmake -D CMAKE_BUILD_TYPE=Release ..\n make\n cd ..  Windows\n You're probably better off just copying all files into a Visual Studio\n project", 
            "title": "How to compile the code"
        }, 
        {
            "location": "/semantic_segmentation/src/densecrf/#how-to-run-the-example", 
            "text": "An example on how to use the DenseCRF can be found in\nexamples/dense_inference.cpp. The example loads an image and some annotations.\nIt then uses a very simple classifier to compute a unary term based on those\nannotations. A dense CRF with both color dependent and color independent terms\nfind the final accurate labeling.  Linux, Mac OS X and Windows (cygwin):\n build/examples/dense_inference input_image.ppm annotations.ppm output.ppm  For example:\n build/examples/dense_inference examples/im1.ppm examples/anno1.ppm output1.ppm  An example on how to unse the learning code can be found in \nexamples/dense_learning.cpp. The example loads a color image and ground truth\nannotation. It then learn a CRF model with a logistic regression, a label comp\nand Gaussian kernel.  Linux, Mac OS X and Windows (cygwin):\n build/examples/dense_learning input_image.ppm annotations.ppm output.ppm  For example:\n build/examples/dense_learning examples/im1.ppm examples/anno1.ppm output1.ppm  Please note that this implementation is slightly slower than the one used to\nin our NIPS 2011 paper. Mainly because I tried to keep the code clean and easy\nto understand.", 
            "title": "How to run the example"
        }, 
        {
            "location": "/semantic_segmentation/src/docs/", 
            "text": "References\n\n\nThe following list includes important references used for implementations within libForest:\n\n\n\n\nOnline Random Forests / Decision Trees:\n\n\n\n\n\n\nA. Saffari, C Leistner, J. Santner, M.Godec. On-Line Random Forests. International Conference on Computer Vision Workshops, 2009.\n\n\n\n\n\n\nVariable Importance:\n\n\n\n\n\n\nG. Louppe, L. Wehenkel, A. Sutera, P. Geurts. Understanding Variable Importance in Forests of Randomized Trees. Advances in Neural Information Processing Systems, 2013.\n\n\nG. Louppe. Understanding Random Forests. PhD thesis, Universite de Liege, Belgium, 2014.\n\n\n\n\n\n\nDensity Forests:\n\n\n\n\n\n\nA. Criminisi, J. Shotton. Density Forests. In Decision Forests for Computer Vision and Medical Image Analysis, Springer, 2013.\n\n\n\n\n\n\nKullback-Leibler Divergence:\n\n\n\n\n\n\nF. Perez-Cruz. Kullback-Leibler DIvergence Estimation of Continuous Distributions. International Symposium on Information Theory, 2008.\n\n\n\n\n\n\nKernel Density Estimation:\n\n\n\n\n\n\nB. E. Hansen. Lecture Notes on Nonparametrics. University of Wisconsin, 2009.\n\n\nP. B. Stark. Statistics 240 Lecture Notes, part 10: Density Estimation. University of California Berkeley, 2008.\n\n\nM. C. Jones, J. S. Marron, S. J. Sheather. A Brief Survey of Bandwidth Selection for Density Estimation. Journal of the American Statistical Association, 91(433), 1996.\n\n\nB. A. Turlach. Bandwidth Selection in Kernel Density Estimation: A Review. C.O.R.E. and Intitut de Statistique, Universite Catholique de Louvain, Belgium.\n\n\n\n\n\n\nK-Means:\n\n\n\n\n\n\nD. Arthur, S. Vassilvitskii. k-means++: The Advantages of Careful Seeding. Proceedings of the ACM-SIAM Symposium on Discrete Algorithms, 2007.\n\n\nC. Elkan. Using the Triangle Inequality to Accelerate k-Means. International Conference on Machine Learning, 2003.\n\n\nJ. Han, M. Kamber, J.Pei. Data Mining: Concepts and Techniques. Morgan Kaufmann Publishers Inc. San Francisco, CA, 2005.", 
            "title": "Docs"
        }, 
        {
            "location": "/semantic_segmentation/src/docs/#references", 
            "text": "The following list includes important references used for implementations within libForest:   Online Random Forests / Decision Trees:    A. Saffari, C Leistner, J. Santner, M.Godec. On-Line Random Forests. International Conference on Computer Vision Workshops, 2009.    Variable Importance:    G. Louppe, L. Wehenkel, A. Sutera, P. Geurts. Understanding Variable Importance in Forests of Randomized Trees. Advances in Neural Information Processing Systems, 2013.  G. Louppe. Understanding Random Forests. PhD thesis, Universite de Liege, Belgium, 2014.    Density Forests:    A. Criminisi, J. Shotton. Density Forests. In Decision Forests for Computer Vision and Medical Image Analysis, Springer, 2013.    Kullback-Leibler Divergence:    F. Perez-Cruz. Kullback-Leibler DIvergence Estimation of Continuous Distributions. International Symposium on Information Theory, 2008.    Kernel Density Estimation:    B. E. Hansen. Lecture Notes on Nonparametrics. University of Wisconsin, 2009.  P. B. Stark. Statistics 240 Lecture Notes, part 10: Density Estimation. University of California Berkeley, 2008.  M. C. Jones, J. S. Marron, S. J. Sheather. A Brief Survey of Bandwidth Selection for Density Estimation. Journal of the American Statistical Association, 91(433), 1996.  B. A. Turlach. Bandwidth Selection in Kernel Density Estimation: A Review. C.O.R.E. and Intitut de Statistique, Universite Catholique de Louvain, Belgium.    K-Means:    D. Arthur, S. Vassilvitskii. k-means++: The Advantages of Careful Seeding. Proceedings of the ACM-SIAM Symposium on Discrete Algorithms, 2007.  C. Elkan. Using the Triangle Inequality to Accelerate k-Means. International Conference on Machine Learning, 2003.  J. Han, M. Kamber, J.Pei. Data Mining: Concepts and Techniques. Morgan Kaufmann Publishers Inc. San Francisco, CA, 2005.", 
            "title": "References"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/", 
            "text": "Google C++ Testing Framework\n\n\nhttp://code.google.com/p/googletest/\n\n\nOverview\n\n\nGoogle's framework for writing C++ tests on a variety of platforms\n(Linux, Mac OS X, Windows, Windows CE, Symbian, etc).  Based on the\nxUnit architecture.  Supports automatic test discovery, a rich set of\nassertions, user-defined assertions, death tests, fatal and non-fatal\nfailures, various options for running the tests, and XML test report\ngeneration.\n\n\nPlease see the project page above for more information as well as the\nmailing list for questions, discussions, and development.  There is\nalso an IRC channel on OFTC (irc.oftc.net) #gtest available.  Please\njoin us!\n\n\nRequirements for End Users\n\n\nGoogle Test is designed to have fairly minimal requirements to build\nand use with your projects, but there are some.  Currently, we support\nLinux, Windows, Mac OS X, and Cygwin.  We will also make our best\neffort to support other platforms (e.g. Solaris, AIX, and z/OS).\nHowever, since core members of the Google Test project have no access\nto these platforms, Google Test may have outstanding issues there.  If\nyou notice any problems on your platform, please notify\ngoogletestframework@googlegroups.com.  Patches for fixing them are\neven more welcome!\n\n\nLinux Requirements\n\n\nThese are the base requirements to build and use Google Test from a source\npackage (as described below):\n  * GNU-compatible Make or gmake\n  * POSIX-standard shell\n  * POSIX(-2) Regular Expressions (regex.h)\n  * A C++98-standard-compliant compiler\n\n\nWindows Requirements\n\n\n\n\nMicrosoft Visual C++ 7.1 or newer\n\n\n\n\nCygwin Requirements\n\n\n\n\nCygwin 1.5.25-14 or newer\n\n\n\n\nMac OS X Requirements\n\n\n\n\nMac OS X 10.4 Tiger or newer\n\n\nDeveloper Tools Installed\n\n\n\n\nAlso, you'll need CMake 2.6.4 or higher if you want to build the\nsamples using the provided CMake script, regardless of the platform.\n\n\nRequirements for Contributors\n\n\nWe welcome patches.  If you plan to contribute a patch, you need to\nbuild Google Test and its own tests from an SVN checkout (described\nbelow), which has further requirements:\n\n\n\n\nPython version 2.3 or newer (for running some of the tests and\n    re-generating certain source files from templates)\n\n\nCMake 2.6.4 or newer\n\n\n\n\nGetting the Source\n\n\nThere are two primary ways of getting Google Test's source code: you\ncan download a stable source release in your preferred archive format,\nor directly check out the source from our Subversion (SVN) repositary.\nThe SVN checkout requires a few extra steps and some extra software\npackages on your system, but lets you track the latest development and\nmake patches much more easily, so we highly encourage it.\n\n\nSource Package\n\n\nGoogle Test is released in versioned source packages which can be\ndownloaded from the download page [1].  Several different archive\nformats are provided, but the only difference is the tools used to\nmanipulate them, and the size of the resulting file.  Download\nwhichever you are most comfortable with.\n\n\n[1] http://code.google.com/p/googletest/downloads/list\n\n\nOnce the package is downloaded, expand it using whichever tools you\nprefer for that type.  This will result in a new directory with the\nname \"gtest-X.Y.Z\" which contains all of the source code.  Here are\nsome examples on Linux:\n\n\ntar -xvzf gtest-X.Y.Z.tar.gz\n  tar -xvjf gtest-X.Y.Z.tar.bz2\n  unzip gtest-X.Y.Z.zip\n\n\nSVN Checkout\n\n\nTo check out the main branch (also known as the \"trunk\") of Google\nTest, run the following Subversion command:\n\n\nsvn checkout http://googletest.googlecode.com/svn/trunk/ gtest-svn\n\n\nSetting up the Build\n\n\nTo build Google Test and your tests that use it, you need to tell your\nbuild system where to find its headers and source files.  The exact\nway to do it depends on which build system you use, and is usually\nstraightforward.\n\n\nGeneric Build Instructions\n\n\nSuppose you put Google Test in directory ${GTEST_DIR}.  To build it,\ncreate a library build target (or a project as called by Visual Studio\nand Xcode) to compile\n\n\n${GTEST_DIR}/src/gtest-all.cc\n\n\nwith ${GTEST_DIR}/include in the system header search path and ${GTEST_DIR}\nin the normal header search path.  Assuming a Linux-like system and gcc,\nsomething like the following will do:\n\n\ng++ -isystem ${GTEST_DIR}/include -I${GTEST_DIR} \\\n      -pthread -c ${GTEST_DIR}/src/gtest-all.cc\n  ar -rv libgtest.a gtest-all.o\n\n\n(We need -pthread as Google Test uses threads.)\n\n\nNext, you should compile your test source file with\n${GTEST_DIR}/include in the system header search path, and link it\nwith gtest and any other necessary libraries:\n\n\ng++ -isystem ${GTEST_DIR}/include -pthread path/to/your_test.cc libgtest.a \\\n      -o your_test\n\n\nAs an example, the make/ directory contains a Makefile that you can\nuse to build Google Test on systems where GNU make is available\n(e.g. Linux, Mac OS X, and Cygwin).  It doesn't try to build Google\nTest's own tests.  Instead, it just builds the Google Test library and\na sample test.  You can use it as a starting point for your own build\nscript.\n\n\nIf the default settings are correct for your environment, the\nfollowing commands should succeed:\n\n\ncd ${GTEST_DIR}/make\n  make\n  ./sample1_unittest\n\n\nIf you see errors, try to tweak the contents of make/Makefile to make\nthem go away.  There are instructions in make/Makefile on how to do\nit.\n\n\nUsing CMake\n\n\nGoogle Test comes with a CMake build script (CMakeLists.txt) that can\nbe used on a wide range of platforms (\"C\" stands for cross-platofrm.).\nIf you don't have CMake installed already, you can download it for\nfree from http://www.cmake.org/.\n\n\nCMake works by generating native makefiles or build projects that can\nbe used in the compiler environment of your choice.  The typical\nworkflow starts with:\n\n\nmkdir mybuild       # Create a directory to hold the build output.\n  cd mybuild\n  cmake ${GTEST_DIR}  # Generate native build scripts.\n\n\nIf you want to build Google Test's samples, you should replace the\nlast command with\n\n\ncmake -Dgtest_build_samples=ON ${GTEST_DIR}\n\n\nIf you are on a *nix system, you should now see a Makefile in the\ncurrent directory.  Just type 'make' to build gtest.\n\n\nIf you use Windows and have Vistual Studio installed, a gtest.sln file\nand several .vcproj files will be created.  You can then build them\nusing Visual Studio.\n\n\nOn Mac OS X with Xcode installed, a .xcodeproj file will be generated.\n\n\nLegacy Build Scripts\n\n\nBefore settling on CMake, we have been providing hand-maintained build\nprojects/scripts for Visual Studio, Xcode, and Autotools.  While we\ncontinue to provide them for convenience, they are not actively\nmaintained any more.  We highly recommend that you follow the\ninstructions in the previous two sections to integrate Google Test\nwith your existing build system.\n\n\nIf you still need to use the legacy build scripts, here's how:\n\n\nThe msvc\\ folder contains two solutions with Visual C++ projects.\nOpen the gtest.sln or gtest-md.sln file using Visual Studio, and you\nare ready to build Google Test the same way you build any Visual\nStudio project.  Files that have names ending with -md use DLL\nversions of Microsoft runtime libraries (the /MD or the /MDd compiler\noption).  Files without that suffix use static versions of the runtime\nlibraries (the /MT or the /MTd option).  Please note that one must use\nthe same option to compile both gtest and the test code.  If you use\nVisual Studio 2005 or above, we recommend the -md version as /MD is\nthe default for new projects in these versions of Visual Studio.\n\n\nOn Mac OS X, open the gtest.xcodeproj in the xcode/ folder using\nXcode.  Build the \"gtest\" target.  The universal binary framework will\nend up in your selected build directory (selected in the Xcode\n\"Preferences...\" -\n \"Building\" pane and defaults to xcode/build).\nAlternatively, at the command line, enter:\n\n\nxcodebuild\n\n\nThis will build the \"Release\" configuration of gtest.framework in your\ndefault build location.  See the \"xcodebuild\" man page for more\ninformation about building different configurations and building in\ndifferent locations.\n\n\nIf you wish to use the Google Test Xcode project with Xcode 4.x and\nabove, you need to either:\n * update the SDK configuration options in xcode/Config/General.xconfig.\n   Comment options SDKROOT, MACOS_DEPLOYMENT_TARGET, and GCC_VERSION. If\n   you choose this route you lose the ability to target earlier versions\n   of MacOS X.\n * Install an SDK for an earlier version. This doesn't appear to be\n   supported by Apple, but has been reported to work\n   (http://stackoverflow.com/questions/5378518).\n\n\nTweaking Google Test\n\n\nGoogle Test can be used in diverse environments.  The default\nconfiguration may not work (or may not work well) out of the box in\nsome environments.  However, you can easily tweak Google Test by\ndefining control macros on the compiler command line.  Generally,\nthese macros are named like GTEST_XYZ and you define them to either 1\nor 0 to enable or disable a certain feature.\n\n\nWe list the most frequently used macros below.  For a complete list,\nsee file include/gtest/internal/gtest-port.h.\n\n\nChoosing a TR1 Tuple Library\n\n\nSome Google Test features require the C++ Technical Report 1 (TR1)\ntuple library, which is not yet available with all compilers.  The\ngood news is that Google Test implements a subset of TR1 tuple that's\nenough for its own need, and will automatically use this when the\ncompiler doesn't provide TR1 tuple.\n\n\nUsually you don't need to care about which tuple library Google Test\nuses.  However, if your project already uses TR1 tuple, you need to\ntell Google Test to use the same TR1 tuple library the rest of your\nproject uses, or the two tuple implementations will clash.  To do\nthat, add\n\n\n-DGTEST_USE_OWN_TR1_TUPLE=0\n\n\nto the compiler flags while compiling Google Test and your tests.  If\nyou want to force Google Test to use its own tuple library, just add\n\n\n-DGTEST_USE_OWN_TR1_TUPLE=1\n\n\nto the compiler flags instead.\n\n\nIf you don't want Google Test to use tuple at all, add\n\n\n-DGTEST_HAS_TR1_TUPLE=0\n\n\nand all features using tuple will be disabled.\n\n\nMulti-threaded Tests\n\n\nGoogle Test is thread-safe where the pthread library is available.\nAfter #include \"gtest/gtest.h\", you can check the GTEST_IS_THREADSAFE\nmacro to see whether this is the case (yes if the macro is #defined to\n1, no if it's undefined.).\n\n\nIf Google Test doesn't correctly detect whether pthread is available\nin your environment, you can force it with\n\n\n-DGTEST_HAS_PTHREAD=1\n\n\nor\n\n\n-DGTEST_HAS_PTHREAD=0\n\n\nWhen Google Test uses pthread, you may need to add flags to your\ncompiler and/or linker to select the pthread library, or you'll get\nlink errors.  If you use the CMake script or the deprecated Autotools\nscript, this is taken care of for you.  If you use your own build\nscript, you'll need to read your compiler and linker's manual to\nfigure out what flags to add.\n\n\nAs a Shared Library (DLL)\n\n\nGoogle Test is compact, so most users can build and link it as a\nstatic library for the simplicity.  You can choose to use Google Test\nas a shared library (known as a DLL on Windows) if you prefer.\n\n\nTo compile \ngtest\n as a shared library, add\n\n\n-DGTEST_CREATE_SHARED_LIBRARY=1\n\n\nto the compiler flags.  You'll also need to tell the linker to produce\na shared library instead - consult your linker's manual for how to do\nit.\n\n\nTo compile your \ntests\n that use the gtest shared library, add\n\n\n-DGTEST_LINKED_AS_SHARED_LIBRARY=1\n\n\nto the compiler flags.\n\n\nNote: while the above steps aren't technically necessary today when\nusing some compilers (e.g. GCC), they may become necessary in the\nfuture, if we decide to improve the speed of loading the library (see\nhttp://gcc.gnu.org/wiki/Visibility for details).  Therefore you are\nrecommended to always add the above flags when using Google Test as a\nshared library.  Otherwise a future release of Google Test may break\nyour build script.\n\n\nAvoiding Macro Name Clashes\n\n\nIn C++, macros don't obey namespaces.  Therefore two libraries that\nboth define a macro of the same name will clash if you #include both\ndefinitions.  In case a Google Test macro clashes with another\nlibrary, you can force Google Test to rename its macro to avoid the\nconflict.\n\n\nSpecifically, if both Google Test and some other code define macro\nFOO, you can add\n\n\n-DGTEST_DONT_DEFINE_FOO=1\n\n\nto the compiler flags to tell Google Test to change the macro's name\nfrom FOO to GTEST_FOO.  Currently FOO can be FAIL, SUCCEED, or TEST.\nFor example, with -DGTEST_DONT_DEFINE_TEST=1, you'll need to write\n\n\nGTEST_TEST(SomeTest, DoesThis) { ... }\n\n\ninstead of\n\n\nTEST(SomeTest, DoesThis) { ... }\n\n\nin order to define a test.\n\n\nUpgrating from an Earlier Version\n\n\nWe strive to keep Google Test releases backward compatible.\nSometimes, though, we have to make some breaking changes for the\nusers' long-term benefits.  This section describes what you'll need to\ndo if you are upgrading from an earlier version of Google Test.\n\n\nUpgrading from 1.3.0 or Earlier\n\n\nYou may need to explicitly enable or disable Google Test's own TR1\ntuple library.  See the instructions in section \"Choosing a TR1 Tuple\nLibrary\".\n\n\nUpgrading from 1.4.0 or Earlier\n\n\nThe Autotools build script (configure + make) is no longer officially\nsupportted.  You are encouraged to migrate to your own build system or\nuse CMake.  If you still need to use Autotools, you can find\ninstructions in the README file from Google Test 1.4.0.\n\n\nOn platforms where the pthread library is available, Google Test uses\nit in order to be thread-safe.  See the \"Multi-threaded Tests\" section\nfor what this means to your build script.\n\n\nIf you use Microsoft Visual C++ 7.1 with exceptions disabled, Google\nTest will no longer compile.  This should affect very few people, as a\nlarge portion of STL (including \n) doesn't compile in this mode\nanyway.  We decided to stop supporting it in order to greatly simplify\nGoogle Test's implementation.\n\n\nDeveloping Google Test\n\n\nThis section discusses how to make your own changes to Google Test.\n\n\nTesting Google Test Itself\n\n\nTo make sure your changes work as intended and don't break existing\nfunctionality, you'll want to compile and run Google Test's own tests.\nFor that you can use CMake:\n\n\nmkdir mybuild\n  cd mybuild\n  cmake -Dgtest_build_tests=ON ${GTEST_DIR}\n\n\nMake sure you have Python installed, as some of Google Test's tests\nare written in Python.  If the cmake command complains about not being\nable to find Python (\"Could NOT find PythonInterp (missing:\nPYTHON_EXECUTABLE)\"), try telling it explicitly where your Python\nexecutable can be found:\n\n\ncmake -DPYTHON_EXECUTABLE=path/to/python -Dgtest_build_tests=ON ${GTEST_DIR}\n\n\nNext, you can build Google Test and all of its own tests.  On *nix,\nthis is usually done by 'make'.  To run the tests, do\n\n\nmake test\n\n\nAll tests should pass.\n\n\nRegenerating Source Files\n\n\nSome of Google Test's source files are generated from templates (not\nin the C++ sense) using a script.  A template file is named FOO.pump,\nwhere FOO is the name of the file it will generate.  For example, the\nfile include/gtest/internal/gtest-type-util.h.pump is used to generate\ngtest-type-util.h in the same directory.\n\n\nNormally you don't need to worry about regenerating the source files,\nunless you need to modify them.  In that case, you should modify the\ncorresponding .pump files instead and run the pump.py Python script to\nregenerate them.  You can find pump.py in the scripts/ directory.\nRead the Pump manual [2] for how to use it.\n\n\n[2] http://code.google.com/p/googletest/wiki/PumpManual\n\n\nContributing a Patch\n\n\nWe welcome patches.  Please read the Google Test developer's guide [3]\nfor how you can contribute.  In particular, make sure you have signed\nthe Contributor License Agreement, or we won't be able to accept the\npatch.\n\n\n[3] http://code.google.com/p/googletest/wiki/GoogleTestDevGuide\n\n\nHappy testing!", 
            "title": "Gtest 1.7.0"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#google-c-testing-framework", 
            "text": "http://code.google.com/p/googletest/", 
            "title": "Google C++ Testing Framework"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#overview", 
            "text": "Google's framework for writing C++ tests on a variety of platforms\n(Linux, Mac OS X, Windows, Windows CE, Symbian, etc).  Based on the\nxUnit architecture.  Supports automatic test discovery, a rich set of\nassertions, user-defined assertions, death tests, fatal and non-fatal\nfailures, various options for running the tests, and XML test report\ngeneration.  Please see the project page above for more information as well as the\nmailing list for questions, discussions, and development.  There is\nalso an IRC channel on OFTC (irc.oftc.net) #gtest available.  Please\njoin us!", 
            "title": "Overview"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#requirements-for-end-users", 
            "text": "Google Test is designed to have fairly minimal requirements to build\nand use with your projects, but there are some.  Currently, we support\nLinux, Windows, Mac OS X, and Cygwin.  We will also make our best\neffort to support other platforms (e.g. Solaris, AIX, and z/OS).\nHowever, since core members of the Google Test project have no access\nto these platforms, Google Test may have outstanding issues there.  If\nyou notice any problems on your platform, please notify\ngoogletestframework@googlegroups.com.  Patches for fixing them are\neven more welcome!", 
            "title": "Requirements for End Users"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#linux-requirements", 
            "text": "These are the base requirements to build and use Google Test from a source\npackage (as described below):\n  * GNU-compatible Make or gmake\n  * POSIX-standard shell\n  * POSIX(-2) Regular Expressions (regex.h)\n  * A C++98-standard-compliant compiler", 
            "title": "Linux Requirements"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#windows-requirements", 
            "text": "Microsoft Visual C++ 7.1 or newer", 
            "title": "Windows Requirements"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#cygwin-requirements", 
            "text": "Cygwin 1.5.25-14 or newer", 
            "title": "Cygwin Requirements"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#mac-os-x-requirements", 
            "text": "Mac OS X 10.4 Tiger or newer  Developer Tools Installed   Also, you'll need CMake 2.6.4 or higher if you want to build the\nsamples using the provided CMake script, regardless of the platform.", 
            "title": "Mac OS X Requirements"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#requirements-for-contributors", 
            "text": "We welcome patches.  If you plan to contribute a patch, you need to\nbuild Google Test and its own tests from an SVN checkout (described\nbelow), which has further requirements:   Python version 2.3 or newer (for running some of the tests and\n    re-generating certain source files from templates)  CMake 2.6.4 or newer", 
            "title": "Requirements for Contributors"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#getting-the-source", 
            "text": "There are two primary ways of getting Google Test's source code: you\ncan download a stable source release in your preferred archive format,\nor directly check out the source from our Subversion (SVN) repositary.\nThe SVN checkout requires a few extra steps and some extra software\npackages on your system, but lets you track the latest development and\nmake patches much more easily, so we highly encourage it.", 
            "title": "Getting the Source"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#source-package", 
            "text": "Google Test is released in versioned source packages which can be\ndownloaded from the download page [1].  Several different archive\nformats are provided, but the only difference is the tools used to\nmanipulate them, and the size of the resulting file.  Download\nwhichever you are most comfortable with.  [1] http://code.google.com/p/googletest/downloads/list  Once the package is downloaded, expand it using whichever tools you\nprefer for that type.  This will result in a new directory with the\nname \"gtest-X.Y.Z\" which contains all of the source code.  Here are\nsome examples on Linux:  tar -xvzf gtest-X.Y.Z.tar.gz\n  tar -xvjf gtest-X.Y.Z.tar.bz2\n  unzip gtest-X.Y.Z.zip", 
            "title": "Source Package"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#svn-checkout", 
            "text": "To check out the main branch (also known as the \"trunk\") of Google\nTest, run the following Subversion command:  svn checkout http://googletest.googlecode.com/svn/trunk/ gtest-svn", 
            "title": "SVN Checkout"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#setting-up-the-build", 
            "text": "To build Google Test and your tests that use it, you need to tell your\nbuild system where to find its headers and source files.  The exact\nway to do it depends on which build system you use, and is usually\nstraightforward.", 
            "title": "Setting up the Build"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#generic-build-instructions", 
            "text": "Suppose you put Google Test in directory ${GTEST_DIR}.  To build it,\ncreate a library build target (or a project as called by Visual Studio\nand Xcode) to compile  ${GTEST_DIR}/src/gtest-all.cc  with ${GTEST_DIR}/include in the system header search path and ${GTEST_DIR}\nin the normal header search path.  Assuming a Linux-like system and gcc,\nsomething like the following will do:  g++ -isystem ${GTEST_DIR}/include -I${GTEST_DIR} \\\n      -pthread -c ${GTEST_DIR}/src/gtest-all.cc\n  ar -rv libgtest.a gtest-all.o  (We need -pthread as Google Test uses threads.)  Next, you should compile your test source file with\n${GTEST_DIR}/include in the system header search path, and link it\nwith gtest and any other necessary libraries:  g++ -isystem ${GTEST_DIR}/include -pthread path/to/your_test.cc libgtest.a \\\n      -o your_test  As an example, the make/ directory contains a Makefile that you can\nuse to build Google Test on systems where GNU make is available\n(e.g. Linux, Mac OS X, and Cygwin).  It doesn't try to build Google\nTest's own tests.  Instead, it just builds the Google Test library and\na sample test.  You can use it as a starting point for your own build\nscript.  If the default settings are correct for your environment, the\nfollowing commands should succeed:  cd ${GTEST_DIR}/make\n  make\n  ./sample1_unittest  If you see errors, try to tweak the contents of make/Makefile to make\nthem go away.  There are instructions in make/Makefile on how to do\nit.", 
            "title": "Generic Build Instructions"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#using-cmake", 
            "text": "Google Test comes with a CMake build script (CMakeLists.txt) that can\nbe used on a wide range of platforms (\"C\" stands for cross-platofrm.).\nIf you don't have CMake installed already, you can download it for\nfree from http://www.cmake.org/.  CMake works by generating native makefiles or build projects that can\nbe used in the compiler environment of your choice.  The typical\nworkflow starts with:  mkdir mybuild       # Create a directory to hold the build output.\n  cd mybuild\n  cmake ${GTEST_DIR}  # Generate native build scripts.  If you want to build Google Test's samples, you should replace the\nlast command with  cmake -Dgtest_build_samples=ON ${GTEST_DIR}  If you are on a *nix system, you should now see a Makefile in the\ncurrent directory.  Just type 'make' to build gtest.  If you use Windows and have Vistual Studio installed, a gtest.sln file\nand several .vcproj files will be created.  You can then build them\nusing Visual Studio.  On Mac OS X with Xcode installed, a .xcodeproj file will be generated.", 
            "title": "Using CMake"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#legacy-build-scripts", 
            "text": "Before settling on CMake, we have been providing hand-maintained build\nprojects/scripts for Visual Studio, Xcode, and Autotools.  While we\ncontinue to provide them for convenience, they are not actively\nmaintained any more.  We highly recommend that you follow the\ninstructions in the previous two sections to integrate Google Test\nwith your existing build system.  If you still need to use the legacy build scripts, here's how:  The msvc\\ folder contains two solutions with Visual C++ projects.\nOpen the gtest.sln or gtest-md.sln file using Visual Studio, and you\nare ready to build Google Test the same way you build any Visual\nStudio project.  Files that have names ending with -md use DLL\nversions of Microsoft runtime libraries (the /MD or the /MDd compiler\noption).  Files without that suffix use static versions of the runtime\nlibraries (the /MT or the /MTd option).  Please note that one must use\nthe same option to compile both gtest and the test code.  If you use\nVisual Studio 2005 or above, we recommend the -md version as /MD is\nthe default for new projects in these versions of Visual Studio.  On Mac OS X, open the gtest.xcodeproj in the xcode/ folder using\nXcode.  Build the \"gtest\" target.  The universal binary framework will\nend up in your selected build directory (selected in the Xcode\n\"Preferences...\" -  \"Building\" pane and defaults to xcode/build).\nAlternatively, at the command line, enter:  xcodebuild  This will build the \"Release\" configuration of gtest.framework in your\ndefault build location.  See the \"xcodebuild\" man page for more\ninformation about building different configurations and building in\ndifferent locations.  If you wish to use the Google Test Xcode project with Xcode 4.x and\nabove, you need to either:\n * update the SDK configuration options in xcode/Config/General.xconfig.\n   Comment options SDKROOT, MACOS_DEPLOYMENT_TARGET, and GCC_VERSION. If\n   you choose this route you lose the ability to target earlier versions\n   of MacOS X.\n * Install an SDK for an earlier version. This doesn't appear to be\n   supported by Apple, but has been reported to work\n   (http://stackoverflow.com/questions/5378518).", 
            "title": "Legacy Build Scripts"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#tweaking-google-test", 
            "text": "Google Test can be used in diverse environments.  The default\nconfiguration may not work (or may not work well) out of the box in\nsome environments.  However, you can easily tweak Google Test by\ndefining control macros on the compiler command line.  Generally,\nthese macros are named like GTEST_XYZ and you define them to either 1\nor 0 to enable or disable a certain feature.  We list the most frequently used macros below.  For a complete list,\nsee file include/gtest/internal/gtest-port.h.", 
            "title": "Tweaking Google Test"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#choosing-a-tr1-tuple-library", 
            "text": "Some Google Test features require the C++ Technical Report 1 (TR1)\ntuple library, which is not yet available with all compilers.  The\ngood news is that Google Test implements a subset of TR1 tuple that's\nenough for its own need, and will automatically use this when the\ncompiler doesn't provide TR1 tuple.  Usually you don't need to care about which tuple library Google Test\nuses.  However, if your project already uses TR1 tuple, you need to\ntell Google Test to use the same TR1 tuple library the rest of your\nproject uses, or the two tuple implementations will clash.  To do\nthat, add  -DGTEST_USE_OWN_TR1_TUPLE=0  to the compiler flags while compiling Google Test and your tests.  If\nyou want to force Google Test to use its own tuple library, just add  -DGTEST_USE_OWN_TR1_TUPLE=1  to the compiler flags instead.  If you don't want Google Test to use tuple at all, add  -DGTEST_HAS_TR1_TUPLE=0  and all features using tuple will be disabled.", 
            "title": "Choosing a TR1 Tuple Library"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#multi-threaded-tests", 
            "text": "Google Test is thread-safe where the pthread library is available.\nAfter #include \"gtest/gtest.h\", you can check the GTEST_IS_THREADSAFE\nmacro to see whether this is the case (yes if the macro is #defined to\n1, no if it's undefined.).  If Google Test doesn't correctly detect whether pthread is available\nin your environment, you can force it with  -DGTEST_HAS_PTHREAD=1  or  -DGTEST_HAS_PTHREAD=0  When Google Test uses pthread, you may need to add flags to your\ncompiler and/or linker to select the pthread library, or you'll get\nlink errors.  If you use the CMake script or the deprecated Autotools\nscript, this is taken care of for you.  If you use your own build\nscript, you'll need to read your compiler and linker's manual to\nfigure out what flags to add.", 
            "title": "Multi-threaded Tests"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#as-a-shared-library-dll", 
            "text": "Google Test is compact, so most users can build and link it as a\nstatic library for the simplicity.  You can choose to use Google Test\nas a shared library (known as a DLL on Windows) if you prefer.  To compile  gtest  as a shared library, add  -DGTEST_CREATE_SHARED_LIBRARY=1  to the compiler flags.  You'll also need to tell the linker to produce\na shared library instead - consult your linker's manual for how to do\nit.  To compile your  tests  that use the gtest shared library, add  -DGTEST_LINKED_AS_SHARED_LIBRARY=1  to the compiler flags.  Note: while the above steps aren't technically necessary today when\nusing some compilers (e.g. GCC), they may become necessary in the\nfuture, if we decide to improve the speed of loading the library (see\nhttp://gcc.gnu.org/wiki/Visibility for details).  Therefore you are\nrecommended to always add the above flags when using Google Test as a\nshared library.  Otherwise a future release of Google Test may break\nyour build script.", 
            "title": "As a Shared Library (DLL)"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#avoiding-macro-name-clashes", 
            "text": "In C++, macros don't obey namespaces.  Therefore two libraries that\nboth define a macro of the same name will clash if you #include both\ndefinitions.  In case a Google Test macro clashes with another\nlibrary, you can force Google Test to rename its macro to avoid the\nconflict.  Specifically, if both Google Test and some other code define macro\nFOO, you can add  -DGTEST_DONT_DEFINE_FOO=1  to the compiler flags to tell Google Test to change the macro's name\nfrom FOO to GTEST_FOO.  Currently FOO can be FAIL, SUCCEED, or TEST.\nFor example, with -DGTEST_DONT_DEFINE_TEST=1, you'll need to write  GTEST_TEST(SomeTest, DoesThis) { ... }  instead of  TEST(SomeTest, DoesThis) { ... }  in order to define a test.", 
            "title": "Avoiding Macro Name Clashes"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#upgrating-from-an-earlier-version", 
            "text": "We strive to keep Google Test releases backward compatible.\nSometimes, though, we have to make some breaking changes for the\nusers' long-term benefits.  This section describes what you'll need to\ndo if you are upgrading from an earlier version of Google Test.", 
            "title": "Upgrating from an Earlier Version"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#upgrading-from-130-or-earlier", 
            "text": "You may need to explicitly enable or disable Google Test's own TR1\ntuple library.  See the instructions in section \"Choosing a TR1 Tuple\nLibrary\".", 
            "title": "Upgrading from 1.3.0 or Earlier"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#upgrading-from-140-or-earlier", 
            "text": "The Autotools build script (configure + make) is no longer officially\nsupportted.  You are encouraged to migrate to your own build system or\nuse CMake.  If you still need to use Autotools, you can find\ninstructions in the README file from Google Test 1.4.0.  On platforms where the pthread library is available, Google Test uses\nit in order to be thread-safe.  See the \"Multi-threaded Tests\" section\nfor what this means to your build script.  If you use Microsoft Visual C++ 7.1 with exceptions disabled, Google\nTest will no longer compile.  This should affect very few people, as a\nlarge portion of STL (including  ) doesn't compile in this mode\nanyway.  We decided to stop supporting it in order to greatly simplify\nGoogle Test's implementation.", 
            "title": "Upgrading from 1.4.0 or Earlier"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#developing-google-test", 
            "text": "This section discusses how to make your own changes to Google Test.", 
            "title": "Developing Google Test"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#testing-google-test-itself", 
            "text": "To make sure your changes work as intended and don't break existing\nfunctionality, you'll want to compile and run Google Test's own tests.\nFor that you can use CMake:  mkdir mybuild\n  cd mybuild\n  cmake -Dgtest_build_tests=ON ${GTEST_DIR}  Make sure you have Python installed, as some of Google Test's tests\nare written in Python.  If the cmake command complains about not being\nable to find Python (\"Could NOT find PythonInterp (missing:\nPYTHON_EXECUTABLE)\"), try telling it explicitly where your Python\nexecutable can be found:  cmake -DPYTHON_EXECUTABLE=path/to/python -Dgtest_build_tests=ON ${GTEST_DIR}  Next, you can build Google Test and all of its own tests.  On *nix,\nthis is usually done by 'make'.  To run the tests, do  make test  All tests should pass.", 
            "title": "Testing Google Test Itself"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#regenerating-source-files", 
            "text": "Some of Google Test's source files are generated from templates (not\nin the C++ sense) using a script.  A template file is named FOO.pump,\nwhere FOO is the name of the file it will generate.  For example, the\nfile include/gtest/internal/gtest-type-util.h.pump is used to generate\ngtest-type-util.h in the same directory.  Normally you don't need to worry about regenerating the source files,\nunless you need to modify them.  In that case, you should modify the\ncorresponding .pump files instead and run the pump.py Python script to\nregenerate them.  You can find pump.py in the scripts/ directory.\nRead the Pump manual [2] for how to use it.  [2] http://code.google.com/p/googletest/wiki/PumpManual", 
            "title": "Regenerating Source Files"
        }, 
        {
            "location": "/semantic_segmentation/src/gtest-1.7.0/#contributing-a-patch", 
            "text": "We welcome patches.  Please read the Google Test developer's guide [3]\nfor how you can contribute.  In particular, make sure you have signed\nthe Contributor License Agreement, or we won't be able to accept the\npatch.  [3] http://code.google.com/p/googletest/wiki/GoogleTestDevGuide  Happy testing!", 
            "title": "Contributing a Patch"
        }, 
        {
            "location": "/semantic_segmentation/src/liblbfgs/", 
            "text": "libLBFGS: C library of limited-memory BFGS (L-BFGS)\n\n                                   Copyright (c) 1990, Jorge Nocedal\n                             Copyright (c) 2007-2010, Naoaki Okazaki\n\n\n\n=========================================================================\n1. Introduction\n=========================================================================\nlibLBFGS is a C port of the implementation of Limited-memory\nBroyden-Fletcher-Goldfarb-Shanno (L-BFGS) method written by Jorge Nocedal.\nThe original FORTRAN source code is available at:\nhttp://www.ece.northwestern.edu/~nocedal/lbfgs.html\n\n\nThe L-BFGS method solves the unconstrainted minimization problem:\n    minimize F(x), x = (x1, x2, ..., xN),\nonly if the objective function F(x) and its gradient G(x) are computable.\n\n\nRefer to the libLBFGS web site for more information.\nhttp://www.chokkan.org/software/liblbfgs/\n\n\n=========================================================================\n2. How to build\n=========================================================================\n[Microsoft Visual Studio 2008]\nOpen the solution file \"lbfgs.sln\" and build it.\n\n\n[GCC]\n$ ./configure\n$ make\n$ make install  # To install libLBFGS library and header.\n\n\n=========================================================================\n3. Note on SSE/SSE2 optimization\n=========================================================================\nThis library has SSE/SSE2 optimization routines for vector arithmetic\noperations on Intel/AMD processors. The SSE2 routine is for 64 bit double\nvalues, and the SSE routine is for 32 bit float values. Since the default\nparameters in libLBFGS are tuned for double precision values, it may need\nto modify these parameters to use the SSE optimization routines.\n\n\nTo use the SSE2 optimization routine, specify --enable-sse2 option to the\nconfigure script.\n\n\n$ ./configure --enable-sse2\n\n\nTo build libLBFGS with SSE2 optimization enabled on Microsoft Visual\nStudio 2005, define USE_SSE and \nSSE2\n symbols.\n\n\nMake sure to run libLBFGS on processors where SSE2 instrunctions are\navailable. The library does not check the existence of SSE2 instructions.\n\n\nTo package maintainers,\n\n\nPlease do not enable SSE/SSE2 optimization routine. The library built\nwith SSE/SSE2 optimization will crash without any notice when necessary\nSSE/SSE2 instructions are unavailable on CPUs.\n\n\n=========================================================================\n4. License\n=========================================================================\nlibLBFGS is distributed under the term of the MIT license.\nPlease refer to COPYING file in the distribution.\n\n\n$Id$", 
            "title": "Liblbfgs"
        }, 
        {
            "location": "/sensortag/", 
            "text": "sensortag\n\n\n\n\nMake sure the sensor is on (i.e., the green led blinks, if not press the on/off button) and run:\n\nsudo hcitool lescan\n This command scan for bluetooth devices and you should see on line something like:\nB0:B4:48:BE:CC:06 CC2650 SensorTag, the address might be different depending on the sensor\n\n\ncopy this address\n\n\nrun \nrosrun sensortag sensortagRead.py COPIED_ADDRESS", 
            "title": "Home"
        }, 
        {
            "location": "/sensortag/#sensortag", 
            "text": "Make sure the sensor is on (i.e., the green led blinks, if not press the on/off button) and run: sudo hcitool lescan  This command scan for bluetooth devices and you should see on line something like:\nB0:B4:48:BE:CC:06 CC2650 SensorTag, the address might be different depending on the sensor  copy this address  run  rosrun sensortag sensortagRead.py COPIED_ADDRESS", 
            "title": "sensortag"
        }, 
        {
            "location": "/soma/", 
            "text": "SOMA\n\n\nSemantic Object Map (SOMA) package. SOMA can include objects, regions of interest (ROI) and trajectories. It is based on Mongodb for storing high-level data obtained from perceptual pipeline of a robot. The extracted data could be stored along with spatial and temporal information which can be later used for building high-level queries with spatio-temporal constraints.\n\n\nPrerequisites\n\n\n\n\nMongoDB (\n=2.6)\n\n\nROS mongodb_store package\n\n\nROS navigation stack (only map server)\n\n\nQt5 (sudo apt-get install qtbase5-dev)\n\n\n\n\nGetting started (general steps)\n\n\n\n\n\n\nStart the ros core:\n\n\n$ roscore\n\n2. Launch the ROS datacentre:\n\n\n$ roslaunch mongodb_store mongodb_store.launch db_path:=\npath_to_db\n\nBy default, the SOMA data are stored in \nsomadata\n database. The collections under this database are \nobject\n for SOMA objects, \nroi\n for SOMA rois and \nmap\n for 2D occupancy maps.\n\n\n\n\n\n\nSOMA map manager\n\n\n\n\nSOMA is based on the assumption that all the data are with respect to 2D global map frame. So it is \nmandatory to publish a 2D map using SOMA map manager\n before using SOMA. This node is used for storing, reading and publishing 2D map:\n\n\n\n\n$ rosrun soma_map_manager soma_map_manager_node.py --mapname \nmap_name\n\n\n\n\n\nIf there are any stored 2D occupancy maps in the datacenter, the name of the map could be inputted as an argument to the map manager. Alternatively, user can choose the map to be published from the outputted list. If there are no stored maps, it will wait for a 2D map to be published from map_server. Run the map_server with a 2D map:\n  \n$ rosrun map_server map_server \nmap.yaml\n\nwhere \nmap.yaml\n specifies the map you want to load. After running the \nmap_server\n, you should save the published map using the \nSOMA map manager\n.\n\n\n\n\nIf you want to check the published map, start RVIZ, add a Map display type and subscribe to the \nsoma/map\n topic:\n\n\n\n\n$ rosrun rviz rviz\n\n\nSOMA ROI manager\n\n\n\n\n\n\nIf you want to create SOMA ROIs, run the SOMA ROI manager:\n\n\n$ rosrun soma_roi_manager soma_roi_node.py \nconfig_name\n\nwhere \nconfig_name\n denotes an object configuration name. By default, the configuration file \nsoma_roi_manager/config/default.json\n is used to initialize the list of available ROI types. Alternatively, the following command can be used to use a different configuration file:\n\n\n$ rosrun soma_roi_manager soma_roi.py -t /path/to/config/file \nconfig\n\n2D \nmap\n information will be gathered from \nsoma/map_info\n service of \nSOMA map manager\n.\n6. In RVIZ, add an InteractiveMarker display type, and subscribe to the \n/soma_roi/update\n topic:\n7. Add, delete, modify ROIs in RVIZ using the interactive marker and the context menu (right-mouse-click)\n\n\n\n\n\n\n\n\nROS Services\n\n\nThe other nodes can communicate with SOMA using the SOMA service calls. In order to use these services, one should run the soma data manager:\n\n\nSOMA data manager\n\n\n\n\nRun the soma data manager:\n\n\n\n\n$ rosrun soma_manager data_manager_node.py\n--object_collection_name \ncollection_name\n --object_db_name \ndb_name\n\n\n\n\n\nThe parameters \ndb_name\n and \ncollection_name\n are optional which can be used to define the database and collection name for data storage.\n\n\nSOMA query manager\n\n\n\n\nRun the soma query manager:\n\n\n\n\n$ rosrun soma_query_manager query_manager_node\n\nobject_db_name\n \nobject_collection_name\n \nroi_db_name\n \nroi_collection_name\n\n\n\n\n\nBy default the data is stored under default db and collections :\n\n\n\n\n\n\n\n\n\n\nobject\n\n\nROI\n\n\nmap\n\n\n\n\n\n\n\n\n\n\ndb_name\n\n\nsomadata\n\n\nsomadata\n\n\nsomadata\n\n\n\n\n\n\ncollection_name\n\n\nobject\n\n\nroi\n\n\nmap\n\n\n\n\n\n\n\n\nObject insertion\n\n\nOne or multiple SOMA objects can be inserted using the SOMA service call \n/soma/insert_objects\n. The unique mongodb ids and a boolean value are returned. The boolean return value determines whether the request was successfully completed or not.\n\n\nObject deletion\n\n\nOne or multiple SOMA objects can be deleted using the SOMA service call \n/soma/delete_objects\n. The SOMA object ids are used for deletion. The boolean return value determines whether the request was successfully completed or not.\n\n\nObject update\n\n\nA SOMA object can be updated using the SOMA service call \n/soma/update_object\n. The boolean return value determines whether the request was successfully completed or not.\n\n\nObject query\n\n\nSOMA objects could be queried using SOMA service call \n/soma/query_objects\n. The query request should be filled according to the spatio-temporal constraints. The results are returned based on the query type and constraints.\n\n\nROI query\n\n\nSOMA ROIs could be queried using SOMA service call \n/soma/query_rois\n. The query request should be filled according to the spatio-temporal constraints. The results are returned based on the query type and constraints.", 
            "title": "Home"
        }, 
        {
            "location": "/soma/#soma", 
            "text": "Semantic Object Map (SOMA) package. SOMA can include objects, regions of interest (ROI) and trajectories. It is based on Mongodb for storing high-level data obtained from perceptual pipeline of a robot. The extracted data could be stored along with spatial and temporal information which can be later used for building high-level queries with spatio-temporal constraints.", 
            "title": "SOMA"
        }, 
        {
            "location": "/soma/#prerequisites", 
            "text": "MongoDB ( =2.6)  ROS mongodb_store package  ROS navigation stack (only map server)  Qt5 (sudo apt-get install qtbase5-dev)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/soma/#getting-started-general-steps", 
            "text": "Start the ros core:  $ roscore \n2. Launch the ROS datacentre:  $ roslaunch mongodb_store mongodb_store.launch db_path:= path_to_db \nBy default, the SOMA data are stored in  somadata  database. The collections under this database are  object  for SOMA objects,  roi  for SOMA rois and  map  for 2D occupancy maps.", 
            "title": "Getting started (general steps)"
        }, 
        {
            "location": "/soma/#soma-map-manager", 
            "text": "SOMA is based on the assumption that all the data are with respect to 2D global map frame. So it is  mandatory to publish a 2D map using SOMA map manager  before using SOMA. This node is used for storing, reading and publishing 2D map:   $ rosrun soma_map_manager soma_map_manager_node.py --mapname  map_name   If there are any stored 2D occupancy maps in the datacenter, the name of the map could be inputted as an argument to the map manager. Alternatively, user can choose the map to be published from the outputted list. If there are no stored maps, it will wait for a 2D map to be published from map_server. Run the map_server with a 2D map:\n   $ rosrun map_server map_server  map.yaml \nwhere  map.yaml  specifies the map you want to load. After running the  map_server , you should save the published map using the  SOMA map manager .   If you want to check the published map, start RVIZ, add a Map display type and subscribe to the  soma/map  topic:   $ rosrun rviz rviz", 
            "title": "SOMA map manager"
        }, 
        {
            "location": "/soma/#soma-roi-manager", 
            "text": "If you want to create SOMA ROIs, run the SOMA ROI manager:  $ rosrun soma_roi_manager soma_roi_node.py  config_name \nwhere  config_name  denotes an object configuration name. By default, the configuration file  soma_roi_manager/config/default.json  is used to initialize the list of available ROI types. Alternatively, the following command can be used to use a different configuration file:  $ rosrun soma_roi_manager soma_roi.py -t /path/to/config/file  config \n2D  map  information will be gathered from  soma/map_info  service of  SOMA map manager .\n6. In RVIZ, add an InteractiveMarker display type, and subscribe to the  /soma_roi/update  topic:\n7. Add, delete, modify ROIs in RVIZ using the interactive marker and the context menu (right-mouse-click)", 
            "title": "SOMA ROI manager"
        }, 
        {
            "location": "/soma/#ros-services", 
            "text": "The other nodes can communicate with SOMA using the SOMA service calls. In order to use these services, one should run the soma data manager:", 
            "title": "ROS Services"
        }, 
        {
            "location": "/soma/#soma-data-manager", 
            "text": "Run the soma data manager:   $ rosrun soma_manager data_manager_node.py\n--object_collection_name  collection_name  --object_db_name  db_name   The parameters  db_name  and  collection_name  are optional which can be used to define the database and collection name for data storage.", 
            "title": "SOMA data manager"
        }, 
        {
            "location": "/soma/#soma-query-manager", 
            "text": "Run the soma query manager:   $ rosrun soma_query_manager query_manager_node object_db_name   object_collection_name   roi_db_name   roi_collection_name   By default the data is stored under default db and collections :      object  ROI  map      db_name  somadata  somadata  somadata    collection_name  object  roi  map", 
            "title": "SOMA query manager"
        }, 
        {
            "location": "/soma/#object-insertion", 
            "text": "One or multiple SOMA objects can be inserted using the SOMA service call  /soma/insert_objects . The unique mongodb ids and a boolean value are returned. The boolean return value determines whether the request was successfully completed or not.", 
            "title": "Object insertion"
        }, 
        {
            "location": "/soma/#object-deletion", 
            "text": "One or multiple SOMA objects can be deleted using the SOMA service call  /soma/delete_objects . The SOMA object ids are used for deletion. The boolean return value determines whether the request was successfully completed or not.", 
            "title": "Object deletion"
        }, 
        {
            "location": "/soma/#object-update", 
            "text": "A SOMA object can be updated using the SOMA service call  /soma/update_object . The boolean return value determines whether the request was successfully completed or not.", 
            "title": "Object update"
        }, 
        {
            "location": "/soma/#object-query", 
            "text": "SOMA objects could be queried using SOMA service call  /soma/query_objects . The query request should be filled according to the spatio-temporal constraints. The results are returned based on the query type and constraints.", 
            "title": "Object query"
        }, 
        {
            "location": "/soma/#roi-query", 
            "text": "SOMA ROIs could be queried using SOMA service call  /soma/query_rois . The query request should be filled according to the spatio-temporal constraints. The results are returned based on the query type and constraints.", 
            "title": "ROI query"
        }, 
        {
            "location": "/soma/soma_llsd/", 
            "text": "SOMA Low-Level Sensory Datastore\n\n\nThis package provides a totally optional set of helper tools for the task of composing SOMA objects from individual observations of real-world objects. The tools here, provide a custom set of ROS messages and services. The classic use-case for this package being that of collecting multiple observations of a single or multiple objects before merging them into combined object models. In this use-case, the raw data and low-level segmented observations are stored in the data structures provided by this package, and are used as the source material to create high-level SOMA objects. However, the implementation of how these things are achieved -- data collection, segmentation, processing -- are all left up to the application developer.\n\n\nMessages\n\n\nThe messages are organised in a graph structure. The conceptual root node is the Scene message, which contains various forms of unprocessed robot perception data -- RGB-D data, odometry, arbitrary metadata etc. -- that a system may use in further processing. This represents the raw sensor output of a Robot performing some task such as taking multiple views of a surface, whereby each view would be represented as a Scene message. Multiple Scenes can be grouped together by sharing a common episode_id.\n\n\nGiven a \nScene\n, processing such as segmentation is usually applied to extract objects. Each of these objects can be represented using the \nSegment\n message. There may be multiple \nSegments\n in a single \nScene\n, and the same \nSegment\n representing the same object may be observed in multiple \nScenes\n during a given episode. The message types support this sort of relationship. For each segment, any number of \nObservations\n can be attached to a \nSegment\n. An \nObservation\n represents the data produced by any processing performed on the \nScene\n, and is intended to store things like cropped images and masked point clouds that describe specific observations of objects, and the \nSegment\n links these observations together to the same object across multiple views.\n\n\nServices\n\n\nThere are multiple services designed to make it easy to insert and extract data from the database. These are generally straightforward in use, and there are examples in the /tests/ folder you can try out. The only service that may bear further explanation is \nInsertSceneAuto\n, which allows you to specify robot_pose and camera_info topics, and will automatically record and insert these along with ~2 seconds of output from the /tf topic to your Scene message. Recording a brief burst of /tf allows you to re-use it later and re-calculate things like frame transforms. Transforming to and from a tf transformer is very easy, see the TransformationStore class in server.py for examples. This is the recommended way of putting scenes into the database, as it requires the least amount of effort on the part of the user.\n\n\nIn general, all fields in all messages are optional, and arbitrary extra relationships between messages can be encoded by using the meta_data fields, which are intended to be filled by JSON objects.\n\n\nServices are deliberately kept bare-bones, as in contrast to the main SOMA services that provide feature-rich query services, it is intended that users of the LLSD perform more complicated query tasks by using MongoDB queries.", 
            "title": "Soma llsd"
        }, 
        {
            "location": "/soma/soma_llsd/#soma-low-level-sensory-datastore", 
            "text": "This package provides a totally optional set of helper tools for the task of composing SOMA objects from individual observations of real-world objects. The tools here, provide a custom set of ROS messages and services. The classic use-case for this package being that of collecting multiple observations of a single or multiple objects before merging them into combined object models. In this use-case, the raw data and low-level segmented observations are stored in the data structures provided by this package, and are used as the source material to create high-level SOMA objects. However, the implementation of how these things are achieved -- data collection, segmentation, processing -- are all left up to the application developer.", 
            "title": "SOMA Low-Level Sensory Datastore"
        }, 
        {
            "location": "/soma/soma_llsd/#messages", 
            "text": "The messages are organised in a graph structure. The conceptual root node is the Scene message, which contains various forms of unprocessed robot perception data -- RGB-D data, odometry, arbitrary metadata etc. -- that a system may use in further processing. This represents the raw sensor output of a Robot performing some task such as taking multiple views of a surface, whereby each view would be represented as a Scene message. Multiple Scenes can be grouped together by sharing a common episode_id.  Given a  Scene , processing such as segmentation is usually applied to extract objects. Each of these objects can be represented using the  Segment  message. There may be multiple  Segments  in a single  Scene , and the same  Segment  representing the same object may be observed in multiple  Scenes  during a given episode. The message types support this sort of relationship. For each segment, any number of  Observations  can be attached to a  Segment . An  Observation  represents the data produced by any processing performed on the  Scene , and is intended to store things like cropped images and masked point clouds that describe specific observations of objects, and the  Segment  links these observations together to the same object across multiple views.", 
            "title": "Messages"
        }, 
        {
            "location": "/soma/soma_llsd/#services", 
            "text": "There are multiple services designed to make it easy to insert and extract data from the database. These are generally straightforward in use, and there are examples in the /tests/ folder you can try out. The only service that may bear further explanation is  InsertSceneAuto , which allows you to specify robot_pose and camera_info topics, and will automatically record and insert these along with ~2 seconds of output from the /tf topic to your Scene message. Recording a brief burst of /tf allows you to re-use it later and re-calculate things like frame transforms. Transforming to and from a tf transformer is very easy, see the TransformationStore class in server.py for examples. This is the recommended way of putting scenes into the database, as it requires the least amount of effort on the part of the user.  In general, all fields in all messages are optional, and arbitrary extra relationships between messages can be encoded by using the meta_data fields, which are intended to be filled by JSON objects.  Services are deliberately kept bare-bones, as in contrast to the main SOMA services that provide feature-rich query services, it is intended that users of the LLSD perform more complicated query tasks by using MongoDB queries.", 
            "title": "Services"
        }, 
        {
            "location": "/soma/soma_trajectory/", 
            "text": "SOMA Trajectory\n\n\nSOMA Trajectory is a package to query and display human trajectories.\nIt includes a simple visual interface to query spatio-temporal constraints.\n\n\nPrerequisites\n\n\n\n\nMongoDB (\n=2.6)\n\n\nmongodb_store\n\n\npymongo \n\n\nshapely\n\n\n\n\nGetting started (general steps)\n\n\n\n\n\n\nStart the ros core:\n\n\n$ roscore\n\n2. Launch the ROS datacentre:\n\n\n```\n$ roslaunch mongodb_store mongodb_store.launch db_path:=\n\n\n```\n\n\n\n\n\n\nSOMA map manager\n\n\n\n\n\n\nRun the soma map manager for storing, reading and publishing 2D map. Running this node is essential for running the robot_state_viewer_node:\n  \n$ rosrun soma_map_manager soma_map_manager_node.py --mapname \nmap_name\n\n  If there are any stored 2D occupancy maps in the datacentre then map manager will let you choose the map to be published or you can input the name of the stored map as an argument as \nmap_name\n. If not, it will wait for map_server. Run the map_server with a 2D map:\n    \n$ rosrun map_server map_server \nmap.yaml\n\n  where \nmap.yaml\n specifies the map you want to load. After running the \nmap_server\n, you should save the published map using the \nsoma map manager\n.\n\n\n\n\n\n\nIf you want to check the published map, start RVIZ, add a Map display type and subscribe to the \nsoma/map\n topic:\n\n\n$ rosrun rviz rviz\n\n\n\n\n\n\nSOMA Trajectory Visualizer\n\n\nYou can run the visualizer by calling \nrosrun soma_trajectory soma_trajectory_manager.py\n and \nrosrun soma_trajectory visualiser.py\n\n\n\n\n\n\nAdd an InteractiveMarkers display type in RVIZ and subscribe to the \nsoma_trajectory\n topic. Trajectories will appear once the query submit button on the visualisation is pressed.\n\n\n\n\n\n\nRun rviz to display the results of the queries. You can choose the time interval to be inspected in terms of hours from the hour the first trajectory obtained to the hour the last trajectory obtained. You can also execute temporal periodic queries by setting days of the week, and hours of a day. Whenever a checkbox inside the temporal peridic query is ticked, the regular query with the time interval will be ignored. A simple analysis is displayed in the message box in the bottom of the visualiser for each query.", 
            "title": "Soma trajectory"
        }, 
        {
            "location": "/soma/soma_trajectory/#soma-trajectory", 
            "text": "SOMA Trajectory is a package to query and display human trajectories.\nIt includes a simple visual interface to query spatio-temporal constraints.", 
            "title": "SOMA Trajectory"
        }, 
        {
            "location": "/soma/soma_trajectory/#prerequisites", 
            "text": "MongoDB ( =2.6)  mongodb_store  pymongo   shapely", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/soma/soma_trajectory/#getting-started-general-steps", 
            "text": "Start the ros core:  $ roscore \n2. Launch the ROS datacentre:  ```\n$ roslaunch mongodb_store mongodb_store.launch db_path:=  ```", 
            "title": "Getting started (general steps)"
        }, 
        {
            "location": "/soma/soma_trajectory/#soma-map-manager", 
            "text": "Run the soma map manager for storing, reading and publishing 2D map. Running this node is essential for running the robot_state_viewer_node:\n   $ rosrun soma_map_manager soma_map_manager_node.py --mapname  map_name \n  If there are any stored 2D occupancy maps in the datacentre then map manager will let you choose the map to be published or you can input the name of the stored map as an argument as  map_name . If not, it will wait for map_server. Run the map_server with a 2D map:\n     $ rosrun map_server map_server  map.yaml \n  where  map.yaml  specifies the map you want to load. After running the  map_server , you should save the published map using the  soma map manager .    If you want to check the published map, start RVIZ, add a Map display type and subscribe to the  soma/map  topic:  $ rosrun rviz rviz", 
            "title": "SOMA map manager"
        }, 
        {
            "location": "/soma/soma_trajectory/#soma-trajectory-visualizer", 
            "text": "You can run the visualizer by calling  rosrun soma_trajectory soma_trajectory_manager.py  and  rosrun soma_trajectory visualiser.py    Add an InteractiveMarkers display type in RVIZ and subscribe to the  soma_trajectory  topic. Trajectories will appear once the query submit button on the visualisation is pressed.    Run rviz to display the results of the queries. You can choose the time interval to be inspected in terms of hours from the hour the first trajectory obtained to the hour the last trajectory obtained. You can also execute temporal periodic queries by setting days of the week, and hours of a day. Whenever a checkbox inside the temporal peridic query is ticked, the regular query with the time interval will be ignored. A simple analysis is displayed in the message box in the bottom of the visualiser for each query.", 
            "title": "SOMA Trajectory Visualizer"
        }, 
        {
            "location": "/soma/soma_visualizer/", 
            "text": "SOMA Visualizer\n\n\nSOMA Visualizer is a GUI for querying and visualizing SOMA objects.\n\n\nUsing the visual interface, advanced queries with spatio-temporal constraints  can be specified. The returned SOMA objects are displayed in rviz as point clouds.\n\n\nPrerequisites\n\n\n\n\nMongoDB (\n=2.6)\n\n\nmongodb_store\n\n\nQt5\n\n\n\n\nGetting started (general steps)\n\n\n\n\n\n\nStart the ros core:\n\n\n$ roscore\n\n2. Launch the ROS datacentre:\n\n\n```\n$ roslaunch mongodb_store mongodb_store.launch db_path:=\n\n\n```\n\n\n\n\n\n\nSOMA map manager\n\n\n\n\n\n\nRun the soma map manager for storing, reading and publishing 2D map. Running this node is essential for running the robot_state_viewer_node:\n  \n$ rosrun soma_map_manager soma_map_manager_node.py --mapname \nmap_name\n\n  If there are any stored 2D occupancy maps in the datacentre then map manager will let you choose the map to be published or you can input the name of the stored map as an argument as \nmap_name\n. If not, it will wait for map_server. Run the map_server with a 2D map:\n    \n$ rosrun map_server map_server \nmap.yaml\n\n  where \nmap.yaml\n specifies the map you want to load. After running the \nmap_server\n, you should save the published map using the \nsoma map manager\n.\n\n\n\n\n\n\nIf you want to check the published map, start RVIZ, add a Map display type and subscribe to the \nsoma/map\n topic:\n\n\n$ rosrun rviz rviz\n\n\n\n\n\n\nSOMA Visualizer\n\n\nYou can run the visualizer by calling \nroslaunch soma_visualizer soma_visualizer.launch\n\n\n\n\n\n\nAdd a MarkerArray display type in RVIZ and subscribe to the \nsoma_roi_marker_array\n topic. The drawer will draw when a region is selected in the visualizer. Add a pointcloud2 display type and subscribe to \n/soma_visualizer_node/world_state_3d\n to visualize query results.\n\n\n\n\n\n\nRun rviz to display the results of the queries. You can go back and forth between robot states using the slider. You can choose the time interval to be inspected in terms of days,hours and minutes. You can also execute advanced queries by setting dates, times, etc, enabling them using the checkboxes and by pressing the \nQuery\n button. When you make changes in query constrains, make sure to press \nQuery\n button for updating the query. You can also export the executed query in JSON format using the \nExport JSON\n button. You can reload data from databse using \nReload\n button. The returned objects are also displayed in the table view. You can double click on any of the rows to see the detailed information and images of that object. If there are multiple images, you can go through them by pressing to left and right buttons.\n\n\n\n\n\n\nFor any query, if more than 50 objects are fetched, only first 50 of them are fetched with point clouds and images because of the performance issues\n.", 
            "title": "Soma visualizer"
        }, 
        {
            "location": "/soma/soma_visualizer/#soma-visualizer", 
            "text": "SOMA Visualizer is a GUI for querying and visualizing SOMA objects.  Using the visual interface, advanced queries with spatio-temporal constraints  can be specified. The returned SOMA objects are displayed in rviz as point clouds.", 
            "title": "SOMA Visualizer"
        }, 
        {
            "location": "/soma/soma_visualizer/#prerequisites", 
            "text": "MongoDB ( =2.6)  mongodb_store  Qt5", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/soma/soma_visualizer/#getting-started-general-steps", 
            "text": "Start the ros core:  $ roscore \n2. Launch the ROS datacentre:  ```\n$ roslaunch mongodb_store mongodb_store.launch db_path:=  ```", 
            "title": "Getting started (general steps)"
        }, 
        {
            "location": "/soma/soma_visualizer/#soma-map-manager", 
            "text": "Run the soma map manager for storing, reading and publishing 2D map. Running this node is essential for running the robot_state_viewer_node:\n   $ rosrun soma_map_manager soma_map_manager_node.py --mapname  map_name \n  If there are any stored 2D occupancy maps in the datacentre then map manager will let you choose the map to be published or you can input the name of the stored map as an argument as  map_name . If not, it will wait for map_server. Run the map_server with a 2D map:\n     $ rosrun map_server map_server  map.yaml \n  where  map.yaml  specifies the map you want to load. After running the  map_server , you should save the published map using the  soma map manager .    If you want to check the published map, start RVIZ, add a Map display type and subscribe to the  soma/map  topic:  $ rosrun rviz rviz", 
            "title": "SOMA map manager"
        }, 
        {
            "location": "/soma/soma_visualizer/#soma-visualizer_1", 
            "text": "You can run the visualizer by calling  roslaunch soma_visualizer soma_visualizer.launch    Add a MarkerArray display type in RVIZ and subscribe to the  soma_roi_marker_array  topic. The drawer will draw when a region is selected in the visualizer. Add a pointcloud2 display type and subscribe to  /soma_visualizer_node/world_state_3d  to visualize query results.    Run rviz to display the results of the queries. You can go back and forth between robot states using the slider. You can choose the time interval to be inspected in terms of days,hours and minutes. You can also execute advanced queries by setting dates, times, etc, enabling them using the checkboxes and by pressing the  Query  button. When you make changes in query constrains, make sure to press  Query  button for updating the query. You can also export the executed query in JSON format using the  Export JSON  button. You can reload data from databse using  Reload  button. The returned objects are also displayed in the table view. You can double click on any of the rows to see the detailed information and images of that object. If there are multiple images, you can go through them by pressing to left and right buttons.    For any query, if more than 50 objects are fetched, only first 50 of them are fetched with point clouds and images because of the performance issues .", 
            "title": "SOMA Visualizer"
        }, 
        {
            "location": "/soma/soma_utils/config/", 
            "text": "EXAMPLE OF A ROI-object KB\n\n\n\n\n{ \n  \"1\": { # ROI ID \n    \"name\" : \"kitchen table\", # OPTIONAL NAME OF THE ROI\n\n    \"pos_objects\" : [\"cup\", \"plate\"] # ALLOWED OBJECTS (IF MISSING EVERYTHING EXCEPT NEG-OBJECT ARE ALLOWED)\n  },\n  \"2\": { # ROI ID \n    \"name\" : \"kitchen counter\", # OPTIONAL NAME OF THE ROI\n    \"neg_objects\" : [\"unknown\"] # NOT-ALLOWED OBJECTS (IF MISSING EVERYTHING EXCEPT POS-OBJECTS ARE NOT-ALLOWED)\n  }\n  \"3\": { # ROI ID \n    \"name\" : \"meeting room table\", # OPTIONAL NAME OF THE ROI\n    \"pos_objects\" : [\"projector\"], # ALLOWED OBJECTS (IF MISSING EVERYTHING EXCEPT NEG-OBJECT ARE ALLOWED)\n    \"neg_objects\" : [\"cup\"] # NOT-ALLOWED OBJECTS (IF MISSING EVERYTHING EXCEPT POS-OBJECTS ARE NOT-ALLOWED)\n    }\n}", 
            "title": "Config"
        }, 
        {
            "location": "/soma/soma_utils/robblog/", 
            "text": "STRANDS will produce intelligent mobile robots that are able to run for months in dynamic human environments. We will provide robots with the longevity and behavioural robustness necessary to make them truly useful assistants in a wide range of domains. Such long-lived robots will be able to learn from a wider range of experiences than has previously been possible, creating a whole new generation of autonomous systems able to extract and exploit the structure in their worlds.\n\n\nOur approach is based on understanding 3D space and how it changes over time, from milliseconds to months. We will develop novel approaches to extract spatio-temporal structure from sensor data gathered during months of autonomous operation. Extracted structure will include reoccurring 3D shapes, objects, people, and models of activity. We will also develop control mechanisms which exploit these structures to yield adaptive behaviour in highly demanding, realworld security and care scenarios.\n\n\nYou can find out more on our \nwebsite\n.", 
            "title": "Robblog"
        }, 
        {
            "location": "/strands-docker/", 
            "text": "STRANDS distro docker image(s)\n\n\nDo run an interactive session in the fully installed STRANDS base system, simply make sure you have \ndocker installed on your machine\n, and then you can simply run\n\n\ndocker run -it --rm strands/strands-base /bin/bash\n \n\n\nto launch an interactive session. In there, most STRANDS packages are available, however, access to any local hardware (GPU) is not directly possible, there is more documentation for this at http://wiki.ros.org/docker/Tutorials/Hardware%20Acceleration\n\n\nBut this is enough to have a play with some STRANDS software and connect it to other ROS components as required. \n\n\nRunning on a Linux host\n\n\nIf you want to run this with your local user and actually have the docker container access your X server, run something like:\n\n\ndocker run -it --rm \\\n    --user=`id -u` \\\n    --env=\nDISPLAY\n \\\n    --workdir=\n/home/$USER\n \\\n    --volume=\n/home/$USER:/home/$USER\n \\\n    --volume=\n/etc/group:/etc/group:ro\n \\\n    --volume=\n/etc/passwd:/etc/passwd:ro\n \\\n    --volume=\n/etc/shadow:/etc/shadow:ro\n \\\n    --volume=\n/etc/sudoers.d:/etc/sudoers.d:ro\n \\\n    --volume=\n/tmp/.X11-unix:/tmp/.X11-unix:rw\n \\\n    strands/strands-base /bin/bash\n\n\n\n\nRunning on OSX/Windows\n\n\nThis is a useful guide for running X-enabled docker images on OSX: https://blog.bennycornelissen.nl/bwc-gui-apps-in-docker-on-osx/\n\n\nand here is a gist run this on a MAC: https://gist.github.com/marc-hanheide/d9b4bb6057665acf7524c7b79827f1c8\n\n\nRequirements:\n\n install docker on OSX: https://docs.docker.com/docker-for-mac/\n\n create a docker machine: \ndocker-machine create --driver virtualbox --virtualbox-memory 2048 docker-vm\n\n\n \nsource docker-x.sh\n from https://gist.github.com/marc-hanheide/d9b4bb6057665acf7524c7b79827f1c8\n\n run \ndocker_run strands/strands-base\n\n\nBuilds\n\n\nBuilding locally\n\n\nbuild locally via \ndocker build --tag ros:strands --network host\n\n\nAutomated builds on hub.docker.com\n\n\nThis repository is set up to release automatically a STRANDS docker image into the official docker repository at https://hub.docker.com/r/strands/strands-base/", 
            "title": "Home"
        }, 
        {
            "location": "/strands-docker/#strands-distro-docker-images", 
            "text": "Do run an interactive session in the fully installed STRANDS base system, simply make sure you have  docker installed on your machine , and then you can simply run  docker run -it --rm strands/strands-base /bin/bash    to launch an interactive session. In there, most STRANDS packages are available, however, access to any local hardware (GPU) is not directly possible, there is more documentation for this at http://wiki.ros.org/docker/Tutorials/Hardware%20Acceleration  But this is enough to have a play with some STRANDS software and connect it to other ROS components as required.", 
            "title": "STRANDS distro docker image(s)"
        }, 
        {
            "location": "/strands-docker/#running-on-a-linux-host", 
            "text": "If you want to run this with your local user and actually have the docker container access your X server, run something like:  docker run -it --rm \\\n    --user=`id -u` \\\n    --env= DISPLAY  \\\n    --workdir= /home/$USER  \\\n    --volume= /home/$USER:/home/$USER  \\\n    --volume= /etc/group:/etc/group:ro  \\\n    --volume= /etc/passwd:/etc/passwd:ro  \\\n    --volume= /etc/shadow:/etc/shadow:ro  \\\n    --volume= /etc/sudoers.d:/etc/sudoers.d:ro  \\\n    --volume= /tmp/.X11-unix:/tmp/.X11-unix:rw  \\\n    strands/strands-base /bin/bash", 
            "title": "Running on a Linux host"
        }, 
        {
            "location": "/strands-docker/#running-on-osxwindows", 
            "text": "This is a useful guide for running X-enabled docker images on OSX: https://blog.bennycornelissen.nl/bwc-gui-apps-in-docker-on-osx/  and here is a gist run this on a MAC: https://gist.github.com/marc-hanheide/d9b4bb6057665acf7524c7b79827f1c8  Requirements:  install docker on OSX: https://docs.docker.com/docker-for-mac/  create a docker machine:  docker-machine create --driver virtualbox --virtualbox-memory 2048 docker-vm    source docker-x.sh  from https://gist.github.com/marc-hanheide/d9b4bb6057665acf7524c7b79827f1c8  run  docker_run strands/strands-base", 
            "title": "Running on OSX/Windows"
        }, 
        {
            "location": "/strands-docker/#builds", 
            "text": "", 
            "title": "Builds"
        }, 
        {
            "location": "/strands-docker/#building-locally", 
            "text": "build locally via  docker build --tag ros:strands --network host", 
            "title": "Building locally"
        }, 
        {
            "location": "/strands-docker/#automated-builds-on-hubdockercom", 
            "text": "This repository is set up to release automatically a STRANDS docker image into the official docker repository at https://hub.docker.com/r/strands/strands-base/", 
            "title": "Automated builds on hub.docker.com"
        }, 
        {
            "location": "/strands_3d_mapping/calibrate_sweeps/", 
            "text": "calibrate_sweeps\n\n\nImplements an action server for calibrating the intermediate sweep positions. Uses the \nstrands_sweep_registration\n package internally. \n\n\nStart node\n\n\nrosrun calibrate_sweeps calibrate_sweep.as\n\n\nCall action server\n\n\nrosrun actionlib axclient.py /calibrate_sweeps\n\n\nAction\n\n\n#goal\nint32 min_num_sweeps\nint32 max_num_sweeps\nstring sweep_location\nstring save_location\n---\n#result\nstring calibration_file\n---\n#feedback\nint32 percentage_done\n\n\n\n\n\n\nmin_num_sweeps\n - the minimum number of sweeps needed to start the calibration\n\n\nmax_num_sweeps\n - the maximum number of sweeps to use when doing the calibration\n\n\nsweep_location\n - where to look for the sweeps. If this argument is left empty, the default path is \n~/.semanticMap/\n\n\nsave_location\n - where to save the registered sweeps after the calibration has finished. If this argument is left empty, the default path is the same as \nsweep_location\n\n\n\n\nSweeps used\n\n\nThe calibration process uses only sweeps recorded with the type \ncomplete\n if using the \ndo_sweeps.py\n action server from the \ncloud_merge\n package, i.e. with 51 positions. \n\n\nIf using the \nptu_action_server_metric_map.py\n action server from the \nscitos_ptu\n package, the parameters are \n-160 20 160 -30 30 30\n. \n\n\nSweeps recorded with different parameters are ignored for the calibration. For registration, sweeps with different parameters are also processed if their parameters are a subset of the \ncomplete\n sweep type parameters (e.g. \ncomlpete\n sweep type parameters are \n-160 20 160 -30 30 30\n; an example subset of those would be \n-160 40 160 -30 30 30\n, i.e. fewer pan stops).\n\n\nResults\n\n\nThe calibration results are saved in \n~/.ros/semanticMap\n. These are:\n\n\n\n\nregistration_transforms.txt\n the result of the 51 transforms for the intermediate poses.\n\n\nregistration_transforms_raw.txt\n legacy - contains the same data as above in a different format, needed by the \nstrands_sweep_registration\n package. \n\n\ncamera_params.txt\n contains the optimized camera parameters. This is currently disabled, and the stored camera parameters should be the same as the input camera parameters.\n\n\nsweep_paramters.txt\n the sweep parameters used by the calibration (\n-160 20 160 -30 30 30\n)", 
            "title": "Calibrate sweeps"
        }, 
        {
            "location": "/strands_3d_mapping/calibrate_sweeps/#calibrate_sweeps", 
            "text": "Implements an action server for calibrating the intermediate sweep positions. Uses the  strands_sweep_registration  package internally.", 
            "title": "calibrate_sweeps"
        }, 
        {
            "location": "/strands_3d_mapping/calibrate_sweeps/#start-node", 
            "text": "rosrun calibrate_sweeps calibrate_sweep.as", 
            "title": "Start node"
        }, 
        {
            "location": "/strands_3d_mapping/calibrate_sweeps/#call-action-server", 
            "text": "rosrun actionlib axclient.py /calibrate_sweeps", 
            "title": "Call action server"
        }, 
        {
            "location": "/strands_3d_mapping/calibrate_sweeps/#action", 
            "text": "#goal\nint32 min_num_sweeps\nint32 max_num_sweeps\nstring sweep_location\nstring save_location\n---\n#result\nstring calibration_file\n---\n#feedback\nint32 percentage_done   min_num_sweeps  - the minimum number of sweeps needed to start the calibration  max_num_sweeps  - the maximum number of sweeps to use when doing the calibration  sweep_location  - where to look for the sweeps. If this argument is left empty, the default path is  ~/.semanticMap/  save_location  - where to save the registered sweeps after the calibration has finished. If this argument is left empty, the default path is the same as  sweep_location", 
            "title": "Action"
        }, 
        {
            "location": "/strands_3d_mapping/calibrate_sweeps/#sweeps-used", 
            "text": "The calibration process uses only sweeps recorded with the type  complete  if using the  do_sweeps.py  action server from the  cloud_merge  package, i.e. with 51 positions.   If using the  ptu_action_server_metric_map.py  action server from the  scitos_ptu  package, the parameters are  -160 20 160 -30 30 30 .   Sweeps recorded with different parameters are ignored for the calibration. For registration, sweeps with different parameters are also processed if their parameters are a subset of the  complete  sweep type parameters (e.g.  comlpete  sweep type parameters are  -160 20 160 -30 30 30 ; an example subset of those would be  -160 40 160 -30 30 30 , i.e. fewer pan stops).", 
            "title": "Sweeps used"
        }, 
        {
            "location": "/strands_3d_mapping/calibrate_sweeps/#results", 
            "text": "The calibration results are saved in  ~/.ros/semanticMap . These are:   registration_transforms.txt  the result of the 51 transforms for the intermediate poses.  registration_transforms_raw.txt  legacy - contains the same data as above in a different format, needed by the  strands_sweep_registration  package.   camera_params.txt  contains the optimized camera parameters. This is currently disabled, and the stored camera parameters should be the same as the input camera parameters.  sweep_paramters.txt  the sweep parameters used by the calibration ( -160 20 160 -30 30 30 )", 
            "title": "Results"
        }, 
        {
            "location": "/strands_3d_mapping/cloud_merge/", 
            "text": "Package for building local metric maps\n\n\ncloud_merge_node\n\n\nDependencies\n\n\nMake sure you have Qt installed on the robot by getting the rqt packages:\n\n\nsudo apt-get install ros-hydro-qt-ros\nsudo apt-get install ros-hydro-rqt\nsudo apt-get install ros-hydro-qt-build\n\n\n\n\nDescription\n\n\nThe \ncloud_merge_node\n acquires data from the RGBD sensor, as the PTU unit does a sweep, stopping at various positions as provided as input to the \nscitos_ptu ptu_pan_tilt_metric_map.py\n action server. (As an alternative, one can use the \ndo_sweep.py\n action server from this package, which provides a higher level interface to doing a sweep). \n\n\nAs the PTU stops at a position, a number of RGBD frames are collected and averaged, with the purpose of reducing noise. Each one of these frames are converted to the global frame of reference, and merged together to form an observation point cloud, which is further processed by the \nsemantic_map semantic_map_node\n node. \n\n\nIf the sweep intermediate positions have been calibrated (using the \ncalibrate_sweeps calibrate_sweep_as\n action server) and the parameter \nregister_and_correct_sweep\n is set to \ntrue\n, the collected sweeps are also registered. Note that this registration is for the intermediate point clouds making up the sweep, and not between two sweeps.\n\n\nThe observations are stored on the disk, in the folder\n\n\n~.semanticMap/ \n\n\n\n\nTo start the \ncloud_merge_node\n, run:\n\n\nroslaunch cloud_merge cloud_merge.launch\n\n\n\n\nInput topics\n\n\n\n\n/ptu/log\n  : this topic provides information about the sweep (i.e. parameters, started, position reached, finished).\n\n\n/current_node\n : the waypoint id received on this topic is associated with the sweep collected\n\n\n\n\nOutput topics\n\n\n\n\n/local_metric_map/intermediate_point_cloud\n - RGBD point cloud corresponding to an intermediate position\n\n\n/local_metric_map/merged_point_cloud\n - merged point cloud with resolution specified by the \nvoxel_size_table_top\n parameter\n\n\n/local_metric_map/merged_point_cloud_downsampled\n - merged point cloud with resolution specified by the \nvoxel_size_observation\n parameter\n\n\n/local_metric_map/depth/depth_filtered\n - averaged depth frames corresponding to an intermediate position\n\n\n/local_metric_map/rgb/rgb_filtered\n - averaged RGB frames corresponding to an intermediate position\n\n\n/local_metric_map/depth/camera_info\n - camera info message corresponding to the image published on the \n/local_metric_map/depth/depth_filtered\n topic\n\n\n/local_metric_map/rgb/camera_info\n - camera info message corresponding to the image published on the \n/local_metric_map/rgb/rgb_filtered\n topic\n\n\n/local_metric_map/room_observations\n - string message containing the absolute path to the xml file corresponding to the collected sweep. This is used by the \nsemantic_map semantic_map_node\n to trigger a Meta-Room update. \n\n\n\n\nParameters:\n\n\n\n\nsave_intermediate\n (true/false)- whether to save the intermediate point clouds to disk; default \ntrue\n\n\ncleanup\n (true/false) - whether to remove previously saved data from \n~/.semanticMap/\n; default \nfalse\n\n\ngenerate_pointclouds\n (true/false) - generate point clouds from RGBD images or use the point clouds produced by the camera driver directly; default \ntrue\n. Note that setting \nfalse\n here has not been used for a while and might not work as expected. \n\n\nlog_to_db\n (true/false) - whether to log data to mongodb database; default \ntrue\n\n\nvoxel_size_table_top\n (double) - the cell size to downsample the merged point cloud to before being published for detecting table tops; default \n0.01 m\n\n\nvoxel_size_observation\n (double) - the cell size to downsample the merge point cloud to for visualisation purposes in rviz; default \n0.03 m\n\n\npoint_cutoff_distance\n (double) - maximum distance after which data should be discarded when constructing the merged point cloud; default \n4.0 m\n\n\nmax_instances\n (int) - how many instances of each observation to keep stored on disk; default \n2\n\n\ninput_cloud\n - name of the topic for the RGBD input point clouds (this is used when \ngenerate_pointclouds\n is \nfalse\n); default \n/depth_registered/points\n. Note: this has not been used for a while and might not work as expected.\n\n\ninput_rgb\n - name of the topic for the RGB image (this is used when generate_pointclouds is true); default \n/head_xtion/rgb/image_color\n\n\ninput_depth\n - name of the topic for the depth image (this is used when generate_pointclouds is true); default \n/head_xtion/depth/image_raw\n \n\n\ninput_caminfo\n - name of the topic for the camera parameters (this is used when generate_pointclouds is true); default \n/head_xtion/rgb/camera_info\n\n\n\n\nExtracting data from mongodb\n\n\nAfter logging some data, you can extract if from the database and saved it to disk in a folder of your choice using:\n\n\nrosrun semantic_map load_from_mongo /path/where/to/save/\n\n\n\n\nAfter extracting data from the database, you can load all the recorded observations in appropriate datastructures (containing the waypoint_id, merged cloud, individual point clouds, individual rgb and depth images and camera parameters):\n\n\nrosrun metaroom_xml_parser load_multiple_files /path/where/to/load/from/\n\n\n\n\n(Note the \n/\n at the end of the path in the command above). \n\n\ndo_sweeps.py\n\n\nTo start the action server manually:\n\n\nrosrun cloud_merge do_sweep.py\n\n\nUse:\n\n\nrosrun actionlib axclient.py /do_sweep\n\n\nThis action server takes as input a string, with the following values defined: \"complete\", \"medium\", \"short\", \"shortest\". Internally the action server from \nscitos_ptu ptu_action_server_metric_map.py\n is used, so make sure that is running.\n\n\nThe behavior is the following:\n\n\nIf sweep type is \ncomplete\n, the sweep is started with parameters \n-160 20 160 -30 30 30\n -\n 51 positions\nIf sweep type is \nmedium\n, the sweep is started with parameters \n-160 20 160 -30 30 -30\n -\n 17 positions\nIf sweep type is \nshort\n, the sweep is started with parameters \n-160 40 160 -30 30 -30\n -\n 9 positions\nIf sweep type is \nshortest\n, the sweep is started with parameters \n-160 60 140 -30 30 -30\n -\n 6 positions (there might be blank areas with this sweep type, depending on the environment).", 
            "title": "Cloud merge"
        }, 
        {
            "location": "/strands_3d_mapping/cloud_merge/#package-for-building-local-metric-maps", 
            "text": "", 
            "title": "Package for building local metric maps"
        }, 
        {
            "location": "/strands_3d_mapping/cloud_merge/#cloud_merge_node", 
            "text": "", 
            "title": "cloud_merge_node"
        }, 
        {
            "location": "/strands_3d_mapping/cloud_merge/#dependencies", 
            "text": "Make sure you have Qt installed on the robot by getting the rqt packages:  sudo apt-get install ros-hydro-qt-ros\nsudo apt-get install ros-hydro-rqt\nsudo apt-get install ros-hydro-qt-build", 
            "title": "Dependencies"
        }, 
        {
            "location": "/strands_3d_mapping/cloud_merge/#description", 
            "text": "The  cloud_merge_node  acquires data from the RGBD sensor, as the PTU unit does a sweep, stopping at various positions as provided as input to the  scitos_ptu ptu_pan_tilt_metric_map.py  action server. (As an alternative, one can use the  do_sweep.py  action server from this package, which provides a higher level interface to doing a sweep).   As the PTU stops at a position, a number of RGBD frames are collected and averaged, with the purpose of reducing noise. Each one of these frames are converted to the global frame of reference, and merged together to form an observation point cloud, which is further processed by the  semantic_map semantic_map_node  node.   If the sweep intermediate positions have been calibrated (using the  calibrate_sweeps calibrate_sweep_as  action server) and the parameter  register_and_correct_sweep  is set to  true , the collected sweeps are also registered. Note that this registration is for the intermediate point clouds making up the sweep, and not between two sweeps.  The observations are stored on the disk, in the folder  ~.semanticMap/   To start the  cloud_merge_node , run:  roslaunch cloud_merge cloud_merge.launch", 
            "title": "Description"
        }, 
        {
            "location": "/strands_3d_mapping/cloud_merge/#input-topics", 
            "text": "/ptu/log   : this topic provides information about the sweep (i.e. parameters, started, position reached, finished).  /current_node  : the waypoint id received on this topic is associated with the sweep collected", 
            "title": "Input topics"
        }, 
        {
            "location": "/strands_3d_mapping/cloud_merge/#output-topics", 
            "text": "/local_metric_map/intermediate_point_cloud  - RGBD point cloud corresponding to an intermediate position  /local_metric_map/merged_point_cloud  - merged point cloud with resolution specified by the  voxel_size_table_top  parameter  /local_metric_map/merged_point_cloud_downsampled  - merged point cloud with resolution specified by the  voxel_size_observation  parameter  /local_metric_map/depth/depth_filtered  - averaged depth frames corresponding to an intermediate position  /local_metric_map/rgb/rgb_filtered  - averaged RGB frames corresponding to an intermediate position  /local_metric_map/depth/camera_info  - camera info message corresponding to the image published on the  /local_metric_map/depth/depth_filtered  topic  /local_metric_map/rgb/camera_info  - camera info message corresponding to the image published on the  /local_metric_map/rgb/rgb_filtered  topic  /local_metric_map/room_observations  - string message containing the absolute path to the xml file corresponding to the collected sweep. This is used by the  semantic_map semantic_map_node  to trigger a Meta-Room update.", 
            "title": "Output topics"
        }, 
        {
            "location": "/strands_3d_mapping/cloud_merge/#parameters", 
            "text": "save_intermediate  (true/false)- whether to save the intermediate point clouds to disk; default  true  cleanup  (true/false) - whether to remove previously saved data from  ~/.semanticMap/ ; default  false  generate_pointclouds  (true/false) - generate point clouds from RGBD images or use the point clouds produced by the camera driver directly; default  true . Note that setting  false  here has not been used for a while and might not work as expected.   log_to_db  (true/false) - whether to log data to mongodb database; default  true  voxel_size_table_top  (double) - the cell size to downsample the merged point cloud to before being published for detecting table tops; default  0.01 m  voxel_size_observation  (double) - the cell size to downsample the merge point cloud to for visualisation purposes in rviz; default  0.03 m  point_cutoff_distance  (double) - maximum distance after which data should be discarded when constructing the merged point cloud; default  4.0 m  max_instances  (int) - how many instances of each observation to keep stored on disk; default  2  input_cloud  - name of the topic for the RGBD input point clouds (this is used when  generate_pointclouds  is  false ); default  /depth_registered/points . Note: this has not been used for a while and might not work as expected.  input_rgb  - name of the topic for the RGB image (this is used when generate_pointclouds is true); default  /head_xtion/rgb/image_color  input_depth  - name of the topic for the depth image (this is used when generate_pointclouds is true); default  /head_xtion/depth/image_raw    input_caminfo  - name of the topic for the camera parameters (this is used when generate_pointclouds is true); default  /head_xtion/rgb/camera_info", 
            "title": "Parameters:"
        }, 
        {
            "location": "/strands_3d_mapping/cloud_merge/#extracting-data-from-mongodb", 
            "text": "After logging some data, you can extract if from the database and saved it to disk in a folder of your choice using:  rosrun semantic_map load_from_mongo /path/where/to/save/  After extracting data from the database, you can load all the recorded observations in appropriate datastructures (containing the waypoint_id, merged cloud, individual point clouds, individual rgb and depth images and camera parameters):  rosrun metaroom_xml_parser load_multiple_files /path/where/to/load/from/  (Note the  /  at the end of the path in the command above).", 
            "title": "Extracting data from mongodb"
        }, 
        {
            "location": "/strands_3d_mapping/cloud_merge/#do_sweepspy", 
            "text": "To start the action server manually:  rosrun cloud_merge do_sweep.py  Use:  rosrun actionlib axclient.py /do_sweep  This action server takes as input a string, with the following values defined: \"complete\", \"medium\", \"short\", \"shortest\". Internally the action server from  scitos_ptu ptu_action_server_metric_map.py  is used, so make sure that is running.  The behavior is the following:  If sweep type is  complete , the sweep is started with parameters  -160 20 160 -30 30 30  -  51 positions\nIf sweep type is  medium , the sweep is started with parameters  -160 20 160 -30 30 -30  -  17 positions\nIf sweep type is  short , the sweep is started with parameters  -160 40 160 -30 30 -30  -  9 positions\nIf sweep type is  shortest , the sweep is started with parameters  -160 60 140 -30 30 -30  -  6 positions (there might be blank areas with this sweep type, depending on the environment).", 
            "title": "do_sweeps.py"
        }, 
        {
            "location": "/strands_3d_mapping/ekz-public-lib/", 
            "text": "Section 1: License\nSection 2: Dependencies\nSection 3: BUILD\nSection 4: SOURCE CODE\nSection 5: EXAMPLES\nSection 6: TODOLIST\n\n\n-------------------------Section 1: License------------------------------------\n\n\nLicense: BSD\n\n\n-------------------------Section 2: Dependencies-------------------------------\n\n\nDependencies required:\n1: Install ros hydro from:  http://wiki.ros.org/hydro/Installation/\n2: Install ros-pcl with:    apt-get install ros-hydro-perception-pcl\n3: Install ros-opencv2:     apt-get install ros-hydro-opencv2\n\n\nOptional(recommended):\n4: Install ros openni:      apt-get install ros-hydro-openni-launch\n-------------------------Section 3: BUILD--------------------------------------\nPlace in catkin src folder.\nUse catkin_make from catkin folder.\n\n\n-------------------------Section 4: SOURCE CODE--------------------------------\nfound in installation directory + /src/\nThis section summarizes the contents of the src directory. For each folder a summary of the contents are provided with a short list important files that the user is likely to interact with.\n\n\n--\nFile: ekz.h\nInfo: To include library just include ekz.h. Includes of other files for the library.\n\n\n--\nFolder:core\nInfo: Contains core classes for the library.\nImportant Files:\n\n\nCalibration.h       //Calibration class, controls focal length etc\n\n\nFrameInput.h        //Contains the input of one frame. Got functions like getXYZ(int w, int h), getPointCloud() etc.\n\n\nRGBDFrame.h         //Representation of a frame. Contains extracted information for the frame such as segmentation and keypoints etc.\n\n\nTransformation.h    //Represents a transformation between two frames.\n\n\n--\nFolder:FeatureDescriptor\nInfo: Contains different types of feature descriptors used by the library.\nImportant files:\nFeatureDescriptor.h //Base class for Feature descriptors, contains functions such as distance(FeatureDescriptor * other_descriptor).\n\n\n--\nFolder:FeatureExtractor\nInfo: Contains different types of feature extractors to be chosen from by the library.\nImportant files:\nFeatureExtractor.h  //Core class for feature extractors\nOrbExtractor.h      //Class used to extract Orb keypoints\nSurfExtractor.h     //Class used to extract Surf keypoints\n\n\n--\nFolder:FrameMatcher\nInfo: Contains registration algorithms that takes two frames as inputs(no initial guess given).\nImportant files:\nFrameMatcher.h  //Core class for Frame matchers\nAICK.h          //AICK base implementation without heuristic matching algorithm.\nbowAICK.h       //AICK implementation with heuristic matching algorithm. Faster than AICK.h.\n\n\n--\nFolder:Map\nInfo: Contains Map3D classes.\nImportant files:\nMap3D.h         //Basic map class. Contains many usefull functions to reduce the complexity for the user. Registers frames added sequentially.\n\n\n--\nFolder: mygeometry\nInfo: Contains geometry classes such as planes and points. Also contains Keypoints base class.\n\n\n--\nFolder:RGBDSegmentation\nInfo: Contains RGBD segmentation algorithms to be used on the RGBDFrames. Currently unused.\n\n\n--\nFolder:TransformationFilter\nInfo:Contains registration algorithms that takes two frames as inputs with an initial transformation and improves the solution. Currently unused.\n\n\n--\nFolder:apps\nInfo:Contains example code of how to use the library. See Section 5 for details.\n\n\n-------------------------Section 5: EXAMPLES-----------------------------------\nfound in installation directory + /src/apps/\nUnzip testdata.7z to gain access to some test data to run the examples with.\n\n\n====================image_recorder.cpp====================\nSummary:\n    Records data and stores it in .pcd files from a the rostopic /camera/depth_registered/points\nInput:\n    path where to store recorded data\nOutput:\n    png image pairs with RGBD data captured from a the rostopic /camera/depth_registered/points\nUSAGE:\n    Run roscore\n    Run roslaunch openni_launch openni.launch\n    Run image_recorder program with an argument telling the recorder where to store the data\n\n\n====================pcd_recorder.cpp====================\nSummary:\n    Records data and stores it in .png files from a the rostopic /camera/depth_registered/points\nInput:\n    path where to store recorded data\nOutput:\n    captured pairs(depth and RGB) of .png files\nUSAGE:\n    Run roscore\n    Run roslaunch openni_launch openni.launch\n    Run pcd_recorder program with an argument telling the recorder where to store the data\n\n\n====================example_register_pcd_map.cpp====================\nSummary:\n    Minimalistic example for registering data provided in .pcd files sequentially using a Map3D object.\nInput:\n    a set of paths to pcd files\nOutput:\n    .pcd file of aligned data\nUSAGE:\n    Run example_register_pcd_map program with a set of paths to pcd files to be registed\n\n\n====================example_register_images_map.cpp====================\nSummary:\n    Minimalistic example for registering data provided in .png files sequentially using a Map3D object.\nInput:\n    a path to a folder where png files with the correct names are located\nOutput:\n    .pcd file of aligned data\nUSAGE:\n    Run example_register_images_map program with an argument telling the program where to find the data\n\n\n====================example_register_pcd_standalone.cpp====================\nSummary:\n    Example for registering data provided in .pcd files sequentially.\nInput:\n    a set of paths to pcd files\nOutput:\n    .pcd file of aligned data\nUSAGE:\n    Run example_register_pcd_map program with a set of paths to pcd files to be registed\n\n\n====================example_register_images_standalone.cpp====================\nSummary:\n    Example for registering data provided in .png files sequentially.\nInput:\n    a path to a folder where png files with the correct names are located\nOutput:\n    .pcd file of aligned data\nUSAGE:\n    Run example_register_images_map program with an argument telling the program where to find the data\n\n\n====================example_register_images_fast_map.cpp====================\nSummary:\n    Example for registering data provided in .png files sequentially using a Map3D object using ORB features and AICK with bag of words.\nInput:\n    a path to a folder where png files with the correct names are located\n    a path+fileprefix to a folder where a pre trained bag of words model is located\nOutput:\n    .pcd file of aligned data\nUSAGE:\n    Run example_register_images_fast_map program with an argument telling the program where to find the data\n\n\n====================example_bow_images.cpp====================\nSummary:\n    Example for training a bag of words model for data provided in .png files using a Map3Dbow.\nInput:\n    a path to a folder where png files with the correct names are located, a path/name for output, number of files to read and a number to controll how what part of the frames given will be used.\nOutput:\n    .pcd file of aligned data\nUSAGE:\n    Run example_bow_images program with a path to a folder where png files with the correct names are located, a path/name for output, number of files to read and a number to controll how what part of the frames given will be used.\n\n\n-------------------------Section 1: TODOLIST   -------------------------\nUse trees to speed up word association during frame generation.\nProvide more maptypes.\nGive option to provide initial guess for poses in map.", 
            "title": "Ekz public lib"
        }, 
        {
            "location": "/strands_3d_mapping/", 
            "text": "scitos_3d_mapping\n\n\nTools for building 3D maps and using these maps for navigation and visualization.\n\n\nStart the system\n\n\nStart all the nodes in this repository using:\n\n\nroslaunch semantic_map_launcher semantic_map.launch\n\n\nData acquisition\n\n\nTo collect sweeps, use the action server from: \ncloud_merge do_sweep.py\n\n\nTo start the action server manually (already launched with \nroslaunch semantic_map_launcher semantic_map.launch\n):\n\n\nrosrun cloud_merge do_sweep.py\n\n\nUse:\n\n\nrosrun actionlib axclient.py /do_sweep\n\n\nThis action server takes as input a string, with the following values defined: \"complete\", \"medium\", \"short\", \"shortest\". Internally the action server from \nscitos_ptu\n called \nptu_action_server_metric_map.py\n is used, so make sure that is running. \n\n\nThe behavior is the following:\n\n If sweep type is \ncomplete\n, the sweep is started with parameters \n-160 20 160 -30 30 30\n -\n 51 positions\n\n If sweep type is \nmedium\n, the sweep is started with parameters \n-160 20 160 -30 30 -30\n -\n 17 positions\n\n If sweep type is \nshort\n, the sweep is started with parameters \n-160 40 160 -30 30 -30\n -\n 9 positions\n\n If sweep type is \nshortest\n, the sweep is started with parameters \n-160 60 140 -30 30 -30\n -\n 6 positions (there might be blank areas with this sweep type, depending on the environment).\n\n\nCalibrate sweep poses\n\n\nOnce a number of sweeps of type \"complete\" have been collected, you can run the calibration routine which will compute the registration transformations for the 51 poses. Afterwards, you can execute sweeps of any type (from the types defined above) and the correct transformations will be loaded so that the sweeps are registered.\n\n\nTo start the action server manually (already launched with \nroslaunch semantic_map_launcher semantic_map.launch\n):\n\n\nrosrun calibrate_sweeps calibrate_sweep_as\n\n\nUse:\n\n\nrosrun actionlib axclient.py /calibrate_sweeps\n\n\n(Here you have to specify the minimum and maximum number of sweeps to use for the optimization. To get good registration results you should have collected \n 5 sweeps. Note that only sweeps of type \"complete\" are used here, all others are ignored). \n\n\nOnce the calibration has been executed, the parameters are saved in \n~/.ros/semanticMap/\n from where they are loaded whenever needed. All sweeps recorded up to this point are automatically corrected using the registered sweeps.\n\n\nMeta-Rooms\n\n\nThe Meta-Rooms are created by the \nsemantic_map semantic_map_node\n. To start, run:\n\n\nroslaunch semantic_map semantic_map.launch\n\n\nFor more information check out the \nsemantic_map\n package. \n\n\nThe dynamic clusters are published on the \n/local_metric_map/dynamic_clusters\n topic and the Meta-Rooms are published on the \n/local_metric_map/metaroom\n topic. \n\n\nReinitialize the Meta-Rooms\n\n\nAfter the calibration you can re-initialize the metarooms (in general a good idea, as the registration between the sweeps should be better now that the poses have been calibrated).\n\n\nrosservice call /local_metric_map/ClearMetaroomService \"waypoint_id: - 'WayPointXYZ' initialize: true\"\n\n\nSet the argument initialize to \ntrue\n and provide all the waypoints for which you want to re-initialize the metarooms in the \nwaypoint_id\n list. \n\n\nAccess invidual dynamic clusters\n\n\nThe package \nobject_manager\n allows access to individual dynamic clusters, via a number of services. To start use:\n\n\nrosrun object_manager object_manager_node\n\n\nFor more information check out the \nobject_manager\n package.\n\n\nsemantic_map_publisher\n\n\nThe package \nsemantic_map_publisher\n provides a number of services for accessing previously collected data which is stored on the disk. To start use:\n\n\nrosrun semantic_map_publisher semantic_map_publisher\n\n\nFor more information check out the \nsemantic_map_publisher\n package.\n\n\nAccessing saved data\n\n\nThe package \nmetaroom_xml_parser\n provides a number of utilities for reading previously saved sweep data. These include utilities for accessing:\n\n\n\n\nmerged point clouds\n\n\nindividual point clouds\n\n\ndynamic clusters\n\n\nlabelled data\n\n\nsweep xml files.\n\n\n\n\nCheck out the \nmetaroom_xml_parser\n package for more information.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_3d_mapping/#scitos_3d_mapping", 
            "text": "Tools for building 3D maps and using these maps for navigation and visualization.", 
            "title": "scitos_3d_mapping"
        }, 
        {
            "location": "/strands_3d_mapping/#start-the-system", 
            "text": "Start all the nodes in this repository using:  roslaunch semantic_map_launcher semantic_map.launch", 
            "title": "Start the system"
        }, 
        {
            "location": "/strands_3d_mapping/#data-acquisition", 
            "text": "To collect sweeps, use the action server from:  cloud_merge do_sweep.py  To start the action server manually (already launched with  roslaunch semantic_map_launcher semantic_map.launch ):  rosrun cloud_merge do_sweep.py  Use:  rosrun actionlib axclient.py /do_sweep  This action server takes as input a string, with the following values defined: \"complete\", \"medium\", \"short\", \"shortest\". Internally the action server from  scitos_ptu  called  ptu_action_server_metric_map.py  is used, so make sure that is running.   The behavior is the following:  If sweep type is  complete , the sweep is started with parameters  -160 20 160 -30 30 30  -  51 positions  If sweep type is  medium , the sweep is started with parameters  -160 20 160 -30 30 -30  -  17 positions  If sweep type is  short , the sweep is started with parameters  -160 40 160 -30 30 -30  -  9 positions  If sweep type is  shortest , the sweep is started with parameters  -160 60 140 -30 30 -30  -  6 positions (there might be blank areas with this sweep type, depending on the environment).", 
            "title": "Data acquisition"
        }, 
        {
            "location": "/strands_3d_mapping/#calibrate-sweep-poses", 
            "text": "Once a number of sweeps of type \"complete\" have been collected, you can run the calibration routine which will compute the registration transformations for the 51 poses. Afterwards, you can execute sweeps of any type (from the types defined above) and the correct transformations will be loaded so that the sweeps are registered.  To start the action server manually (already launched with  roslaunch semantic_map_launcher semantic_map.launch ):  rosrun calibrate_sweeps calibrate_sweep_as  Use:  rosrun actionlib axclient.py /calibrate_sweeps  (Here you have to specify the minimum and maximum number of sweeps to use for the optimization. To get good registration results you should have collected   5 sweeps. Note that only sweeps of type \"complete\" are used here, all others are ignored).   Once the calibration has been executed, the parameters are saved in  ~/.ros/semanticMap/  from where they are loaded whenever needed. All sweeps recorded up to this point are automatically corrected using the registered sweeps.", 
            "title": "Calibrate sweep poses"
        }, 
        {
            "location": "/strands_3d_mapping/#meta-rooms", 
            "text": "The Meta-Rooms are created by the  semantic_map semantic_map_node . To start, run:  roslaunch semantic_map semantic_map.launch  For more information check out the  semantic_map  package.   The dynamic clusters are published on the  /local_metric_map/dynamic_clusters  topic and the Meta-Rooms are published on the  /local_metric_map/metaroom  topic.", 
            "title": "Meta-Rooms"
        }, 
        {
            "location": "/strands_3d_mapping/#reinitialize-the-meta-rooms", 
            "text": "After the calibration you can re-initialize the metarooms (in general a good idea, as the registration between the sweeps should be better now that the poses have been calibrated).  rosservice call /local_metric_map/ClearMetaroomService \"waypoint_id: - 'WayPointXYZ' initialize: true\"  Set the argument initialize to  true  and provide all the waypoints for which you want to re-initialize the metarooms in the  waypoint_id  list.", 
            "title": "Reinitialize the Meta-Rooms"
        }, 
        {
            "location": "/strands_3d_mapping/#access-invidual-dynamic-clusters", 
            "text": "The package  object_manager  allows access to individual dynamic clusters, via a number of services. To start use:  rosrun object_manager object_manager_node  For more information check out the  object_manager  package.", 
            "title": "Access invidual dynamic clusters"
        }, 
        {
            "location": "/strands_3d_mapping/#semantic_map_publisher", 
            "text": "The package  semantic_map_publisher  provides a number of services for accessing previously collected data which is stored on the disk. To start use:  rosrun semantic_map_publisher semantic_map_publisher  For more information check out the  semantic_map_publisher  package.", 
            "title": "semantic_map_publisher"
        }, 
        {
            "location": "/strands_3d_mapping/#accessing-saved-data", 
            "text": "The package  metaroom_xml_parser  provides a number of utilities for reading previously saved sweep data. These include utilities for accessing:   merged point clouds  individual point clouds  dynamic clusters  labelled data  sweep xml files.   Check out the  metaroom_xml_parser  package for more information.", 
            "title": "Accessing saved data"
        }, 
        {
            "location": "/strands_3d_mapping/learn_objects_action/", 
            "text": "This package provides an action server for performing object learning, as demonstrated at the Y2 review and published in (RA-L reference). It depends heavily on the STRANDS setup, in particular the robot PTU configuration and topological navigation.\n\n\nDependencies\n\n\nThere are a large number of STRANDS package dependencies for this package:\n\n\n\n\nstrands_navigation_msgs\n\nis required as monitored_navigation is used for all robot motion.\n\n\nobject_view_generator\n\nis used to generate the position for the robot to move to observe the object.\n\n\nptu_follow_frame\n\nis used to keep the object in the centre of the PTU throughout the learning.\n\n\nstatic_transform_manager\n\nis used to set a TF from for the object to allow tracking.\n\n\nscitos_ptu\n\nis used to control the ptu.\n\n\nobject_manager\n\nis used to store and retrieve information about dynamic clusters.\n\n\nsemantic_map_to_2d\n\nis used to obtain a down-projected 3D map of the area around an object for planning.\n\n\ncloud_merge\n\nis used to trigger a metric map action.\n\n\ncamera_srv_definitions\n\nis used to track the camera pose as the robot drives around.\n\n\nincremental_object_learning_srv_definitions\n\nis used to stitch together views of the object and learn a model.\n\n\nrosbag_openni_compression\n is used to record a rosbag containing most of the published ros topics (including images from the \nhead_xtion\ncamera).\n\n\nrecognition_srv_definitions\n\nis (not yet) used to re-recognise the learned object.\n\n\n\n\nStarting the server node\n\n\nA lot of nodes need to be running before this action server will start. If any are missing then a warning will be printed out.\nThe required nodes are as follows:\n\n\nStandard nodes/launches:\n- \nstrands_bringup / strands_core\n is needed to provide MongoDB access with message store\n- \nstrands_bringup / strands_robot\n is needed for robot motion and to enable the PTU to be controlled by velocity\n- \nstrands_bringup / strands_cameras\n is needed to provide \nhead_xtion\n, using the latest OpenNI2 launch approach.\n- \nstrands_bringup / strands_navigation\n is required for monitored_navigation etc.\n\n\nAdditional specific nodes to launch:\n- metric mapping: \nroslaunch semantic_map_launcher semantic_map.launch\n\n- metric map ptu: \nrosrun scitos_ptu ptu_action_server_metric_map.py\n\n- map down projector: \nrosrun semantic_map_to_2d semantic_map_2d_server\n\n- camera tracker: \nrosrun camera_tracker camera_track_service  _camera_topic:=/head_xtion/depth_registered\n\n- object learning node: \nrosrun incremental_object_learning incremental_object_learning_service\n\n- ptu tracker: \nrosrun ptu_follow_frame ptu_follow.py\n\n- transform manager: \nrosrun static_transform_manager static_tf_services.py\n\n- view planner: \nrosrun object_view_generator view_points_service.py _map_topic:=/waypoint_map\n\n\nFinally the learn object action can be started:\n\n\nrosrun learn_objects_action server.py _model_path:=/home/strands/test_models _record_run:=False\n\n\n\n\nThere node can be started with the following parameters:\n- \nmodel_path\n: this is non-optional and must provide the directory where the learnt model should be stored.\n- \nrois_file\n: this is the optional path to a file detailing the SOMA roi to use for which waypoint. If not \nprovided then no SOMA regions will be needed. (see below for example file)\n- \ndebug_mode\n: by default this is False, but if set True then the action will step through the various parts of the \nlearning process. At each stage the action server will need confirmation to proceed, supplied over a ros topic.\n- \nplanning_method\n: This selects which planning method to use to aquire the additional object views. Currently just the \ndefault 'ral16' is working, but there is a skeleton method 'infogain' that is ready to add the \nnbv_planning\n code to.\n- \nrecord_run\n : This denotes whether or not to record the ros topics as the robot navigates around a cluster collecting additional views. By default this is False. \n\n\nLaunch file\n\n\nAll the dependencies, including the \nlearn_objects_action\n action server can be started with:\n\n\nroslaunch learn_objects_action learn_objects_dependencies.launch model_path:=/path/to/models/folder \n\n\n\n\nTriggering a learning session\n\n\nThe object learning is triggered by the action \n/learn_object\n. This takes the waypoint name as the only field in it's goal definition. This sould be the waypoint that the robot is already at when triggering the learning.\n\n\nOnce started, the robot will perform a short metric map sweep, calculate the difference between the new map and the previous one at that location, select a cluster to learn based on how many views it can get of it, then drive around the object acquiring views. These views will be stitched together by the incremental object learning code from V4r, and finally a model will be save in the specified (by a parameter) folder. \n\n\nRViz montoring\n\n\nThere are several topics that can be monitored in RViz to get an idea of what is happening:\n- \n/waypoint_map\n shows the free space that the robot can plan in. Add it as a costmap to make it easier to see ontop of the base map.\n- \n/object_view_goals : PoseArray\n shows the view points that the robot will try and reach\n- \n/local_metric_map/dynamic_clusters : PointCloud2\n shows the difference between this map and the last one. Once of the shown clusters will be the learning target.\n- \n/local_metric_map/observations : PointCloud2\n shows the new observations of the target object as they arrive.\n\n\nDebug mode\n\n\nWhen in debug mode the action server will wait for confirmation to proceed between states, and confirmation of which dynamic cluster to select. If running in debug mode then it is best to \nlook at the code\n to see what is required.\n\n\nSOMA Rois file format\n\n\nThe optional file detailing which ROI to use for which region should be of the following format:\n\n\nWayPointName: 'conf/map/region_no`\nOtherPointName: 'conf/map/another_region`\nWayPointN: ''\n\n\n\n\nwhere every waypoint is covered, ones that should not be constrained to a region are given empty strings.\n\n\nLimitations\n\n\n\n\nSometimes, if the object is too far behind the robot, the robot turning completely will be too fast and the camera tracker fail. This results in bad models due to failed registration.\n\n\nIf the object crosses the back of the robot while driving, then the PTU has to do a full 360 degree spin to keep tracking it. During this the camera tracker will likely fail. Therefore runs with objects infront of the waypoint are more likely to be nice.\n\n\nIf monitored navigation fails to move the robot, only one re-attempt is made. If that fails the action fails.\n\n\nThe PTU tilt angle is super restricted. Very often objects are too low down, so the PTU can not see them at a reasonable close distance to the object, resulting in tracking of the object without it actually being in view. Make sure objects to learn are at chest height.", 
            "title": "Learn objects action"
        }, 
        {
            "location": "/strands_3d_mapping/learn_objects_action/#dependencies", 
            "text": "There are a large number of STRANDS package dependencies for this package:   strands_navigation_msgs \nis required as monitored_navigation is used for all robot motion.  object_view_generator \nis used to generate the position for the robot to move to observe the object.  ptu_follow_frame \nis used to keep the object in the centre of the PTU throughout the learning.  static_transform_manager \nis used to set a TF from for the object to allow tracking.  scitos_ptu \nis used to control the ptu.  object_manager \nis used to store and retrieve information about dynamic clusters.  semantic_map_to_2d \nis used to obtain a down-projected 3D map of the area around an object for planning.  cloud_merge \nis used to trigger a metric map action.  camera_srv_definitions \nis used to track the camera pose as the robot drives around.  incremental_object_learning_srv_definitions \nis used to stitch together views of the object and learn a model.  rosbag_openni_compression  is used to record a rosbag containing most of the published ros topics (including images from the  head_xtion camera).  recognition_srv_definitions \nis (not yet) used to re-recognise the learned object.", 
            "title": "Dependencies"
        }, 
        {
            "location": "/strands_3d_mapping/learn_objects_action/#starting-the-server-node", 
            "text": "A lot of nodes need to be running before this action server will start. If any are missing then a warning will be printed out.\nThe required nodes are as follows:  Standard nodes/launches:\n-  strands_bringup / strands_core  is needed to provide MongoDB access with message store\n-  strands_bringup / strands_robot  is needed for robot motion and to enable the PTU to be controlled by velocity\n-  strands_bringup / strands_cameras  is needed to provide  head_xtion , using the latest OpenNI2 launch approach.\n-  strands_bringup / strands_navigation  is required for monitored_navigation etc.  Additional specific nodes to launch:\n- metric mapping:  roslaunch semantic_map_launcher semantic_map.launch \n- metric map ptu:  rosrun scitos_ptu ptu_action_server_metric_map.py \n- map down projector:  rosrun semantic_map_to_2d semantic_map_2d_server \n- camera tracker:  rosrun camera_tracker camera_track_service  _camera_topic:=/head_xtion/depth_registered \n- object learning node:  rosrun incremental_object_learning incremental_object_learning_service \n- ptu tracker:  rosrun ptu_follow_frame ptu_follow.py \n- transform manager:  rosrun static_transform_manager static_tf_services.py \n- view planner:  rosrun object_view_generator view_points_service.py _map_topic:=/waypoint_map  Finally the learn object action can be started:  rosrun learn_objects_action server.py _model_path:=/home/strands/test_models _record_run:=False  There node can be started with the following parameters:\n-  model_path : this is non-optional and must provide the directory where the learnt model should be stored.\n-  rois_file : this is the optional path to a file detailing the SOMA roi to use for which waypoint. If not \nprovided then no SOMA regions will be needed. (see below for example file)\n-  debug_mode : by default this is False, but if set True then the action will step through the various parts of the \nlearning process. At each stage the action server will need confirmation to proceed, supplied over a ros topic.\n-  planning_method : This selects which planning method to use to aquire the additional object views. Currently just the \ndefault 'ral16' is working, but there is a skeleton method 'infogain' that is ready to add the  nbv_planning  code to.\n-  record_run  : This denotes whether or not to record the ros topics as the robot navigates around a cluster collecting additional views. By default this is False.", 
            "title": "Starting the server node"
        }, 
        {
            "location": "/strands_3d_mapping/learn_objects_action/#launch-file", 
            "text": "All the dependencies, including the  learn_objects_action  action server can be started with:  roslaunch learn_objects_action learn_objects_dependencies.launch model_path:=/path/to/models/folder", 
            "title": "Launch file"
        }, 
        {
            "location": "/strands_3d_mapping/learn_objects_action/#triggering-a-learning-session", 
            "text": "The object learning is triggered by the action  /learn_object . This takes the waypoint name as the only field in it's goal definition. This sould be the waypoint that the robot is already at when triggering the learning.  Once started, the robot will perform a short metric map sweep, calculate the difference between the new map and the previous one at that location, select a cluster to learn based on how many views it can get of it, then drive around the object acquiring views. These views will be stitched together by the incremental object learning code from V4r, and finally a model will be save in the specified (by a parameter) folder.", 
            "title": "Triggering a learning session"
        }, 
        {
            "location": "/strands_3d_mapping/learn_objects_action/#rviz-montoring", 
            "text": "There are several topics that can be monitored in RViz to get an idea of what is happening:\n-  /waypoint_map  shows the free space that the robot can plan in. Add it as a costmap to make it easier to see ontop of the base map.\n-  /object_view_goals : PoseArray  shows the view points that the robot will try and reach\n-  /local_metric_map/dynamic_clusters : PointCloud2  shows the difference between this map and the last one. Once of the shown clusters will be the learning target.\n-  /local_metric_map/observations : PointCloud2  shows the new observations of the target object as they arrive.", 
            "title": "RViz montoring"
        }, 
        {
            "location": "/strands_3d_mapping/learn_objects_action/#debug-mode", 
            "text": "When in debug mode the action server will wait for confirmation to proceed between states, and confirmation of which dynamic cluster to select. If running in debug mode then it is best to  look at the code  to see what is required.", 
            "title": "Debug mode"
        }, 
        {
            "location": "/strands_3d_mapping/learn_objects_action/#soma-rois-file-format", 
            "text": "The optional file detailing which ROI to use for which region should be of the following format:  WayPointName: 'conf/map/region_no`\nOtherPointName: 'conf/map/another_region`\nWayPointN: ''  where every waypoint is covered, ones that should not be constrained to a region are given empty strings.", 
            "title": "SOMA Rois file format"
        }, 
        {
            "location": "/strands_3d_mapping/learn_objects_action/#limitations", 
            "text": "Sometimes, if the object is too far behind the robot, the robot turning completely will be too fast and the camera tracker fail. This results in bad models due to failed registration.  If the object crosses the back of the robot while driving, then the PTU has to do a full 360 degree spin to keep tracking it. During this the camera tracker will likely fail. Therefore runs with objects infront of the waypoint are more likely to be nice.  If monitored navigation fails to move the robot, only one re-attempt is made. If that fails the action fails.  The PTU tilt angle is super restricted. Very often objects are too low down, so the PTU can not see them at a reasonable close distance to the object, resulting in tracking of the object without it actually being in view. Make sure objects to learn are at chest height.", 
            "title": "Limitations"
        }, 
        {
            "location": "/strands_3d_mapping/metaroom_xml_parser/", 
            "text": "Package for parsing saved room observations\n\n\nDescription\n\n\nThe \nmetaroom_xml_parser\n package is used to parse previously saved room observations. The data will be read into an appropriate data structure containing: merged point cloud, individual point clouds, individual RGB and depth images and corresponding camera parameters. \n\n\nUsage\n\n\n\n\nParsing one file\n\n\n\n\nThe \nload_single_file\n application reads in one room observation.\n\n\nrosrun metaroom_xml_parser load_single_file /path/to/xml\n\n\n\n\n\n\nParsing multiple observations\n\n\n\n\nThe 'load_multiple_files' application reads in multiple room observations and returns a vector. It takes as input the root folder where the observations are stored. \n\n\nrosrun metaroom_xml_parser load_multiple_files /path/to/folder\n\n\n\n\n\n\nParsing labelled data\n\n\n\n\nThe \nload_labelled_data\n application reads labelled data for observations taken at a particular waypoint. The waypoint id and the folder where the observations are stored are taken in as parameters.\n\n\nrosrun metaroom_xml_parser load_labelled_data` /path/to/sweeps WayPointXYZ\n\n\n\n\nUtilities\n\n\nA number of utilities are provided by this package, for easy data manipulation. The definitions can be seen in the file \nload_utilities.h\n\n\nMerged cloud utilities\n\n\nThe complete cloud datatype is:\n\n\ntemplate \nclass PointType\n boost::shared_ptr\npcl::PointCloud\nPointType\n\n\nThe utilities for loading only the merged cloud are:\n\n \nloadMergedCloudFromSingleSweep\n # returns one cloud\n\n \nloadMergedCloudFromMultipleSweeps\n # returns a vector of merged clouds, one for each sweep\n* \nloadMergedCloudForTopologicalWaypoint\n # same as above\n\n\nIntermediate cloud utilities\n\n\nThe intermediate cloud datatype is:\n\n\n    template \nclass PointType\n\n    struct IntermediateCloudCompleteData\n    {\n        std::vector\nboost::shared_ptr\npcl::PointCloud\nPointType\n  vIntermediateRoomClouds;\n        std::vector\ntf::StampedTransform\n                           vIntermediateRoomCloudTransforms;\n        std::vector\nimage_geometry::PinholeCameraModel\n             vIntermediateRoomCloudCamParams;\n        std::vector\ntf::StampedTransform\n                           vIntermediateRoomCloudTransformsRegistered;\n        std::vector\nimage_geometry::PinholeCameraModel\n             vIntermediateRoomCloudCamParamsCorrected;\n        std::vector\ncv::Mat\n                                        vIntermediateRGBImages; // type CV_8UC3\n        std::vector\ncv::Mat\n                                        vIntermediateDepthImages; // type CV_16UC1\n    };\n\n\n\n\nThe utilities for loading the intermediate clouds are:\n\n \nloadIntermediateCloudsFromSingleSweep\n                  # just the point clouds\n\n \nloadIntermediateCloudsCompleteDataFromSingleSweep\n      # complete data, with transforms and images\n\n \nloadIntermediateCloudsFromMultipleSweeps\n\n\n \nloadIntermediateCloudsCompleteDataFromMultipleSweeps\n\n\n \nloadIntermediateCloudsForTopologicalWaypoint\n\n\n \nloadIntermediateCloudsCompleteDataForTopologicalWaypoint\n\n\nSweep XML utilities\n\n\nThe sweep XML is an \nstd::string\n\n\nThe utilities for finding sweep XMLS are:\n\n \ngetSweepXmls\n # takes a folder where to search as argument. Returns a \nvector\nstring\n\n\n \ngetSweepXmlsForTopologicalWaypoint\n\n\nDynamic cluster utilities\n\n\nThe dynamic clusters type is:\n\n\ntemplate \nclass PointType\n std::vector\nboost::shared_ptr\npcl::PointCloud\nPointType\n\n\nThe dynamic cluster  utilities are:\n\n \nloadDynamicClustersFromSingleSweep\n\n\n \nloadDynamicClustersFromMultipleSweeps\n\n* \nloadDynamicClustersForTopologicalWaypoint\n\n\nLabelled data utilities\n\n\nThe labelled data type is:\n\n\n    template \nclass PointType\n\n    struct LabelledData\n    {\n        boost::shared_ptr\npcl::PointCloud\nPointType\n               completeCloud;\n        tf::StampedTransform                                        transformToGlobal;\n        tf::Vector3                                                 sweepCenter;\n        std::vector\nboost::shared_ptr\npcl::PointCloud\nPointType\n  objectClouds;\n        std::vector\nstd::string\n                                    objectLabels;\n        boost::posix_time::ptime                                    sweepTime;\n        std::string                                                 waypoint;\n\n    };\n\n\n\n\nThe labelled data utilities are:\n* \nloadLabelledDataFromSingleSweep", 
            "title": "Metaroom xml parser"
        }, 
        {
            "location": "/strands_3d_mapping/metaroom_xml_parser/#package-for-parsing-saved-room-observations", 
            "text": "", 
            "title": "Package for parsing saved room observations"
        }, 
        {
            "location": "/strands_3d_mapping/metaroom_xml_parser/#description", 
            "text": "The  metaroom_xml_parser  package is used to parse previously saved room observations. The data will be read into an appropriate data structure containing: merged point cloud, individual point clouds, individual RGB and depth images and corresponding camera parameters.", 
            "title": "Description"
        }, 
        {
            "location": "/strands_3d_mapping/metaroom_xml_parser/#usage", 
            "text": "Parsing one file   The  load_single_file  application reads in one room observation.  rosrun metaroom_xml_parser load_single_file /path/to/xml   Parsing multiple observations   The 'load_multiple_files' application reads in multiple room observations and returns a vector. It takes as input the root folder where the observations are stored.   rosrun metaroom_xml_parser load_multiple_files /path/to/folder   Parsing labelled data   The  load_labelled_data  application reads labelled data for observations taken at a particular waypoint. The waypoint id and the folder where the observations are stored are taken in as parameters.  rosrun metaroom_xml_parser load_labelled_data` /path/to/sweeps WayPointXYZ", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_3d_mapping/metaroom_xml_parser/#utilities", 
            "text": "A number of utilities are provided by this package, for easy data manipulation. The definitions can be seen in the file  load_utilities.h", 
            "title": "Utilities"
        }, 
        {
            "location": "/strands_3d_mapping/metaroom_xml_parser/#merged-cloud-utilities", 
            "text": "The complete cloud datatype is:  template  class PointType  boost::shared_ptr pcl::PointCloud PointType  The utilities for loading only the merged cloud are:   loadMergedCloudFromSingleSweep  # returns one cloud   loadMergedCloudFromMultipleSweeps  # returns a vector of merged clouds, one for each sweep\n*  loadMergedCloudForTopologicalWaypoint  # same as above", 
            "title": "Merged cloud utilities"
        }, 
        {
            "location": "/strands_3d_mapping/metaroom_xml_parser/#intermediate-cloud-utilities", 
            "text": "The intermediate cloud datatype is:      template  class PointType \n    struct IntermediateCloudCompleteData\n    {\n        std::vector boost::shared_ptr pcl::PointCloud PointType   vIntermediateRoomClouds;\n        std::vector tf::StampedTransform                            vIntermediateRoomCloudTransforms;\n        std::vector image_geometry::PinholeCameraModel              vIntermediateRoomCloudCamParams;\n        std::vector tf::StampedTransform                            vIntermediateRoomCloudTransformsRegistered;\n        std::vector image_geometry::PinholeCameraModel              vIntermediateRoomCloudCamParamsCorrected;\n        std::vector cv::Mat                                         vIntermediateRGBImages; // type CV_8UC3\n        std::vector cv::Mat                                         vIntermediateDepthImages; // type CV_16UC1\n    };  The utilities for loading the intermediate clouds are:   loadIntermediateCloudsFromSingleSweep                   # just the point clouds   loadIntermediateCloudsCompleteDataFromSingleSweep       # complete data, with transforms and images   loadIntermediateCloudsFromMultipleSweeps    loadIntermediateCloudsCompleteDataFromMultipleSweeps    loadIntermediateCloudsForTopologicalWaypoint    loadIntermediateCloudsCompleteDataForTopologicalWaypoint", 
            "title": "Intermediate cloud utilities"
        }, 
        {
            "location": "/strands_3d_mapping/metaroom_xml_parser/#sweep-xml-utilities", 
            "text": "The sweep XML is an  std::string  The utilities for finding sweep XMLS are:   getSweepXmls  # takes a folder where to search as argument. Returns a  vector string    getSweepXmlsForTopologicalWaypoint", 
            "title": "Sweep XML utilities"
        }, 
        {
            "location": "/strands_3d_mapping/metaroom_xml_parser/#dynamic-cluster-utilities", 
            "text": "The dynamic clusters type is:  template  class PointType  std::vector boost::shared_ptr pcl::PointCloud PointType  The dynamic cluster  utilities are:   loadDynamicClustersFromSingleSweep    loadDynamicClustersFromMultipleSweeps \n*  loadDynamicClustersForTopologicalWaypoint", 
            "title": "Dynamic cluster utilities"
        }, 
        {
            "location": "/strands_3d_mapping/metaroom_xml_parser/#labelled-data-utilities", 
            "text": "The labelled data type is:      template  class PointType \n    struct LabelledData\n    {\n        boost::shared_ptr pcl::PointCloud PointType                completeCloud;\n        tf::StampedTransform                                        transformToGlobal;\n        tf::Vector3                                                 sweepCenter;\n        std::vector boost::shared_ptr pcl::PointCloud PointType   objectClouds;\n        std::vector std::string                                     objectLabels;\n        boost::posix_time::ptime                                    sweepTime;\n        std::string                                                 waypoint;\n\n    };  The labelled data utilities are:\n*  loadLabelledDataFromSingleSweep", 
            "title": "Labelled data utilities"
        }, 
        {
            "location": "/strands_3d_mapping/nbv_planning/", 
            "text": "This package provides a implementation of the paper \n\n\n\"A probabilistic framework for next best view estimation in a cluttered environment.\", Potthas \n Sukhatme, 2014\n\n\nInstead of using s simple 3D array to store occupancy probabilities, an octomap is used. This more efficient and avoids the need for the \nmarkov random field gap filling method in the paper.\n\n\nThe code provides two interfaces, a C++ class and a ROS node. The C++ class is documented in source, with an example for running offline on PCD files.\n\n\nRunning an example on PCDs\n\n\nThe example program \"nbv_pcds\" can be used to run offline with some logged PCD files.  Run this with a single command line argument:\n\n\nrosrun nbv_planning nbv_pcds path_to_yaml\n\n\n\n\nWhere the YAML file supplied gives the location of the pointclouds, target volume and sensor model. See \ntest_files/out.yaml\n\nfor an example YAML file.\nThe pointclouds supplied need to contain the pose of the camera within the VIEWPOINT field. Compatible point clouds can be\ncaptured using \nscripts/capture_some_clouds.py\n.\nThe output of the program will be the view scores and the selected view. To visualise the progress in RViz, subscribe\nto \n/nbv_planner/views\n, \n/nbv_planner/octomap\n, and \n/nbv_planner/volume\n.\n\n\nRunning and using the planning as a ROS node\n\n\nThis package provides a single ROS node that can be used to do NBV planning. To start the node:\n\n\nrosrun nbv_planning nbv_server _camera_info_topic:=.ead_xtion/depth/camera_info\n\n\n\n\nThe camera info topic needs to be correct so that the planner can get the intrinsic parameters of the camera.\n\n\n/nbv_planner/set_target_volume\n\n\nThis sets the volume that views should be selected for. The request takes the centroid of a bounding box, and \nthe extents. See \nthe service definition here\n\n\n/nbv_planner/update\n\n\nThis updates the current knowledge of the target volume, so that the next best view can be selected based \non updated information from the last selected view. See \nthe service definition here\n\n\n/nbv_planner/select_next_view\n\n\nThis returns the view that should be used next, based on how much information gain is predicted to be achieved \nby it. This returns the view as a pose (for the camera), the index of the view and the score. As argument it takes a \nboolean to say if the selected view should be disabled after selection - i.e not selected again later. \nSee \nthe service definition here\n \n\n\n/nbv_planner/set_views\n\n\nThis service sets the 'candidate' views that the planner should select from. Each view should be a geometry_msgs/Pose, which is the pose of the camera not the robot.\nSee \nthe service definition here\n\n\nRViz\n\n\nThere are some topics to visualise the planning:\n- \n/nbv_planner/octomap\n : this is an octomap_msgs/Octomap, subscribe using the RViz plugin to see the current volume knowledge\n- \n/nbv_planner/volume\n : this is a MarkerArray showing the target region to select views for\n- \n/nbv_planner/views\n : this is a MarkerArray showing the candidate views the planner is working with. \n\n\nLimitations\n\n\nThis code has not been thoroughly tested. In particular, there is likely to be bugs in relation to formulae (6) and (7), and the advantage of the method stated in the paper over \"simple_inf_gain\" was not so apparant in tests that I carried out. None the less, hopefully this package can be the basis for some better implementation or alternative method.", 
            "title": "Nbv planning"
        }, 
        {
            "location": "/strands_3d_mapping/nbv_planning/#running-an-example-on-pcds", 
            "text": "The example program \"nbv_pcds\" can be used to run offline with some logged PCD files.  Run this with a single command line argument:  rosrun nbv_planning nbv_pcds path_to_yaml  Where the YAML file supplied gives the location of the pointclouds, target volume and sensor model. See  test_files/out.yaml \nfor an example YAML file.\nThe pointclouds supplied need to contain the pose of the camera within the VIEWPOINT field. Compatible point clouds can be\ncaptured using  scripts/capture_some_clouds.py .\nThe output of the program will be the view scores and the selected view. To visualise the progress in RViz, subscribe\nto  /nbv_planner/views ,  /nbv_planner/octomap , and  /nbv_planner/volume .", 
            "title": "Running an example on PCDs"
        }, 
        {
            "location": "/strands_3d_mapping/nbv_planning/#running-and-using-the-planning-as-a-ros-node", 
            "text": "This package provides a single ROS node that can be used to do NBV planning. To start the node:  rosrun nbv_planning nbv_server _camera_info_topic:=.ead_xtion/depth/camera_info  The camera info topic needs to be correct so that the planner can get the intrinsic parameters of the camera.", 
            "title": "Running and using the planning as a ROS node"
        }, 
        {
            "location": "/strands_3d_mapping/nbv_planning/#nbv_plannerset_target_volume", 
            "text": "This sets the volume that views should be selected for. The request takes the centroid of a bounding box, and \nthe extents. See  the service definition here", 
            "title": "/nbv_planner/set_target_volume"
        }, 
        {
            "location": "/strands_3d_mapping/nbv_planning/#nbv_plannerupdate", 
            "text": "This updates the current knowledge of the target volume, so that the next best view can be selected based \non updated information from the last selected view. See  the service definition here", 
            "title": "/nbv_planner/update"
        }, 
        {
            "location": "/strands_3d_mapping/nbv_planning/#nbv_plannerselect_next_view", 
            "text": "This returns the view that should be used next, based on how much information gain is predicted to be achieved \nby it. This returns the view as a pose (for the camera), the index of the view and the score. As argument it takes a \nboolean to say if the selected view should be disabled after selection - i.e not selected again later. \nSee  the service definition here", 
            "title": "/nbv_planner/select_next_view"
        }, 
        {
            "location": "/strands_3d_mapping/nbv_planning/#nbv_plannerset_views", 
            "text": "This service sets the 'candidate' views that the planner should select from. Each view should be a geometry_msgs/Pose, which is the pose of the camera not the robot.\nSee  the service definition here", 
            "title": "/nbv_planner/set_views"
        }, 
        {
            "location": "/strands_3d_mapping/nbv_planning/#rviz", 
            "text": "There are some topics to visualise the planning:\n-  /nbv_planner/octomap  : this is an octomap_msgs/Octomap, subscribe using the RViz plugin to see the current volume knowledge\n-  /nbv_planner/volume  : this is a MarkerArray showing the target region to select views for\n-  /nbv_planner/views  : this is a MarkerArray showing the candidate views the planner is working with.", 
            "title": "RViz"
        }, 
        {
            "location": "/strands_3d_mapping/nbv_planning/#limitations", 
            "text": "This code has not been thoroughly tested. In particular, there is likely to be bugs in relation to formulae (6) and (7), and the advantage of the method stated in the paper over \"simple_inf_gain\" was not so apparant in tests that I carried out. None the less, hopefully this package can be the basis for some better implementation or alternative method.", 
            "title": "Limitations"
        }, 
        {
            "location": "/strands_3d_mapping/object_manager/", 
            "text": "object_manager\n\n\nThis package allows interaction with the dynamic clusters segmented by the \nsemantic_map\n package. \n\n\nParameters\n\n\n\n\nlog_objects_to_db\n - whether the clusters segmented should be logged to mongodb. The default value is \nTrue\n\n\nobject_folder\n - the folder where to look for dynamic clusters. The default path is \n~/.semanticMap/\n\n\nmin_object_size\n - clusters with fewer points than this threshold are discarded. The default value is \n500\n.\n\n\nadditional_views_topic\n - the topic on which the additional views are published. The default is \n/object_learning/object_view\n \n\n\nadditional_views_status_topic\n - the topic on which status messages when collecting additional views are published. The default is \n/object_learning/status\n. The topic messages supported are \nstart_viewing\n (only accepted if a cluster has been previously selected) and \nstop_viewing\n\n\n\n\nDynamicObjectsService\n\n\nMessage tpe:\n\n\nstring waypoint_id\n---\nstring[] object_id\nsensor_msgs/PointCloud2[] objects\ngeometry_msgs/Point[] centroids\n\n\n\n\nGiven a waypoint id, this service returns all the dynamic clusters segmented at that waypoint, with their ids, point clouds and centroid. \n\n\nService topic: \nObjectManager/DynamicObjectsService\n\n\nThe point cloud corresponding to all the dynamic clusters is also published on the topic \n\"/object_manager/objects\n \n\n\nGetDynamicObjectService\n\n\nMessage type:\n\n\nstring waypoint_id\nstring object_id\n---\nsensor_msgs/PointCloud2 object_cloud\nint32[] object_mask\ngeometry_msgs/Transform transform_to_map\nint32 pan_angle\nint32 tilt_angle\n\n\n\n\nGiven a waypoint id and a cluster id (should correspond to the ids received after calling the \nDynamicObjectsService\n), this service returns the point cloud corresponding to that dynamic cluster in the camera frame of reference and a transform to get the point cloud in the map frame of refence. In addition, a set of angles (\npan_angle\n, and \ntilt_angle\n) to which to turn the PTU, and a set of indices representing image pixels corresponding to the dynamic cluster in the image obtained after turning the PTU to the specified angles. \nAfter calling this service, the requested dynamic cluster is \"selected\", and after receiving the \nstart_viewing\n mesasge on the \nobject_learning/status\n topic, additional views received on the \n/object_learning/object_view\n topic will be added and logged together with this cluster.\n\n\nService topic: \nObjectManager/GetDynamicObjectService\n\n\nThe point cloud corresponding to the requested dynamic cluster is also published on the topic \n/object_manager/requested_object\n.\n\n\nThe cluster mask is also published as an image on the topic: \n/object_manager/requested_object_mask\n\n\nNote that the clusters are logged to the database when calling the \nDynamicObjectsService\n or  the \nGetDynamicObjectService\n (if the \nlog_to_db\n argument is set to \nTrue\n). Calling these services multiple times does not affect (negatively) the logging. \n\n\nExport logged dynamic clusters from mongodb\n\n\nrosrun object_manager load_objects_from_mongo /path/where/to/export/data/\n\n\nThe data exported is saved according to the sweeps where the clusters were extracted (i.e. \nYYYYMMDD/patrol_run_#/room_#/...\n)", 
            "title": "Object manager"
        }, 
        {
            "location": "/strands_3d_mapping/object_manager/#object_manager", 
            "text": "This package allows interaction with the dynamic clusters segmented by the  semantic_map  package.", 
            "title": "object_manager"
        }, 
        {
            "location": "/strands_3d_mapping/object_manager/#parameters", 
            "text": "log_objects_to_db  - whether the clusters segmented should be logged to mongodb. The default value is  True  object_folder  - the folder where to look for dynamic clusters. The default path is  ~/.semanticMap/  min_object_size  - clusters with fewer points than this threshold are discarded. The default value is  500 .  additional_views_topic  - the topic on which the additional views are published. The default is  /object_learning/object_view    additional_views_status_topic  - the topic on which status messages when collecting additional views are published. The default is  /object_learning/status . The topic messages supported are  start_viewing  (only accepted if a cluster has been previously selected) and  stop_viewing", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_3d_mapping/object_manager/#dynamicobjectsservice", 
            "text": "Message tpe:  string waypoint_id\n---\nstring[] object_id\nsensor_msgs/PointCloud2[] objects\ngeometry_msgs/Point[] centroids  Given a waypoint id, this service returns all the dynamic clusters segmented at that waypoint, with their ids, point clouds and centroid.   Service topic:  ObjectManager/DynamicObjectsService  The point cloud corresponding to all the dynamic clusters is also published on the topic  \"/object_manager/objects", 
            "title": "DynamicObjectsService"
        }, 
        {
            "location": "/strands_3d_mapping/object_manager/#getdynamicobjectservice", 
            "text": "Message type:  string waypoint_id\nstring object_id\n---\nsensor_msgs/PointCloud2 object_cloud\nint32[] object_mask\ngeometry_msgs/Transform transform_to_map\nint32 pan_angle\nint32 tilt_angle  Given a waypoint id and a cluster id (should correspond to the ids received after calling the  DynamicObjectsService ), this service returns the point cloud corresponding to that dynamic cluster in the camera frame of reference and a transform to get the point cloud in the map frame of refence. In addition, a set of angles ( pan_angle , and  tilt_angle ) to which to turn the PTU, and a set of indices representing image pixels corresponding to the dynamic cluster in the image obtained after turning the PTU to the specified angles. \nAfter calling this service, the requested dynamic cluster is \"selected\", and after receiving the  start_viewing  mesasge on the  object_learning/status  topic, additional views received on the  /object_learning/object_view  topic will be added and logged together with this cluster.  Service topic:  ObjectManager/GetDynamicObjectService  The point cloud corresponding to the requested dynamic cluster is also published on the topic  /object_manager/requested_object .  The cluster mask is also published as an image on the topic:  /object_manager/requested_object_mask  Note that the clusters are logged to the database when calling the  DynamicObjectsService  or  the  GetDynamicObjectService  (if the  log_to_db  argument is set to  True ). Calling these services multiple times does not affect (negatively) the logging.", 
            "title": "GetDynamicObjectService"
        }, 
        {
            "location": "/strands_3d_mapping/object_manager/#export-logged-dynamic-clusters-from-mongodb", 
            "text": "rosrun object_manager load_objects_from_mongo /path/where/to/export/data/  The data exported is saved according to the sweeps where the clusters were extracted (i.e.  YYYYMMDD/patrol_run_#/room_#/... )", 
            "title": "Export logged dynamic clusters from mongodb"
        }, 
        {
            "location": "/strands_3d_mapping/object_view_generator/", 
            "text": "This package contains a service providing node that generates view poses around an object/region/arbitrary point of interest. \nThe poses it generates are for the robot base, no PTU angles are provided - these are either easily calculated \nseparately to have the PTU point at the target, or the \n\nptu_follow_frame\n\npackage can be used in conjunction with the\n\nstatic_transform_manager\n\npackage to have the PTU always looking at the target point. The generated poses are collision checked against a 2D map \nto be reasonably sure that the robot is capable of navigating to those positions. In the STRANDS work using this code\nwe use a down projected 3D map (generated by the \n\nsemantic_map_to_2d\n\npackage) to effectively collision check in 3D.\n\n\nThe node provides two methods of generating the target points, one in which the aim is to generate a sequence of \nviews - i.e a robot trajectory, and the other is just to generate a set of view points. The later method is the initial step of\nthe first method, details are available in !!RA-L paper!!. The method to use is selected via a parameter in the service call.\n\n\nRunning\n\n\nstart the service node with\n\n\nrosrun object_view_generator view_points_service.py\n \n\n\nThis will start the service with the default parameter values, namely:\n\n\n\n\nmap_topic\n : default \n'/map'\n  - This specifies the topic to pull the map from for testing candidate poses for collision. The topic type must be nav_msgs/OccupancyGrid, and a fresh copy of the map will be waited for at each service call.\n\n\nis_costmap\n : default \nFalse\n  - This specifies the map type, as costmaps usually have different value meanings than standard maps. This allows the inflated costmap to be used in place of the static 2D map if desired.\n\n\n\n\nUsage\n\n\nOnce started the node will provide a single service \n/generate_object_views\n with the following request fields:\n\n\n\n\ngeometry_msgs/Pose target_pose\n - The pose of the target to generate views for. Currently only the position is used, the orientation can be left default.\n\n\nfloat32 min_dist\n  - This is the minimum distance that the view poses should be from the tartget.\n\n\nfloat32 max_dist\n - The maximum distance that the view poses should be from the object.\n\n\nint16 number_views\n - How many views (maximum) should be generated. If some of the views are not valid then less than this number might be returned.\n\n\nfloat32 inflation_radius\n - How much space should the robot footprint be expanded by when collision checking the candidate poses.\n\n\nbool return_as_trajectory\n - This selects the method for generating the views. If \nTrue\n, then only a smaller subset of connected views will be returned, thereby returning a trajectory. If \nFalse\n then a larger set of all possible views is provided, but the path between these views is not considered when selecting them.\n\n\nstring SOMA_region\n - This optional parameter, if supplied, will cause the generator to restrict views to the specified SOMA region. If specified then the format must be \"soma_map/soma_config/region_number\" - the parsing of this might be sensitive :-). If you supply an empty string then SOMA will not be required on your system.\n\n\n\n\nThe return of the service is as follows:\n\n\n\n\ngeometry_msgs/PoseArray goals\n - This is simply the list of goals.\n\n\n\n\nFor convenience the service handle will also publish the generated goals on the topic \n/object_view_goals\n of type \nPoseArray\n.\n\n\nTesting\n\n\nThe test client can be started with \n\n\nrosrun object_view_generator test_with_rviz.py\n\n\nThis provides a simple text interface to call the service with different parameters. Subscribe to \n/object_view_goals\n in RViz to see the results.", 
            "title": "Object view generator"
        }, 
        {
            "location": "/strands_3d_mapping/object_view_generator/#running", 
            "text": "start the service node with  rosrun object_view_generator view_points_service.py    This will start the service with the default parameter values, namely:   map_topic  : default  '/map'   - This specifies the topic to pull the map from for testing candidate poses for collision. The topic type must be nav_msgs/OccupancyGrid, and a fresh copy of the map will be waited for at each service call.  is_costmap  : default  False   - This specifies the map type, as costmaps usually have different value meanings than standard maps. This allows the inflated costmap to be used in place of the static 2D map if desired.", 
            "title": "Running"
        }, 
        {
            "location": "/strands_3d_mapping/object_view_generator/#usage", 
            "text": "Once started the node will provide a single service  /generate_object_views  with the following request fields:   geometry_msgs/Pose target_pose  - The pose of the target to generate views for. Currently only the position is used, the orientation can be left default.  float32 min_dist   - This is the minimum distance that the view poses should be from the tartget.  float32 max_dist  - The maximum distance that the view poses should be from the object.  int16 number_views  - How many views (maximum) should be generated. If some of the views are not valid then less than this number might be returned.  float32 inflation_radius  - How much space should the robot footprint be expanded by when collision checking the candidate poses.  bool return_as_trajectory  - This selects the method for generating the views. If  True , then only a smaller subset of connected views will be returned, thereby returning a trajectory. If  False  then a larger set of all possible views is provided, but the path between these views is not considered when selecting them.  string SOMA_region  - This optional parameter, if supplied, will cause the generator to restrict views to the specified SOMA region. If specified then the format must be \"soma_map/soma_config/region_number\" - the parsing of this might be sensitive :-). If you supply an empty string then SOMA will not be required on your system.   The return of the service is as follows:   geometry_msgs/PoseArray goals  - This is simply the list of goals.   For convenience the service handle will also publish the generated goals on the topic  /object_view_goals  of type  PoseArray .", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_3d_mapping/object_view_generator/#testing", 
            "text": "The test client can be started with   rosrun object_view_generator test_with_rviz.py  This provides a simple text interface to call the service with different parameters. Subscribe to  /object_view_goals  in RViz to see the results.", 
            "title": "Testing"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map/", 
            "text": "Package for building metarooms and extracting dynamic clusters\n\n\nsemantic_map_node\n\n\nDescription\n\n\nThe local metric map consists of a series of meta-rooms, each corresponding to a different location. A meta-room contains only those parts of the scene which are observed to be static, and it is created incrementally as the robot re-observes the same location over time.\n\n\nThis package takes room observations as they are constructed by the \ncloud_merge\n package and extracts the corresponding metaroom and also computes dynamic clusters at each waypoint. \n\n\nSome data is stored on the disk, in the folder\n\n\n~.semanticMap/metarooms\n\n\n\n\nTo start this node run:\n\n\nroslaunch semantic_map semantic_map.launch\n\n\nInput topics\n\n\n\n\n/local_metric_map/room_observations\n : on this topic the observation xml is published. After receiving the xml, the appropriate Meta-Room will be loaded and updated.\n\n\n\n\nOutput topics\n\n\n\n\n/local_metric_map/metaroom\n - RGBD point cloud corresponding to a meta-room, published after a new observation has been processed. \n\n\n/local_metric_map/dynamic_clusters\n - RGBD point cloud corresponding to the dynamic clusters, published after a new observation has been processed.\n\n\n\n\nClearMetaroomService\n\n\nThis service resets the Meta-Rooms at specific waypoints.\n\n\nMessage type:\n\n\nstring[] waypoint_id\nbool initialize\n---\n\n\n\n\nIf \ninitialize\n is set to \nTrue\n, the Meta-Rooms at the specific waypoints are re-initialized with the latest observations collected at those waypoints. Otherwise, the Meta-Rooms are just deleted. \n\n\nParameters\n\n\n\n\nsave_intermediate\n : whether to save intermediate Meta-Room update steps to the disk. Default \nfalse\n\n\nlog_to_db\n : log the Meta-Rooms to mongodb. Default \nfalse\n\n\nupdate_metaroom\n : update the Meta-Rooms (if \nfalse\n they will only be initialized and used as they are for dynamic cluster computation). Default \ntrue\n\n\nmin_object_size\n : a dynamic cluster will be reported only if it has more points than this threshold. Default \n500\n\n\nnewest_dynamic_clusters\n : compute dynamic clusters by comparing the latest sweep with the previous one (as opposed to comparing the latest sweep to the metaroom). Default \nfalse\n\n\n\n\nExport sweeps from mongodb to the disk\n\n\nrosrun semantic_map load_from_mongo /path/where/to/export/data/\n\n\n\n\nImport sweeps from the disk into mongodb\n\n\nrosrun semantic_map add_to_mongo /path/where/to/load/data/from/", 
            "title": "Semantic map"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map/#package-for-building-metarooms-and-extracting-dynamic-clusters", 
            "text": "", 
            "title": "Package for building metarooms and extracting dynamic clusters"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map/#semantic_map_node", 
            "text": "", 
            "title": "semantic_map_node"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map/#description", 
            "text": "The local metric map consists of a series of meta-rooms, each corresponding to a different location. A meta-room contains only those parts of the scene which are observed to be static, and it is created incrementally as the robot re-observes the same location over time.  This package takes room observations as they are constructed by the  cloud_merge  package and extracts the corresponding metaroom and also computes dynamic clusters at each waypoint.   Some data is stored on the disk, in the folder  ~.semanticMap/metarooms  To start this node run:  roslaunch semantic_map semantic_map.launch", 
            "title": "Description"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map/#input-topics", 
            "text": "/local_metric_map/room_observations  : on this topic the observation xml is published. After receiving the xml, the appropriate Meta-Room will be loaded and updated.", 
            "title": "Input topics"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map/#output-topics", 
            "text": "/local_metric_map/metaroom  - RGBD point cloud corresponding to a meta-room, published after a new observation has been processed.   /local_metric_map/dynamic_clusters  - RGBD point cloud corresponding to the dynamic clusters, published after a new observation has been processed.", 
            "title": "Output topics"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map/#clearmetaroomservice", 
            "text": "This service resets the Meta-Rooms at specific waypoints.  Message type:  string[] waypoint_id\nbool initialize\n---  If  initialize  is set to  True , the Meta-Rooms at the specific waypoints are re-initialized with the latest observations collected at those waypoints. Otherwise, the Meta-Rooms are just deleted.", 
            "title": "ClearMetaroomService"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map/#parameters", 
            "text": "save_intermediate  : whether to save intermediate Meta-Room update steps to the disk. Default  false  log_to_db  : log the Meta-Rooms to mongodb. Default  false  update_metaroom  : update the Meta-Rooms (if  false  they will only be initialized and used as they are for dynamic cluster computation). Default  true  min_object_size  : a dynamic cluster will be reported only if it has more points than this threshold. Default  500  newest_dynamic_clusters  : compute dynamic clusters by comparing the latest sweep with the previous one (as opposed to comparing the latest sweep to the metaroom). Default  false", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map/#export-sweeps-from-mongodb-to-the-disk", 
            "text": "rosrun semantic_map load_from_mongo /path/where/to/export/data/", 
            "title": "Export sweeps from mongodb to the disk"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map/#import-sweeps-from-the-disk-into-mongodb", 
            "text": "rosrun semantic_map add_to_mongo /path/where/to/load/data/from/", 
            "title": "Import sweeps from the disk into mongodb"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_launcher/", 
            "text": "semantic_map_launcher\n\n\nThis launches the metric mapping / semantic mapping nodes from the \nstrands_3d_mapping\n repository.\n\n\nroslaunch semantic_map_launcher semantic_map.launch\n\n\nNodes started\n\n\n\n\ncloud_merge\n\n\nsemantic_map\n\n\ncalibrate_sweep_as\n\n\nsemantic_map_publisher\n\n\nobjcet_manager\n\n\ndo_sweep\n\n\nptu_action_server_metric_map\n\n\ndynamic_object_compute_mask_server\n\n\n\n\nParameters\n\n\nThe parameters accepted by the launch file are:\n\n\n\n\nsave_intermediate_clouds\n : whether to save the intermediate point clouds from the sweeps to the disk. Default \ntrue\n\n\nsave_intermediate_images\n : whether to save all the images making up an intermediate cloud to the disk (this takes a lot of space!!). Default \nfalse\n\n\nlog_to_db\n : log the sweeps to mongodb. Default \ntrue\n\n\nlog_objects_to_db\n : log the dynamic clusters to mongodb. Default \ntrue\n\n\ncleanup\n : at startup, delete everything in the \n~/.semanticMap/\n folder. Default \nfalse\n \n\n\nmax_instances\n : maximum number of sweeps, per waypoint to keep in the \n~/.semanticMap/\n folder. Default: \n10\n\n\ncache_old_data\n : if there are more sweeps per waypoint than the \nmax_instances\n parameter, delete them or move them to the cache folder \n~/.semanticMap/cache/\n. Default  \nfalse\n, i.e. delete older sweeps.\n\n\nupdate_metaroom\n : update the metaroom with new sweeps. Default \ntrue\n\n\nnewest_dynamic_clusters\n : compute dynamic clusters by comparing the latest sweep with the previous one (as opposed to comparing the latest sweep to the metaroom). Default \ntrue\n\n\nmin_object_size\n : the minimum number of points for a cluster to be reported. Default \n500\n\n\nsegmentation_method\n : the segmentation method used to segment the object from the additional views collected by the value. Supported methods: \nconvex_segmentation\n and \nmeta_room\n. Default: \nmeta_room\n.", 
            "title": "Semantic map launcher"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_launcher/#semantic_map_launcher", 
            "text": "This launches the metric mapping / semantic mapping nodes from the  strands_3d_mapping  repository.  roslaunch semantic_map_launcher semantic_map.launch", 
            "title": "semantic_map_launcher"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_launcher/#nodes-started", 
            "text": "cloud_merge  semantic_map  calibrate_sweep_as  semantic_map_publisher  objcet_manager  do_sweep  ptu_action_server_metric_map  dynamic_object_compute_mask_server", 
            "title": "Nodes started"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_launcher/#parameters", 
            "text": "The parameters accepted by the launch file are:   save_intermediate_clouds  : whether to save the intermediate point clouds from the sweeps to the disk. Default  true  save_intermediate_images  : whether to save all the images making up an intermediate cloud to the disk (this takes a lot of space!!). Default  false  log_to_db  : log the sweeps to mongodb. Default  true  log_objects_to_db  : log the dynamic clusters to mongodb. Default  true  cleanup  : at startup, delete everything in the  ~/.semanticMap/  folder. Default  false    max_instances  : maximum number of sweeps, per waypoint to keep in the  ~/.semanticMap/  folder. Default:  10  cache_old_data  : if there are more sweeps per waypoint than the  max_instances  parameter, delete them or move them to the cache folder  ~/.semanticMap/cache/ . Default   false , i.e. delete older sweeps.  update_metaroom  : update the metaroom with new sweeps. Default  true  newest_dynamic_clusters  : compute dynamic clusters by comparing the latest sweep with the previous one (as opposed to comparing the latest sweep to the metaroom). Default  true  min_object_size  : the minimum number of points for a cluster to be reported. Default  500  segmentation_method  : the segmentation method used to segment the object from the additional views collected by the value. Supported methods:  convex_segmentation  and  meta_room . Default:  meta_room .", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_publisher/", 
            "text": "semantic_map_publisher\n\n\nThis package provides an interface to observation data previously recorded and stored on the disk. The data can be queried using the services described below.\n\n\nWaypointInfoService\n\n\nMessage type:\n\n\n---\nstring[] waypoint_id\nint32[] observation_count\n\n\n\n\nReturns a list of waypoints along with the number of observations collected at those waypoints.\n\n\nService name: \nSemanticMapPublisher/WaypointInfoService\n\n\nSensorOriginService\n\n\nMessage type:\n\n\nstring waypoint_id\n---\ngeometry_msgs/Vector3 origin\n\n\n\n\nGiven a waypoint this service returns the origin from where the latest observation was acquired at that waypoint.\n\n\nService name: \nSemanticMapPublisher/SensorOriginService\n\n\nObservationService\n\n\nMessage type:\n\n\nstring waypoint_id\nfloat64 resolution\n---\nsensor_msgs/PointCloud2 cloud\n\n\n\n\nGiven a waypoint, and a resolution, this service returns the latest observation collected at that waypoint as a PointCloud with the specified resolution. \n\n\nService name: \nSemanticMapPublisher/ObservationService\n\n\nObservationOctomapService\n\n\nMessage type:\n\n\nstring waypoint_id\nfloat64 resolution\n---\noctomap_msgs/Octomap octomap\n\n\n\n\nSame as \nObservationService\n but returns the latest observation as an Octomap.\n\n\nService name: \nSemanticMapPublisher/ObservationOctomapService\n\n\nWaypointTimestampService\n\n\nMessage type:\n\n\nstring waypoint_id\n---\nstring[] waypoint_timestamps\n\n\n\n\nGiven a waypoint, this service returns the timestamps of all the observations collected at that waypoint, as a list. \n\n\nService name: \nSemanticMapPublisher/WaypointTimestampService\n\n\nObservationInstanceService\n\n\nMessage type:\n\n\nstring waypoint_id\nint64 instance_number # convention 0 - oldest available\nfloat64 resolution\n---\nsensor_msgs/PointCloud2 cloud\nstring observation_timestamp\n\n\n\n\nGiven a waypoint id, an instance number and a resolution, this service returns a particular instance from the observations collected at that particular waypoint, with the desired resolution, along with the timestamp of the observation (as opposed to \nObservationService\n which returns the latest observation at that particular waypoint). \nService name: \nSemanticMapPublisher/ObservationInstanceService\n\n\nObservationOctomapInstanceService\n\n\nMessage type:\n\n\nstring waypoint_id\nint64 instance_number # convention 0 - oldest available\nfloat64 resolution\n---\noctomap_msgs/Octomap octomap\nstring observation_timestamp\n\n\n\n\nSame as \nObservationInstanceService\n, but returns the observation instance as an \nOctomap\n. \n\n\nService name: \nSemanticMapPublisher/ObservationOctomapInstanceService", 
            "title": "Semantic map publisher"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_publisher/#semantic_map_publisher", 
            "text": "This package provides an interface to observation data previously recorded and stored on the disk. The data can be queried using the services described below.", 
            "title": "semantic_map_publisher"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_publisher/#waypointinfoservice", 
            "text": "Message type:  ---\nstring[] waypoint_id\nint32[] observation_count  Returns a list of waypoints along with the number of observations collected at those waypoints.  Service name:  SemanticMapPublisher/WaypointInfoService", 
            "title": "WaypointInfoService"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_publisher/#sensororiginservice", 
            "text": "Message type:  string waypoint_id\n---\ngeometry_msgs/Vector3 origin  Given a waypoint this service returns the origin from where the latest observation was acquired at that waypoint.  Service name:  SemanticMapPublisher/SensorOriginService", 
            "title": "SensorOriginService"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_publisher/#observationservice", 
            "text": "Message type:  string waypoint_id\nfloat64 resolution\n---\nsensor_msgs/PointCloud2 cloud  Given a waypoint, and a resolution, this service returns the latest observation collected at that waypoint as a PointCloud with the specified resolution.   Service name:  SemanticMapPublisher/ObservationService", 
            "title": "ObservationService"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_publisher/#observationoctomapservice", 
            "text": "Message type:  string waypoint_id\nfloat64 resolution\n---\noctomap_msgs/Octomap octomap  Same as  ObservationService  but returns the latest observation as an Octomap.  Service name:  SemanticMapPublisher/ObservationOctomapService", 
            "title": "ObservationOctomapService"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_publisher/#waypointtimestampservice", 
            "text": "Message type:  string waypoint_id\n---\nstring[] waypoint_timestamps  Given a waypoint, this service returns the timestamps of all the observations collected at that waypoint, as a list.   Service name:  SemanticMapPublisher/WaypointTimestampService", 
            "title": "WaypointTimestampService"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_publisher/#observationinstanceservice", 
            "text": "Message type:  string waypoint_id\nint64 instance_number # convention 0 - oldest available\nfloat64 resolution\n---\nsensor_msgs/PointCloud2 cloud\nstring observation_timestamp  Given a waypoint id, an instance number and a resolution, this service returns a particular instance from the observations collected at that particular waypoint, with the desired resolution, along with the timestamp of the observation (as opposed to  ObservationService  which returns the latest observation at that particular waypoint). \nService name:  SemanticMapPublisher/ObservationInstanceService", 
            "title": "ObservationInstanceService"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_publisher/#observationoctomapinstanceservice", 
            "text": "Message type:  string waypoint_id\nint64 instance_number # convention 0 - oldest available\nfloat64 resolution\n---\noctomap_msgs/Octomap octomap\nstring observation_timestamp  Same as  ObservationInstanceService , but returns the observation instance as an  Octomap .   Service name:  SemanticMapPublisher/ObservationOctomapInstanceService", 
            "title": "ObservationOctomapInstanceService"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_to_2d/", 
            "text": "This package contains a single map server node that publishes a downprojected metric map. \nA service is provided for chaning which waypoint is currently being published.\n\n\nUsage\n\n\nStart it with\n\n\nrosrun semantic_map_to_2d semantic_map_2d_server\n\n\nThis then provides the service \n/set_waypoint\n with the following format [TODO: This should be moved to the nodes namespace...]\n\n\nRequest:\n- \nstring waypoint\n : The name of the waypoint to switch the map to.\n\n\nResponse:\n- \nbool is_ok\n : If the map was switched or not\n- \nstring response\n : Some textual description of what when wrong if not ok.\n\n\nThe map server publishes the map as a \nnav_msgs::OccupancyGrid\n on the topic \n/waypoint_map\n.", 
            "title": "Semantic map to 2d"
        }, 
        {
            "location": "/strands_3d_mapping/semantic_map_to_2d/#usage", 
            "text": "Start it with  rosrun semantic_map_to_2d semantic_map_2d_server  This then provides the service  /set_waypoint  with the following format [TODO: This should be moved to the nodes namespace...]  Request:\n-  string waypoint  : The name of the waypoint to switch the map to.  Response:\n-  bool is_ok  : If the map was switched or not\n-  string response  : Some textual description of what when wrong if not ok.  The map server publishes the map as a  nav_msgs::OccupancyGrid  on the topic  /waypoint_map .", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_3d_mapping/strands_sweep_registration/", 
            "text": "strands_sweep_registration\n\n\nComponent that calibrates the intermediate positions of the point clouds making up a sweep. Currently only accepts sweeps of type \ncomplete\n, as recorded by the \ndo_sweep.py\n action server from the \ncloud_merge\n package, i.e. with 51 intermediate positions. \n\n\nInternally it uses the Ceres optimization engine.", 
            "title": "Strands sweep registration"
        }, 
        {
            "location": "/strands_3d_mapping/strands_sweep_registration/#strands_sweep_registration", 
            "text": "Component that calibrates the intermediate positions of the point clouds making up a sweep. Currently only accepts sweeps of type  complete , as recorded by the  do_sweep.py  action server from the  cloud_merge  package, i.e. with 51 intermediate positions.   Internally it uses the Ceres optimization engine.", 
            "title": "strands_sweep_registration"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/cereal/", 
            "text": "cereal - A C++11 library for serialization\n\n\ncereal is a header-only C++11 serialization library.  cereal takes arbitrary data types and reversibly turns them into different representations, such as compact binary encodings, XML, or JSON.  cereal was designed to be fast, light-weight, and easy to extend - it has no external dependencies and can be easily bundled with other code or used standalone.\n\n\ncereal has great documentation\n\n\nLooking for more information on how cereal works and its documentation?  Visit \ncereal's web page\n to get the latest information.\n\n\ncereal is easy to use\n\n\nInstallation and use of of cereal is fully documented on the \nmain web page\n, but this is a quick and dirty version:\n\n\n\n\nDownload cereal and place the headers somewhere your code can see them\n\n\nWrite serialization functions for your custom types or use the built in support for the standard library cereal provides\n\n\nUse the serialization archives to load and save data\n\n\n\n\n#include \ncereal/types/unordered_map.hpp\n\n#include \ncereal/types/memory.hpp\n\n#include \ncereal/archives/binary.hpp\n\n#include \nfstream\n\n\nstruct MyRecord\n{\n  uint8_t x, y;\n  float z;\n\n  template \nclass Archive\n\n  void serialize( Archive \n ar )\n  {\n    ar( x, y, z );\n  }\n};\n\nstruct SomeData\n{\n  int32_t id;\n  std::shared_ptr\nstd::unordered_map\nuint32_t, MyRecord\n data;\n\n  template \nclass Archive\n\n  void save( Archive \n ar ) const\n  {\n    ar( data );\n  }\n\n  template \nclass Archive\n\n  void load( Archive \n ar )\n  {\n    static int32_t idGen = 0;\n    id = idGen++;\n    ar( data );\n  }\n};\n\nint main()\n{\n  std::ofstream os(\nout.cereal\n, std::ios::binary);\n  cereal::BinaryOutputArchive archive( os );\n\n  SomeData myData;\n  archive( myData );\n\n  return 0;\n}\n\n\n\n\ncereal has a mailing list\n\n\nEither get in touch over \nemail\n or \non the web\n.\n\n\ncereal has a permissive license\n\n\ncereal is licensed under the \nBSD license\n.\n\n\ncereal build status\n\n\n\n\ndevelop : \n\n\n\n\n\n\nWere you looking for the Haskell cereal?  Go \nhere\n.", 
            "title": "Cereal"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/cereal/#cereal-a-c11-library-for-serialization", 
            "text": "cereal is a header-only C++11 serialization library.  cereal takes arbitrary data types and reversibly turns them into different representations, such as compact binary encodings, XML, or JSON.  cereal was designed to be fast, light-weight, and easy to extend - it has no external dependencies and can be easily bundled with other code or used standalone.", 
            "title": "cereal - A C++11 library for serialization"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/cereal/#cereal-has-great-documentation", 
            "text": "Looking for more information on how cereal works and its documentation?  Visit  cereal's web page  to get the latest information.", 
            "title": "cereal has great documentation"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/cereal/#cereal-is-easy-to-use", 
            "text": "Installation and use of of cereal is fully documented on the  main web page , but this is a quick and dirty version:   Download cereal and place the headers somewhere your code can see them  Write serialization functions for your custom types or use the built in support for the standard library cereal provides  Use the serialization archives to load and save data   #include  cereal/types/unordered_map.hpp \n#include  cereal/types/memory.hpp \n#include  cereal/archives/binary.hpp \n#include  fstream \n\nstruct MyRecord\n{\n  uint8_t x, y;\n  float z;\n\n  template  class Archive \n  void serialize( Archive   ar )\n  {\n    ar( x, y, z );\n  }\n};\n\nstruct SomeData\n{\n  int32_t id;\n  std::shared_ptr std::unordered_map uint32_t, MyRecord  data;\n\n  template  class Archive \n  void save( Archive   ar ) const\n  {\n    ar( data );\n  }\n\n  template  class Archive \n  void load( Archive   ar )\n  {\n    static int32_t idGen = 0;\n    id = idGen++;\n    ar( data );\n  }\n};\n\nint main()\n{\n  std::ofstream os( out.cereal , std::ios::binary);\n  cereal::BinaryOutputArchive archive( os );\n\n  SomeData myData;\n  archive( myData );\n\n  return 0;\n}", 
            "title": "cereal is easy to use"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/cereal/#cereal-has-a-mailing-list", 
            "text": "Either get in touch over  email  or  on the web .", 
            "title": "cereal has a mailing list"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/cereal/#cereal-has-a-permissive-license", 
            "text": "cereal is licensed under the  BSD license .", 
            "title": "cereal has a permissive license"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/cereal/#cereal-build-status", 
            "text": "develop :     Were you looking for the Haskell cereal?  Go  here .", 
            "title": "cereal build status"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/", 
            "text": "object_3d_retrieval\n\n\nThis contains our implementation of a vocabulary tree (Nister \n Stewenius) together with some modifications for our paper. It also includes some functions e.g. for extracting segments \n features and querying on our data structure.\n\n\nTo use the system, follow the instructions in \nscripts/menu.py\n. There are submenus, for example for querying that can be run separately, i.e. \nscripts/training_menu.py\n and \nscripts/querying_menu.py\n.\n\n\nInstructions for stepping through menu.py\n\n\nIf you build the project and go to your build folder, you should see the\npython scripts mentioned above. To use the scripts you will need data\ncaptured according to the meta room description, defined in \nstrands_3d_mapping\n.\n\n\nWhen you run \nmenu.py\n, you are first asked for the data path. This is the base\nfolder of your sweeps and it should contain folders with different dates\nas names. Once you enter the name you should get the menu itself:\n\n\nPlease supply the data path: /home/nbore/Data/KTH_longterm_dataset_labels\n\nPlease make sure to do the following in order both for the noise data folder and for the annotated data. Note that you should choose the a or b option consistently.\n\nWorking on data path /home/nbore/Data/KTH_longterm_dataset_labels\n\n1.  Set data path (if you want to change)\n2.  Init sweep segment folders\n3a. Create convex segments (alternative to 3b \n 5b)\n3b. Create convex segments and supervoxels (alternative to 3a \n 5a)\n4.  Extract PFHRGB features\n5a. Segment keypoints into subsegments (alternative to 5b)\n5b. Create PFHRGB feature for supervoxels (alternative to 5a)\n6.  Extract SIFT from sweeps (for re-weighting)\n7.  Vocabulary training menu\n8.  Querying \n benchmarking menu\n9.  Exit\n\nPlease enter an option 1-9(a/b):\n\n\n\n\nNow you should step through the numbers in order. Note that if you\nchoose e.g. \n3a\n you should also choose \n5a\n. Start by inputting \n2\n.\nThis will set up all the folders. Then choose either \n3a\n or \n3b\n. This will\nperform a convex segmentation of the data. It will take a while. Then input\n\n4\n to extract features, this will also take a while. The depending on if you\nchose \na\n or \nb\n previously, input \n5a\n or \n5b\n. Finally, input \n6\n to finish\nthe data processing by extracting \nsift\n features.\n\n\nYou are now ready to go on to training the vocabulary tree representation!\n\n\nInstructions for running training_menu.py\n\n\nTo go into the training menu, either input \n7\n in the previous menu or\nrun \ntraining_menu.py\n separately. This time you are asked for the path\nto your vocabulary representation. Simply give the path to some empty folder:\n\n\nPlease supply the path to the vocabulary: /home/nbore/Data/KTH_longterm_dataset_labels_convex\n\nPlease make sure to do the following in order\n\nWorking on vocabulary path /home/nbore/Data/KTH_longterm_dataset_labels_convex\n\n1. Set vocabulary path (if you want to change)\n2. Initialize vocabulary folders and files\n3. Build vocabulary tree representation\n4. Exit\n\nPlease enter an option 1-4:\n\n\n\n\nFirst, input \n2\n. This will ask you for paths to an annotated meta room data\nset and another \"noise\" data set which does not need to be annotated.\nIt will also ask if you want a \"standard\" or \"incremental\" type vocabulary.\nThe two types are detailed in the paper, basically the standard works on\nthe convex segments extracted earlier while incremental is more flexible.\nOnce this is done, simply enter \n3\n to train and build the representation.\nYou are now ready to query the representation, see\n\nhttps://github.com/nilsbore/quasimodo/tree/master/quasimodo_retrieval\n\nfor information on the ROS interface. Also check out example usage in\n\nhttps://github.com/nilsbore/quasimodo/tree/master/quasimodo_test\n.\n\n\nHappy querying!\n\n\nDependencies\n\n\nRight now, we do not use the debian package of \nstrands_3d_mapping\n (see below),\ninstead you should compile\n\nhttps://github.com/RaresAmbrus/strands_3d_mapping\n manually in a catkin\nworkspace, be sure to check out the \nhydro-devel\n branch. There is a commented\nsection in the cmake file where you can set the variable \nparser_workspace\n,\nwhich should point to the catkin workspace where your \nstrands_3d_mapping\n\ncheckout lives. See the lines\n\nhttps://github.com/nilsbore/dynamic_object_retrieval/blob/dynamic/CMakeLists.txt#L52\n\nand\n\nhttps://github.com/nilsbore/dynamic_object_retrieval/blob/dynamic/benchmark/CMakeLists.txt#L31\n.\n\n\nIn the future, you will instead use the packaged version of \nstrands_3d_mapping\n\nfrom the STRANDS project \nhttp://strands.acin.tuwien.ac.at/\n. Follow the instructions on \nhttps://github.com/strands-project-releases/strands-releases/wiki\n to add the debian package repository. Then install the \nmetaroom_xml_parser\n by typing \nsudo apt-get install ros-indigo-metaroom-xml-parser\n.\n\n\nTested with Ubuntu 14.04 with ROS Indigo and corresponding OpenCV + PCL, QT4. The repos \nhttps://github.com/nilsbore/k_means_tree\n, \nhttps://github.com/USCiLab/cereal\n\nand \nhttps://github.com/mp3guy/Stopwatch\n are included in the repo as subtrees.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/#object_3d_retrieval", 
            "text": "This contains our implementation of a vocabulary tree (Nister   Stewenius) together with some modifications for our paper. It also includes some functions e.g. for extracting segments   features and querying on our data structure.  To use the system, follow the instructions in  scripts/menu.py . There are submenus, for example for querying that can be run separately, i.e.  scripts/training_menu.py  and  scripts/querying_menu.py .", 
            "title": "object_3d_retrieval"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/#instructions-for-stepping-through-menupy", 
            "text": "If you build the project and go to your build folder, you should see the\npython scripts mentioned above. To use the scripts you will need data\ncaptured according to the meta room description, defined in  strands_3d_mapping .  When you run  menu.py , you are first asked for the data path. This is the base\nfolder of your sweeps and it should contain folders with different dates\nas names. Once you enter the name you should get the menu itself:  Please supply the data path: /home/nbore/Data/KTH_longterm_dataset_labels\n\nPlease make sure to do the following in order both for the noise data folder and for the annotated data. Note that you should choose the a or b option consistently.\n\nWorking on data path /home/nbore/Data/KTH_longterm_dataset_labels\n\n1.  Set data path (if you want to change)\n2.  Init sweep segment folders\n3a. Create convex segments (alternative to 3b   5b)\n3b. Create convex segments and supervoxels (alternative to 3a   5a)\n4.  Extract PFHRGB features\n5a. Segment keypoints into subsegments (alternative to 5b)\n5b. Create PFHRGB feature for supervoxels (alternative to 5a)\n6.  Extract SIFT from sweeps (for re-weighting)\n7.  Vocabulary training menu\n8.  Querying   benchmarking menu\n9.  Exit\n\nPlease enter an option 1-9(a/b):  Now you should step through the numbers in order. Note that if you\nchoose e.g.  3a  you should also choose  5a . Start by inputting  2 .\nThis will set up all the folders. Then choose either  3a  or  3b . This will\nperform a convex segmentation of the data. It will take a while. Then input 4  to extract features, this will also take a while. The depending on if you\nchose  a  or  b  previously, input  5a  or  5b . Finally, input  6  to finish\nthe data processing by extracting  sift  features.  You are now ready to go on to training the vocabulary tree representation!", 
            "title": "Instructions for stepping through menu.py"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/#instructions-for-running-training_menupy", 
            "text": "To go into the training menu, either input  7  in the previous menu or\nrun  training_menu.py  separately. This time you are asked for the path\nto your vocabulary representation. Simply give the path to some empty folder:  Please supply the path to the vocabulary: /home/nbore/Data/KTH_longterm_dataset_labels_convex\n\nPlease make sure to do the following in order\n\nWorking on vocabulary path /home/nbore/Data/KTH_longterm_dataset_labels_convex\n\n1. Set vocabulary path (if you want to change)\n2. Initialize vocabulary folders and files\n3. Build vocabulary tree representation\n4. Exit\n\nPlease enter an option 1-4:  First, input  2 . This will ask you for paths to an annotated meta room data\nset and another \"noise\" data set which does not need to be annotated.\nIt will also ask if you want a \"standard\" or \"incremental\" type vocabulary.\nThe two types are detailed in the paper, basically the standard works on\nthe convex segments extracted earlier while incremental is more flexible.\nOnce this is done, simply enter  3  to train and build the representation.\nYou are now ready to query the representation, see https://github.com/nilsbore/quasimodo/tree/master/quasimodo_retrieval \nfor information on the ROS interface. Also check out example usage in https://github.com/nilsbore/quasimodo/tree/master/quasimodo_test .  Happy querying!", 
            "title": "Instructions for running training_menu.py"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/#dependencies", 
            "text": "Right now, we do not use the debian package of  strands_3d_mapping  (see below),\ninstead you should compile https://github.com/RaresAmbrus/strands_3d_mapping  manually in a catkin\nworkspace, be sure to check out the  hydro-devel  branch. There is a commented\nsection in the cmake file where you can set the variable  parser_workspace ,\nwhich should point to the catkin workspace where your  strands_3d_mapping \ncheckout lives. See the lines https://github.com/nilsbore/dynamic_object_retrieval/blob/dynamic/CMakeLists.txt#L52 \nand https://github.com/nilsbore/dynamic_object_retrieval/blob/dynamic/benchmark/CMakeLists.txt#L31 .  In the future, you will instead use the packaged version of  strands_3d_mapping \nfrom the STRANDS project  http://strands.acin.tuwien.ac.at/ . Follow the instructions on  https://github.com/strands-project-releases/strands-releases/wiki  to add the debian package repository. Then install the  metaroom_xml_parser  by typing  sudo apt-get install ros-indigo-metaroom-xml-parser .  Tested with Ubuntu 14.04 with ROS Indigo and corresponding OpenCV + PCL, QT4. The repos  https://github.com/nilsbore/k_means_tree ,  https://github.com/USCiLab/cereal \nand  https://github.com/mp3guy/Stopwatch  are included in the repo as subtrees.", 
            "title": "Dependencies"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/k_means_tree/", 
            "text": "k_means_tree\n\n\nHierarchical tree organization determined by k means segmentation at each level", 
            "title": "K means tree"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/k_means_tree/#k_means_tree", 
            "text": "Hierarchical tree organization determined by k means segmentation at each level", 
            "title": "k_means_tree"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/stopwatch/", 
            "text": "This whole directory is a subtree of https://github.com/mp3guy/Stopwatch .\nAs such, it is created by Thomas Whelan et al. (https://github.com/mp3guy) without\nany changes by us. The code is re-distributed here to be able to use it within the larger package structure.\n\n\nSince some of the code is attributed to the b-human code release, we include the following license,\nwhich applies to those parts:\n\n\nLICENSE\n\n\nCopyright (c) 2016 B-Human.  All rights reserved.\n\n\nPreamble: B-Human releases most of the software it uses at RoboCup\ncompetitions to allow teams participating in the Standard Platform\nLeague that do not have the resources to develop a complete robot\nsoccer system on their own, but still have important contributions\nto make to the goals of RoboCup. We intend to enable such teams to\nbenchmark their own scientific approaches in RoboCup competitions.\nWe also hope that the scientific community will benefit from their\nwork through the publication of their findings.\nA second reason for B-Human releasing its code is that source code\nis the most solid documentation of how problems were actually\nsolved.\n\n\nParts of this distribution were not developed by B-Human.\nThis license doesn't apply to these parts, the rights of the\ncopyright owners remain.\n\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n\n\n\n\n\nRedistributions in binary form must reproduce the above\n   copyright notice, this list of conditions and the following\n   disclaimer in the documentation and/or other materials provided\n   with the distribution.\n\n\n\n\n\n\nThe end-user documentation included with the redistribution, if\n   any, must include the following acknowledgment:\n   \"This product includes software developed by B-Human\n    (http://www.b-human.de).\"\n   Alternately, this acknowledgment may appear in the software\n   itself, if and wherever such third-party acknowledgments\n   normally appear.\n\n\n\n\n\n\nIf the source code or parts of the source code shall be used\n   for a RoboCup competition, the competing program must differ in\n   at least multiple major parts from the original distribution.\n   \"Major parts\" means own contributions to the RoboCup goal, which\n   are potentially publishable and usually manifest themselves as\n   new modules (i.e. source code) and not just as a parameterization\n   of existing technology (e.g. walking parameters, kicks, behavior\n   options).\n\n\n\n\n\n\nFor each B-Human code release from which parts are used in a\n   RoboCup competition, the usage shall be announced in the SPL\n   mailing list (currently robocup-nao@cc.gatech.edu) one month\n   before the first competition in which you are using it. The\n   announcement shall name which parts of this code are used.\n   It shall also contain a description of the own contribution\n   that addresses the criterions mentioned above.\n\n\n\n\n\n\nIf you are using this source code or parts of this source code\n   and happen to meet members of B-Human at a RoboCup competition,\n   please provide these members with a few bottles of your favorite\n   beer.\n\n\n\n\n\n\nBug fixes regarding existing code shall be sent back to B-Human via\n   GitHub pull request (https://github.com/bhuman).\n\n\n\n\n\n\nTHIS SOFTWARE IS PROVIDED BY B-HUMAN ``AS IS'' AND ANY\nEXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL\nB-HUMAN NOR ITS MEMBERS BE LIABLE FOR ANY DIRECT, INDIRECT,\nINCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\nOR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\nTHIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n\nStopwatch\n\n\nEasy to use simple benchmarking tool.\n\n\nSends UDP packets to localhost, which StopwatchViewer receives and displays stats on, including plots (credit to \nFiachra Matthews\n here).\n\n\nStopwatchViewer needs Qt4 to build. Simply include Stopwatch.h in whatever code you want to benchmark and use as such;\n\n\n#include \nstring\n\n#include \nstdio.h\n\n#include \nStopwatch.h\n\n\nint main(int argc, char *argv[])\n{\n  //This stops duplicate timings on multiple runs\n  Stopwatch::getInstance().setCustomSignature(32434);\n\n  STOPWATCH(\nTiming1\n,\n\n  if(argc \n= 1)\n  {\n    sleep(1);\n  }\n\n  );\n\n\n  TICK(\nTiming2\n);\n\n  while(argc \n 0)\n  {\n    sleep(2);\n    argc--;\n  }\n\n  TOCK(\nTiming2\n);\n\n  Stopwatch::getInstance().sendAll();\n}\n\n\n\n\nThen just watch the stats in StopwatchViewer.\n\n\nUses some code from the B-Human code release (http://www.b-human.de/).", 
            "title": "Stopwatch"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/stopwatch/#license", 
            "text": "Copyright (c) 2016 B-Human.  All rights reserved.  Preamble: B-Human releases most of the software it uses at RoboCup\ncompetitions to allow teams participating in the Standard Platform\nLeague that do not have the resources to develop a complete robot\nsoccer system on their own, but still have important contributions\nto make to the goals of RoboCup. We intend to enable such teams to\nbenchmark their own scientific approaches in RoboCup competitions.\nWe also hope that the scientific community will benefit from their\nwork through the publication of their findings.\nA second reason for B-Human releasing its code is that source code\nis the most solid documentation of how problems were actually\nsolved.  Parts of this distribution were not developed by B-Human.\nThis license doesn't apply to these parts, the rights of the\ncopyright owners remain.  Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.    Redistributions in binary form must reproduce the above\n   copyright notice, this list of conditions and the following\n   disclaimer in the documentation and/or other materials provided\n   with the distribution.    The end-user documentation included with the redistribution, if\n   any, must include the following acknowledgment:\n   \"This product includes software developed by B-Human\n    (http://www.b-human.de).\"\n   Alternately, this acknowledgment may appear in the software\n   itself, if and wherever such third-party acknowledgments\n   normally appear.    If the source code or parts of the source code shall be used\n   for a RoboCup competition, the competing program must differ in\n   at least multiple major parts from the original distribution.\n   \"Major parts\" means own contributions to the RoboCup goal, which\n   are potentially publishable and usually manifest themselves as\n   new modules (i.e. source code) and not just as a parameterization\n   of existing technology (e.g. walking parameters, kicks, behavior\n   options).    For each B-Human code release from which parts are used in a\n   RoboCup competition, the usage shall be announced in the SPL\n   mailing list (currently robocup-nao@cc.gatech.edu) one month\n   before the first competition in which you are using it. The\n   announcement shall name which parts of this code are used.\n   It shall also contain a description of the own contribution\n   that addresses the criterions mentioned above.    If you are using this source code or parts of this source code\n   and happen to meet members of B-Human at a RoboCup competition,\n   please provide these members with a few bottles of your favorite\n   beer.    Bug fixes regarding existing code shall be sent back to B-Human via\n   GitHub pull request (https://github.com/bhuman).    THIS SOFTWARE IS PROVIDED BY B-HUMAN ``AS IS'' AND ANY\nEXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL\nB-HUMAN NOR ITS MEMBERS BE LIABLE FOR ANY DIRECT, INDIRECT,\nINCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\nOR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\nTHIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.", 
            "title": "LICENSE"
        }, 
        {
            "location": "/strands_3d_mapping/dynamic_object_retrieval/stopwatch/#stopwatch", 
            "text": "Easy to use simple benchmarking tool.  Sends UDP packets to localhost, which StopwatchViewer receives and displays stats on, including plots (credit to  Fiachra Matthews  here).  StopwatchViewer needs Qt4 to build. Simply include Stopwatch.h in whatever code you want to benchmark and use as such;  #include  string \n#include  stdio.h \n#include  Stopwatch.h \n\nint main(int argc, char *argv[])\n{\n  //This stops duplicate timings on multiple runs\n  Stopwatch::getInstance().setCustomSignature(32434);\n\n  STOPWATCH( Timing1 ,\n\n  if(argc  = 1)\n  {\n    sleep(1);\n  }\n\n  );\n\n\n  TICK( Timing2 );\n\n  while(argc   0)\n  {\n    sleep(2);\n    argc--;\n  }\n\n  TOCK( Timing2 );\n\n  Stopwatch::getInstance().sendAll();\n}  Then just watch the stats in StopwatchViewer.  Uses some code from the B-Human code release (http://www.b-human.de/).", 
            "title": "Stopwatch"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/additional_view_registration_server/", 
            "text": "Additional view registration\n\n\nThis package provides services for the registration of additional views of an object. The services are defined \nhere\n. \n\n\n\n\nObjectAdditionalViewRegistrationService\n\n\n\n\nstring observation_xml # can be blank\nstring object_xml\n---\ngeometry_msgs/Transform[] additional_view_transforms\nint32[] additional_view_correspondences\ngeometry_msgs/Transform observation_transform\nint32 observation_correspondences\n\n\n\n\n\n\nAdditionalViewRegistrationService\n\n\n\n\nstring observation_xml # can be blank\nsensor_msgs/PointCloud2[] additional_views\ngeometry_msgs/Transform[] additional_views_odometry_transforms\n---\ngeometry_msgs/Transform[] additional_view_transforms\nint32[] additional_view_correspondences\ngeometry_msgs/Transform observation_transform\nint32 observation_correspondences\n\n\n\n\nTwo interfaces are provides: the first takes as input an \nobject_xml\n file which points to the additional views acquired by the robot (loaded with the \nmetaroom_xml_parser\n), while the second one takes as input directly the additional views and the initial odometry poses. \nNote\n that the system will attempt registration even when the odometry transforms are not available, however in general the results are exepcted to be worse. Both interfaces take as input a second parameter \nobservation_xml\n which points to an observation that the additional views should be registered to. \n\n\nThe result is stored in \nadditional_view_transforms\n - corresponding to the transforms which align the views with each other, along with \nadditional_view_correspondences\n denoting the number of \nSIFT\n correspondences used to compute the transforms. Similary, \nobservation_transform\n stores the transform which aligns the additional views to the observation, and \nobservation_correspondences\n denotes the number of correspondences used to compute it.", 
            "title": "Additional view registration server"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/additional_view_registration_server/#additional-view-registration", 
            "text": "This package provides services for the registration of additional views of an object. The services are defined  here .    ObjectAdditionalViewRegistrationService   string observation_xml # can be blank\nstring object_xml\n---\ngeometry_msgs/Transform[] additional_view_transforms\nint32[] additional_view_correspondences\ngeometry_msgs/Transform observation_transform\nint32 observation_correspondences   AdditionalViewRegistrationService   string observation_xml # can be blank\nsensor_msgs/PointCloud2[] additional_views\ngeometry_msgs/Transform[] additional_views_odometry_transforms\n---\ngeometry_msgs/Transform[] additional_view_transforms\nint32[] additional_view_correspondences\ngeometry_msgs/Transform observation_transform\nint32 observation_correspondences  Two interfaces are provides: the first takes as input an  object_xml  file which points to the additional views acquired by the robot (loaded with the  metaroom_xml_parser ), while the second one takes as input directly the additional views and the initial odometry poses.  Note  that the system will attempt registration even when the odometry transforms are not available, however in general the results are exepcted to be worse. Both interfaces take as input a second parameter  observation_xml  which points to an observation that the additional views should be registered to.   The result is stored in  additional_view_transforms  - corresponding to the transforms which align the views with each other, along with  additional_view_correspondences  denoting the number of  SIFT  correspondences used to compute the transforms. Similary,  observation_transform  stores the transform which aligns the additional views to the observation, and  observation_correspondences  denotes the number of correspondences used to compute it.", 
            "title": "Additional view registration"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/", 
            "text": "observation_registration\n\n\nThis contains packages (nodes) for the registration of:\n\n \nobservations\n\n\n \nadditional views\n\n\nThese packages should be called using the services defined \nhere\n.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/#observation_registration", 
            "text": "This contains packages (nodes) for the registration of:   observations    additional views  These packages should be called using the services defined  here .", 
            "title": "observation_registration"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_launcher/", 
            "text": "observation_registration_launcher\n\n\nThis launches the metric mapping / semantic mapping nodes required for the registration of meta-rooms and additional views.\n\n\nroslaunch observation_registration_launcher observation_registration.launch\n\n\nNodes started\n\n\n\n\nobservation_registration_server\n\n\nadditional_view_registration_server", 
            "title": "Observation registration launcher"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_launcher/#observation_registration_launcher", 
            "text": "This launches the metric mapping / semantic mapping nodes required for the registration of meta-rooms and additional views.  roslaunch observation_registration_launcher observation_registration.launch", 
            "title": "observation_registration_launcher"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_launcher/#nodes-started", 
            "text": "observation_registration_server  additional_view_registration_server", 
            "title": "Nodes started"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_server/", 
            "text": "Observation registration server\n\n\nThis node provides a service which registers two observations. \nThe service used\n is defined \nhere\n:\n\n\n\nstring source_observation_xml\nstring target_observation_xml\n---\ngeometry_msgs/Transform transform\nint32 total_correspondences\n\n\n\n\nThe service returns the transform which aligns the source observation to the target observation. The underlying registration computes correspondences between pairs of images from the source and target observations from which the registration transformation is computed. \n\n\nTest observation registration server\n\n\nFor convenience, a test routine is provided in this package which takes as input two observation xml files, calls the \nobservation_registration_server\n and visualizes the registration results. To run execute:\n\n\nrosrun observation_registration_server test_observation_registration XML_1 XML_2", 
            "title": "Observation registration server"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_server/#observation-registration-server", 
            "text": "This node provides a service which registers two observations.  The service used  is defined  here :  \nstring source_observation_xml\nstring target_observation_xml\n---\ngeometry_msgs/Transform transform\nint32 total_correspondences  The service returns the transform which aligns the source observation to the target observation. The underlying registration computes correspondences between pairs of images from the source and target observations from which the registration transformation is computed.", 
            "title": "Observation registration server"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_server/#test-observation-registration-server", 
            "text": "For convenience, a test routine is provided in this package which takes as input two observation xml files, calls the  observation_registration_server  and visualizes the registration results. To run execute:  rosrun observation_registration_server test_observation_registration XML_1 XML_2", 
            "title": "Test observation registration server"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_services/", 
            "text": "Observation registration services\n\n\nThis package defines a number of service messages, which should be included by the packages using observation/additional view registration (thus avoiding a dependency on the registration packages, which in turn depend on CUDA). \n\n\nObservationRegistrationService\n\n\nstring source_observation_xml\nstring target_observation_xml\n---\ngeometry_msgs/Transform transform\nint32 total_correspondences\n\n\n\n\nThis service returns the 3D transformation aligning the source observation to the target observation (i.e. \nsource * transform = target\n). The number of correspondences in \ntotal_correspondences\n gives an indication of the quality of the registration (specifically, if \ntotal_correspondences = 0\n the registration has failed). \n\n\nObjectAdditionalViewRegistrationService\n\n\nstring observation_xml # can be blank\nstring object_xml\n---\ngeometry_msgs/Transform[] additional_view_transforms\nint32[] additional_view_correspondences\ngeometry_msgs/Transform observation_transform\nint32 observation_correspondences\n\n\n\n\nThis service registers the additional views acquired for a specific object a) with respect to each other and b) with an observation. The inputs to this service are the \nobject_xml\n file, pointing to the object containing the additional views to be registered. The object is loaded from the disk using the \nmetaroom_xml_parser\n. The second parameter is the \nobservation_xml\n previously acquired using the \nstrands_3d_mapping\n pipeline. If this parameters is not blank, the additional views of the object will also be registered to the observation intermediate clouds. The underlying registration is done using \nsiftgpu\n and the CERES optimization engine. \n\n\nThe registered poses of the object additional views are recorded in the vector \nadditional_view_transforms\n, while the number of correspondences used per view are stored in \nadditional_view_correspondences\n. Similarly, the transform that aligns the views to the observation is stored in \nobservation_transform\n and the number of correspondences used to computed it in \nobservation_correspondences\n.\n\n\nAdditionalViewRegistrationService\n\n\n\nstring observation_xml # can be blank\nsensor_msgs/PointCloud2[] additional_views\ngeometry_msgs/Transform[] additional_views_odometry_transforms\n---\ngeometry_msgs/Transform[] additional_view_transforms\nint32[] additional_view_correspondences\ngeometry_msgs/Transform observation_transform\nint32 observation_correspondences\n\n\n\n\nThis service can be used for the same purpose as the previous one. The difference is that, instead of passing an \nobject_xml\n file, one can pass a number of \nadditional_view\n (in the form of point clouds), and, optionally (though strongly advised), odometry poses for the additional views, which the optimizer will use as initial guesses for the additional view poses.", 
            "title": "Observation registration services"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_services/#observation-registration-services", 
            "text": "This package defines a number of service messages, which should be included by the packages using observation/additional view registration (thus avoiding a dependency on the registration packages, which in turn depend on CUDA).", 
            "title": "Observation registration services"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_services/#observationregistrationservice", 
            "text": "string source_observation_xml\nstring target_observation_xml\n---\ngeometry_msgs/Transform transform\nint32 total_correspondences  This service returns the 3D transformation aligning the source observation to the target observation (i.e.  source * transform = target ). The number of correspondences in  total_correspondences  gives an indication of the quality of the registration (specifically, if  total_correspondences = 0  the registration has failed).", 
            "title": "ObservationRegistrationService"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_services/#objectadditionalviewregistrationservice", 
            "text": "string observation_xml # can be blank\nstring object_xml\n---\ngeometry_msgs/Transform[] additional_view_transforms\nint32[] additional_view_correspondences\ngeometry_msgs/Transform observation_transform\nint32 observation_correspondences  This service registers the additional views acquired for a specific object a) with respect to each other and b) with an observation. The inputs to this service are the  object_xml  file, pointing to the object containing the additional views to be registered. The object is loaded from the disk using the  metaroom_xml_parser . The second parameter is the  observation_xml  previously acquired using the  strands_3d_mapping  pipeline. If this parameters is not blank, the additional views of the object will also be registered to the observation intermediate clouds. The underlying registration is done using  siftgpu  and the CERES optimization engine.   The registered poses of the object additional views are recorded in the vector  additional_view_transforms , while the number of correspondences used per view are stored in  additional_view_correspondences . Similarly, the transform that aligns the views to the observation is stored in  observation_transform  and the number of correspondences used to computed it in  observation_correspondences .", 
            "title": "ObjectAdditionalViewRegistrationService"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/observation_registration_services/#additionalviewregistrationservice", 
            "text": "string observation_xml # can be blank\nsensor_msgs/PointCloud2[] additional_views\ngeometry_msgs/Transform[] additional_views_odometry_transforms\n---\ngeometry_msgs/Transform[] additional_view_transforms\nint32[] additional_view_correspondences\ngeometry_msgs/Transform observation_transform\nint32 observation_correspondences  This service can be used for the same purpose as the previous one. The difference is that, instead of passing an  object_xml  file, one can pass a number of  additional_view  (in the form of point clouds), and, optionally (though strongly advised), odometry poses for the additional views, which the optimizer will use as initial guesses for the additional view poses.", 
            "title": "AdditionalViewRegistrationService"
        }, 
        {
            "location": "/strands_3d_mapping/observation_registration/siftgpu/", 
            "text": "A GPU implementation of David Lowe's Scale Invariant Feature Transform\n\n\nChangchang wu\n\n\nhttp://cs.unc.edu/~ccwu\n\n\nUniversity of North Carolina at Chapel Hill\n\n\n\n\n\n\nSIFT \n\n\nSIFTGPU is an implementation of SIFT for GPU. SiftGPU uses GPU to process pixels and features \nparallely in Gaussian pyramid construction, DoG keypoint detection and descriptor generation \nfor SIFT. Compact feature list is efficiently build through a GPU/CPU mixed reduction.  \n\n\nSIFTGPU is inspired by Andrea Vedaldi's sift++ and Sudipta N Sinha et al's GPU-SIFT. Many \nparameters of sift++ ( for example, number of octaves,number of DOG levels, edge threshold,\netc) are available in SiftGPU. \n\n\nSIFTGPU also includes a GPU exhaustive/guided sift matcher SiftMatchGPU. It basically multiplies \nthe descriptor matrix on GPU and find closest feature matches on GPU.  GLSL/CUDA/CG implementations\nare all provided. \n\n\nNEW: The latest SIFTGPU also enables you to use Multi-GPUs and GPUS on different computers.\nCheck doc/manual.pdf for more information. You can modify some marcros definition in \nSimpleSIFT.cpp and speed.cpp to enable the testing of the new functions. \n\n\n\n\n\n\nRequirements\n\n\nThe default implemntation uses GLSL, and it requires a GPU that has large memory and supports\ndynamic branching. For nVidia graphic cards, you can optionally use CG(require fp40) or \nCUDA implementation. You can try different implementations and to find out the fastest one \n    for different image sizes and parameters. \n\n\nThe GLSL version may not work on ATI now. They did compile sucessfully with ATI Catalyst 8.9, \nbut not any more with 9.x versions.\n\nSiftGPU uses DevIl Image library, GLEW and GLUT. You'll need to make sure your system has\n\n\n\nall the dependening libraries. SiftGPU should be able to run on any operation system that supports \nthe above libraries\n\n\nFor windows system visual studio solution are provided as msvc/SiftGPU.dsw, msvc/SiftGPU.sln and\n    msvc/SiftGPU_CUDA_Enabled.sln. Linux/Mac makefile is in folder Linux of the package. \n\n\n\n\n\n\nHelps \n\n\nUse -help to get parameter information. Check /doc/manual.pdf for samples and explanations. \nIn the vc workspace, there is a project called SimpleSIF that gives an example of simple \nSiftGPU usage. There are more examples of different ways of using SiftGPU in manual.pdf \n\n\nCheck /doc/manual.pdf for help on the viewer.", 
            "title": "Siftgpu"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/", 
            "text": "quasimodo\n\n\nQuasimodo - Qu(erying) A(nd) S(patio-Temporal) I(ncremental) Mod(el building of) O(bjects)\n\n\nBuild \\\n Setup\n\n\nThe packes distributed here do note require any special setup, simply build them in your catkin workspace.\nNote that you will need the \n2.0\n branch of \nhttps://github.com/strands-project/soma\n\n(which in turn requires an up-to-date version of \nhttps://github.com/strands-project/mongodb_store\n).\nHowever, our packages require graphics for data fusion and visualization, the following describes how\nto set that up on a computer without display.\n\n\nHeadless Display \\\n ElasticFusion\n\n\nThis package is meant to run on a robot with no screen but with Nvidia graphics card.\nThis requires some setup every time the package is run (see the last two lines of code).\n\n\nRight now, this package has a few dependencies that have to be installed manually.\nIn the future, our ElasticFusion fork will be replaced by a new version implemented in CUDA.\nFor now, you need to clone the repo\n\nhttps://github.com/stevenlovegrove/Pangolin\n\nanywhere in your computer home folder. Follow the build instructions in the readme.\n\n\nThen you need to install our fork of ElasticFusion. For project members, please contact one of\nthe maintainers and we will give you access to the code. Note that you need at least\nCUDA version 7 to run ElasticFusion. To get a graphics driver compatible with this version,\nthe easiest way (under Ubuntu 14.04) is to \nsudo apt-get install nvidia-352\n.\n\n\nTo run these programs on a headless computer, we need to perform the following steps.\nFirst, do\n\n\nsudo nvidia-xconfig -a --use-display-device=None --virtual=1280x1024\n\n\n\n\nand then restart the computer. Further, we need to point to the new display that will be used\nin the X server that will be used by typing\n\n\nexport DISPLAY=:0\n\n\n\n\nIf you are gonna run the programs multiple times, you might consider putting this in the \n.bashrc\n.\nNote that this will mean that you have to set the \nDISPLAY\n again manually if you want to \nssh\n with \n-X\n.\nThen, every time you restarted the computer and run the nodes, you need to run\n\n\nsudo service lightdm stop\nsudo /usr/bin/X :0 \n\n\n\n\n\nThis will kill the previous X server and start a new one that works in a headless state.\n\n\nApart from these acrobatics, all you should need to do is \ncatkin_make\n in your workspace.\n\n\nGeneral Info \\\n Launching\n\n\nThe packages in this repo provides functionality for building a database of objects from observations\nin an unsupervised manner. It builds on the Metaroom paradigm for data collection. The libraries\nused for retrieving point clouds from several weeks of data collection can be found in\n\nhttps://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/dynamic_object_retrieval\n.\n\n\nFor launching the pipeline for building the representation and querying on the robot, do\n\n\nroslaunch quasimodo_launch quasimodo.launch data_folder:=/path/to/.semanticMap\n\n\n\n\nHere, \n/path/to/.semanticMap\n is typically located in \n~/.semanticMap\n.\nPlease type the complete path.\nThis will launch the necessary nodes and launch files, both for maintaining a data base of object\nmodels and for retrieving point clouds across weeks of exploration.\n\n\nVisualization\n\n\nRetrieval component\n\n\nThe easiest way to visualize the output of the retrieval (point cloud history search) pipeline\nis to look at the image published on the \n/quasimodo_retrieval/visualization\n topic.\nThe leftmost image shows the masked RGB image of the query object and to the right are rendered views of the\nten closest matches represented as 3D surfel clouds.\n\n\nYou can manually trigger a search (i.e. without using the incremental object building framework)\nof an object with additional views by starting\n\n\nrosrun quasimodo_retrieval quasimodo_retrieve_observation\n\n\n\n\nand then, in another window specifying the path to the xml of the additional views:\n\n\nrostopic pub /object_learning/learned_object_xml std_msgs/String \ndata: '/path/to/.semanticMap/201422/patrol_run_56/room_0/2016-Apr-22 14:58:33.536964_object_0.xml'\n\n\n\n\n\nYou can also use soma to visualize the queries over time.\n\n\nModel building component\n\n\n@jekekrantz Add some stuff here!\n\n\nDetailed description of packages, nodes, launch files and messages\n\n\nretrieval_processing\n\n\nThis package runs in conjunction with the metaroom nodes online on the robot. As metarooms are collected,\nthe package continuously segments, extracts features and adds them to a feature vocabulary representation.\nTo launch the entire pipeline, do\n\n\nroslaunch retrieval_processing processing.launch data_folder:=~/.semanticMap\n\n\n\n\nNodes\n\n\n\n\nretrieval_segmentation\n - uses the \nconvex_segmentation\n package, \nhttps://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/dynamic_object_retrieval/convex_segmentation\n, to segment a point cloud with normals into smaller convex segments\n\n\nretrieval_features\n - uses the \ndynamic_object_retrieval\n package, \nhttps://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/dynamic_object_retrieval/dynamic_object_retrieval\n, to extract PFHRGB features from the convex segments\n\n\nretrieval_vocabulary\n - uses the \nk_means_tree\n package, \nhttps://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/dynamic_object_retrieval/k_means_tree\n, to organize the features into a hierarchical vocabulary representation for fast querying\n\n\nretrieval_simulate_observations\n - this node is used if you have already collected data that you want to process, or if you want to debug without doing sweeps on the robot\n\n\n\n\nLaunch files\n\n\n\n\nprocessing.launch\n - this file launches all of the above nodes. By default it does not launch \nretrieval_simulate_observations\n, this is trigged with the parameter \nsimulate_observations:=true\n.\nIt is important to set the parameter \ndata_folder:=/path/to/metarooms\n to the location of the metarooms,\ntypically \n~/.semanticMap\n.\n\n\n\n\nquasimodo_retrieval\n\n\nThis package provides the nodes for retrieving point clouds from the memory created by \nretrieval_processing\n.\nLaunch everything simply with\n\n\nroslaunch quasimodo_retrieval retrieval.launch vocabulary_path:=/path/to/vocabulary\n\n\n\n\nwhere the vocabulary is most often located in \n~/.semanticMap/vocabulary\n.\n\n\nNodes\n\n\n\n\nquasimodo_retrieval_node\n - provides the service \n/query_normal_cloud\n and subscribes to the topic \n/models/new\n. If something is published on the topic, it returns the result on \n/retrieval_result\n.\n\n\nquasimodo_visualization_server\n - this node simply subscribes to \n/retrieval_result\n and visualizes the query result using the tools in the package \nobject_3d_benchmark\n, \nhttps://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/dynamic_object_retrieval/benchmark\n. The resulting image is published on \n/visualization_image\n.\n\n\nquasimodo_retrieve_observation\n - allows the system to bypass the model building component, instead searching for results directly using the retrieval framework. Simply publish something like \nrostopic pub /object_learning/learned_object_xml std_msgs/String \"data: '/path/to/.semanticMap/201422/patrol_run_56/room_0/2016-Apr-22 14:58:33.536964_object_0.xml'\"\n to retrieve more views of that object.\n\n\n\n\nOther Nodes\n\n\n\n\nquasimodo_visualize_model\n - this node simply visualizes the topic \n/models/new\n by integrating it into a point cloud and showing a PCL visualizer\n\n\nquasimodo_retrieval_publisher\n - this node queries for all the labeled objects in a particular metaroom sweep, given by the parameter \ndata_path\n.\n\n\nquasimodo_retrieval_server\n - a barebones version of \nquasimodo_retrieval_node\n, simply returns the retrieved clouds without loading any images or objects masks\n\n\n\n\nLaunch files\n\n\n\n\nretrieval.launch\n - launches \nquasimodo_retrieval_node\n, \nquasimodo_visualization_server\n and a node for fusing the incoming RGB-D frames. Takes the parameter \nvocabulary_path\n, most often this is \n~/.semanticMap/vocabulary\n.\n\n\n\n\nquasimodo_optimization\n\n\nThis package is a general tool for optimizing some value by evaluating some metric that comes from analyzing a rosbag.\nThe tool package uses \ndynamic_reconfigure\n to play back the rosbag with different parameters and record the\nvalues associated with the parameters.\n\n\nCan be launched with\n\n\nroslaunch quasimodo_optimizer optimizer.launch\n\n\n\n\nAfterwards, run \nplot_values.py\n in the folder where you ran the launch file.\n\n\nNodes\n\n\n\n\noptimizer.py\n - steps through the parameters and plays back the rosbags for every parameter configuration\n\n\nrosbag_player.py\n - an action server for playing back ros bags on demand\n\n\nplot_values.py\n - plots the values as a heat map in parameter space\n\n\n\n\nLaunch files\n\n\n\n\noptimizer.launch\n - launches \noptimizer.py\n and \nrosbag_player.py\n.\n\n\n\n\nquasimodo_msgs\n\n\nAll the message and service types required for the Quasimodo framework.\n\n\nMessage types\n\n\n\n\nimage_array.msg\n - an array of images\n\n\nint_array.msg\n - an array of ints\n\n\nmodel.msg\n - a model object, consisting of point clouds, frames, camera parameters and relative transforms\n\n\nretrieval_query.msg\n - message type for querying \nquasimodo_retrieval\n\n\nretrieval_result.msg\n- message type result from querying \nquasimodo_retrieval\n\n\nretrieval_query_result.msg\n - a combined message for querying and result\n\n\nrgbd_frame.msg\n - RGD images, depth images and camera parameters\n\n\nstring_array.msg\n - an array of strings\n\n\n\n\nService types\n\n\n\n\ncloud_from_model.srv\n - service for fusing models into clouds\n\n\nfuse_models.srv\n - several models to one fused model\n\n\nget_model.srv\n - get model for identifier\n\n\nindex_frame.srv\n - add frame to model data base\n\n\nmodel_from_frame.srv\n - turn frame into model\n\n\nquery_cloud.srv\n - query retrieval using \nretrieval_query.msg\n\n\nsimple_query_cloud.srv\n - query retrieval using \nsensor_msgs/PointCloud2\n pointcloud with normals\n\n\nvisualize_query.srv\n- visualize a \nretrieval_result.msg\n\n\n\n\nquasimodo_brain\n\n\nThis package controlls the flow of data in the quasimodo system and maintains the database of object models. Relies heavily on the quasimodo_models package. The package also contains loaders for different formats of data, such as for example the metarooms.\n\n\nroslaunch quasimodo_brain modelserver.launch\n\n\n\n\nroslaunch quasimodo_brain robot_listener.launch\n\n\n\n\nNodes\n\n\n\n\n\n\npreload_object_data\n - Reads data in the metarooms format. Uppon requests publishes data for the \nmodelserver\n. Input: paths to a set of folders containing data. \n\n\n\n\n\n\nrobot_listener\n - Listenes to topic. Whenever it recieves the path to an xml file it reads data in the metarooms format from the file and publishes data for the \nmodelserver\n. Input: topicname to listen at. \n\n\n\n\n\n\nmodelserver\n - Listens to data from input modules, uses the \nquasimodo_models\n package to register and merge models into more complete models and thereby maintain the database of objects. Input: '-v' for visualization, '-p /path/to/folder' to set a folder where the database is read/stored, '-m' initializes the database with the data from /path/to/folder, '-massreg_timeout value' sets the stopping time for the multiview registration, '-occlusion_penalty value' sets the penalty value for occlusions(controlling how likeley the database is to merge models).\n\n\n\n\n\n\nLaunch files\n\n\n\n\n\n\nmodelserver.launch\n - this file launches the modelserver node without the visualization flag.\n\n\n\n\n\n\nrobot_listener.launch\n - this file launches the robot_listener node without the topicname set to \"/some/topic\".\n\n\n\n\n\n\nbrain.launch\n - Launches the modelserver and the preload_object_data nodes. On automatic restart.\n\n\n\n\n\n\nquasimodo_models\n\n\nThis package is contains libraries for registering, splitting, merging and optimizing quasimodo object models. Quasimodo object models contain RGBDFrames, Segmentation masks, Depthimages and relative poses between the data for the frames.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#quasimodo", 
            "text": "Quasimodo - Qu(erying) A(nd) S(patio-Temporal) I(ncremental) Mod(el building of) O(bjects)", 
            "title": "quasimodo"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#build-setup", 
            "text": "The packes distributed here do note require any special setup, simply build them in your catkin workspace.\nNote that you will need the  2.0  branch of  https://github.com/strands-project/soma \n(which in turn requires an up-to-date version of  https://github.com/strands-project/mongodb_store ).\nHowever, our packages require graphics for data fusion and visualization, the following describes how\nto set that up on a computer without display.", 
            "title": "Build \\&amp; Setup"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#headless-display-elasticfusion", 
            "text": "This package is meant to run on a robot with no screen but with Nvidia graphics card.\nThis requires some setup every time the package is run (see the last two lines of code).  Right now, this package has a few dependencies that have to be installed manually.\nIn the future, our ElasticFusion fork will be replaced by a new version implemented in CUDA.\nFor now, you need to clone the repo https://github.com/stevenlovegrove/Pangolin \nanywhere in your computer home folder. Follow the build instructions in the readme.  Then you need to install our fork of ElasticFusion. For project members, please contact one of\nthe maintainers and we will give you access to the code. Note that you need at least\nCUDA version 7 to run ElasticFusion. To get a graphics driver compatible with this version,\nthe easiest way (under Ubuntu 14.04) is to  sudo apt-get install nvidia-352 .  To run these programs on a headless computer, we need to perform the following steps.\nFirst, do  sudo nvidia-xconfig -a --use-display-device=None --virtual=1280x1024  and then restart the computer. Further, we need to point to the new display that will be used\nin the X server that will be used by typing  export DISPLAY=:0  If you are gonna run the programs multiple times, you might consider putting this in the  .bashrc .\nNote that this will mean that you have to set the  DISPLAY  again manually if you want to  ssh  with  -X .\nThen, every time you restarted the computer and run the nodes, you need to run  sudo service lightdm stop\nsudo /usr/bin/X :0    This will kill the previous X server and start a new one that works in a headless state.  Apart from these acrobatics, all you should need to do is  catkin_make  in your workspace.", 
            "title": "Headless Display \\&amp; ElasticFusion"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#general-info-launching", 
            "text": "The packages in this repo provides functionality for building a database of objects from observations\nin an unsupervised manner. It builds on the Metaroom paradigm for data collection. The libraries\nused for retrieving point clouds from several weeks of data collection can be found in https://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/dynamic_object_retrieval .  For launching the pipeline for building the representation and querying on the robot, do  roslaunch quasimodo_launch quasimodo.launch data_folder:=/path/to/.semanticMap  Here,  /path/to/.semanticMap  is typically located in  ~/.semanticMap .\nPlease type the complete path.\nThis will launch the necessary nodes and launch files, both for maintaining a data base of object\nmodels and for retrieving point clouds across weeks of exploration.", 
            "title": "General Info \\&amp; Launching"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#visualization", 
            "text": "", 
            "title": "Visualization"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#retrieval-component", 
            "text": "The easiest way to visualize the output of the retrieval (point cloud history search) pipeline\nis to look at the image published on the  /quasimodo_retrieval/visualization  topic.\nThe leftmost image shows the masked RGB image of the query object and to the right are rendered views of the\nten closest matches represented as 3D surfel clouds.  You can manually trigger a search (i.e. without using the incremental object building framework)\nof an object with additional views by starting  rosrun quasimodo_retrieval quasimodo_retrieve_observation  and then, in another window specifying the path to the xml of the additional views:  rostopic pub /object_learning/learned_object_xml std_msgs/String  data: '/path/to/.semanticMap/201422/patrol_run_56/room_0/2016-Apr-22 14:58:33.536964_object_0.xml'   You can also use soma to visualize the queries over time.", 
            "title": "Retrieval component"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#model-building-component", 
            "text": "@jekekrantz Add some stuff here!", 
            "title": "Model building component"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#detailed-description-of-packages-nodes-launch-files-and-messages", 
            "text": "", 
            "title": "Detailed description of packages, nodes, launch files and messages"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#retrieval_processing", 
            "text": "This package runs in conjunction with the metaroom nodes online on the robot. As metarooms are collected,\nthe package continuously segments, extracts features and adds them to a feature vocabulary representation.\nTo launch the entire pipeline, do  roslaunch retrieval_processing processing.launch data_folder:=~/.semanticMap", 
            "title": "retrieval_processing"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#nodes", 
            "text": "retrieval_segmentation  - uses the  convex_segmentation  package,  https://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/dynamic_object_retrieval/convex_segmentation , to segment a point cloud with normals into smaller convex segments  retrieval_features  - uses the  dynamic_object_retrieval  package,  https://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/dynamic_object_retrieval/dynamic_object_retrieval , to extract PFHRGB features from the convex segments  retrieval_vocabulary  - uses the  k_means_tree  package,  https://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/dynamic_object_retrieval/k_means_tree , to organize the features into a hierarchical vocabulary representation for fast querying  retrieval_simulate_observations  - this node is used if you have already collected data that you want to process, or if you want to debug without doing sweeps on the robot", 
            "title": "Nodes"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#launch-files", 
            "text": "processing.launch  - this file launches all of the above nodes. By default it does not launch  retrieval_simulate_observations , this is trigged with the parameter  simulate_observations:=true .\nIt is important to set the parameter  data_folder:=/path/to/metarooms  to the location of the metarooms,\ntypically  ~/.semanticMap .", 
            "title": "Launch files"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#quasimodo_retrieval", 
            "text": "This package provides the nodes for retrieving point clouds from the memory created by  retrieval_processing .\nLaunch everything simply with  roslaunch quasimodo_retrieval retrieval.launch vocabulary_path:=/path/to/vocabulary  where the vocabulary is most often located in  ~/.semanticMap/vocabulary .", 
            "title": "quasimodo_retrieval"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#nodes_1", 
            "text": "quasimodo_retrieval_node  - provides the service  /query_normal_cloud  and subscribes to the topic  /models/new . If something is published on the topic, it returns the result on  /retrieval_result .  quasimodo_visualization_server  - this node simply subscribes to  /retrieval_result  and visualizes the query result using the tools in the package  object_3d_benchmark ,  https://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/dynamic_object_retrieval/benchmark . The resulting image is published on  /visualization_image .  quasimodo_retrieve_observation  - allows the system to bypass the model building component, instead searching for results directly using the retrieval framework. Simply publish something like  rostopic pub /object_learning/learned_object_xml std_msgs/String \"data: '/path/to/.semanticMap/201422/patrol_run_56/room_0/2016-Apr-22 14:58:33.536964_object_0.xml'\"  to retrieve more views of that object.", 
            "title": "Nodes"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#other-nodes", 
            "text": "quasimodo_visualize_model  - this node simply visualizes the topic  /models/new  by integrating it into a point cloud and showing a PCL visualizer  quasimodo_retrieval_publisher  - this node queries for all the labeled objects in a particular metaroom sweep, given by the parameter  data_path .  quasimodo_retrieval_server  - a barebones version of  quasimodo_retrieval_node , simply returns the retrieved clouds without loading any images or objects masks", 
            "title": "Other Nodes"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#launch-files_1", 
            "text": "retrieval.launch  - launches  quasimodo_retrieval_node ,  quasimodo_visualization_server  and a node for fusing the incoming RGB-D frames. Takes the parameter  vocabulary_path , most often this is  ~/.semanticMap/vocabulary .", 
            "title": "Launch files"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#quasimodo_optimization", 
            "text": "This package is a general tool for optimizing some value by evaluating some metric that comes from analyzing a rosbag.\nThe tool package uses  dynamic_reconfigure  to play back the rosbag with different parameters and record the\nvalues associated with the parameters.  Can be launched with  roslaunch quasimodo_optimizer optimizer.launch  Afterwards, run  plot_values.py  in the folder where you ran the launch file.", 
            "title": "quasimodo_optimization"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#nodes_2", 
            "text": "optimizer.py  - steps through the parameters and plays back the rosbags for every parameter configuration  rosbag_player.py  - an action server for playing back ros bags on demand  plot_values.py  - plots the values as a heat map in parameter space", 
            "title": "Nodes"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#launch-files_2", 
            "text": "optimizer.launch  - launches  optimizer.py  and  rosbag_player.py .", 
            "title": "Launch files"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#quasimodo_msgs", 
            "text": "All the message and service types required for the Quasimodo framework.", 
            "title": "quasimodo_msgs"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#message-types", 
            "text": "image_array.msg  - an array of images  int_array.msg  - an array of ints  model.msg  - a model object, consisting of point clouds, frames, camera parameters and relative transforms  retrieval_query.msg  - message type for querying  quasimodo_retrieval  retrieval_result.msg - message type result from querying  quasimodo_retrieval  retrieval_query_result.msg  - a combined message for querying and result  rgbd_frame.msg  - RGD images, depth images and camera parameters  string_array.msg  - an array of strings", 
            "title": "Message types"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#service-types", 
            "text": "cloud_from_model.srv  - service for fusing models into clouds  fuse_models.srv  - several models to one fused model  get_model.srv  - get model for identifier  index_frame.srv  - add frame to model data base  model_from_frame.srv  - turn frame into model  query_cloud.srv  - query retrieval using  retrieval_query.msg  simple_query_cloud.srv  - query retrieval using  sensor_msgs/PointCloud2  pointcloud with normals  visualize_query.srv - visualize a  retrieval_result.msg", 
            "title": "Service types"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#quasimodo_brain", 
            "text": "This package controlls the flow of data in the quasimodo system and maintains the database of object models. Relies heavily on the quasimodo_models package. The package also contains loaders for different formats of data, such as for example the metarooms.  roslaunch quasimodo_brain modelserver.launch  roslaunch quasimodo_brain robot_listener.launch", 
            "title": "quasimodo_brain"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#nodes_3", 
            "text": "preload_object_data  - Reads data in the metarooms format. Uppon requests publishes data for the  modelserver . Input: paths to a set of folders containing data.     robot_listener  - Listenes to topic. Whenever it recieves the path to an xml file it reads data in the metarooms format from the file and publishes data for the  modelserver . Input: topicname to listen at.     modelserver  - Listens to data from input modules, uses the  quasimodo_models  package to register and merge models into more complete models and thereby maintain the database of objects. Input: '-v' for visualization, '-p /path/to/folder' to set a folder where the database is read/stored, '-m' initializes the database with the data from /path/to/folder, '-massreg_timeout value' sets the stopping time for the multiview registration, '-occlusion_penalty value' sets the penalty value for occlusions(controlling how likeley the database is to merge models).", 
            "title": "Nodes"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#launch-files_3", 
            "text": "modelserver.launch  - this file launches the modelserver node without the visualization flag.    robot_listener.launch  - this file launches the robot_listener node without the topicname set to \"/some/topic\".    brain.launch  - Launches the modelserver and the preload_object_data nodes. On automatic restart.", 
            "title": "Launch files"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/#quasimodo_models", 
            "text": "This package is contains libraries for registering, splitting, merging and optimizing quasimodo object models. Quasimodo object models contain RGBDFrames, Segmentation masks, Depthimages and relative poses between the data for the frames.", 
            "title": "quasimodo_models"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/quasimodo_retrieval/", 
            "text": "Quasimodo Retrieval\n\n\nThis package contains nodes to interface with the \nobject_3d_retrieval\n and\n\nobject_3d_benchmark\n packages. They are meant to be used together with\nwith the scripts in \nobject_3d_benchmark\n to build a feature representation\nthan can be used by these nodes for querying. Please look at the package\n\nquasimodo_test\n (specifically the node \ntest_msgs\n) for an example of this in action.\n\n\nRetrieval Server\n\n\nThe retrieval server basically takes a point cloud and queries for\nsimilar objects observed previously within our feature representation.\nThe definition of the service looks like:\n\n\nsensor_msgs/PointCloud2 cloud\nsensor_msgs/Image image\nsensor_msgs/CameraInfo camera\n---\nretrieval_result result\n\n\n\n\nPlease check the message \nquasimodo_msgs/retrieval_result\n for the exact format\nof the return type. Invoke a server instance that can be called with the\nabove service definition by running\n\n\nrosrun quasimodo_retrieval quasimodo_retrieval_server _vocabulary_path:=/path/to/vocabulary\n.\n\n\nVisualization Server\n\n\nThe idea is that the output from the retrieval server can be sent on to\nanother server for visualization. The original query data, together with the\nresult from the retrieval server is passed on to the visualization server.\nIt offers the following service:\n\n\nsensor_msgs/Image image\nsensor_msgs/CameraInfo camera\ngeometry_msgs/Transform room_transform\nretrieval_result result\n---\nsensor_msgs/Image image\n\n\n\n\nRun the node by simply typing\n\n\nrosrun quasimodo_retrieval quasimodo_visualization_server\n.", 
            "title": "Quasimodo retrieval"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/quasimodo_retrieval/#quasimodo-retrieval", 
            "text": "This package contains nodes to interface with the  object_3d_retrieval  and object_3d_benchmark  packages. They are meant to be used together with\nwith the scripts in  object_3d_benchmark  to build a feature representation\nthan can be used by these nodes for querying. Please look at the package quasimodo_test  (specifically the node  test_msgs ) for an example of this in action.", 
            "title": "Quasimodo Retrieval"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/quasimodo_retrieval/#retrieval-server", 
            "text": "The retrieval server basically takes a point cloud and queries for\nsimilar objects observed previously within our feature representation.\nThe definition of the service looks like:  sensor_msgs/PointCloud2 cloud\nsensor_msgs/Image image\nsensor_msgs/CameraInfo camera\n---\nretrieval_result result  Please check the message  quasimodo_msgs/retrieval_result  for the exact format\nof the return type. Invoke a server instance that can be called with the\nabove service definition by running  rosrun quasimodo_retrieval quasimodo_retrieval_server _vocabulary_path:=/path/to/vocabulary .", 
            "title": "Retrieval Server"
        }, 
        {
            "location": "/strands_3d_mapping/quasimodo/quasimodo_retrieval/#visualization-server", 
            "text": "The idea is that the output from the retrieval server can be sent on to\nanother server for visualization. The original query data, together with the\nresult from the retrieval server is passed on to the visualization server.\nIt offers the following service:  sensor_msgs/Image image\nsensor_msgs/CameraInfo camera\ngeometry_msgs/Transform room_transform\nretrieval_result result\n---\nsensor_msgs/Image image  Run the node by simply typing  rosrun quasimodo_retrieval quasimodo_visualization_server .", 
            "title": "Visualization Server"
        }, 
        {
            "location": "/strands_3d_mapping/wiki/Get-up-and-running/", 
            "text": "The repo contains the catkin package package octomap_launch. To be able to run it you need the octomap stack. You can get it by running sudo apt-get install ros-groovy-octomap-mapping. If you also want to visualize the maps you create you can get the octovis visualizer tool by running sudo apt-get install ros-groovy-octovis.\n\n\nroslaunch octomap_launch octomap.launch map:=(PATH_TO_MAP)/map.yaml - for running the scitos nav stack package for localization and octomap for map building. You need to have the scitos drivers, including the laser scanner and the state_publisher up and running before doing this.\n\n\nIf you want to view the map while building it using octomap, you can visualize it in rviz by adding a MarkerArray on topic occupied_cells_vis_array.\n\n\nTo save the map created by octomap, just do rosrun octomap_server octomap_saver -f (FILENAME).bt. You can view the map file by just typing octovis (FILENAME).bt in the command line. There is an example 3D map created on the robot in octomap_launch/example_maps/map3d.bt that you can try this out with.", 
            "title": "Get up and running"
        }, 
        {
            "location": "/strands_3d_mapping/wiki/Home/", 
            "text": "Welcome to the scitos_3d_mapping_and_navigation wiki!\n\n\nFor instructions on how to get the dependencies and run octomap, have a look at:\nhttps://github.com/strands-project/scitos_3d_mapping_and_navigation/wiki/Get-up-and-running", 
            "title": "Home"
        }, 
        {
            "location": "/strands_apps/door_pass/", 
            "text": "Overview\n\n\nSimple behaviour for passing through narrow doors. \nJust position the robot near to the ramp and send the Monitored nav goal with any parameters.", 
            "title": "Door pass"
        }, 
        {
            "location": "/strands_apps/door_pass/#overview", 
            "text": "Simple behaviour for passing through narrow doors. \nJust position the robot near to the ramp and send the Monitored nav goal with any parameters.", 
            "title": "Overview"
        }, 
        {
            "location": "/strands_apps/", 
            "text": "strands_apps\n\n\nProject specific apps and tools", 
            "title": "Home"
        }, 
        {
            "location": "/strands_apps/#strands_apps", 
            "text": "Project specific apps and tools", 
            "title": "strands_apps"
        }, 
        {
            "location": "/strands_apps/marathon_reporter/", 
            "text": "Overview\n\n\nThis package provides a script to report marathon run attempts to the STRANDS Robot Marathon webserver. The script monitors the robot's odometry to find out how far the the robot has moved. Starting the script signals the beginning of a run, ending the script finishes the run.\n\n\nPre-requisites\n\n\n\n\nThe robot needs to have internet access at the beginning of the run. This is so that it can close any outstanding previous attempts and request a new run key from the server.\n\n\nThe robot must publish odometry data as \nnav_msgs/Odometry\n messages on the \n/odom\n topic.\n\n\nThe monogodb_store must be running.\n\n\nYour marathon teamname/password must be set in \n~/.marathon_auth\n\n\n\n\nSetting up \n.marathon_auth\n\n\nThe team name and password supplied to you must be placed in the file \n~/.marathon_auth\n in the format:\n\n\nteam: \nyour_team_name\n\npassword: \nyour_pa33w0rd", 
            "title": "Marathon reporter"
        }, 
        {
            "location": "/strands_apps/marathon_reporter/#overview", 
            "text": "This package provides a script to report marathon run attempts to the STRANDS Robot Marathon webserver. The script monitors the robot's odometry to find out how far the the robot has moved. Starting the script signals the beginning of a run, ending the script finishes the run.", 
            "title": "Overview"
        }, 
        {
            "location": "/strands_apps/marathon_reporter/#pre-requisites", 
            "text": "The robot needs to have internet access at the beginning of the run. This is so that it can close any outstanding previous attempts and request a new run key from the server.  The robot must publish odometry data as  nav_msgs/Odometry  messages on the  /odom  topic.  The monogodb_store must be running.  Your marathon teamname/password must be set in  ~/.marathon_auth", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/strands_apps/marathon_reporter/#setting-up-marathon_auth", 
            "text": "The team name and password supplied to you must be placed in the file  ~/.marathon_auth  in the format:  team:  your_team_name \npassword:  your_pa33w0rd", 
            "title": "Setting up .marathon_auth"
        }, 
        {
            "location": "/strands_apps/odometry_mileage/", 
            "text": "Odometry Mileometre\n\n\nThis very simple node subscribes to \n/odom\n and calculates the travelled distance since start in metres.\nThe result is publish on \n/odom_mileage\n.\n\n\nStart with \nrosrun odomoetry_mileage odometry_mileage\n\n\nIf the parametere \nsaved_mileage\n is present on the rosparam server (e.g. via the datacentre), then the node will use the stored mileage to intialise itself.\nThe node will also check against the actual \n/mileage\n topic on startup and will choose the large of both (parameter or published value). The \nmileage\n topic might be actually higher then the \nodom_mileage\n topic after restarting the robot because at that point the mileage is read directly from the EEPROM and the odometry_mileage node might not have been running all the time when the system crashes. \n\n\nWhile the node is running the mileage parameter is constantly set to the current value and saved to the datacentre. The default save interval\nis every 500 odometry messages, which is every ~10 seconds. This can be change via the \nsave_interval\n parametre.", 
            "title": "Odometry mileage"
        }, 
        {
            "location": "/strands_apps/odometry_mileage/#odometry-mileometre", 
            "text": "This very simple node subscribes to  /odom  and calculates the travelled distance since start in metres.\nThe result is publish on  /odom_mileage .  Start with  rosrun odomoetry_mileage odometry_mileage  If the parametere  saved_mileage  is present on the rosparam server (e.g. via the datacentre), then the node will use the stored mileage to intialise itself.\nThe node will also check against the actual  /mileage  topic on startup and will choose the large of both (parameter or published value). The  mileage  topic might be actually higher then the  odom_mileage  topic after restarting the robot because at that point the mileage is read directly from the EEPROM and the odometry_mileage node might not have been running all the time when the system crashes.   While the node is running the mileage parameter is constantly set to the current value and saved to the datacentre. The default save interval\nis every 500 odometry messages, which is every ~10 seconds. This can be change via the  save_interval  parametre.", 
            "title": "Odometry Mileometre"
        }, 
        {
            "location": "/strands_apps/pose_extractor/", 
            "text": "Pose extractor\n\n\nCurrently contains one node which extracts the last poseStamped from a \nnav_msgs/Path\n. This is used to gaze at the position the robot is driving towards and to make its movement more legible.", 
            "title": "Pose extractor"
        }, 
        {
            "location": "/strands_apps/pose_extractor/#pose-extractor", 
            "text": "Currently contains one node which extracts the last poseStamped from a  nav_msgs/Path . This is used to gaze at the position the robot is driving towards and to make its movement more legible.", 
            "title": "Pose extractor"
        }, 
        {
            "location": "/strands_apps/ramp_climb/", 
            "text": "Overview\n\n\nSimple behaviour for passing thorugh narrow doors.\nJust position the robot near to the door and send the monitored nav goal.", 
            "title": "Ramp climb"
        }, 
        {
            "location": "/strands_apps/ramp_climb/#overview", 
            "text": "Simple behaviour for passing thorugh narrow doors.\nJust position the robot near to the door and send the monitored nav goal.", 
            "title": "Overview"
        }, 
        {
            "location": "/strands_apps/roslaunch_axserver/", 
            "text": "roslaunch_axserver: Running roslaunch files via the actionlib interface\n\n\ngoal definition\n\n\nThis component allows to launch ROS launch files via an actionlib server. A simple goal (of type \nlaunchServerGoal\n) would look like this:\n\n\npkg: 'strands_morse'\nlaunch_file: 'uol_mht_nav2d.launch'\nmonitored_topics: ['/map']\n\n\n\n\nthis would launch the launch file \nuol_mht_nav2d.launch\n in package \nstrands_morse\n. \n\n\nmonitoring the roslaunch process\n\n\nThe action server implements a monitor thread that will provide feedback on the actionlib feedback channel as a boolean (parameter \nready\n in the feedback), indicating if \nall\n the topics given via \nmonitored_topics\n (in the example above just one: \n/map\n) are actually advertised. This allows to wait until all components have been launched. If \nmonitored_topics\n is empty, the feedback is always \nready=True\n. \n\n\nstate transitions\n\n\nThe goal status will be set ACTIVE when it was possible to launch a roslaunch process (and will be REJECTED if this was not possible). If the roslaunch process terminates with an error (e.g. when the given roslaunch file doesn't exist), the goal is set to ABORTED. If the roslaunch process terminates successfully, then the goal is set to SUCCESS (usually, we will never see this, as the roslaunch process will likely be running until we actively preempt it).\n\n\nusing in Python\n\n\nIn order to use this in your code, include something like the following in your python code:\n\n\nimport rospy\nimport roslaunch_axserver.msg\nimport actionlib\n\n...\n\n    client = actionlib.SimpleActionClient('launchServer', roslaunch_axserver.msg.launchAction)\n\n    # Waits until the action server has started up and started\n    # listening for goals.\n    client.wait_for_server()\n\n    # Creates a goal to send to the action server.\n    goal = roslaunch_axserver.msg.launchGoal(pkg='pkg', launch_file='filename', monitored_topics=[])\n\n    # Sends the goal to the action server.\n    client.send_goal(goal)\n\n\n\n\n\nThe server is reentrant, meaning that you can run several roslaunch processes in parallel (\nnot\n a \nSimpleActionServer\n)", 
            "title": "Roslaunch axserver"
        }, 
        {
            "location": "/strands_apps/roslaunch_axserver/#roslaunch_axserver-running-roslaunch-files-via-the-actionlib-interface", 
            "text": "", 
            "title": "roslaunch_axserver: Running roslaunch files via the actionlib interface"
        }, 
        {
            "location": "/strands_apps/roslaunch_axserver/#goal-definition", 
            "text": "This component allows to launch ROS launch files via an actionlib server. A simple goal (of type  launchServerGoal ) would look like this:  pkg: 'strands_morse'\nlaunch_file: 'uol_mht_nav2d.launch'\nmonitored_topics: ['/map']  this would launch the launch file  uol_mht_nav2d.launch  in package  strands_morse .", 
            "title": "goal definition"
        }, 
        {
            "location": "/strands_apps/roslaunch_axserver/#monitoring-the-roslaunch-process", 
            "text": "The action server implements a monitor thread that will provide feedback on the actionlib feedback channel as a boolean (parameter  ready  in the feedback), indicating if  all  the topics given via  monitored_topics  (in the example above just one:  /map ) are actually advertised. This allows to wait until all components have been launched. If  monitored_topics  is empty, the feedback is always  ready=True .", 
            "title": "monitoring the roslaunch process"
        }, 
        {
            "location": "/strands_apps/roslaunch_axserver/#state-transitions", 
            "text": "The goal status will be set ACTIVE when it was possible to launch a roslaunch process (and will be REJECTED if this was not possible). If the roslaunch process terminates with an error (e.g. when the given roslaunch file doesn't exist), the goal is set to ABORTED. If the roslaunch process terminates successfully, then the goal is set to SUCCESS (usually, we will never see this, as the roslaunch process will likely be running until we actively preempt it).", 
            "title": "state transitions"
        }, 
        {
            "location": "/strands_apps/roslaunch_axserver/#using-in-python", 
            "text": "In order to use this in your code, include something like the following in your python code:  import rospy\nimport roslaunch_axserver.msg\nimport actionlib\n\n...\n\n    client = actionlib.SimpleActionClient('launchServer', roslaunch_axserver.msg.launchAction)\n\n    # Waits until the action server has started up and started\n    # listening for goals.\n    client.wait_for_server()\n\n    # Creates a goal to send to the action server.\n    goal = roslaunch_axserver.msg.launchGoal(pkg='pkg', launch_file='filename', monitored_topics=[])\n\n    # Sends the goal to the action server.\n    client.send_goal(goal)  The server is reentrant, meaning that you can run several roslaunch processes in parallel ( not  a  SimpleActionServer )", 
            "title": "using in Python"
        }, 
        {
            "location": "/strands_apps/static_transform_manager/", 
            "text": "static_transform_manager\n\n\nThis provides a transformation manager node that broadcasts static transformations at 30Hz, in the same way that static transformations can\nbe created using static_tf_broadcaster. Using this node static transformations can be easier setup and removed than managing a static_tf_broadcasters as sub processes.\n\n\nUsage\n\n\nrosrun static_transform_manager static_tf_services.py\n\n\n\n\nwill start the manager node. This provides two services:\n\n\n\n\n/static_transforms_manager/set_tf\n\n\n/static_transforms_manager/stop_tf\n\n\n\n\nset_tf\n takes a single argument \ngeometry_msgs/TransformStamped transform\n,\nand returns a debug string \nresponse\n and a bool \nsuccess\n. The supplied transform is broadcast at 30Hz.\n\n\nstop_tf\n takes a single argument \nstring child_frame_id\n which is the child frame to stop broadcasting.\n\n\nExample\n\n\nrosrun static_transform_manager rviz_click_to_tf.py\n will listen to points clicked in rviz and turn them into a transformation in the TF tree.", 
            "title": "Static transform manager"
        }, 
        {
            "location": "/strands_apps/static_transform_manager/#static_transform_manager", 
            "text": "This provides a transformation manager node that broadcasts static transformations at 30Hz, in the same way that static transformations can\nbe created using static_tf_broadcaster. Using this node static transformations can be easier setup and removed than managing a static_tf_broadcasters as sub processes.", 
            "title": "static_transform_manager"
        }, 
        {
            "location": "/strands_apps/static_transform_manager/#usage", 
            "text": "rosrun static_transform_manager static_tf_services.py  will start the manager node. This provides two services:   /static_transforms_manager/set_tf  /static_transforms_manager/stop_tf   set_tf  takes a single argument  geometry_msgs/TransformStamped transform ,\nand returns a debug string  response  and a bool  success . The supplied transform is broadcast at 30Hz.  stop_tf  takes a single argument  string child_frame_id  which is the child frame to stop broadcasting.", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_apps/static_transform_manager/#example", 
            "text": "rosrun static_transform_manager rviz_click_to_tf.py  will listen to points clicked in rviz and turn them into a transformation in the TF tree.", 
            "title": "Example"
        }, 
        {
            "location": "/strands_apps/topological_roslaunch/", 
            "text": "A node which makes brings up and tears down roslaunch files depend on the topological location of the robot. The STRANDS use case is to stop certain subsystems when the robot is charging.", 
            "title": "Topological roslaunch"
        }, 
        {
            "location": "/strands_apps/watchdog_node/", 
            "text": "watchdog_node\n\n\nThis package provides a 'watchdog' node that monitors for specified conditions,\nand takes a specified action if any condition becomes true. \n\n\nUsage\n\n\nrosrun watchdog_node watchdog_node _config:=/path/to/config.yaml\n\n\nwhere \nconfig.yaml\n is the configuration specifying a list of watchdogs in \nterms of 'monitors' and 'actions':\n\n\n- name: check_battery\n  description: Monitors the battery level.\n  restart_timeout: 30\n  monitors:\n      - monitor_type: TopicFieldCondition\n        topic: /battery_state/lifePercent\n        condition: \n{value} \n 97\n\n  actions:\n      - action_type: SendEmail\n        to_addresses:  [cburbridge@gmail.com]\n        from_address: cburbridge@gmail.com\n        message: \nWarning: robot battery is critically low.\n\n        server: mail-relay.cs.bham.ac.uk\n        port: 25\n- name: check_scitos_node\n  description: Checks the /odom topic, kills scitos node if not published for 20s.\n  restart_timeout: 30\n  monitors:\n      - monitor_type: TopicAlive\n        topic: /odom\n        max_duration: 20\n  actions:\n      - action_type: KillNode\n        node_name: /scitos_mira\n\n\n\n\nEach watchdog must have the following fields:\n\n\n\n\nname\n: any name chosen by the user\n\n\ndescription\n: some hopefully useful description string\n\n\nrestart_timeout\n: how many seconds to wait after this watchdog has fired\nbefore restarting the watchdog. If value is -1 then the watchdog will stop running after\nit has been fired.\n\n\nmonitors\n: a list of monitors that make up the watchdog. If any one of the \nmonitors fires then the watchdog will fire and stop, restarting after \nrestart_timeout\n.\n\n\nactions\n: a list of actions to take when this watchdog fires.\n\n\n\n\nMonitors\n\n\nThe following monitors are available:\n\n\n- type:  TopicPublished\n  description:  This monitor triggers the actions if a message is published on a given topic\n  configuration fields:\n    -  topic  :  The topic to listen to\n\n- type:  TopicAlive\n  description:  This monitor triggers the actions if there are no messages on the given topic for a given period\n  configuration fields:\n    -  topic  :  The topic to monitor\n    -  max_duration  :  The maximum number of seconds to accept not receiving a message\n\n- type:  TopicFieldCondition\n  description:  This monitor checks a field in a given topic and triggers the actions when a condition is met.\n  configuration fields:\n    -  topic  :  The topic \n field to watch, eg /topic/field\n    -  condition  :  A python expression containing {value} as a place holder for the value of the topic field. For example '{value} \n 30'\n\n\n\n\n\nActions\n\n\nThe following actions are available:\n\n\n- type:  KillNode\n  description:  Sends a specified node a kill signal.\n  configuration fields:\n    -  node_name  :  The rosgraph name of the node to kill.\n\n- type:  SendEmail\n  description:  Sends an email using simple unsecure SMTP.\n  configuration fields:\n    -  to_addresses  :  A list of email addresses to send to.\n    -  from_address  :  The email address to be sent from.\n    -  message  :  The message body of the email.\n    -  server  :  The SMTP server.\n    -  port  :  The port the SMTP server uses.\n\n\n\n\nDeveloping new actions/monitors\n\n\nMonitors\n\n\nMonitors are classes that derive from the base classes \n\nwatchdog_node.MonitorType\n. They must provide \nname\n, \ndescription\n and \nconfig_keys\n\nfields at the class level and implement a \nstart()\n and \nstop()\n method. \nstart()\n \nwill be called when the monitor is started or restarted following the firing of \nthe watchdog. \nstop()\n will be called when the watchdog fires or is shutdown.\n\n\nIn order to signal that the watchdog should fire, the class should call 'self.set_invalid()`.\n\n\nTopicPublished provides a straightforward example.\n\n\nNote that all modules defining monitors need to be imported in \nmonitors/__init__.py\n\n\nActions\n\n\nActions are classes that derive from the base class \nwatchdog_node.ActionType\n. Similar to \nmonitors they must provide \nname\n, \ndescription\n and \nconfig_keys\n\nfields at the class level. They must also provide a \nexecute(self)\n method that\nwill be called when the watchdog fires.\n\n\nKillNode\n provides a straightforward example.\n\n\nNote that all modules defining actions need to be imported in \nactions/__init__.py", 
            "title": "Watchdog node"
        }, 
        {
            "location": "/strands_apps/watchdog_node/#watchdog_node", 
            "text": "This package provides a 'watchdog' node that monitors for specified conditions,\nand takes a specified action if any condition becomes true.", 
            "title": "watchdog_node"
        }, 
        {
            "location": "/strands_apps/watchdog_node/#usage", 
            "text": "rosrun watchdog_node watchdog_node _config:=/path/to/config.yaml  where  config.yaml  is the configuration specifying a list of watchdogs in \nterms of 'monitors' and 'actions':  - name: check_battery\n  description: Monitors the battery level.\n  restart_timeout: 30\n  monitors:\n      - monitor_type: TopicFieldCondition\n        topic: /battery_state/lifePercent\n        condition:  {value}   97 \n  actions:\n      - action_type: SendEmail\n        to_addresses:  [cburbridge@gmail.com]\n        from_address: cburbridge@gmail.com\n        message:  Warning: robot battery is critically low. \n        server: mail-relay.cs.bham.ac.uk\n        port: 25\n- name: check_scitos_node\n  description: Checks the /odom topic, kills scitos node if not published for 20s.\n  restart_timeout: 30\n  monitors:\n      - monitor_type: TopicAlive\n        topic: /odom\n        max_duration: 20\n  actions:\n      - action_type: KillNode\n        node_name: /scitos_mira  Each watchdog must have the following fields:   name : any name chosen by the user  description : some hopefully useful description string  restart_timeout : how many seconds to wait after this watchdog has fired\nbefore restarting the watchdog. If value is -1 then the watchdog will stop running after\nit has been fired.  monitors : a list of monitors that make up the watchdog. If any one of the \nmonitors fires then the watchdog will fire and stop, restarting after  restart_timeout .  actions : a list of actions to take when this watchdog fires.", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_apps/watchdog_node/#monitors", 
            "text": "The following monitors are available:  - type:  TopicPublished\n  description:  This monitor triggers the actions if a message is published on a given topic\n  configuration fields:\n    -  topic  :  The topic to listen to\n\n- type:  TopicAlive\n  description:  This monitor triggers the actions if there are no messages on the given topic for a given period\n  configuration fields:\n    -  topic  :  The topic to monitor\n    -  max_duration  :  The maximum number of seconds to accept not receiving a message\n\n- type:  TopicFieldCondition\n  description:  This monitor checks a field in a given topic and triggers the actions when a condition is met.\n  configuration fields:\n    -  topic  :  The topic   field to watch, eg /topic/field\n    -  condition  :  A python expression containing {value} as a place holder for the value of the topic field. For example '{value}   30'", 
            "title": "Monitors"
        }, 
        {
            "location": "/strands_apps/watchdog_node/#actions", 
            "text": "The following actions are available:  - type:  KillNode\n  description:  Sends a specified node a kill signal.\n  configuration fields:\n    -  node_name  :  The rosgraph name of the node to kill.\n\n- type:  SendEmail\n  description:  Sends an email using simple unsecure SMTP.\n  configuration fields:\n    -  to_addresses  :  A list of email addresses to send to.\n    -  from_address  :  The email address to be sent from.\n    -  message  :  The message body of the email.\n    -  server  :  The SMTP server.\n    -  port  :  The port the SMTP server uses.", 
            "title": "Actions"
        }, 
        {
            "location": "/strands_apps/watchdog_node/#developing-new-actionsmonitors", 
            "text": "", 
            "title": "Developing new actions/monitors"
        }, 
        {
            "location": "/strands_apps/watchdog_node/#monitors_1", 
            "text": "Monitors are classes that derive from the base classes  watchdog_node.MonitorType . They must provide  name ,  description  and  config_keys \nfields at the class level and implement a  start()  and  stop()  method.  start()  \nwill be called when the monitor is started or restarted following the firing of \nthe watchdog.  stop()  will be called when the watchdog fires or is shutdown.  In order to signal that the watchdog should fire, the class should call 'self.set_invalid()`.  TopicPublished provides a straightforward example.  Note that all modules defining monitors need to be imported in  monitors/__init__.py", 
            "title": "Monitors"
        }, 
        {
            "location": "/strands_apps/watchdog_node/#actions_1", 
            "text": "Actions are classes that derive from the base class  watchdog_node.ActionType . Similar to \nmonitors they must provide  name ,  description  and  config_keys \nfields at the class level. They must also provide a  execute(self)  method that\nwill be called when the watchdog fires.  KillNode  provides a straightforward example.  Note that all modules defining actions need to be imported in  actions/__init__.py", 
            "title": "Actions"
        }, 
        {
            "location": "/strands_data_to_qsrlib/", 
            "text": "STRANDS data tools that work with QSRlib\n\n\nThis package aims to provide easy parsing of various datasets mainly keeping\nthem in QSRlib format.\n\n\nIt might end up being part of the QSRlib given its functionality and strong\ndependency to it.\n\n\nThere are usually two types of files:\n\n \nreaders\n: responsible for reading from the raw data or from saved files\nand keeping the raw data (and other info as needed) in some QSRlib\nfriendly format.\n\n \nkeepers\n: responsible for taking a reader and making QSRs via QSRlib.\nThey usually have the option to load the QSRs directly from some file.\n\n\nAvailable tools for the following datasets\n\n\n\n\nCAD120 (\ninfo\n)\n\n\n\n\nUsage and Help\n\n\nInstallation\n\n\nYou do need to have\n\nQSRlib\n somewhere installed\nwhere it can be found by the \nreaders\n and the \nkeepers\n. Easiest way is probably to \nmodify your \nPYTHONPATH\n or if you are using an IDE then check its documentation\non how to resolve dependencies.\n\n\nCAD120 reader\n\n\ncad120_data_reader.py\n provides the class \nCAD120_Data_Reader\n. In most cases\nit is enough to just call the constructor without any of the optional arguments,\nand if you have a suitable \nconfig.ini\n then things should go smoothly.\n\n\nAbout the \nconfig.ini\n\n\nCreate a \nconfig.ini\n base on the following template that tells where the CAD120\nfolder is. If the notion of corrected \nlabeling.txt\n files makes no sense then\njust use the same path for both \npath1\n and \npath2\n.\n\n\n[cad120_data_reader]\npath = \npath1\n/CAD_120\ncorrected_labeling_path = \npath2\n/CAD_120\n; use load_from_files=True in the constructor to load from the following files\nsub_sequences_filename = \npickle_file.p\n\nsub_time_segmentation_filename = \npickle_file.p\n\nground_truth_tracks_filename = \npickle_file.p\n\n\n\n\n\nJust make sure that your program can find your \nconfig.ini\n. If you are not \nfamiliar how to do this then an easy way is to pass the directory of\n\nconfig.ini\n in the constructor, e.g.:\n\n\nreader = CAD120_Data_Reader(config_path=\npath string to config.ini\n)\n\n\n\n\nCAD120 keeper\n\n\ncad120_qsr_keeper.py\n provides the class \nCAD120_QSR_Keeper\n. If you want to make\nQSRs from the reader then you need to pass some parameters. See the main part for\nan example and you will need a \nlocal.ini\n file. If you want to load the QSRs from\na file simply call with argument `-l \n.\n\n\nRunning \ncad120_qsr_keeper.py -h\n will give sufficient help also.\n\n\nAbout the \nlocal.ini\n\n\n[local]\nreader_ini = \nreader config.ini file\n\nreader_load = true\n\n\n\n\nreader_load\n true if you want the \nreader\n to load the data from the files in\nthe \nconfig.ini", 
            "title": "Home"
        }, 
        {
            "location": "/strands_data_to_qsrlib/#strands-data-tools-that-work-with-qsrlib", 
            "text": "This package aims to provide easy parsing of various datasets mainly keeping\nthem in QSRlib format.  It might end up being part of the QSRlib given its functionality and strong\ndependency to it.  There are usually two types of files:   readers : responsible for reading from the raw data or from saved files\nand keeping the raw data (and other info as needed) in some QSRlib\nfriendly format.   keepers : responsible for taking a reader and making QSRs via QSRlib.\nThey usually have the option to load the QSRs directly from some file.", 
            "title": "STRANDS data tools that work with QSRlib"
        }, 
        {
            "location": "/strands_data_to_qsrlib/#available-tools-for-the-following-datasets", 
            "text": "CAD120 ( info )", 
            "title": "Available tools for the following datasets"
        }, 
        {
            "location": "/strands_data_to_qsrlib/#usage-and-help", 
            "text": "", 
            "title": "Usage and Help"
        }, 
        {
            "location": "/strands_data_to_qsrlib/#installation", 
            "text": "You do need to have QSRlib  somewhere installed\nwhere it can be found by the  readers  and the  keepers . Easiest way is probably to \nmodify your  PYTHONPATH  or if you are using an IDE then check its documentation\non how to resolve dependencies.", 
            "title": "Installation"
        }, 
        {
            "location": "/strands_data_to_qsrlib/#cad120-reader", 
            "text": "cad120_data_reader.py  provides the class  CAD120_Data_Reader . In most cases\nit is enough to just call the constructor without any of the optional arguments,\nand if you have a suitable  config.ini  then things should go smoothly.", 
            "title": "CAD120 reader"
        }, 
        {
            "location": "/strands_data_to_qsrlib/#about-the-configini", 
            "text": "Create a  config.ini  base on the following template that tells where the CAD120\nfolder is. If the notion of corrected  labeling.txt  files makes no sense then\njust use the same path for both  path1  and  path2 .  [cad120_data_reader]\npath =  path1 /CAD_120\ncorrected_labeling_path =  path2 /CAD_120\n; use load_from_files=True in the constructor to load from the following files\nsub_sequences_filename =  pickle_file.p \nsub_time_segmentation_filename =  pickle_file.p \nground_truth_tracks_filename =  pickle_file.p   Just make sure that your program can find your  config.ini . If you are not \nfamiliar how to do this then an easy way is to pass the directory of config.ini  in the constructor, e.g.:  reader = CAD120_Data_Reader(config_path= path string to config.ini )", 
            "title": "About the config.ini"
        }, 
        {
            "location": "/strands_data_to_qsrlib/#cad120-keeper", 
            "text": "cad120_qsr_keeper.py  provides the class  CAD120_QSR_Keeper . If you want to make\nQSRs from the reader then you need to pass some parameters. See the main part for\nan example and you will need a  local.ini  file. If you want to load the QSRs from\na file simply call with argument `-l  .  Running  cad120_qsr_keeper.py -h  will give sufficient help also.", 
            "title": "CAD120 keeper"
        }, 
        {
            "location": "/strands_data_to_qsrlib/#about-the-localini", 
            "text": "[local]\nreader_ini =  reader config.ini file \nreader_load = true  reader_load  true if you want the  reader  to load the data from the files in\nthe  config.ini", 
            "title": "About the local.ini"
        }, 
        {
            "location": "/strands_data_to_qsrlib/novelTrajectories/", 
            "text": "STRANDS data tools that work with QSRlib\n\n\nTrajectory reader\n\n\ntraj_data_reader.py\n provides the class \nTrajectory_Data_Reader\n. \n\nconfig.ini\n needs to include qsr options. It can optionally contain Activity Graph options.\n\n\nAbout the \nconfig.ini\n\n\nCreate a \nconfig.ini\n based on the following template:\n\n\n[trajectory_data_reader]\npath = \npath1\n\n; use load_from_files=True in the constructor to load from the following files\ndate = date\nqsr = qtcb\nq = 0_01\nv = False\nn = True\n\n\n\n\nYou can add any parameters you like in the config file, but also initiate them in the data_reader config \n???\n.", 
            "title": "novelTrajectories"
        }, 
        {
            "location": "/strands_data_to_qsrlib/novelTrajectories/#strands-data-tools-that-work-with-qsrlib", 
            "text": "", 
            "title": "STRANDS data tools that work with QSRlib"
        }, 
        {
            "location": "/strands_data_to_qsrlib/novelTrajectories/#trajectory-reader", 
            "text": "traj_data_reader.py  provides the class  Trajectory_Data_Reader .  config.ini  needs to include qsr options. It can optionally contain Activity Graph options.", 
            "title": "Trajectory reader"
        }, 
        {
            "location": "/strands_data_to_qsrlib/novelTrajectories/#about-the-configini", 
            "text": "Create a  config.ini  based on the following template:  [trajectory_data_reader]\npath =  path1 \n; use load_from_files=True in the constructor to load from the following files\ndate = date\nqsr = qtcb\nq = 0_01\nv = False\nn = True  You can add any parameters you like in the config file, but also initiate them in the data_reader config  ??? .", 
            "title": "About the config.ini"
        }, 
        {
            "location": "/strands_executive/doc/", 
            "text": "The STRANDS Executive Framework -- Tutorial\n\n\nThis tutorial was originally written for the STRANDS Project \nLong-term Autonomy for Mobile Robots Summer School 2015\n.\n\n\nPreliminaries\n\n\nIn addition to assuming \na working knowledge of ROS\n, this tutorial also assumes you are comfortable working with:\n\n\n\n\nactionlib\n -- if not there are \ntutorials here\n\n\nmongodb_store\n -- in particular the \nmessage store\n which provides persistent storage of ROS messages.\n\n\n\n\nThis tutorial also assumes you have the \nstrands_executive\n software installed. You should have done this when preparing for the summer school. If not, following the \ninstallation instructions\n.\n\n\nTasks\n\n\nThis executive framework, schedules and manages the execution of \ntasks\n. An instance of a task object describes the desired exection of an \nactionlib action server\n, usually within a \ntime window\n and at a given robot location. In the first part of this tutorial you will create instances of tasks and manage their execution via the executive framework.\n\n\nNote\n, whilst any actionlib server can be used with the execution framework, it is important that the server correctly responds to preemption requests.\n\n\nThe Wait Action\n\n\nTo save the work of implementing an action server, we will work with one which is provided by the execution framework: the wait action. This action server simply either waits until a particular time or for a given duration before returning. The definition of action is as follows:\n\n\n# the time to wait until\ntime wait_until\n# if wait until is 0 then try waiting for this long, if this is 0 too, wait until explicitly woken or cancelled\nduration wait_duration\n---\n---\n# feedback message\nfloat32 percent_complete\n\n\n\n\nThe server has three behaviours. If \nwait_until\n is specified, then the server will wait until this time until returning. If \nwait_until\n is not specified (i.e. it is a default \ntime\n instance, one with 0 in both fields) then \nwait_duration\n is used to determine how long the server should wait before returning. If this is also unspecified (i.e. it is a default \nduration\n instance, one with 0 in both fields) then the server simply waits until a the \nEmpty\n service \n/wait_action/end_wait\n is called and then returns. \n\n\nTo see this in action, start the wait server as follows (assuming \nroscore\n is already running):\n\n\nrosrun wait_action wait_node.py\n\n\n\n\nYou can then use the actionlib GUI client to explore different values:\n\n\nrosrun actionlib axclient.py /wait_node\n\n\n\n\nThe \nwait_duration\n argument is self explanatory. The \nwait_until\n argument is seconds since epoch in UTC (which is one hour behind BST if you're doing this at the summer school). \n\n\nTask Specification\n\n\nNow we have an action server we'd like to execute, we can write a task to have its execution managed by the executive framework. To create a task, first create an instance of the \nTask\n message type. Examples are given in Python, as the helper functions currently only exist for Python, but C++ is also possible (and C++ helpers will be added if someone asks for them in the future).\n\n\n#!/usr/bin/env python\n\nimport rospy\n\nfrom strands_executive_msgs.msg import Task\ntask = Task()\n\n\n\n\nTo specify that this task should trigger the wait action, set the action field to the topic prefix of the action server (i.e. the string you use in the action server constructor, or the one you used as the last argument to axclient previously):\n\n\ntask.action = '/wait_action'\n\n\n\n\nYou must also set the maximum length of time you expect your task to execute for. This is used by the execution framework to determine whether your task is executing correctly, and by the scheduler to work out execution times. The duration is a \nrospy.Duration\n instance and is defined in seconds. The \nmax_duration\n field is not connected with any argument to the action server itself. \n\n\nmax_wait_minutes = 60 * 60\ntask.max_duration = rospy.Duration(max_wait_minutes)\n\n\n\n\nAs our action server requires arguments (i.e. wait_until or wait_duration), we must add these to the task too. Arguments must be added \nin the order they are defined in your action message\n. Arguments are added to the task using the helper functions from \nstrands_executive_msgs.task_utils\n. For our wait_action, this means we must add a value for wait_until then a value for wait_duration (as this is the order defined in the action definition included above). The following code specifies an action that waits for 10 seconds. \n\n\nfrom strands_executive_msgs import task_utils\n\ntask_utils.add_time_argument(task, rospy.Time())\ntask_utils.add_duration_argument(task, rospy.Duration(10))\n\n\n\n\nTasks can be assigned any argument type that is required by the action server. This is done via the mongodb_store and is explained in more detail \nhere\n.\n\n\nTask Execution\n\n\nWe have now specified enough information to allow the task to be executed. For this to happen we must do three things. First, we must start up the execution system. The \nstrands_executive_tutorial\n package contains a launch file which runs everything you need to get started: \nmongodb_store\n; a \ndummy topological navigation system\n (we will replace this with the robot's navigation system later); and the \nexecutive framework\n itself. Run this in a separate terminal as follows:\n\n\nroslaunch strands_executive_tutorial tutorial_dependencies.launch\n\n\n\n\nSecond, we must tell the execution system that it can start executing task. This is done using the \nSetExecutionStatus\n service which can be used to pause and resume execution at runtime. The following provides functions which return the correct ROS service, after waiting for it to exist.\n\n\nfrom strands_executive_msgs.srv import AddTasks, DemandTask, SetExecutionStatus\n\ndef get_service(service_name, service_type):    \n    rospy.loginfo('Waiting for %s service...' % service_name)\n    rospy.wait_for_service(service_name)\n    rospy.loginfo(\nDone\n)        \n    return rospy.ServiceProxy(service_name, service_type)\n\ndef get_execution_status_service():\n    return get_service('/task_executor/set_execution_status', SetExecutionStatus)\n\n\n\n\nThe resulting service can then be used to enable execution:\n\n\nset_execution_status = get_execution_status_service()\nset_execution_status(True)\n\n\n\n\nNote\n, that this only needs to be done once after the execution system has been started. If you don't set it to \nTrue\n then tasks can be added but nothing will happen. Once it has been set to \nTrue\n then it won't be set to \nFalse\n unless the service is called again (which will pause execution), or the task executor is shut down and started again.\n\n\nFinally, we can request the \nimmediate\n execution of the task using the \nDemandTask\n service. This service interrupts any task that is currently executing (provided the associated action server supports pre-emption) and starts the execution of the specified one.  The following code demonstrates this.\n\n\ndef get_demand_task_service():\n    return get_service('/task_executor/demand_task', DemandTask)\n\ndemand_task = get_demand_task_service()\ndemand_task(task) \n\n\n\n\nIf you run all of the code provided so far (structured in an appropriate way), you should see some output like the following from the launch file you previously started:\n\n\n[INFO] [WallTime: 1439285826.115682] State machine starting in initial state 'TASK_INITIALISATION' with userdata: \n    ['task']\n[INFO] [WallTime: 1439285826.118216] State machine transitioning 'TASK_INITIALISATION':'succeeded'--\n'TASK_EXECUTION'\n[INFO] [WallTime: 1439285826.118692] Concurrence starting with userdata: \n    ['task']\n[INFO] [WallTime: 1439285826.118842] Action /wait_action started for task 1\n[WARN] [WallTime: 1439285826.121560] Still waiting for action server '/wait_action' to start... is it running?\n[INFO] [WallTime: 1439285826.155626] Connected to action server '/wait_action'.\n[INFO] [WallTime: 1439285826.192815] target wait time: 2015-08-11 10:37:16\n[INFO] [WallTime: 1439285836.196404] waited until: 2015-08-11 10:37:16\n[INFO] [WallTime: 1439285836.207261] Concurrent state 'MONITORED' returned outcome 'succeeded' on termination.\n[INFO] [WallTime: 1439285837.166333] Concurrent state 'MONITORING' returned outcome 'preempted' on termination.\n[INFO] [WallTime: 1439285837.181308] Concurrent Outcomes: {'MONITORED': 'succeeded', 'MONITORING': 'preempted'}\n[INFO] [WallTime: 1439285837.181774] Action terminated with outcome succeeded\n[INFO] [WallTime: 1439285837.186778] State machine transitioning 'TASK_EXECUTION':'succeeded'--\n'TASK_SUCCEEDED'\n[INFO] [WallTime: 1439285837.190386] Execution of task 1 succeeded\n[INFO] [WallTime: 1439285837.190591] State machine terminating 'TASK_SUCCEEDED':'succeeded':'succeeded'\n\n\n\n\nMost of this is output from the task execution node as it steps through the \nfinite state machine\n used for task execution. The following lines show the task is working as we want:\n\n\n[INFO] [WallTime: 1439285826.192815] target wait time: 2015-08-11 10:37:16\n[INFO] [WallTime: 1439285836.196404] waited until: 2015-08-11 10:37:16\n\n\n\n\nTask Scheduling\n\n\nWe can now extend our task specification to include information about \nwhen\n the task should be executed. This is an essential part of our long-term autonomy approach, which requires behaviours to happen at times specified either by a user, or the robot's on subsystems. Each task should be given a time window during which task execution should occur. The task scheduler which is part of the executive framework sequences all the tasks it is managing to ensure that the time window of every task is respected. If this is not possible then the most recently added tasks are dropped (unless you are using priorities, in which case lower priority tasks are dropped until a schedule can be found).\n\n\nThe opening time of the task's execution window is specified using the \nstart_after\n field. The code below sets the start window to ten seconds into the future.\n\n\ntask.start_after = rospy.get_rostime() + rospy.Duration(10)\n\n\n\n\nThe execution window should provide enough time to execute the task, but should also offer some slack as other tasks added to the system may have to executed first. The closing time of the the execution window is specified using the \nend_before\n field. The code below sets the end of the window such that the window is three times the maximum duration of the task. \n\n\ntask.end_before = task.start_after + rospy.Duration(task.max_duration.to_sec() * 3)\n\n\n\n\nPreviously we \ndemanded\n task execution using the \nDemandTask\n service. This ignores the time window of the task and executes it immediately. To respect the time window we must use the service \n/task_executor/add_tasks\n of type \nAddTasks\n. \n\n\ndef get_add_tasks_service():\n    return get_service('/task_executor/add_tasks', AddTasks)\n\n\n\n\nThis adds a list of tasks to the executive framework, which in turn triggers scheduling and updates the robot's execution schedule (assuming execution status has already been set to \nTrue\n). \n\n\nadd_tasks = get_add_tasks_service()\nadd_tasks([task])\n\n\n\n\nWhen this script is executed, rather than the immediate execution of your task as previously, you should see the executor delay for some time (not quite ten seconds as there is an assumption of a minimum travel time before task execution) before executing the wait action. The output should be similar to that shown below, which shows the scheduler creating a schedule (which only contains the wait task) then the executor delaying execution until the star window of the task is open.\n\n\n[ INFO] [1439548004.349319000]: SCHEDULER: Going to solve\n[ INFO] [1439548004.397583000]: SCHEDULER: found a solution\n[INFO] [WallTime: 1439548004.398210] start window: 11:26:53.412277\n[INFO] [WallTime: 1439548004.398465]          now: 11:26:44.398102\n[INFO] [WallTime: 1439548004.398639]  travel time:  0:00:02\n[INFO] [WallTime: 1439548004.398825] need to delay 7.14174938 for execution\n[INFO] [WallTime: 1439548004.399264] Added 1 tasks into the schedule to get total of 1\n\n\n\n\nExecution Information\n\n\nYou can use the following sources of information to inspect the current state of the executive framework.\n\n\nThe current execution status can be obtained using the service \nGetExecutionStatus\n on \n/task_executor/get_execution_status\n. A return value of \ntrue\n means the execution system is running, whereas \nfalse\n means that the execution system has either not been started or it has been paused.\n\n\nTo see the execution schedule, subscribe to the topic \n/current_schedule\n which gets the list of tasks in execution order. If \ncurrently_executing\n is \ntrue\n then this means the first element of \nexecution_queue\n is the currently active task. If it is false then the system is delaying until it starts executing that task.\n\n\nTo just get the currently active task, use the service \nstrands_executive_msgs/GetActiveTask\n on \n/task_executor/get_active_task\n. If the returned task has a \ntask_id\n of \n0\n then there is no active task (as you can't return \nNone\n over a service).\n\n\nTo get print outs in the terminal describing the operation of the executive framework, you can use the following scripts:\n\n\nrosrun task_executor schedule_status.py\n\n\n\n\nschedule_status.py\n prints out the current schedule and execution information. For example, for the above wait action, you might see something like\n\n\n[INFO] [WallTime: 1439549489.280268] Waiting to execute /wait_action (task 3)\n[INFO] [WallTime: 1439549489.280547] Execution to start at 2015-08-14 11:51:39\n[INFO] [WallTime: 1439549489.280785] A further 0 tasks queued for execution\n[INFO] [WallTime: 1439549494.295445] \n\n\n\n\n\nrosrun task_executor task_status.py\n\n\n\n\ntask_status.py\n prints out the events that occur internally in the framework. For one of our wait tasks, you might see the following:\n\n\ntask 3    /wait_action    ADDED   14/08/15 11:51:29\ntask 3    /wait_action    TASK_STARTED    14/08/15 11:51:37\ntask 3    /wait_action    EXECUTION_STARTED   14/08/15 11:51:37\ntask 3    /wait_action    EXECUTION_SUCCEEDED   14/08/15 11:51:47\ntask 3    /wait_action    TASK_SUCCEEDED    14/08/15 11:51:47\n\n\n\n\nYou can also subscribe to these events on the topic \n/task_executor/events\n.\n\n\nExercise 1\n\n\na) Given what you know so far, create a script which schedules a configurable number of wait action tasks. All tasks should have the same duration and time window, but you should make sure that the time window is created such that it allows all the tasks to be executed. Use \nschedule_status.py\n and \ntask_status.py\n to monitor the progress of execution and debug your script if necessary.\n\n\nb) Optional. Create an action server which prints out all the details of a task, then uses this in place of the wait action for 1(a). See the \ntask documentation\n for how to add user-defined message types as task arguments.\n\n\nTasks with Locations\n\n\nSo far we have ignored the robot entirely. Now we will extend our task to feature the location of the robot when the task is performed. Each task can optionally have a start location and an end location. Locations are names of nodes in the robot's topological map (which you should have worked with yesterday). If the start location is provided, the task executive drives the robot to this location before starting the execution of the task action. If navigation fails, the task will not be executed. The end location describes the location the executive system expects the robot to be at when the task execution is completed. If left blank, this is assumed to be the same as the start location. The start and end locations are used by the scheduler to determine what order to execute the tasks.\n\n\nFor this part of the tutorial, we will work with the UOL MHT simulation and the associated topological map. From here on the examples assume you are running the correct mongodb store, simulation and navigation launch files for this simulated setup.\n\n\nSince you now have a simulated robot running (see note above), you now \nshould not\n use the \ntutorial_dependencies\n launch file. Instead, launch the executive framework as follows:\n\n\nroslaunch task_executor task-scheduler.launch scheduler_version:=1\n\n\n\n\n\nThe \nscheduler_version:=1\n parameter ensures the optimal version of the scheduler is used (the original Coltin et al. algorithm). This version produces optimal schedules (including the travel time between locations), although it starts to take a long time to produce schedules once you go above 20 or so tasks in a single scheduling problem. You can omit this parameter to run the non-optimal version which uses preprocessing to produce non-optimal schedules quickly.\n\n\nTo configure a task's location, set the following fields in the task object:\n\n\ntask.start_node_id = `Waypoint3`\ntask.end_node_id = `Waypoint3`\n\n\n\n\nMake sure you add the above lines to your existing code \nbefore\n the task is send to the scheduler. You can then use the \nadd_tasks\n service as before, and you should see the robot drive to \nWaypoint3\n before executing a wait action (assuming you're extending the code from above).\n\n\nNote\n that now the robot must travel between locations you need to be make sure your time windows for execution are updated appropriately. The time window specifies the time in which the action is executed, not the navigation time. However, whereas you could previously easily fit two 10 second wait tasks in a 30 second time window (if you gave them both the same window), if it takes 120 seconds to travel between the two locations then the scheduler will not be able to include them both in the same schedule as the travel time between them will place one task outside its time window. \n\n\nExercise 2\n\n\na) Extend your previous multi-task code to include locations in the tasks, with the robot travelling to at least three different locations.\n\n\nChanges to Execution\n\n\nYou now know how to have a list of tasks scheduled and executed. If a task is \ndemanded\n using the \nDemandTask\n service (as in the first part of this tutorial), the currently executing task is interrupted and the demanded one is executed in its place. In parallel, a new schedule is created which contains the remaining scheduled tasks and the interrupted one if possible (i.e. if the time windows can be respected). In all cases, if all tasks cannot be scheduled some are dropped until a valid schedule can be produced. \n\n\nYou can also pause the execution of the current task by sending a value of \nfalse\n to the \nSetExecutionStatus\n service. This will pause the task execution in the current state it is in, either navigation or action execution, until a value of \ntrue\n is sent to the service.\n\n\nIf you have written a task which should not be interrupted by these methods, you can create a service which informs the executive whether or not interruption is possible. The instructions for this are \nhere\n.\n\n\nExercise 3\n\n\na) Use your previous code to start the simulated robot executing scheduled tasks. Once it is executing, experiment with setting the execution status and demand tasks. Monitor the effects of these interactions using \n\n\nrosrun task_executor schedule_status.py\n\n\n\n\nYou can alter the execution status from the command line using \n\n\nrosservice call /task_executor/set_execution_status \nstatus: true\n\n\n\n\n\nRoutines\n\n\nFor long-term operation, we often want the robot to execute tasks according to some \nroutine\n, e.g. visit all the rooms in the building between 9am and 11am. The STRANDS executive framework supports the creations of such routines through two mechanisms. The \nDailyRoutine\n and \nDailyRoutineRunner\n classes (described \nhere\n) support the creation of routines of daily repeating tasks. The \nRobotRoutine\n class (described \nhere\n) builds on these objects to provide managed routine behaviour for a robot. This class also manages the battery level of the robot, provides a hook for when the robot is idle (i.e. it has no tasks to execute in the near future), and also manages the difference between day and night for the robot (at night the robot docks on a charger and can then only execute tasks without movement). \n\n\nThe \nPatrolRoutine\n class provides an example subclass of \nRobotRoutine\n which simply generates a patrol behaviour which creates tasks to wait at every node in the topological map. You can see this in operation by running:\n\n\nrosrun routine_behaviours patroller_routine_node.py\n\n\n\n\nWith this node running you should see that the robot creates a schedule to visit all the nodes, or it fails to create on (if the time windows are not satisfiable) and instead visits a random node when idle.\n\n\nBefore going further, we can use the code from this node to understand the basic properties of a routine.\n\n\nimport rospy\n\nfrom datetime import time\nfrom dateutil.tz import tzlocal\n\nfrom routine_behaviours.patrol_routine import PatrolRoutine\n\n\nif __name__ == '__main__':\n    rospy.init_node(\npatroller_routine\n)\n\n    # start and end times -- all times should be in local timezone\n    localtz = tzlocal()\n    start = time(8,00, tzinfo=localtz)\n    end = time(20,00, tzinfo=localtz)\n\n    idle_duration=rospy.Duration(20)\n\n    routine = PatrolRoutine(daily_start=start, daily_end=end, idle_duration=idle_duration)    \n\n    routine.create_routine()\n\n    routine.start_routine()\n\n    rospy.spin()\n\n\n\n\n\nLooking at the code in more detail:\n\n\n    start = time(8,00, tzinfo=localtz)\n    end = time(20,00, tzinfo=localtz)\n\n\n\n\nThe above code defines the working day of the robot. No task execution will happen before \nstart\n, and when \nend\n is reached, all execution will cease and the robot will dock for the night.\n\n\n    idle_duration=rospy.Duration(20)\n\n\n\n\nThe above code is used to configure how long the robot should not be executing a task before it considers itself idle.\n\n\n    routine = PatrolRoutine(daily_start=start, daily_end=end, idle_duration=idle_duration)    \n\n\n\n\nThis creates the routine object, passing the defined values.\n\n\n\n   routine.create_routine()\n\n   routine.start_routine()\n\n\n\n\ncreate_routine()\n runs code which specifies the tasks and populates the routine within the object. You can see the code for this \nhere\n. Following this, \nstart_routine\n triggers the execution of the populated routine from the previous step.\n\n\nIf you want to create a routine that performs tasks at all, or a selection of, nodes in the map, you can create a subclass of \nPatrolRoutine\n and override methods such as \ncreate_patrol_routine\n and \ncreate_patrol_task\n to create task-specific behaviour. For the final part of this tutorial we will eschew this approach, and instead create a simple routine from \nRobotRoutine\n.  Use the following code as a structure for your routine.\n\n\n\n#!/usr/bin/env python\n\nimport rospy\n\nfrom routine_behaviours.robot_routine import RobotRoutine\n\nfrom datetime import time, date, timedelta\nfrom dateutil.tz import tzlocal\n\nfrom strands_executive_msgs.msg import Task\nfrom strands_executive_msgs import task_utils\n\n\nclass ExampleRoutine(RobotRoutine):\n    \n Creates a routine which simply visits nodes. \n\n\n    def __init__(self, daily_start, daily_end, idle_duration=rospy.Duration(5), charging_point = 'ChargingPoint'):\n        # super(PatrolRoutine, self).__init__(daily_start, daily_end)        \n        RobotRoutine.__init__(self, daily_start, daily_end, idle_duration=idle_duration, charging_point=charging_point)\n\n    def create_routine(self):\n        pass        \n\n    def on_idle(self):\n        \n\n            Called when the routine is idle. Default is to trigger travel to the charging. As idleness is determined by the current schedule, if this call doesn't utlimately cause a task schedule to be generated this will be called repeatedly.\n        \n\n        rospy.loginfo('I am idle')    \n\n\n\nif __name__ == '__main__':\n\n    rospy.init_node('tutorial_4')\n\n    # start and end times -- all times should be in local timezone\n    localtz = tzlocal()\n    start = time(8,00, tzinfo=localtz)\n    end = time(20,00, tzinfo=localtz)\n\n    # how long to stand idle before doing something\n    idle_duration=rospy.Duration(20)\n\n    routine = ExampleRoutine(daily_start=start, daily_end=end, idle_duration=idle_duration)    \n\n    routine.create_routine()\n\n    routine.start_routine()\n\n    rospy.spin()\n\n\n\n\nThis is similar to the \npatroller_routine_node\n code we saw previously, except we now see the (currently empty) implementation of some parts of routine class. If you run this (alongside the executive framework, simulation, navigation etc.) you should see something like the follows:\n\n\n[INFO] [WallTime: 1440522933.015355] Waiting for task_executor service...\n[INFO] [WallTime: 1440522933.018245] Done\n[INFO] [WallTime: 1440522933.023142] Fetching parameters from dynamic_reconfigure\n[INFO] [WallTime: 1440522933.027528] Config set to 30, 10\n[INFO] [WallTime: 1440522933.533364] Current day starts at 2015-08-25 08:00:00+01:00\n[INFO] [WallTime: 1440522933.533773] Current day ends at 2015-08-25 20:00:00+01:00\n[INFO] [WallTime: 1440522933.547498] extra_tasks_for_today\n[INFO] [WallTime: 1440522933.547924] Scheduling 0 tasks now and 0 later\n[INFO] [WallTime: 1440522933.548184] triggering day start cb at 2015-08-25 18:15:33.547396+01:00\n[INFO] [WallTime: 1440522933.548409] Good morning\n[INFO] [WallTime: 1440522933.554520] Scheduling 0 tasks now and 0 later\n[INFO] [WallTime: 1440522958.590649] I am idle\n\n\n\n\nThe \nI am idle\n line should appear after 20 seconds, as configured by \nidle_duration\n. Nothing else should happen, as we have not yet added tasks. \n\n\nTo add tasks, we will fill in the \ncreate_routine\n method. In this method we will use the instance of \nDailyRoutine\n contained within \nRobotRoutine\n. Tasks are added using the \nrepeat_every*\n methods from this object. These take a list of tasks and store them such that they can be correctly instantiated with start and end times every day. When creating tasks for a routine, you should not specify \nstart_after\n or \nend_before\n as these will be determined by the routine itself. Let's assume we have a function called \nwait_task_at_waypoint\n which creates a \nTask\n object to perform a  10 second wait at a given waypoint. We can then get the routine to schedule two such wait tasks to be performed every day.\n\n\n    def create_routine(self):\n        daily_wps = ['WayPoint6', 'WayPoint3']\n        daily_wp_tasks = [wait_task_at_waypoint(wp) for wp in daily_wps]\n        self.routine.repeat_every_day(daily_wp_tasks)\n\n\n\n\nIf you use this code in your previous script, you should see the robot execute the two tasks, then go idle. If you wait long enough (until the start of the next day!) the robot will repeat this execution. Rather that wait that long, let's add some tasks with a shorter repeat rate into \ncreate_routine\n.\n\n\n        minute_wps = ['WayPoint5']\n        minute_wp_tasks = [wait_task_at_waypoint(wp) for wp in minute_wps]\n        self.routine.repeat_every_delta(minute_wp_tasks, delta=timedelta(minutes=5))\n\n\n\n\nThis causes a task to wait at the given waypoint to be scheduled at five minute intervals throughout the day. These tasks will be scheduled alongside the ones from the \ndaily_wp_tasks\n (provided you didn't remove the code). \nrepeat_every_delta\n is the most flexible of the routine creation tools, but you can also use \nother methods from \nDailyRoutine\n.\n\n\nExercise 4\n\n\na) Create a routine that mixes tasks with hourly and daily repeats.\n\n\nb) Add tasks that either happen in the morning or afternoon. You will need to use the \nstart_time\n argument to \nrepeat_every\n. \n\n\nc) Once you have all of this working in simulation, move your code to your group's robot. Create a routine to patrol the nodes in your topological map. Extend this code to create meta-room maps at certain waypoints at regular intervals. You can also try creating your own tasks/action servers, or scheduling the execution of existing ones (speech, PTU movement, topological edge traversal etc.).\n\n\nAdditional Resources\n\n\n\n\n\n\nframework README\n\n\n\n\n\n\ntask_executor documentation\n and \nAPI\n\n\n\n\n\n\nscheduler documentation\n \n\n\n\n\n\n\nroutine_behaviours documentation\n and \nAPI", 
            "title": "Doc"
        }, 
        {
            "location": "/strands_executive/doc/#the-strands-executive-framework-tutorial", 
            "text": "This tutorial was originally written for the STRANDS Project  Long-term Autonomy for Mobile Robots Summer School 2015 .", 
            "title": "The STRANDS Executive Framework -- Tutorial"
        }, 
        {
            "location": "/strands_executive/doc/#preliminaries", 
            "text": "In addition to assuming  a working knowledge of ROS , this tutorial also assumes you are comfortable working with:   actionlib  -- if not there are  tutorials here  mongodb_store  -- in particular the  message store  which provides persistent storage of ROS messages.   This tutorial also assumes you have the  strands_executive  software installed. You should have done this when preparing for the summer school. If not, following the  installation instructions .", 
            "title": "Preliminaries"
        }, 
        {
            "location": "/strands_executive/doc/#tasks", 
            "text": "This executive framework, schedules and manages the execution of  tasks . An instance of a task object describes the desired exection of an  actionlib action server , usually within a  time window  and at a given robot location. In the first part of this tutorial you will create instances of tasks and manage their execution via the executive framework.  Note , whilst any actionlib server can be used with the execution framework, it is important that the server correctly responds to preemption requests.", 
            "title": "Tasks"
        }, 
        {
            "location": "/strands_executive/doc/#the-wait-action", 
            "text": "To save the work of implementing an action server, we will work with one which is provided by the execution framework: the wait action. This action server simply either waits until a particular time or for a given duration before returning. The definition of action is as follows:  # the time to wait until\ntime wait_until\n# if wait until is 0 then try waiting for this long, if this is 0 too, wait until explicitly woken or cancelled\nduration wait_duration\n---\n---\n# feedback message\nfloat32 percent_complete  The server has three behaviours. If  wait_until  is specified, then the server will wait until this time until returning. If  wait_until  is not specified (i.e. it is a default  time  instance, one with 0 in both fields) then  wait_duration  is used to determine how long the server should wait before returning. If this is also unspecified (i.e. it is a default  duration  instance, one with 0 in both fields) then the server simply waits until a the  Empty  service  /wait_action/end_wait  is called and then returns.   To see this in action, start the wait server as follows (assuming  roscore  is already running):  rosrun wait_action wait_node.py  You can then use the actionlib GUI client to explore different values:  rosrun actionlib axclient.py /wait_node  The  wait_duration  argument is self explanatory. The  wait_until  argument is seconds since epoch in UTC (which is one hour behind BST if you're doing this at the summer school).", 
            "title": "The Wait Action"
        }, 
        {
            "location": "/strands_executive/doc/#task-specification", 
            "text": "Now we have an action server we'd like to execute, we can write a task to have its execution managed by the executive framework. To create a task, first create an instance of the  Task  message type. Examples are given in Python, as the helper functions currently only exist for Python, but C++ is also possible (and C++ helpers will be added if someone asks for them in the future).  #!/usr/bin/env python\n\nimport rospy\n\nfrom strands_executive_msgs.msg import Task\ntask = Task()  To specify that this task should trigger the wait action, set the action field to the topic prefix of the action server (i.e. the string you use in the action server constructor, or the one you used as the last argument to axclient previously):  task.action = '/wait_action'  You must also set the maximum length of time you expect your task to execute for. This is used by the execution framework to determine whether your task is executing correctly, and by the scheduler to work out execution times. The duration is a  rospy.Duration  instance and is defined in seconds. The  max_duration  field is not connected with any argument to the action server itself.   max_wait_minutes = 60 * 60\ntask.max_duration = rospy.Duration(max_wait_minutes)  As our action server requires arguments (i.e. wait_until or wait_duration), we must add these to the task too. Arguments must be added  in the order they are defined in your action message . Arguments are added to the task using the helper functions from  strands_executive_msgs.task_utils . For our wait_action, this means we must add a value for wait_until then a value for wait_duration (as this is the order defined in the action definition included above). The following code specifies an action that waits for 10 seconds.   from strands_executive_msgs import task_utils\n\ntask_utils.add_time_argument(task, rospy.Time())\ntask_utils.add_duration_argument(task, rospy.Duration(10))  Tasks can be assigned any argument type that is required by the action server. This is done via the mongodb_store and is explained in more detail  here .", 
            "title": "Task Specification"
        }, 
        {
            "location": "/strands_executive/doc/#task-execution", 
            "text": "We have now specified enough information to allow the task to be executed. For this to happen we must do three things. First, we must start up the execution system. The  strands_executive_tutorial  package contains a launch file which runs everything you need to get started:  mongodb_store ; a  dummy topological navigation system  (we will replace this with the robot's navigation system later); and the  executive framework  itself. Run this in a separate terminal as follows:  roslaunch strands_executive_tutorial tutorial_dependencies.launch  Second, we must tell the execution system that it can start executing task. This is done using the  SetExecutionStatus  service which can be used to pause and resume execution at runtime. The following provides functions which return the correct ROS service, after waiting for it to exist.  from strands_executive_msgs.srv import AddTasks, DemandTask, SetExecutionStatus\n\ndef get_service(service_name, service_type):    \n    rospy.loginfo('Waiting for %s service...' % service_name)\n    rospy.wait_for_service(service_name)\n    rospy.loginfo( Done )        \n    return rospy.ServiceProxy(service_name, service_type)\n\ndef get_execution_status_service():\n    return get_service('/task_executor/set_execution_status', SetExecutionStatus)  The resulting service can then be used to enable execution:  set_execution_status = get_execution_status_service()\nset_execution_status(True)  Note , that this only needs to be done once after the execution system has been started. If you don't set it to  True  then tasks can be added but nothing will happen. Once it has been set to  True  then it won't be set to  False  unless the service is called again (which will pause execution), or the task executor is shut down and started again.  Finally, we can request the  immediate  execution of the task using the  DemandTask  service. This service interrupts any task that is currently executing (provided the associated action server supports pre-emption) and starts the execution of the specified one.  The following code demonstrates this.  def get_demand_task_service():\n    return get_service('/task_executor/demand_task', DemandTask)\n\ndemand_task = get_demand_task_service()\ndemand_task(task)   If you run all of the code provided so far (structured in an appropriate way), you should see some output like the following from the launch file you previously started:  [INFO] [WallTime: 1439285826.115682] State machine starting in initial state 'TASK_INITIALISATION' with userdata: \n    ['task']\n[INFO] [WallTime: 1439285826.118216] State machine transitioning 'TASK_INITIALISATION':'succeeded'-- 'TASK_EXECUTION'\n[INFO] [WallTime: 1439285826.118692] Concurrence starting with userdata: \n    ['task']\n[INFO] [WallTime: 1439285826.118842] Action /wait_action started for task 1\n[WARN] [WallTime: 1439285826.121560] Still waiting for action server '/wait_action' to start... is it running?\n[INFO] [WallTime: 1439285826.155626] Connected to action server '/wait_action'.\n[INFO] [WallTime: 1439285826.192815] target wait time: 2015-08-11 10:37:16\n[INFO] [WallTime: 1439285836.196404] waited until: 2015-08-11 10:37:16\n[INFO] [WallTime: 1439285836.207261] Concurrent state 'MONITORED' returned outcome 'succeeded' on termination.\n[INFO] [WallTime: 1439285837.166333] Concurrent state 'MONITORING' returned outcome 'preempted' on termination.\n[INFO] [WallTime: 1439285837.181308] Concurrent Outcomes: {'MONITORED': 'succeeded', 'MONITORING': 'preempted'}\n[INFO] [WallTime: 1439285837.181774] Action terminated with outcome succeeded\n[INFO] [WallTime: 1439285837.186778] State machine transitioning 'TASK_EXECUTION':'succeeded'-- 'TASK_SUCCEEDED'\n[INFO] [WallTime: 1439285837.190386] Execution of task 1 succeeded\n[INFO] [WallTime: 1439285837.190591] State machine terminating 'TASK_SUCCEEDED':'succeeded':'succeeded'  Most of this is output from the task execution node as it steps through the  finite state machine  used for task execution. The following lines show the task is working as we want:  [INFO] [WallTime: 1439285826.192815] target wait time: 2015-08-11 10:37:16\n[INFO] [WallTime: 1439285836.196404] waited until: 2015-08-11 10:37:16", 
            "title": "Task Execution"
        }, 
        {
            "location": "/strands_executive/doc/#task-scheduling", 
            "text": "We can now extend our task specification to include information about  when  the task should be executed. This is an essential part of our long-term autonomy approach, which requires behaviours to happen at times specified either by a user, or the robot's on subsystems. Each task should be given a time window during which task execution should occur. The task scheduler which is part of the executive framework sequences all the tasks it is managing to ensure that the time window of every task is respected. If this is not possible then the most recently added tasks are dropped (unless you are using priorities, in which case lower priority tasks are dropped until a schedule can be found).  The opening time of the task's execution window is specified using the  start_after  field. The code below sets the start window to ten seconds into the future.  task.start_after = rospy.get_rostime() + rospy.Duration(10)  The execution window should provide enough time to execute the task, but should also offer some slack as other tasks added to the system may have to executed first. The closing time of the the execution window is specified using the  end_before  field. The code below sets the end of the window such that the window is three times the maximum duration of the task.   task.end_before = task.start_after + rospy.Duration(task.max_duration.to_sec() * 3)  Previously we  demanded  task execution using the  DemandTask  service. This ignores the time window of the task and executes it immediately. To respect the time window we must use the service  /task_executor/add_tasks  of type  AddTasks .   def get_add_tasks_service():\n    return get_service('/task_executor/add_tasks', AddTasks)  This adds a list of tasks to the executive framework, which in turn triggers scheduling and updates the robot's execution schedule (assuming execution status has already been set to  True ).   add_tasks = get_add_tasks_service()\nadd_tasks([task])  When this script is executed, rather than the immediate execution of your task as previously, you should see the executor delay for some time (not quite ten seconds as there is an assumption of a minimum travel time before task execution) before executing the wait action. The output should be similar to that shown below, which shows the scheduler creating a schedule (which only contains the wait task) then the executor delaying execution until the star window of the task is open.  [ INFO] [1439548004.349319000]: SCHEDULER: Going to solve\n[ INFO] [1439548004.397583000]: SCHEDULER: found a solution\n[INFO] [WallTime: 1439548004.398210] start window: 11:26:53.412277\n[INFO] [WallTime: 1439548004.398465]          now: 11:26:44.398102\n[INFO] [WallTime: 1439548004.398639]  travel time:  0:00:02\n[INFO] [WallTime: 1439548004.398825] need to delay 7.14174938 for execution\n[INFO] [WallTime: 1439548004.399264] Added 1 tasks into the schedule to get total of 1", 
            "title": "Task Scheduling"
        }, 
        {
            "location": "/strands_executive/doc/#execution-information", 
            "text": "You can use the following sources of information to inspect the current state of the executive framework.  The current execution status can be obtained using the service  GetExecutionStatus  on  /task_executor/get_execution_status . A return value of  true  means the execution system is running, whereas  false  means that the execution system has either not been started or it has been paused.  To see the execution schedule, subscribe to the topic  /current_schedule  which gets the list of tasks in execution order. If  currently_executing  is  true  then this means the first element of  execution_queue  is the currently active task. If it is false then the system is delaying until it starts executing that task.  To just get the currently active task, use the service  strands_executive_msgs/GetActiveTask  on  /task_executor/get_active_task . If the returned task has a  task_id  of  0  then there is no active task (as you can't return  None  over a service).  To get print outs in the terminal describing the operation of the executive framework, you can use the following scripts:  rosrun task_executor schedule_status.py  schedule_status.py  prints out the current schedule and execution information. For example, for the above wait action, you might see something like  [INFO] [WallTime: 1439549489.280268] Waiting to execute /wait_action (task 3)\n[INFO] [WallTime: 1439549489.280547] Execution to start at 2015-08-14 11:51:39\n[INFO] [WallTime: 1439549489.280785] A further 0 tasks queued for execution\n[INFO] [WallTime: 1439549494.295445]   rosrun task_executor task_status.py  task_status.py  prints out the events that occur internally in the framework. For one of our wait tasks, you might see the following:  task 3    /wait_action    ADDED   14/08/15 11:51:29\ntask 3    /wait_action    TASK_STARTED    14/08/15 11:51:37\ntask 3    /wait_action    EXECUTION_STARTED   14/08/15 11:51:37\ntask 3    /wait_action    EXECUTION_SUCCEEDED   14/08/15 11:51:47\ntask 3    /wait_action    TASK_SUCCEEDED    14/08/15 11:51:47  You can also subscribe to these events on the topic  /task_executor/events .", 
            "title": "Execution Information"
        }, 
        {
            "location": "/strands_executive/doc/#exercise-1", 
            "text": "a) Given what you know so far, create a script which schedules a configurable number of wait action tasks. All tasks should have the same duration and time window, but you should make sure that the time window is created such that it allows all the tasks to be executed. Use  schedule_status.py  and  task_status.py  to monitor the progress of execution and debug your script if necessary.  b) Optional. Create an action server which prints out all the details of a task, then uses this in place of the wait action for 1(a). See the  task documentation  for how to add user-defined message types as task arguments.", 
            "title": "Exercise 1"
        }, 
        {
            "location": "/strands_executive/doc/#tasks-with-locations", 
            "text": "So far we have ignored the robot entirely. Now we will extend our task to feature the location of the robot when the task is performed. Each task can optionally have a start location and an end location. Locations are names of nodes in the robot's topological map (which you should have worked with yesterday). If the start location is provided, the task executive drives the robot to this location before starting the execution of the task action. If navigation fails, the task will not be executed. The end location describes the location the executive system expects the robot to be at when the task execution is completed. If left blank, this is assumed to be the same as the start location. The start and end locations are used by the scheduler to determine what order to execute the tasks.  For this part of the tutorial, we will work with the UOL MHT simulation and the associated topological map. From here on the examples assume you are running the correct mongodb store, simulation and navigation launch files for this simulated setup.  Since you now have a simulated robot running (see note above), you now  should not  use the  tutorial_dependencies  launch file. Instead, launch the executive framework as follows:  roslaunch task_executor task-scheduler.launch scheduler_version:=1  The  scheduler_version:=1  parameter ensures the optimal version of the scheduler is used (the original Coltin et al. algorithm). This version produces optimal schedules (including the travel time between locations), although it starts to take a long time to produce schedules once you go above 20 or so tasks in a single scheduling problem. You can omit this parameter to run the non-optimal version which uses preprocessing to produce non-optimal schedules quickly.  To configure a task's location, set the following fields in the task object:  task.start_node_id = `Waypoint3`\ntask.end_node_id = `Waypoint3`  Make sure you add the above lines to your existing code  before  the task is send to the scheduler. You can then use the  add_tasks  service as before, and you should see the robot drive to  Waypoint3  before executing a wait action (assuming you're extending the code from above).  Note  that now the robot must travel between locations you need to be make sure your time windows for execution are updated appropriately. The time window specifies the time in which the action is executed, not the navigation time. However, whereas you could previously easily fit two 10 second wait tasks in a 30 second time window (if you gave them both the same window), if it takes 120 seconds to travel between the two locations then the scheduler will not be able to include them both in the same schedule as the travel time between them will place one task outside its time window.", 
            "title": "Tasks with Locations"
        }, 
        {
            "location": "/strands_executive/doc/#exercise-2", 
            "text": "a) Extend your previous multi-task code to include locations in the tasks, with the robot travelling to at least three different locations.", 
            "title": "Exercise 2"
        }, 
        {
            "location": "/strands_executive/doc/#changes-to-execution", 
            "text": "You now know how to have a list of tasks scheduled and executed. If a task is  demanded  using the  DemandTask  service (as in the first part of this tutorial), the currently executing task is interrupted and the demanded one is executed in its place. In parallel, a new schedule is created which contains the remaining scheduled tasks and the interrupted one if possible (i.e. if the time windows can be respected). In all cases, if all tasks cannot be scheduled some are dropped until a valid schedule can be produced.   You can also pause the execution of the current task by sending a value of  false  to the  SetExecutionStatus  service. This will pause the task execution in the current state it is in, either navigation or action execution, until a value of  true  is sent to the service.  If you have written a task which should not be interrupted by these methods, you can create a service which informs the executive whether or not interruption is possible. The instructions for this are  here .", 
            "title": "Changes to Execution"
        }, 
        {
            "location": "/strands_executive/doc/#exercise-3", 
            "text": "a) Use your previous code to start the simulated robot executing scheduled tasks. Once it is executing, experiment with setting the execution status and demand tasks. Monitor the effects of these interactions using   rosrun task_executor schedule_status.py  You can alter the execution status from the command line using   rosservice call /task_executor/set_execution_status  status: true", 
            "title": "Exercise 3"
        }, 
        {
            "location": "/strands_executive/doc/#routines", 
            "text": "For long-term operation, we often want the robot to execute tasks according to some  routine , e.g. visit all the rooms in the building between 9am and 11am. The STRANDS executive framework supports the creations of such routines through two mechanisms. The  DailyRoutine  and  DailyRoutineRunner  classes (described  here ) support the creation of routines of daily repeating tasks. The  RobotRoutine  class (described  here ) builds on these objects to provide managed routine behaviour for a robot. This class also manages the battery level of the robot, provides a hook for when the robot is idle (i.e. it has no tasks to execute in the near future), and also manages the difference between day and night for the robot (at night the robot docks on a charger and can then only execute tasks without movement).   The  PatrolRoutine  class provides an example subclass of  RobotRoutine  which simply generates a patrol behaviour which creates tasks to wait at every node in the topological map. You can see this in operation by running:  rosrun routine_behaviours patroller_routine_node.py  With this node running you should see that the robot creates a schedule to visit all the nodes, or it fails to create on (if the time windows are not satisfiable) and instead visits a random node when idle.  Before going further, we can use the code from this node to understand the basic properties of a routine.  import rospy\n\nfrom datetime import time\nfrom dateutil.tz import tzlocal\n\nfrom routine_behaviours.patrol_routine import PatrolRoutine\n\n\nif __name__ == '__main__':\n    rospy.init_node( patroller_routine )\n\n    # start and end times -- all times should be in local timezone\n    localtz = tzlocal()\n    start = time(8,00, tzinfo=localtz)\n    end = time(20,00, tzinfo=localtz)\n\n    idle_duration=rospy.Duration(20)\n\n    routine = PatrolRoutine(daily_start=start, daily_end=end, idle_duration=idle_duration)    \n\n    routine.create_routine()\n\n    routine.start_routine()\n\n    rospy.spin()  Looking at the code in more detail:      start = time(8,00, tzinfo=localtz)\n    end = time(20,00, tzinfo=localtz)  The above code defines the working day of the robot. No task execution will happen before  start , and when  end  is reached, all execution will cease and the robot will dock for the night.      idle_duration=rospy.Duration(20)  The above code is used to configure how long the robot should not be executing a task before it considers itself idle.      routine = PatrolRoutine(daily_start=start, daily_end=end, idle_duration=idle_duration)      This creates the routine object, passing the defined values.  \n   routine.create_routine()\n\n   routine.start_routine()  create_routine()  runs code which specifies the tasks and populates the routine within the object. You can see the code for this  here . Following this,  start_routine  triggers the execution of the populated routine from the previous step.  If you want to create a routine that performs tasks at all, or a selection of, nodes in the map, you can create a subclass of  PatrolRoutine  and override methods such as  create_patrol_routine  and  create_patrol_task  to create task-specific behaviour. For the final part of this tutorial we will eschew this approach, and instead create a simple routine from  RobotRoutine .  Use the following code as a structure for your routine.  \n#!/usr/bin/env python\n\nimport rospy\n\nfrom routine_behaviours.robot_routine import RobotRoutine\n\nfrom datetime import time, date, timedelta\nfrom dateutil.tz import tzlocal\n\nfrom strands_executive_msgs.msg import Task\nfrom strands_executive_msgs import task_utils\n\n\nclass ExampleRoutine(RobotRoutine):\n      Creates a routine which simply visits nodes.  \n\n    def __init__(self, daily_start, daily_end, idle_duration=rospy.Duration(5), charging_point = 'ChargingPoint'):\n        # super(PatrolRoutine, self).__init__(daily_start, daily_end)        \n        RobotRoutine.__init__(self, daily_start, daily_end, idle_duration=idle_duration, charging_point=charging_point)\n\n    def create_routine(self):\n        pass        \n\n    def on_idle(self):\n         \n            Called when the routine is idle. Default is to trigger travel to the charging. As idleness is determined by the current schedule, if this call doesn't utlimately cause a task schedule to be generated this will be called repeatedly.\n         \n        rospy.loginfo('I am idle')    \n\n\n\nif __name__ == '__main__':\n\n    rospy.init_node('tutorial_4')\n\n    # start and end times -- all times should be in local timezone\n    localtz = tzlocal()\n    start = time(8,00, tzinfo=localtz)\n    end = time(20,00, tzinfo=localtz)\n\n    # how long to stand idle before doing something\n    idle_duration=rospy.Duration(20)\n\n    routine = ExampleRoutine(daily_start=start, daily_end=end, idle_duration=idle_duration)    \n\n    routine.create_routine()\n\n    routine.start_routine()\n\n    rospy.spin()  This is similar to the  patroller_routine_node  code we saw previously, except we now see the (currently empty) implementation of some parts of routine class. If you run this (alongside the executive framework, simulation, navigation etc.) you should see something like the follows:  [INFO] [WallTime: 1440522933.015355] Waiting for task_executor service...\n[INFO] [WallTime: 1440522933.018245] Done\n[INFO] [WallTime: 1440522933.023142] Fetching parameters from dynamic_reconfigure\n[INFO] [WallTime: 1440522933.027528] Config set to 30, 10\n[INFO] [WallTime: 1440522933.533364] Current day starts at 2015-08-25 08:00:00+01:00\n[INFO] [WallTime: 1440522933.533773] Current day ends at 2015-08-25 20:00:00+01:00\n[INFO] [WallTime: 1440522933.547498] extra_tasks_for_today\n[INFO] [WallTime: 1440522933.547924] Scheduling 0 tasks now and 0 later\n[INFO] [WallTime: 1440522933.548184] triggering day start cb at 2015-08-25 18:15:33.547396+01:00\n[INFO] [WallTime: 1440522933.548409] Good morning\n[INFO] [WallTime: 1440522933.554520] Scheduling 0 tasks now and 0 later\n[INFO] [WallTime: 1440522958.590649] I am idle  The  I am idle  line should appear after 20 seconds, as configured by  idle_duration . Nothing else should happen, as we have not yet added tasks.   To add tasks, we will fill in the  create_routine  method. In this method we will use the instance of  DailyRoutine  contained within  RobotRoutine . Tasks are added using the  repeat_every*  methods from this object. These take a list of tasks and store them such that they can be correctly instantiated with start and end times every day. When creating tasks for a routine, you should not specify  start_after  or  end_before  as these will be determined by the routine itself. Let's assume we have a function called  wait_task_at_waypoint  which creates a  Task  object to perform a  10 second wait at a given waypoint. We can then get the routine to schedule two such wait tasks to be performed every day.      def create_routine(self):\n        daily_wps = ['WayPoint6', 'WayPoint3']\n        daily_wp_tasks = [wait_task_at_waypoint(wp) for wp in daily_wps]\n        self.routine.repeat_every_day(daily_wp_tasks)  If you use this code in your previous script, you should see the robot execute the two tasks, then go idle. If you wait long enough (until the start of the next day!) the robot will repeat this execution. Rather that wait that long, let's add some tasks with a shorter repeat rate into  create_routine .          minute_wps = ['WayPoint5']\n        minute_wp_tasks = [wait_task_at_waypoint(wp) for wp in minute_wps]\n        self.routine.repeat_every_delta(minute_wp_tasks, delta=timedelta(minutes=5))  This causes a task to wait at the given waypoint to be scheduled at five minute intervals throughout the day. These tasks will be scheduled alongside the ones from the  daily_wp_tasks  (provided you didn't remove the code).  repeat_every_delta  is the most flexible of the routine creation tools, but you can also use  other methods from  DailyRoutine .", 
            "title": "Routines"
        }, 
        {
            "location": "/strands_executive/doc/#exercise-4", 
            "text": "a) Create a routine that mixes tasks with hourly and daily repeats.  b) Add tasks that either happen in the morning or afternoon. You will need to use the  start_time  argument to  repeat_every .   c) Once you have all of this working in simulation, move your code to your group's robot. Create a routine to patrol the nodes in your topological map. Extend this code to create meta-room maps at certain waypoints at regular intervals. You can also try creating your own tasks/action servers, or scheduling the execution of existing ones (speech, PTU movement, topological edge traversal etc.).", 
            "title": "Exercise 4"
        }, 
        {
            "location": "/strands_executive/doc/#additional-resources", 
            "text": "framework README    task_executor documentation  and  API    scheduler documentation      routine_behaviours documentation  and  API", 
            "title": "Additional Resources"
        }, 
        {
            "location": "/strands_executive/gcal_routine/", 
            "text": "Google Calendar Routine\n\n\nThis package reads dates from a \nGoogle Calendar\n and submits them to the scheduling service via the \nAddTasks\n service. \n\n\nThis requires the \ntask_scheduler\n to be running, including its requirements. \n\n\nStart it with \nrosrun gcal_routine gcal_routine_node.py\n. You may have to also enable task execution via \nrosservice call /task_executor/set_execution_status \"status: true\"\n\n\nThe \ngcal_routine\n submits task to the scheduler queried from Google calendar using an API key via the official Google REST API. Goggle Calendar resolves all recurrences and this node gets a \"linearised\" view of events. By default, the event \ntitle\n of the event on GCal describes the \naction\n and the \nWhere\n describes the start and end \nplace id\n. Further arguments of the \nTask\n structure can be set in YAML in the \ndescription\n of the event. \nIn addition, to make the user experience easier, tasks can be configured via a predefined template YAML file like this:\n\n\n{\n    'wait': {\n        'action': 'wait_action',\n        'max_duration': {secs: 300, nsecs: 0},\n    },\n    'patrol': {\n        'action': 'wait_action',\n        'max_duration': {secs: 30, nsecs: 0}\n    }\n}\n\n\n\n\nThis allows to define pre-defined values depending on the \naction\n entered in the \ntitle\n of the event. E.g. if \nwait\n is entered, then the attributes of the \nTask\n structure are being automatically populated from the template file.\n\n\nThe following parameters are being recognised:\n\n\n\n\n~calendar\n: The ID of the google calendar, defaults to\nhenry.strands%40hanheide.net\n (find the correct calendar ID in Calendar setting on Google Calendar)\n\n\n~key\n: The private access key to gain access to GCal, defaults to \nAIzaSyC1rqV2yecWwV0eLgmoQH7m7PdLNX1p6a0\n (This is the App key for the \nhenry.strands@hanheide.net\n account. You may want to use other accounts.)\n\n\n~available_tasks_conf_file\n: The path to a yaml file providing the template as shown above\n\n\n~pre_start_window_min\n: Number of minutes that tasks are submitted to the scheduler \nprior\n to their \nstart_after\n time. (default: 15 minutes)\n\n\nroutine_update_wait_sec\n: seconds to sleep between checks for new tasks to be valid to be sent to the scheduler (default: 10s)\n\n\n~gcal_poll_wait_sec\n: seconds between polls to the Google calendar. Default is 60s (i.e. 1 poll per minute). You don't want this to be too small or Google will ban the account due to high bandwidth. Once per minute is a good start.\n\n\n~priority\n: Priority to assign to task. Default is 2.\n\n\n\n\nAll events in a Google calendar are translated into UTC time for the generation of tasks. So it should work in any time zone. \n\n\nHere is an example call:\n\n\nrosrun  gcal_routine gcal_routine_node.py _stand_alone_test:=true _calendar:=\nhanheide.net_6hgulf44ij7ctjrf2iscj0m24o@group.calendar.google.com\n _pre_start_window_min:=30 _gcal_poll_wait_sec:=10 _routine_update_wait_sec:=1", 
            "title": "Gcal routine"
        }, 
        {
            "location": "/strands_executive/gcal_routine/#google-calendar-routine", 
            "text": "This package reads dates from a  Google Calendar  and submits them to the scheduling service via the  AddTasks  service.   This requires the  task_scheduler  to be running, including its requirements.   Start it with  rosrun gcal_routine gcal_routine_node.py . You may have to also enable task execution via  rosservice call /task_executor/set_execution_status \"status: true\"  The  gcal_routine  submits task to the scheduler queried from Google calendar using an API key via the official Google REST API. Goggle Calendar resolves all recurrences and this node gets a \"linearised\" view of events. By default, the event  title  of the event on GCal describes the  action  and the  Where  describes the start and end  place id . Further arguments of the  Task  structure can be set in YAML in the  description  of the event. \nIn addition, to make the user experience easier, tasks can be configured via a predefined template YAML file like this:  {\n    'wait': {\n        'action': 'wait_action',\n        'max_duration': {secs: 300, nsecs: 0},\n    },\n    'patrol': {\n        'action': 'wait_action',\n        'max_duration': {secs: 30, nsecs: 0}\n    }\n}  This allows to define pre-defined values depending on the  action  entered in the  title  of the event. E.g. if  wait  is entered, then the attributes of the  Task  structure are being automatically populated from the template file.", 
            "title": "Google Calendar Routine"
        }, 
        {
            "location": "/strands_executive/gcal_routine/#the-following-parameters-are-being-recognised", 
            "text": "~calendar : The ID of the google calendar, defaults to henry.strands%40hanheide.net  (find the correct calendar ID in Calendar setting on Google Calendar)  ~key : The private access key to gain access to GCal, defaults to  AIzaSyC1rqV2yecWwV0eLgmoQH7m7PdLNX1p6a0  (This is the App key for the  henry.strands@hanheide.net  account. You may want to use other accounts.)  ~available_tasks_conf_file : The path to a yaml file providing the template as shown above  ~pre_start_window_min : Number of minutes that tasks are submitted to the scheduler  prior  to their  start_after  time. (default: 15 minutes)  routine_update_wait_sec : seconds to sleep between checks for new tasks to be valid to be sent to the scheduler (default: 10s)  ~gcal_poll_wait_sec : seconds between polls to the Google calendar. Default is 60s (i.e. 1 poll per minute). You don't want this to be too small or Google will ban the account due to high bandwidth. Once per minute is a good start.  ~priority : Priority to assign to task. Default is 2.   All events in a Google calendar are translated into UTC time for the generation of tasks. So it should work in any time zone.   Here is an example call:  rosrun  gcal_routine gcal_routine_node.py _stand_alone_test:=true _calendar:= hanheide.net_6hgulf44ij7ctjrf2iscj0m24o@group.calendar.google.com  _pre_start_window_min:=30 _gcal_poll_wait_sec:=10 _routine_update_wait_sec:=1", 
            "title": "The following parameters are being recognised:"
        }, 
        {
            "location": "/strands_executive/", 
            "text": "The STRANDS Executive Framework\n\n\nAn executive framework for a mobile robot. The basic unit of behaviour for the framework is a \ntask\n as defined by \nstrands_executive_msgs/Task\n. The framework triggers and manages the execution of tasks plus navigation between them. The framework includes a task scheduler to optimise the execution time of a list of times. \n\n\nInstallation\n\n\nBinary\n\n\nIf you are using Ubuntu, the easiest way to install the executive framework is to \nadd the STRANDS apt releases repository\n. You can then run \nsudo apt-get install ros-indigo-task-executor\n. This will install the framework and it's dependencies alongside your existing ROS install under \n/opt/ros/indigo\n.\n\n\nSource\n\n\nTo compile from source you should clone this repository into your catkin workspace and compile as normal. For dependencies you will also need (at least) the following repsositories: \nstrands_navigation\n and \nmongodb_store\n. Source installs have been tested on Ubuntu 12.04, 14.04 and OS X.\n\n\nRuntime Dependencies\n\n\nFor the executive framework to function correctly, you must have the mongodb_store nodes running. These are used by the framework to store tasks with arbitrary arguments.\n\n\nroslaunch mongodb_store mongodb_store.launch\n\n\n\n\nor with path specifying, where should the db is stored:\n\n\nroslaunch mongodb_store mongodb_store.launch db_path:=/...\n\n\n\n\nCurrently the framework abstracts over navigation actions using \nthe STRANDS topological navigation framework\n. Therefore you must have this framework running. For testing, or if you're not running the full topological navigation system, you can run a simulple simulated topological system with the following command:\n\n\nroslaunch topological_utils dummy_topological_navigation.launch\n\n\n\n\nThis produces a map with 9 nodes: \nChargingPoint\n in the centre, with \nv_-2\n, \nv_-1\n to \nv_2\n running vertically and \nh_-2\n to \nh_2\n running horizontally, joining \nChargingPoint\n in the middle.\n\n\nRunning the executive framework\n\n\nTo start the executive framework, launch the following launch file.\n\n\nroslaunch task_executor task-scheduler-top.launch\n\n\n\n\nThis launches using topolgical navigation for moving the robot. If you wish to use the MDP execution (which has additional runtime dependencies) replace \ntop\n with \nmdp\n in the launch file name.\n\n\nRunning scheduled patrols\n\n\nTo test the executive framework you can try running the robot around the topological map.\n\n\nStart the executive framework:\n\n\nroslaunch task_executor task-scheduler.launch\n\n\n\n\nWith this up and running you can start the robot running continuous patrols using:\n\n\nrorun task_executor continuous_patrolling.py\n\n\n\n\nIf this runs successfully, then your basic systems is up and running safely.\n\n\nTasks\n\n\nThis executive framework, schedules and manages the execution of \ntasks\n. A task maps directly to the execution of an \nactionlib action server\n, allowing you to resuse or integrate your desired robot behaviours in a widely used ROS framework.\n\n\nMost task instances will contain both the name of a \ntopological map node\n where the task should be executed, plus the name of a \nSimpleActionServer\n to be called at the node and its associated arguments. Tasks must contain one of these, but not necessarily both.\n\n\nTo create a task, first create an instance of the \nTask\n message type. Examples are given in Python, as the helper functions currently only exist for Python, but C++ is also possible (and C++ helpers will be added if someone asks for them).\n\n\nfrom strands_executive_msgs.msg import Task\ntask = Task()\n\n\n\n\nThen you can set the node id for where the task will be executed (or you can do this inline in the constructor):\n\n\ntask.start_node_id = 'WayPoint1'\n\n\n\n\nIf you don't add a start node id then the task will be executed wherever the robot is located when it starts executing the task. If your task will end at a different location than it starts you can also specify \nend_node_id\n. This allows the scheduler to make better estimates of travel time between tasks.\n\n\nTo add the name of the action server, do:\n\n\ntask.action = 'do_dishes'\n\n\n\n\nWhere 'do_dishes' is replaced by the action name argument you would give to the \nactionlib.SimpleActionClient\n constructor. If you do not specify an action, the executive will assume the task is to simply visit the location indicated by \nstart_node_id\n.\n\n\nYou must also set the maximum length of time you expect your task to execute for. This is be used by the execution framework to determine whether your task is executing correctly, and by the scheduler to work out execution times. The duration is a \nrospy.Duration\n instance and is defined in seconds. \n\n\n# wash dishes for an hour\ndishes_duration = 60 * 60\ntask.max_duration = rospy.Duration(dishes_duration)\n\n\n\n\nYou can also specify the time window during which the task should be executed. \n\n\n# don't start the task until 10 minutes in the future\ntask.start_after = rospy.get_rostime() + rospy.Duration(10 * 60)\n# and give a window of three times the max execution time in which to execute\ntask.end_before = task.start_after + rospy.Duration(task.start_after.to_sec() * 3)\n\n\n\n\nIf the goal of the actionlib server related to your task  needs arguments, you must then add them to the task \nin the order they are used in your goal type constructor\n. Arguments are added to the task using the provided helper functions from \nstrands_executive_msgs.task_utils\n. For example, for the following action which is available under \ntask_executor/action/TestExecution.action\n, you need to supply a string argument followed by a pose, then an int then a float.\n\n\n# something to print\nstring some_goal_string\n# something to test typing\ngeometry_msgs/Pose test_pose\n# something for numbers\nint32 an_int\nfloat32 a_float\n---\n---\n# feedback message\nfloat32 percent_complete\n\n\n\n\nTo add the string, do the following\n\n\nfrom strands_executive_msgs import task_utils\ntask_utils.add_string_argument(task, 'my string argument goes here')\n\n\n\n\nFor the pose, this must be added to the \nmongodb_store message\n store and then the \nObjectID\n of the pose is used to communicate its location. This is done as follows\n\n\nfrom mongodb_store.message_store import MessageStoreProxy\nmsg_store = MessageStoreProxy()\n\np = Pose()\nobject_id = msg_store.insert(p)\ntask_utils.add_object_id_argument(task, object_id, Pose)\n\n\n\n\nInts and floats can be added as follows\n\n\ntask_utils.add_int_argument(task, 24)\ntask_utils.add_float_argument(task, 63.678)\n\n\n\n\nAdding a Task\n\n\nTasks can be added to the task executor for future execution via the \nadd_tasks\n service. These tasks are queued or scheduled for execution, and may not be executed immediately.\n\n\nadd_tasks_srv_name = '/task_executor/add_tasks'\nset_exe_stat_srv_name = '/task_executor/set_execution_status'\nrospy.wait_for_service(add_tasks_srv_name)\nrospy.wait_for_service(set_exe_stat_srv_name)\nadd_tasks_srv = rospy.ServiceProxy(add_tasks_srv_name, strands_executive_msgs.srv.AddTask)\nset_execution_status = rospy.ServiceProxy(set_exe_stat_srv_name, strands_executive_msgs.srv.SetExecutionStatus)\n\ntry:\n    # add task to the execution framework\n    task_id = add_tasks_srv([task])\n    # make sure the executive is running -- this only needs to be done once for the whole system not for every task\n    set_execution_status(True)\nexcept rospy.ServiceException, e: \n    print \nService call failed: %s\n%e       \n\n\n\n\nDemanding a Task\n\n\nIf you want your task to be executed immediately, pre-empting the current task execution (or navigation to that task), you can use the \ndemand_task\n service:\n\n\ndemand_task_srv_name = '/task_executor/demand_task'\nset_exe_stat_srv_name = '/task_executor/set_execution_status'\nrospy.wait_for_service(demand_task_srv_name)\nrospy.wait_for_service(set_exe_stat_srv_name)\ndemand_task_srv = rospy.ServiceProxy(demand_task_srv_name, strands_executive_msgs.srv.DemandTask)\nset_execution_status = rospy.ServiceProxy(set_exe_stat_srv_name, strands_executive_msgs.srv.SetExecutionStatus)\n\ntry:\n    # demand task execution immedidately\n    task_id = demand_task_srv([task])\n    # make sure the executive is running -- this only needs to be done once for the whole system not for every task\n    set_execution_status(True)\nexcept rospy.ServiceException, e: \n    print \nService call failed: %s\n%e       \n\n\n\n\nExecution Information\n\n\nThe current execution status can be obtained using the service \nstrands_executive_msgs/GetExecutionStatus\n typically on \n/task_executor/get_execution_status\n. True means the execution system is running, false means that the execution system has either not been started or it has been paused (see below).\n\n\nTo see the full schedule subscribe to the topic \n/current_schedule\n which gets the list of tasks in execution order. If \ncurrently_executing\n that means the first element of \nexecution_queue\n is the currently active task. If it is false then the system is delaying until it starts executing that task.\n\n\nTo just get the currently active task, use the service \nstrands_executive_msgs/GetActiveTask\n on \n/task_executor/get_active_task\n. If the returned task has a \ntask_id\n of \n0\n then there is no active task (as you can't return \nNone\n over a service).\n\n\nInterruptibility at Execution Time\n\n\nBy default the execution of tasks is interruptible (via actionlib preempt). Interruptions happen if another task is demanded while a task is running, or if the task exceeds its execution duration. If you do not wish your task to be interrupted in these condition you can provide the \nIsTaskInterruptible.srv\n service at the name \ntask name\n_is_interruptible\n, e.g. \ndo_dishes_is_interruptible\n from the example above. You can change the return value at runtime as this will be checked prior to interruption. \n\n\nHere's an example from the node which provides the \nwait_action\n.\n\n\n\nclass WaitServer:\n    def __init__(self):         \n        self.server = actionlib.SimpleActionServer('wait_action', WaitAction, self.execute, False) \n        self.server.start()\n        # this is not necessary in this node, but included for testing purposes\n        rospy.Service('wait_action_is_interruptible', IsTaskInterruptible, self.is_interruptible)\n\n    def is_interruptible(self, req):\n        # rospy.loginfo('Yes, interrupt me, go ahead')\n        # return True\n        rospy.loginfo('No, I will never stop')\n        return False\n\n\n\n\n\nCreating a Routine\n\n\nOur use case for task execution is that the robot has a \ndaily routine\n which is a list of tasks which it carries out every day. Such are routine can be created with the \ntask_routine.DailyRoutine\n object which is configured with start and end times for the robot's daily activities:\n\n\n    # some useful times\n    localtz = tzlocal()\n    # the time the robot will be active\n    start = time(8,30, tzinfo=localtz)\n    end = time(17,00, tzinfo=localtz)\n    midday = time(12,00, tzinfo=localtz)\n\n    morning = (start, midday)\n    afternoon = (midday, end)\n\n    routine = task_routine.DailyRoutine(start, end)\n ```\n\nTasks are then added using the `repeat_every*` methods. These take the given task and store it such that it can be correctly instantiated with start and end times every day:\n\n```python\n    # do this task every day\n    routine.repeat_every_day(task)\n    # and every two hours during the day\n    routine.repeat_every_hour(task, hours=2)\n    # once in the morning\n    routine.repeat_every(task, *morning)\n    # and twice in the afternoon\n    routine.repeat_every(task, *afternoon, times=2)\n\n\n\n\n\nThe \nDailyRoutine\n declares the structure of the routine. The routine tasks must be passed to the \nDailyRoutineRunner\n to manage the creation of specific task instances and their addition to the task executor. \n\n\n\n    # this uses the newer AddTasks service which excepts tasks as a batch\n    add_tasks_srv_name = '/task_executor/add_tasks'\n    add_tasks_srv = rospy.ServiceProxy(add_tasks_srv_name, AddTasks)\n\n\n    # create the object which will talk to the scheduler\n    runner = task_routine.DailyRoutineRunner(start, end, add_tasks_srv)\n    # pass the routine tasks on to the runner which handles the daily instantiation of actual tasks\n    runner.add_tasks(routine.get_routine_tasks())\n\n    # Set the task executor running (if it's not already)\n    set_execution_status(True)", 
            "title": "Home"
        }, 
        {
            "location": "/strands_executive/#the-strands-executive-framework", 
            "text": "An executive framework for a mobile robot. The basic unit of behaviour for the framework is a  task  as defined by  strands_executive_msgs/Task . The framework triggers and manages the execution of tasks plus navigation between them. The framework includes a task scheduler to optimise the execution time of a list of times.", 
            "title": "The STRANDS Executive Framework"
        }, 
        {
            "location": "/strands_executive/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/strands_executive/#binary", 
            "text": "If you are using Ubuntu, the easiest way to install the executive framework is to  add the STRANDS apt releases repository . You can then run  sudo apt-get install ros-indigo-task-executor . This will install the framework and it's dependencies alongside your existing ROS install under  /opt/ros/indigo .", 
            "title": "Binary"
        }, 
        {
            "location": "/strands_executive/#source", 
            "text": "To compile from source you should clone this repository into your catkin workspace and compile as normal. For dependencies you will also need (at least) the following repsositories:  strands_navigation  and  mongodb_store . Source installs have been tested on Ubuntu 12.04, 14.04 and OS X.", 
            "title": "Source"
        }, 
        {
            "location": "/strands_executive/#runtime-dependencies", 
            "text": "For the executive framework to function correctly, you must have the mongodb_store nodes running. These are used by the framework to store tasks with arbitrary arguments.  roslaunch mongodb_store mongodb_store.launch  or with path specifying, where should the db is stored:  roslaunch mongodb_store mongodb_store.launch db_path:=/...  Currently the framework abstracts over navigation actions using  the STRANDS topological navigation framework . Therefore you must have this framework running. For testing, or if you're not running the full topological navigation system, you can run a simulple simulated topological system with the following command:  roslaunch topological_utils dummy_topological_navigation.launch  This produces a map with 9 nodes:  ChargingPoint  in the centre, with  v_-2 ,  v_-1  to  v_2  running vertically and  h_-2  to  h_2  running horizontally, joining  ChargingPoint  in the middle.", 
            "title": "Runtime Dependencies"
        }, 
        {
            "location": "/strands_executive/#running-the-executive-framework", 
            "text": "To start the executive framework, launch the following launch file.  roslaunch task_executor task-scheduler-top.launch  This launches using topolgical navigation for moving the robot. If you wish to use the MDP execution (which has additional runtime dependencies) replace  top  with  mdp  in the launch file name.", 
            "title": "Running the executive framework"
        }, 
        {
            "location": "/strands_executive/#running-scheduled-patrols", 
            "text": "To test the executive framework you can try running the robot around the topological map.  Start the executive framework:  roslaunch task_executor task-scheduler.launch  With this up and running you can start the robot running continuous patrols using:  rorun task_executor continuous_patrolling.py  If this runs successfully, then your basic systems is up and running safely.", 
            "title": "Running scheduled patrols"
        }, 
        {
            "location": "/strands_executive/#tasks", 
            "text": "This executive framework, schedules and manages the execution of  tasks . A task maps directly to the execution of an  actionlib action server , allowing you to resuse or integrate your desired robot behaviours in a widely used ROS framework.  Most task instances will contain both the name of a  topological map node  where the task should be executed, plus the name of a  SimpleActionServer  to be called at the node and its associated arguments. Tasks must contain one of these, but not necessarily both.  To create a task, first create an instance of the  Task  message type. Examples are given in Python, as the helper functions currently only exist for Python, but C++ is also possible (and C++ helpers will be added if someone asks for them).  from strands_executive_msgs.msg import Task\ntask = Task()  Then you can set the node id for where the task will be executed (or you can do this inline in the constructor):  task.start_node_id = 'WayPoint1'  If you don't add a start node id then the task will be executed wherever the robot is located when it starts executing the task. If your task will end at a different location than it starts you can also specify  end_node_id . This allows the scheduler to make better estimates of travel time between tasks.  To add the name of the action server, do:  task.action = 'do_dishes'  Where 'do_dishes' is replaced by the action name argument you would give to the  actionlib.SimpleActionClient  constructor. If you do not specify an action, the executive will assume the task is to simply visit the location indicated by  start_node_id .  You must also set the maximum length of time you expect your task to execute for. This is be used by the execution framework to determine whether your task is executing correctly, and by the scheduler to work out execution times. The duration is a  rospy.Duration  instance and is defined in seconds.   # wash dishes for an hour\ndishes_duration = 60 * 60\ntask.max_duration = rospy.Duration(dishes_duration)  You can also specify the time window during which the task should be executed.   # don't start the task until 10 minutes in the future\ntask.start_after = rospy.get_rostime() + rospy.Duration(10 * 60)\n# and give a window of three times the max execution time in which to execute\ntask.end_before = task.start_after + rospy.Duration(task.start_after.to_sec() * 3)  If the goal of the actionlib server related to your task  needs arguments, you must then add them to the task  in the order they are used in your goal type constructor . Arguments are added to the task using the provided helper functions from  strands_executive_msgs.task_utils . For example, for the following action which is available under  task_executor/action/TestExecution.action , you need to supply a string argument followed by a pose, then an int then a float.  # something to print\nstring some_goal_string\n# something to test typing\ngeometry_msgs/Pose test_pose\n# something for numbers\nint32 an_int\nfloat32 a_float\n---\n---\n# feedback message\nfloat32 percent_complete  To add the string, do the following  from strands_executive_msgs import task_utils\ntask_utils.add_string_argument(task, 'my string argument goes here')  For the pose, this must be added to the  mongodb_store message  store and then the  ObjectID  of the pose is used to communicate its location. This is done as follows  from mongodb_store.message_store import MessageStoreProxy\nmsg_store = MessageStoreProxy()\n\np = Pose()\nobject_id = msg_store.insert(p)\ntask_utils.add_object_id_argument(task, object_id, Pose)  Ints and floats can be added as follows  task_utils.add_int_argument(task, 24)\ntask_utils.add_float_argument(task, 63.678)", 
            "title": "Tasks"
        }, 
        {
            "location": "/strands_executive/#adding-a-task", 
            "text": "Tasks can be added to the task executor for future execution via the  add_tasks  service. These tasks are queued or scheduled for execution, and may not be executed immediately.  add_tasks_srv_name = '/task_executor/add_tasks'\nset_exe_stat_srv_name = '/task_executor/set_execution_status'\nrospy.wait_for_service(add_tasks_srv_name)\nrospy.wait_for_service(set_exe_stat_srv_name)\nadd_tasks_srv = rospy.ServiceProxy(add_tasks_srv_name, strands_executive_msgs.srv.AddTask)\nset_execution_status = rospy.ServiceProxy(set_exe_stat_srv_name, strands_executive_msgs.srv.SetExecutionStatus)\n\ntry:\n    # add task to the execution framework\n    task_id = add_tasks_srv([task])\n    # make sure the executive is running -- this only needs to be done once for the whole system not for every task\n    set_execution_status(True)\nexcept rospy.ServiceException, e: \n    print  Service call failed: %s %e", 
            "title": "Adding a Task"
        }, 
        {
            "location": "/strands_executive/#demanding-a-task", 
            "text": "If you want your task to be executed immediately, pre-empting the current task execution (or navigation to that task), you can use the  demand_task  service:  demand_task_srv_name = '/task_executor/demand_task'\nset_exe_stat_srv_name = '/task_executor/set_execution_status'\nrospy.wait_for_service(demand_task_srv_name)\nrospy.wait_for_service(set_exe_stat_srv_name)\ndemand_task_srv = rospy.ServiceProxy(demand_task_srv_name, strands_executive_msgs.srv.DemandTask)\nset_execution_status = rospy.ServiceProxy(set_exe_stat_srv_name, strands_executive_msgs.srv.SetExecutionStatus)\n\ntry:\n    # demand task execution immedidately\n    task_id = demand_task_srv([task])\n    # make sure the executive is running -- this only needs to be done once for the whole system not for every task\n    set_execution_status(True)\nexcept rospy.ServiceException, e: \n    print  Service call failed: %s %e", 
            "title": "Demanding a Task"
        }, 
        {
            "location": "/strands_executive/#execution-information", 
            "text": "The current execution status can be obtained using the service  strands_executive_msgs/GetExecutionStatus  typically on  /task_executor/get_execution_status . True means the execution system is running, false means that the execution system has either not been started or it has been paused (see below).  To see the full schedule subscribe to the topic  /current_schedule  which gets the list of tasks in execution order. If  currently_executing  that means the first element of  execution_queue  is the currently active task. If it is false then the system is delaying until it starts executing that task.  To just get the currently active task, use the service  strands_executive_msgs/GetActiveTask  on  /task_executor/get_active_task . If the returned task has a  task_id  of  0  then there is no active task (as you can't return  None  over a service).", 
            "title": "Execution Information"
        }, 
        {
            "location": "/strands_executive/#interruptibility-at-execution-time", 
            "text": "By default the execution of tasks is interruptible (via actionlib preempt). Interruptions happen if another task is demanded while a task is running, or if the task exceeds its execution duration. If you do not wish your task to be interrupted in these condition you can provide the  IsTaskInterruptible.srv  service at the name  task name _is_interruptible , e.g.  do_dishes_is_interruptible  from the example above. You can change the return value at runtime as this will be checked prior to interruption.   Here's an example from the node which provides the  wait_action .  \nclass WaitServer:\n    def __init__(self):         \n        self.server = actionlib.SimpleActionServer('wait_action', WaitAction, self.execute, False) \n        self.server.start()\n        # this is not necessary in this node, but included for testing purposes\n        rospy.Service('wait_action_is_interruptible', IsTaskInterruptible, self.is_interruptible)\n\n    def is_interruptible(self, req):\n        # rospy.loginfo('Yes, interrupt me, go ahead')\n        # return True\n        rospy.loginfo('No, I will never stop')\n        return False", 
            "title": "Interruptibility at Execution Time"
        }, 
        {
            "location": "/strands_executive/#creating-a-routine", 
            "text": "Our use case for task execution is that the robot has a  daily routine  which is a list of tasks which it carries out every day. Such are routine can be created with the  task_routine.DailyRoutine  object which is configured with start and end times for the robot's daily activities:      # some useful times\n    localtz = tzlocal()\n    # the time the robot will be active\n    start = time(8,30, tzinfo=localtz)\n    end = time(17,00, tzinfo=localtz)\n    midday = time(12,00, tzinfo=localtz)\n\n    morning = (start, midday)\n    afternoon = (midday, end)\n\n    routine = task_routine.DailyRoutine(start, end)\n ```\n\nTasks are then added using the `repeat_every*` methods. These take the given task and store it such that it can be correctly instantiated with start and end times every day:\n\n```python\n    # do this task every day\n    routine.repeat_every_day(task)\n    # and every two hours during the day\n    routine.repeat_every_hour(task, hours=2)\n    # once in the morning\n    routine.repeat_every(task, *morning)\n    # and twice in the afternoon\n    routine.repeat_every(task, *afternoon, times=2)  The  DailyRoutine  declares the structure of the routine. The routine tasks must be passed to the  DailyRoutineRunner  to manage the creation of specific task instances and their addition to the task executor.   \n    # this uses the newer AddTasks service which excepts tasks as a batch\n    add_tasks_srv_name = '/task_executor/add_tasks'\n    add_tasks_srv = rospy.ServiceProxy(add_tasks_srv_name, AddTasks)\n\n\n    # create the object which will talk to the scheduler\n    runner = task_routine.DailyRoutineRunner(start, end, add_tasks_srv)\n    # pass the routine tasks on to the runner which handles the daily instantiation of actual tasks\n    runner.add_tasks(routine.get_routine_tasks())\n\n    # Set the task executor running (if it's not already)\n    set_execution_status(True)", 
            "title": "Creating a Routine"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/", 
            "text": "MDP Plan Exec\n\n\nAn optimal, high-level robot route planner and travel time estimator, using probabilistic model-checking techniques.\n\n\nOverview\n\n\nThis package provides single-robot optimal route planning capabilities, along with execution time expectations based on the method presented on:\n\n\n\n\nB. Lacerda, D. Parker, and N. Hawes. Optimal and Dynamic Planning for Markov Decision Processes with Co-Safe LTL Specifications In \n20114 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014)\n, Chicago, Illinois, USA, September 2014.\n\n\n\n\nThis uses MDP models of the environment and an \nextension\n of the probabilistic model checker tool \nPRISM\n that allows for communication with a ROS node via sockets, to generate the cost-optimal policies.\n\n\nIn order to fill the costs and probabilities of the MDP models with predictions from real-world data, we use the approach presented on:\n\n\n\n\nJ. Fentanes, B. Lacerda, T. Krajn\nk, N. Hawes, and M. Hanheide. Now or later? Predicting and Maximising Success of Navigation Actions from Long-Term Experience. In \n2015 IEEE International Conference on Robotics and Automation (ICRA 2015)\n, Seattle, Washington, USA, May 2015.\n\n\n\n\nUsage\n\n\nYou can use this package ithin the \nscheduled_task_executor node\n. This allows for a  complete framework that allows scheduling and execution of tasks taking into account the time of day. If you wish to use it in a standalone fashion, you can  do \nroslaunch mdp_plan_exec mdp_plan_exec.launch topological_map:=t\n where \nt\nis the name of the map saved in the \nmongo_db\n using the \ntopological_utils\n package. \n\n\nNodes\n\n\nspecial_waypoints_manager.py\n\n\nDynamically manages special waypoints defined, for example, by an end user.\n\n\nServices\n\n\n/mdp_plan_exec/add_delete_special_waypoint\n (\nstrands_executive_msgs/AddDeleteSpecialWaypoint\n) \n\n\nAdds \"special\" waypoints. These can be \"forbidden\" waypoints (i.e., waypoints to avoind during task execution, e.g., for safety reasons), or \"safe\" waypoints, for which the robot should navigate to when told to get out of the way by an end-user. These sets of waypoints are used to automatically generate the co-safe LTL specifications for the action server provided by the  \nmdp_travel_time_estimator.py\n node (see below). The boolean \nis_addition\n defines if the service call is adding or removing a waypoint, and the \nwaypoint_type\nconstant defines if the waypoint in the service call is forbidden or safe.\n\n\n/mdp_plan_exec/get_special_waypoints\n (\nstrands_executive_msgs/GetSpecialWaypoints\n) \n\n\nReturns the current lists of forbidden and safe waypoints, along with the corresponding LTL subformulas. For safe waypoints the corresponding subformula is the disjunction of all waypoints in the safe waypoint list, and for forbidden waypoints the corresponding subformula is the conjunction of negation of all waypoints in the forbidden waypoints list.\n\n\nmdp_travel_time_estimator.py\n\n\nProvides expected times to a target waypoint, by performing value iteration on a product MDP obtained from the MDP model representing the topological map.\n\n\nServices\n\n\n/mdp_plan_exec/get_expected_travel_times_to_waypoint\n (\nstrands_executive_msgs/GetExpectedTravelTimesToWaypoint\n) \n\n\nGiven a string \ntarget_waypoint\n identifying an element of  the topological map, and a ROS timestamp, this service returns the expected times to travel from all waypoints to the target waypoint. These are enconded by vectors \nstring[] source_waypoints\n and \nfloat64[] travel_times\n, where \ntravel_times[i]\nrepresents the expected travel time from \nsource_waypoints[i]\nto \ntarget_waypoint\n.\n\n\nmdp_policy_executor.py\n\n\nGenerates and executes a policy for a given input task,  by performing value iteration on a product MDP obtained from the MDP model representing the topological map.\n\n\nActions\n\n\n/mdp_plan_exec/execute_policy\n (\nstrands_executive_msgs/ExecutePolicy\n) \n\n\nGiven a \ntask_type\n and \ntarget_id\n, generates a cost optimal policy and executes the navigation actions associated to it. The specification is a co-safe LTL formula generated taking into account the \ntask_type\n. In the following \nforbidden_waypoints_ltl_string\n and \nsafe_waypoints_ltl_string\n are the strings obtained from the \nget_special_waypoints\nservice:\n\n\n\n\ntask_type = GOTO_WAYPOINT\n. This generates and executes a policy for formula \nF target_id\n (i.e., \"reach \ntarget_id\n\") if there are no forbidden  waypoints, or for formula \nforbidden_waypoints_ltl_string U target_id\n  (i.e., reach \ntarget_id\nwhile avoiding the forbidden waypoints).\n\n\ntask_type = LEAVE_FORBIDDEN_AREA\n. This generates and executes a policy for formula \nF forbidden_waypoints_ltl_string\n (i.e., get out of the forbidden waypoints as soon as possible). It should be called when the robot ends up in forbidden areas of the environment due to some failure in execution.\n\n\ntask_type = GOTO_CLOSEST_SAFE_WAYPOINT\n. This generates and executes a policy for formula \nF safe_waypoints_string\n (i.e., reach a safe waypoint as soon as possible). It should be called when the robot is sent to a safe zone, for example by an end-user.\n\n\ntask_type = COSAFE_LTL\n. This generates and executes a policy for formula \ntarget_id\n. In this case, \ntarget_id\nis a general co-safe LTL formula written in the \nPRISM specification language\n.\n\n\n\n\nThe action server also provides the expected execution time as feedback.", 
            "title": "Mdp plan exec"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/#mdp-plan-exec", 
            "text": "An optimal, high-level robot route planner and travel time estimator, using probabilistic model-checking techniques.", 
            "title": "MDP Plan Exec"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/#overview", 
            "text": "This package provides single-robot optimal route planning capabilities, along with execution time expectations based on the method presented on:   B. Lacerda, D. Parker, and N. Hawes. Optimal and Dynamic Planning for Markov Decision Processes with Co-Safe LTL Specifications In  20114 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014) , Chicago, Illinois, USA, September 2014.   This uses MDP models of the environment and an  extension  of the probabilistic model checker tool  PRISM  that allows for communication with a ROS node via sockets, to generate the cost-optimal policies.  In order to fill the costs and probabilities of the MDP models with predictions from real-world data, we use the approach presented on:   J. Fentanes, B. Lacerda, T. Krajn k, N. Hawes, and M. Hanheide. Now or later? Predicting and Maximising Success of Navigation Actions from Long-Term Experience. In  2015 IEEE International Conference on Robotics and Automation (ICRA 2015) , Seattle, Washington, USA, May 2015.", 
            "title": "Overview"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/#usage", 
            "text": "You can use this package ithin the  scheduled_task_executor node . This allows for a  complete framework that allows scheduling and execution of tasks taking into account the time of day. If you wish to use it in a standalone fashion, you can  do  roslaunch mdp_plan_exec mdp_plan_exec.launch topological_map:=t  where  t is the name of the map saved in the  mongo_db  using the  topological_utils  package.", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/#nodes", 
            "text": "", 
            "title": "Nodes"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/#special_waypoints_managerpy", 
            "text": "Dynamically manages special waypoints defined, for example, by an end user.", 
            "title": "special_waypoints_manager.py"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/#services", 
            "text": "/mdp_plan_exec/add_delete_special_waypoint  ( strands_executive_msgs/AddDeleteSpecialWaypoint )   Adds \"special\" waypoints. These can be \"forbidden\" waypoints (i.e., waypoints to avoind during task execution, e.g., for safety reasons), or \"safe\" waypoints, for which the robot should navigate to when told to get out of the way by an end-user. These sets of waypoints are used to automatically generate the co-safe LTL specifications for the action server provided by the   mdp_travel_time_estimator.py  node (see below). The boolean  is_addition  defines if the service call is adding or removing a waypoint, and the  waypoint_type constant defines if the waypoint in the service call is forbidden or safe.  /mdp_plan_exec/get_special_waypoints  ( strands_executive_msgs/GetSpecialWaypoints )   Returns the current lists of forbidden and safe waypoints, along with the corresponding LTL subformulas. For safe waypoints the corresponding subformula is the disjunction of all waypoints in the safe waypoint list, and for forbidden waypoints the corresponding subformula is the conjunction of negation of all waypoints in the forbidden waypoints list.", 
            "title": "Services"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/#mdp_travel_time_estimatorpy", 
            "text": "Provides expected times to a target waypoint, by performing value iteration on a product MDP obtained from the MDP model representing the topological map.", 
            "title": "mdp_travel_time_estimator.py"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/#services_1", 
            "text": "/mdp_plan_exec/get_expected_travel_times_to_waypoint  ( strands_executive_msgs/GetExpectedTravelTimesToWaypoint )   Given a string  target_waypoint  identifying an element of  the topological map, and a ROS timestamp, this service returns the expected times to travel from all waypoints to the target waypoint. These are enconded by vectors  string[] source_waypoints  and  float64[] travel_times , where  travel_times[i] represents the expected travel time from  source_waypoints[i] to  target_waypoint .", 
            "title": "Services"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/#mdp_policy_executorpy", 
            "text": "Generates and executes a policy for a given input task,  by performing value iteration on a product MDP obtained from the MDP model representing the topological map.", 
            "title": "mdp_policy_executor.py"
        }, 
        {
            "location": "/strands_executive/mdp_plan_exec/#actions", 
            "text": "/mdp_plan_exec/execute_policy  ( strands_executive_msgs/ExecutePolicy )   Given a  task_type  and  target_id , generates a cost optimal policy and executes the navigation actions associated to it. The specification is a co-safe LTL formula generated taking into account the  task_type . In the following  forbidden_waypoints_ltl_string  and  safe_waypoints_ltl_string  are the strings obtained from the  get_special_waypoints service:   task_type = GOTO_WAYPOINT . This generates and executes a policy for formula  F target_id  (i.e., \"reach  target_id \") if there are no forbidden  waypoints, or for formula  forbidden_waypoints_ltl_string U target_id   (i.e., reach  target_id while avoiding the forbidden waypoints).  task_type = LEAVE_FORBIDDEN_AREA . This generates and executes a policy for formula  F forbidden_waypoints_ltl_string  (i.e., get out of the forbidden waypoints as soon as possible). It should be called when the robot ends up in forbidden areas of the environment due to some failure in execution.  task_type = GOTO_CLOSEST_SAFE_WAYPOINT . This generates and executes a policy for formula  F safe_waypoints_string  (i.e., reach a safe waypoint as soon as possible). It should be called when the robot is sent to a safe zone, for example by an end-user.  task_type = COSAFE_LTL . This generates and executes a policy for formula  target_id . In this case,  target_id is a general co-safe LTL formula written in the  PRISM specification language .   The action server also provides the expected execution time as feedback.", 
            "title": "Actions"
        }, 
        {
            "location": "/strands_executive/scheduler/", 
            "text": "Scheduler\n\n\nA task scheduler implemented using mixed integer programming.\n\n\nOverview\n\n\nThis package provides single-robot task scheduling capabilities based on the algorithm presented in:\n\n\n\n\nL. Mudrova and N. Hawes. Task scheduling for mobile robots using interval algebra. In \n2015 IEEE International Conference on Robotics and Automation (ICRA 2015)\n, Seattle, Washington, USA, May 2015.\n\n\n\n\nThis extends prior work presented in \n\n\n\n\nB. Coltin, M. Veloso, and R. Ventura. Dynamic user task scheduling for mobile robots. In \nAAAI Workshop on Automated Action Planning for Autonomous Mobile Robots\n. AAAI, 2011.\n\n\n\n\nUsage\n\n\nIf you want your tasks to be executed, then you should use the scheduler within the \nscheduled_task_executor node\n. If you wish to use it in a standalone fashion, you can call the \nget_schedule\n service described below, and demonstrated in the \nscheduler/tests/test_scheduler_node.py\n.\n\n\nLibraries\n\n\nThe library \nlibscheduler\n provides the scheduling functionality outside of ROS. Use this if you want to include the scheduler in our own C++ library without using ROS. An example of its usage is provided in the \nscheduler_example\n executable, compiled from  \nscheduler/src/main.cpp\n.\n\n\nNodes\n\n\nscheduler_node\n\n\nThe \nscheduler_node\n provides task scheduling as a ROS service. Examples of using this service are available in the tests for the scheduler \nscheduler/tests/test_scheduler_node.py\n.\n\n\nServices\n\n\nget_schedule\n (\nstrands_executive_msgs/GetSchedule\n) \n\n\nGiven a list of tasks (\nTask[] tasks\n), this service call returns the order in which they should be executed and the desired execution start time of each task. Execution order is specified by the return value \nuint64[] task_order\n which contains the ids of input tasks in the order they should be executed. Execution times are provided by \ntime[] execution_times\n which is in the same order as \ntask_order\n and states the ideal time for task execution assuming all the input durations to the scheduler are as expected. The \nDurationMatrix\n argument specifies the travel time between task locations. It is a 2D array (implemented as a list of lists) where \ndurations[i].durations[j]\n specifies the travel time between the end location of task \ni\n and the start location of task \nj\n.\n\n\nParameters\n\n\n~save_problems\n (\nbool\n)\n\n\nIf true, the scheduling problems received by the scheduler (via \nget_schedule\n) are stored into the \nmongodb_store message store\n collection 'scheduling_problems'. This allows for later replay via \nrosrun scheduler replay_scheduling_problems.py\n. Defaults to true.\n\n\n~scheduler_version\n (\nint\n)\n\n\nSwitch the version of the algorithm the scheduler is running. If not specified, the Mudrova and Hawes algorithm is used. If the value 1 specified, then the original Coltin et al algorothm is used.\n\n\n~output_file\n (\nstring\n)\n\n\nPath to a desired output file. If set, output from the scheduler is saved to this file.\n\n\n~timeout\n (\nint\n)\n\n\nHow many seconds to allow the scheduler to run before timing out and returning (still potentially returning a result). The default value is to not have a timeout.", 
            "title": "Scheduler"
        }, 
        {
            "location": "/strands_executive/scheduler/#scheduler", 
            "text": "A task scheduler implemented using mixed integer programming.", 
            "title": "Scheduler"
        }, 
        {
            "location": "/strands_executive/scheduler/#overview", 
            "text": "This package provides single-robot task scheduling capabilities based on the algorithm presented in:   L. Mudrova and N. Hawes. Task scheduling for mobile robots using interval algebra. In  2015 IEEE International Conference on Robotics and Automation (ICRA 2015) , Seattle, Washington, USA, May 2015.   This extends prior work presented in    B. Coltin, M. Veloso, and R. Ventura. Dynamic user task scheduling for mobile robots. In  AAAI Workshop on Automated Action Planning for Autonomous Mobile Robots . AAAI, 2011.", 
            "title": "Overview"
        }, 
        {
            "location": "/strands_executive/scheduler/#usage", 
            "text": "If you want your tasks to be executed, then you should use the scheduler within the  scheduled_task_executor node . If you wish to use it in a standalone fashion, you can call the  get_schedule  service described below, and demonstrated in the  scheduler/tests/test_scheduler_node.py .", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_executive/scheduler/#libraries", 
            "text": "The library  libscheduler  provides the scheduling functionality outside of ROS. Use this if you want to include the scheduler in our own C++ library without using ROS. An example of its usage is provided in the  scheduler_example  executable, compiled from   scheduler/src/main.cpp .", 
            "title": "Libraries"
        }, 
        {
            "location": "/strands_executive/scheduler/#nodes", 
            "text": "", 
            "title": "Nodes"
        }, 
        {
            "location": "/strands_executive/scheduler/#scheduler_node", 
            "text": "The  scheduler_node  provides task scheduling as a ROS service. Examples of using this service are available in the tests for the scheduler  scheduler/tests/test_scheduler_node.py .", 
            "title": "scheduler_node"
        }, 
        {
            "location": "/strands_executive/scheduler/#services", 
            "text": "get_schedule  ( strands_executive_msgs/GetSchedule )   Given a list of tasks ( Task[] tasks ), this service call returns the order in which they should be executed and the desired execution start time of each task. Execution order is specified by the return value  uint64[] task_order  which contains the ids of input tasks in the order they should be executed. Execution times are provided by  time[] execution_times  which is in the same order as  task_order  and states the ideal time for task execution assuming all the input durations to the scheduler are as expected. The  DurationMatrix  argument specifies the travel time between task locations. It is a 2D array (implemented as a list of lists) where  durations[i].durations[j]  specifies the travel time between the end location of task  i  and the start location of task  j .", 
            "title": "Services"
        }, 
        {
            "location": "/strands_executive/scheduler/#parameters", 
            "text": "~save_problems  ( bool )  If true, the scheduling problems received by the scheduler (via  get_schedule ) are stored into the  mongodb_store message store  collection 'scheduling_problems'. This allows for later replay via  rosrun scheduler replay_scheduling_problems.py . Defaults to true.  ~scheduler_version  ( int )  Switch the version of the algorithm the scheduler is running. If not specified, the Mudrova and Hawes algorithm is used. If the value 1 specified, then the original Coltin et al algorothm is used.  ~output_file  ( string )  Path to a desired output file. If set, output from the scheduler is saved to this file.  ~timeout  ( int )  How many seconds to allow the scheduler to run before timing out and returning (still potentially returning a result). The default value is to not have a timeout.", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_executive/task_executor/", 
            "text": "Task Executive\n\n\nA set of nodes and libraries for defining and executing long-term task-level behaviour.\n\n\nOverview\n\n\nThe task executor is the central element of the \nstrands_executive\n framework. It receives tasks from clients, queues them for execution, then manages their execution and the required navigation between tasks. This package provides two executors: the simple \nfifo_task_executor\n which simply executes tasks in the order it receives them (ignoring their timing constraints) and the \nscheduled_task_executor\n which uses the \nscheduler node\n to create and maintain an execution schedule based on the time windows of the tasks. This package also provides the \ntask_routine\n Python module which facilitates the creation of daily task routines with repeating temporal structure. \n\n\nModules\n\n\nFor the task routine module see \nthe description on the overview page\n, and \nthe API documentation\n.\n\n\nRuntime Dependencies\n\n\nFor the executive framework to function correctly, you must have the \nmongodb_store\n nodes running. These are used by the framework to store tasks with arbitrary arguments.\n\n\nroslaunch mongodb_store mongodb_store.launch\n\n\n\n\nor with path specifying, where should the db is stored:\n\n\nroslaunch mongodb_store mongodb_store.launch db_path:=/...\n\n\n\n\nCurrently the task executive abstracts over navigation actions using \nthe STRANDS topological navigation framework\n. Therefore you must have this framework running. For testing, or if you're not running the full topological navigation system, you can run a simple simulated topological system with the following command:\n\n\nroslaunch topological_utils dummy_topological_navigation.launch\n\n\n\n\nThis produces a map with 9 nodes: \nChargingPoint\n in the centre, with \nv_-2\n, \nv_-1\n to \nv_2\n running vertically and \nh_-2\n to \nh_2\n running horizontally, joining \nChargingPoint\n in the middle.\n\n\nNodes\n\n\nscheduled_task_executor.py\n\n\nThis node receives tasks via the services, schedules them for execution, then executes them in the order defined by the schedule.\n\n\nExecution Status\n\n\nWhen the executor is started, it will not start executing any tasks until the execution status is set to \nTrue\n via a call to the \n/task_executor/set_execution_status\n (\nstrands_executive_msgs/GetExecutionStatus\n) service. If execution status is set to \nFalse\n then execution pauses, interrupting any currently executing task. \n\n\nTask Addition and Scheduling\n\n\nTasks are added using the \nadd_task\n (single task) and \nadd_tasks\n (multiple tasks) services. All received tasks are added to a queue for scheduling which is monitored by the executor. When then queue contains tasks, the new tasks, plus the tasks already scheduled are sent to the \nscheduler node\n. If a task is successfully created, this replaces the previous schedule and execution continues. If scheduling fails then the newly added tasks are dropped by the executor and the previous schedule is reinstated. Adding new tasks does not interrupt the currently executing task.\n\n\nTask Demanding\n\n\nIf a task should be executed immediately, the \ndemand_task\n (\nstrands_executive_msgs/DemandTask\n) service can be used. This interrupts the currently executing task and replaces it with the demanded task. The schedule is then recreated to respect the execution constraints of the demanded task, and, if possible the interrupted task is included in this new schedule.\n\n\nInterruptibility\n\n\nBy default the execution of tasks is interruptible (via actionlib preempt). If you do not wish your task to be interrupted in these condition you can provide the \nIsTaskInterruptible.srv\n service at the name \ntask name\n_is_interruptible\n, e.g. \ndo_dishes_is_interruptible\n from the example above. You can change the return value at runtime as this will be checked prior to interruption. \n\n\nHere's an example from the node which provides the \nwait_action\n.\n\n\n\nclass WaitServer:\n    def __init__(self):         \n        self.server = actionlib.SimpleActionServer('wait_action', WaitAction, self.execute, False) \n        self.server.start()\n        # this is not necessary in this node, but included for testing purposes\n        rospy.Service('wait_action_is_interruptible', IsTaskInterruptible, self.is_interruptible)\n\n    def is_interruptible(self, req):\n        # rospy.loginfo('Yes, interrupt me, go ahead')\n        # return True\n        rospy.loginfo('No, I will never stop')\n        return False\n\n\n\n\n\nTask Execution and Monitoring\n\n\nWhen a task is executed it pass through two phases: navigation and action execution. If the task has a \nstart_node_id\n set then the executor uses topological navigation to move the robot to this start node. Before doing so it obtains an estimate of the travel time from the \ntopological_navigation/travel_time_estimator\n service. If the travel time greatly exceeds this estimate, the topological navigation action is preempted and the task execution is failed. If the topological_navigation action reports anything but success on completion then the task execution is failed. If it reports success then the task moves on to the action execution phase. This phases triggers the action server described by the task. If execution of the action server greatly exceeds the \nmax_duration\n of the task, it is preempted and execution is considered failed. The overall execution state machine is pictured below. \n\n\n\n\nServices\n\n\ntask_executor/add_tasks\n (\nstrands_executive_msgs/AddTasks\n) \n\n\nAdd a list of tasks to be scheduled for execution.\n\n\ntask_executor/add_task\n (\nstrands_executive_msgs/AddTask\n) \n\n\nAdd a single task to be scheduled for execution.\n\n\ntask_executor/demand_task\n (\nstrands_executive_msgs/DemandTask\n) \n\n\nTriggers the immediate execution of a task, interrupting the currently executing task.\n\n\ntask_executor/set_execution_status\n (\nstrands_executive_msgs/SetExecutionStatus\n) \n\n\nSets the execution status of the executor. Set to false to pause execution. Starts at false so much be set to true on start-up.\n\n\ntask_executor/get_execution_status\n (\nstrands_executive_msgs/GetExecutionStatus\n) \n\n\nGets the current execution status of the executor.\n\n\ntask_executor/clear_schedule\n (\nstd_srvs/Empty\n) \n\n\nClears all tasks scheduled for execution. Cancels any active task.\n\n\ntask_executor/cancel_task\n (\nstrands_executive_msgs/CancelTask\n) \n\n\nRemoves the task with the given id from the schedule. If this task is currently executing, execution is interrupted.\n\n\ntask_executor/get_active_task\n (\nstrands_executive_msgs/GetActiveTask\n) \n\n\nGets the task which is currently executing.\n\n\nPublished Topics\n\n\ntask_executor/events\n \nstrands_executive_msgs/TaskEvent\n)\n\n\nEvents that happen as the task executor passes through its state machine for each task.\n\n\ncurrent_schedule\n \nstrands_executive_msgs/ExecutionStatus\n)\n\n\nThe list of upcoming tasks and what is currently being executed.\n\n\nfifo_task_executor.py\n\n\nA greatly simplified task executor that executes tasks in the order they are added, and only supports task addition and very little else when compared to the \nscheduled_task_executor\n.\n\n\nschedule_status.py\n\n\nPrints a summary of the \ncurrent_schedule\n topic.", 
            "title": "Task executor"
        }, 
        {
            "location": "/strands_executive/task_executor/#task-executive", 
            "text": "A set of nodes and libraries for defining and executing long-term task-level behaviour.", 
            "title": "Task Executive"
        }, 
        {
            "location": "/strands_executive/task_executor/#overview", 
            "text": "The task executor is the central element of the  strands_executive  framework. It receives tasks from clients, queues them for execution, then manages their execution and the required navigation between tasks. This package provides two executors: the simple  fifo_task_executor  which simply executes tasks in the order it receives them (ignoring their timing constraints) and the  scheduled_task_executor  which uses the  scheduler node  to create and maintain an execution schedule based on the time windows of the tasks. This package also provides the  task_routine  Python module which facilitates the creation of daily task routines with repeating temporal structure.", 
            "title": "Overview"
        }, 
        {
            "location": "/strands_executive/task_executor/#modules", 
            "text": "For the task routine module see  the description on the overview page , and  the API documentation .", 
            "title": "Modules"
        }, 
        {
            "location": "/strands_executive/task_executor/#runtime-dependencies", 
            "text": "For the executive framework to function correctly, you must have the  mongodb_store  nodes running. These are used by the framework to store tasks with arbitrary arguments.  roslaunch mongodb_store mongodb_store.launch  or with path specifying, where should the db is stored:  roslaunch mongodb_store mongodb_store.launch db_path:=/...  Currently the task executive abstracts over navigation actions using  the STRANDS topological navigation framework . Therefore you must have this framework running. For testing, or if you're not running the full topological navigation system, you can run a simple simulated topological system with the following command:  roslaunch topological_utils dummy_topological_navigation.launch  This produces a map with 9 nodes:  ChargingPoint  in the centre, with  v_-2 ,  v_-1  to  v_2  running vertically and  h_-2  to  h_2  running horizontally, joining  ChargingPoint  in the middle.", 
            "title": "Runtime Dependencies"
        }, 
        {
            "location": "/strands_executive/task_executor/#nodes", 
            "text": "", 
            "title": "Nodes"
        }, 
        {
            "location": "/strands_executive/task_executor/#scheduled_task_executorpy", 
            "text": "This node receives tasks via the services, schedules them for execution, then executes them in the order defined by the schedule.", 
            "title": "scheduled_task_executor.py"
        }, 
        {
            "location": "/strands_executive/task_executor/#execution-status", 
            "text": "When the executor is started, it will not start executing any tasks until the execution status is set to  True  via a call to the  /task_executor/set_execution_status  ( strands_executive_msgs/GetExecutionStatus ) service. If execution status is set to  False  then execution pauses, interrupting any currently executing task.", 
            "title": "Execution Status"
        }, 
        {
            "location": "/strands_executive/task_executor/#task-addition-and-scheduling", 
            "text": "Tasks are added using the  add_task  (single task) and  add_tasks  (multiple tasks) services. All received tasks are added to a queue for scheduling which is monitored by the executor. When then queue contains tasks, the new tasks, plus the tasks already scheduled are sent to the  scheduler node . If a task is successfully created, this replaces the previous schedule and execution continues. If scheduling fails then the newly added tasks are dropped by the executor and the previous schedule is reinstated. Adding new tasks does not interrupt the currently executing task.", 
            "title": "Task Addition and Scheduling"
        }, 
        {
            "location": "/strands_executive/task_executor/#task-demanding", 
            "text": "If a task should be executed immediately, the  demand_task  ( strands_executive_msgs/DemandTask ) service can be used. This interrupts the currently executing task and replaces it with the demanded task. The schedule is then recreated to respect the execution constraints of the demanded task, and, if possible the interrupted task is included in this new schedule.", 
            "title": "Task Demanding"
        }, 
        {
            "location": "/strands_executive/task_executor/#interruptibility", 
            "text": "By default the execution of tasks is interruptible (via actionlib preempt). If you do not wish your task to be interrupted in these condition you can provide the  IsTaskInterruptible.srv  service at the name  task name _is_interruptible , e.g.  do_dishes_is_interruptible  from the example above. You can change the return value at runtime as this will be checked prior to interruption.   Here's an example from the node which provides the  wait_action .  \nclass WaitServer:\n    def __init__(self):         \n        self.server = actionlib.SimpleActionServer('wait_action', WaitAction, self.execute, False) \n        self.server.start()\n        # this is not necessary in this node, but included for testing purposes\n        rospy.Service('wait_action_is_interruptible', IsTaskInterruptible, self.is_interruptible)\n\n    def is_interruptible(self, req):\n        # rospy.loginfo('Yes, interrupt me, go ahead')\n        # return True\n        rospy.loginfo('No, I will never stop')\n        return False", 
            "title": "Interruptibility"
        }, 
        {
            "location": "/strands_executive/task_executor/#task-execution-and-monitoring", 
            "text": "When a task is executed it pass through two phases: navigation and action execution. If the task has a  start_node_id  set then the executor uses topological navigation to move the robot to this start node. Before doing so it obtains an estimate of the travel time from the  topological_navigation/travel_time_estimator  service. If the travel time greatly exceeds this estimate, the topological navigation action is preempted and the task execution is failed. If the topological_navigation action reports anything but success on completion then the task execution is failed. If it reports success then the task moves on to the action execution phase. This phases triggers the action server described by the task. If execution of the action server greatly exceeds the  max_duration  of the task, it is preempted and execution is considered failed. The overall execution state machine is pictured below.", 
            "title": "Task Execution and Monitoring"
        }, 
        {
            "location": "/strands_executive/task_executor/#services", 
            "text": "task_executor/add_tasks  ( strands_executive_msgs/AddTasks )   Add a list of tasks to be scheduled for execution.  task_executor/add_task  ( strands_executive_msgs/AddTask )   Add a single task to be scheduled for execution.  task_executor/demand_task  ( strands_executive_msgs/DemandTask )   Triggers the immediate execution of a task, interrupting the currently executing task.  task_executor/set_execution_status  ( strands_executive_msgs/SetExecutionStatus )   Sets the execution status of the executor. Set to false to pause execution. Starts at false so much be set to true on start-up.  task_executor/get_execution_status  ( strands_executive_msgs/GetExecutionStatus )   Gets the current execution status of the executor.  task_executor/clear_schedule  ( std_srvs/Empty )   Clears all tasks scheduled for execution. Cancels any active task.  task_executor/cancel_task  ( strands_executive_msgs/CancelTask )   Removes the task with the given id from the schedule. If this task is currently executing, execution is interrupted.  task_executor/get_active_task  ( strands_executive_msgs/GetActiveTask )   Gets the task which is currently executing.", 
            "title": "Services"
        }, 
        {
            "location": "/strands_executive/task_executor/#published-topics", 
            "text": "task_executor/events   strands_executive_msgs/TaskEvent )  Events that happen as the task executor passes through its state machine for each task.  current_schedule   strands_executive_msgs/ExecutionStatus )  The list of upcoming tasks and what is currently being executed.", 
            "title": "Published Topics"
        }, 
        {
            "location": "/strands_executive/task_executor/#fifo_task_executorpy", 
            "text": "A greatly simplified task executor that executes tasks in the order they are added, and only supports task addition and very little else when compared to the  scheduled_task_executor .", 
            "title": "fifo_task_executor.py"
        }, 
        {
            "location": "/strands_executive/task_executor/#schedule_statuspy", 
            "text": "Prints a summary of the  current_schedule  topic.", 
            "title": "schedule_status.py"
        }, 
        {
            "location": "/strands_executive_behaviours/automated_routine/", 
            "text": "Automated Routine\n\n\nThis package provides a way of automatically populating a routine with tasks\nbased on a definition specified using a yaml file.", 
            "title": "Automated routine"
        }, 
        {
            "location": "/strands_executive_behaviours/automated_routine/#automated-routine", 
            "text": "This package provides a way of automatically populating a routine with tasks\nbased on a definition specified using a yaml file.", 
            "title": "Automated Routine"
        }, 
        {
            "location": "/strands_executive_behaviours/routine_behaviours/", 
            "text": "Routine Behaviours\n\n\nOverview\n\n\nThis package provides tools to generate long-term routine behaviours for mobile robots. The \nRobotRoutine\n class in this package provides facilities for extending the \nstrands_executive/task_routine\n with useful runtime behaviours. \n\n\nAPI documentation: \nhttp://strands-project.github.io/strands_executive_behaviours/routine_behaviours/html\n\n\nIdle Behaviours\n\n\nWhen the robot is inactive for a configurable amount of time, with sufficient battery charge, the methods \nRobotRoutine.on_idle()\n is called. Overriding this method allows you to generate specific behaviour when the robot is idle. For example \nPatrolRoutine.on_idle()\n moves the robot to random node when idle.\n\n\nNight Behaviours\n\n\nWhen the robot's routine for the day is complete, we assume that the robot is then parked at a charging station. As the routine is complete, tasks which are scheduled as part of the normal routine are not sent to the robot. To provide tasks which are executed by the robot overnight, you can use \nRobotRoutine.add_night_task\n. This adds a task which will be sent for execution every night after the routine finishes. These tasks cannot involve movement and therefore must either have an empty \nstart_node_id\n or be performed at the charging station. A system can have multiple night tasks, but currently no checking is done to ensure the night tasks fit within the time available between the end of one routine day and the start of the next.", 
            "title": "Routine behaviours"
        }, 
        {
            "location": "/strands_executive_behaviours/routine_behaviours/#routine-behaviours", 
            "text": "", 
            "title": "Routine Behaviours"
        }, 
        {
            "location": "/strands_executive_behaviours/routine_behaviours/#overview", 
            "text": "This package provides tools to generate long-term routine behaviours for mobile robots. The  RobotRoutine  class in this package provides facilities for extending the  strands_executive/task_routine  with useful runtime behaviours.   API documentation:  http://strands-project.github.io/strands_executive_behaviours/routine_behaviours/html", 
            "title": "Overview"
        }, 
        {
            "location": "/strands_executive_behaviours/routine_behaviours/#idle-behaviours", 
            "text": "When the robot is inactive for a configurable amount of time, with sufficient battery charge, the methods  RobotRoutine.on_idle()  is called. Overriding this method allows you to generate specific behaviour when the robot is idle. For example  PatrolRoutine.on_idle()  moves the robot to random node when idle.", 
            "title": "Idle Behaviours"
        }, 
        {
            "location": "/strands_executive_behaviours/routine_behaviours/#night-behaviours", 
            "text": "When the robot's routine for the day is complete, we assume that the robot is then parked at a charging station. As the routine is complete, tasks which are scheduled as part of the normal routine are not sent to the robot. To provide tasks which are executed by the robot overnight, you can use  RobotRoutine.add_night_task . This adds a task which will be sent for execution every night after the routine finishes. These tasks cannot involve movement and therefore must either have an empty  start_node_id  or be performed at the charging station. A system can have multiple night tasks, but currently no checking is done to ensure the night tasks fit within the time available between the end of one routine day and the start of the next.", 
            "title": "Night Behaviours"
        }, 
        {
            "location": "/strands_exploration/", 
            "text": "strands_exploration\n\n\nInformation-driven spatio-temporal exploration \n\n\nBuild Status:", 
            "title": "Home"
        }, 
        {
            "location": "/strands_exploration/#strands_exploration", 
            "text": "Information-driven spatio-temporal exploration   Build Status:", 
            "title": "strands_exploration"
        }, 
        {
            "location": "/strands_exploration/spatiotemporal_exploration/", 
            "text": "Spatio-temporal topological exploration\n\n\nThis package provides the nodes necessary to run spatio-temporal exploration over the nodes in the topological map.\n\n\nParameters\n\n\n\n\ntag_node\n: nodes where is not possible to build a ground truth. The exploration task is set by an exploration routine.\n\n\nschedule_directory\n: directory where the schedules are saved.\n\n\ngrids_directory\n: directory where the 3D grids are saved.\n\n\nresolution\n: voxel size.\n\n\ndimX\n: number of cells along the axis X.\n\n\ndimY\n: number of cells along the axis Y.\n\n\ndimZ\n: number of cells along the axis Z.\n\n\nsweep_type\n: \ndo_sweep\n action type (complete, medium, short, shortest). \n\n\ntaskDuration\n: exploration task time duration in seconds. \n\n\n\n\nRunning\n\n\nroslaunch topological_exploration topological_exploration.launch", 
            "title": "Spatiotemporal exploration"
        }, 
        {
            "location": "/strands_exploration/spatiotemporal_exploration/#spatio-temporal-topological-exploration", 
            "text": "This package provides the nodes necessary to run spatio-temporal exploration over the nodes in the topological map.", 
            "title": "Spatio-temporal topological exploration"
        }, 
        {
            "location": "/strands_exploration/spatiotemporal_exploration/#parameters", 
            "text": "tag_node : nodes where is not possible to build a ground truth. The exploration task is set by an exploration routine.  schedule_directory : directory where the schedules are saved.  grids_directory : directory where the 3D grids are saved.  resolution : voxel size.  dimX : number of cells along the axis X.  dimY : number of cells along the axis Y.  dimZ : number of cells along the axis Z.  sweep_type :  do_sweep  action type (complete, medium, short, shortest).   taskDuration : exploration task time duration in seconds.", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_exploration/spatiotemporal_exploration/#running", 
            "text": "roslaunch topological_exploration topological_exploration.launch", 
            "title": "Running"
        }, 
        {
            "location": "/strands_hri/", 
            "text": "strands_hri\n\n\nall the basic HRI packages we are going to use", 
            "title": "Home"
        }, 
        {
            "location": "/strands_hri/#strands_hri", 
            "text": "all the basic HRI packages we are going to use", 
            "title": "strands_hri"
        }, 
        {
            "location": "/strands_hri/strands_gazing/", 
            "text": "Strands gazing package\n\n\nSince gazing is one important part of human-robot interaction this package will provide general applications to perform \ngazing behaviours.\n\n\nGaze at pose\n\n\nThis node takes a given \ngeometry_msgs/PoseStamped\n and moves the head to gaze at this position. This is \nimplemented as an action server.\n\n\nParameters\n\n\n\n\nhead_pose\n \nDefault: /head/commanded_state\n: The topic to which the joint state messages for the head position are \npublished.\n\n\nhead_frame\n \nDefault: /head_base_frame\n: The target coordinate frame.\n\n\n\n\nRunning\n\n\nrosrun strands_gazing gaze_at_pose [_parameter:=value]\n\n\n\n\nTo start the actual gazing you have to use the actionlib client architecture, \ne.g. \nrosrun actionlib axclient.py /gaze_at_pose\n\n\n goal\n * \nint32 runtime_sec\n: The time the gazing should be executed in seconds. Set to 0 for infinit run time (has to be \ncancelled to stop). \nCancelling a goal will trigger the head to move to the zero position.\n\n * \ntopic_name\n: The name of the topic on which to listen for the poses. Currently: \n/move_base/current_goal\n and \n/upper_body_detector/closest_bounding_box_centre\n but really everything that poblishes a stamped pose can be used.\n\n result\n * \nbool expired\n: true if run time is up, false if cancelled.\n* feedback\n * \nfloat32 remaining_time\n: The remaining run time.\n * \ngeometry_msgs/Pose target\n: The pose at which the robot currently gazes.\n\n\nNotes\n\n\nSince some of the topics that publish a PoseStamped do this only once, e.g. \n/move_base/current_goal\n, the transformation runs a infinite loop. This means that move_base publishes the pose once and inside the look the transform will run for the updated robot position with the received pose from move_base. As soon as a new pose is received the transformloop will use this one to calculate the transform. This still runs in real time but accounts for nodes that are lazy publishers.", 
            "title": "Strands gazing"
        }, 
        {
            "location": "/strands_hri/strands_gazing/#strands-gazing-package", 
            "text": "Since gazing is one important part of human-robot interaction this package will provide general applications to perform \ngazing behaviours.", 
            "title": "Strands gazing package"
        }, 
        {
            "location": "/strands_hri/strands_gazing/#gaze-at-pose", 
            "text": "This node takes a given  geometry_msgs/PoseStamped  and moves the head to gaze at this position. This is \nimplemented as an action server.", 
            "title": "Gaze at pose"
        }, 
        {
            "location": "/strands_hri/strands_gazing/#parameters", 
            "text": "head_pose   Default: /head/commanded_state : The topic to which the joint state messages for the head position are \npublished.  head_frame   Default: /head_base_frame : The target coordinate frame.", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_hri/strands_gazing/#running", 
            "text": "rosrun strands_gazing gaze_at_pose [_parameter:=value]  To start the actual gazing you have to use the actionlib client architecture, \ne.g.  rosrun actionlib axclient.py /gaze_at_pose   goal\n *  int32 runtime_sec : The time the gazing should be executed in seconds. Set to 0 for infinit run time (has to be \ncancelled to stop).  Cancelling a goal will trigger the head to move to the zero position. \n *  topic_name : The name of the topic on which to listen for the poses. Currently:  /move_base/current_goal  and  /upper_body_detector/closest_bounding_box_centre  but really everything that poblishes a stamped pose can be used.  result\n *  bool expired : true if run time is up, false if cancelled.\n* feedback\n *  float32 remaining_time : The remaining run time.\n *  geometry_msgs/Pose target : The pose at which the robot currently gazes.", 
            "title": "Running"
        }, 
        {
            "location": "/strands_hri/strands_gazing/#notes", 
            "text": "Since some of the topics that publish a PoseStamped do this only once, e.g.  /move_base/current_goal , the transformation runs a infinite loop. This means that move_base publishes the pose once and inside the look the transform will run for the updated robot position with the received pose from move_base. As soon as a new pose is received the transformloop will use this one to calculate the transform. This still runs in real time but accounts for nodes that are lazy publishers.", 
            "title": "Notes"
        }, 
        {
            "location": "/strands_hri/strands_hri_launch/", 
            "text": "HRI launch package\n\n\nCurrently, this package only contains one launch file for the speech system. See strands_hri_utils README for detailed description.", 
            "title": "Strands hri launch"
        }, 
        {
            "location": "/strands_hri/strands_hri_launch/#hri-launch-package", 
            "text": "Currently, this package only contains one launch file for the speech system. See strands_hri_utils README for detailed description.", 
            "title": "HRI launch package"
        }, 
        {
            "location": "/strands_hri/strands_human_aware_navigation/", 
            "text": "Human aware velocities\n\n\nThis package adjusts the velocity of the robot in the presence of humans.\n\n\nHuman aware navigation\n\n\nThis node sets a navigation target and adjusts the velocity move_base/DWAPlannerROS uses to move the robot to a goal. This is achieved by using the \ndynamic reconfigure callback of the DWAPlannerROS to set the \nmax_vel_x\n, \nmax_rot_vel\n, and \nmax_trans_vel\n according\nto the distance of the robot to the closest human. As a consequence the robot will not be able to move linear or \nangular if a human is too close and will graduately slow down when approaching humans. The rotation speed is only \nadjusted to prevent the robot from spinning in place when not being able to move forward because this behaviour was \nobserved even if the clearing rotation recovery behaviour is turnd off.\n\n\nThis is implemented as an action server.\n\n\nParameters\n\n\n\n\npedestrian_locations\n \nDefault: /pedestrian_localisation/localisations\n: The topic on which the actual pedestrian \nlocations are published by the pedestrian localisation package (bayes_people_tracker/PeopleTracker).\n\n\n/human_aware_navigation/max_dist\n \nDefault: 5.0\n: maximum distance in metres to a human before altering the speed.\n\n\n/human_aware_navigation/min_dist\n \nDefault: 1.2\n: minimum distance in metres to a human. Robot stops at that distance.\n\n\n/human_aware_navigation/timeout\n \nDefault: 5.0\n: time in seconds after which speed is reseted if no human is detected any more.\n\n\n\n\nRunning\n\n\nrosrun strands_human_aware_velocity human_aware_planner_velocity.py\n\n\n\n\nTo start the actual functionality you have to use the actionlib client architecture, \ne.g. \nrosrun actionlib axclient.py /human_aware_planner_velocities\n\n* goal type: move_base/MoveBaseGoal\n\n\nHuman aware cmd_vel\n\n\nA node to adjust the velocity of the robot by taking the /cmd_vel topic and republishing it. In order for this to work \nyou have to remap the /cmd_vel output of your node or navigation stack to the input topic of this node. It relies on \nthe output of the strands_pedestrian_localisation package to provide the actual position of humans in the vicinity of \nthe robot.\n\n\nParameters\n\n\n\n\npedestrian_location\n \nDefault: /pedestrian_localisation/localisations\n: The topic on which the actual pedestrian \nlocations are published by the pedestrian localisation package (bayes_people_tracker/PeopleTracker).\n\n\ncmd_vel_in\n \nDefault: /human_aware_cmd_vel/input/cmd_vel\n: The topic to which the original /cmd_vel should be \npublished.\n\n\ncmd_vel_out\n \nDefault: /cmd_vel\n: The modified /cmd_vel.\n\n\nthreshold\n \nDefault: 3\n: Threshold in seconds to determine which person detections from the cache are too old to use.\n\n\nmax_speed\n \nDefault: 0.7\n: The maximum speed the robot should achiev.\n\n\nmax_dist\n \nDefault: 5.0\n: The distance at which the node starts taking detections of humans into account.\n\n\nmin_dist\n \nDefault: 1.5\n: The cmd_vel will be 0.0 if there is a person closer to the robot than this.\n\n\n\n\nRunning\n\n\nrosrun strands_human_aware_velocity human_aware_cmd_vel [_parameter:=value]", 
            "title": "Strands human aware navigation"
        }, 
        {
            "location": "/strands_hri/strands_human_aware_navigation/#human-aware-velocities", 
            "text": "This package adjusts the velocity of the robot in the presence of humans.", 
            "title": "Human aware velocities"
        }, 
        {
            "location": "/strands_hri/strands_human_aware_navigation/#human-aware-navigation", 
            "text": "This node sets a navigation target and adjusts the velocity move_base/DWAPlannerROS uses to move the robot to a goal. This is achieved by using the \ndynamic reconfigure callback of the DWAPlannerROS to set the  max_vel_x ,  max_rot_vel , and  max_trans_vel  according\nto the distance of the robot to the closest human. As a consequence the robot will not be able to move linear or \nangular if a human is too close and will graduately slow down when approaching humans. The rotation speed is only \nadjusted to prevent the robot from spinning in place when not being able to move forward because this behaviour was \nobserved even if the clearing rotation recovery behaviour is turnd off.  This is implemented as an action server.", 
            "title": "Human aware navigation"
        }, 
        {
            "location": "/strands_hri/strands_human_aware_navigation/#parameters", 
            "text": "pedestrian_locations   Default: /pedestrian_localisation/localisations : The topic on which the actual pedestrian \nlocations are published by the pedestrian localisation package (bayes_people_tracker/PeopleTracker).  /human_aware_navigation/max_dist   Default: 5.0 : maximum distance in metres to a human before altering the speed.  /human_aware_navigation/min_dist   Default: 1.2 : minimum distance in metres to a human. Robot stops at that distance.  /human_aware_navigation/timeout   Default: 5.0 : time in seconds after which speed is reseted if no human is detected any more.", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_hri/strands_human_aware_navigation/#running", 
            "text": "rosrun strands_human_aware_velocity human_aware_planner_velocity.py  To start the actual functionality you have to use the actionlib client architecture, \ne.g.  rosrun actionlib axclient.py /human_aware_planner_velocities \n* goal type: move_base/MoveBaseGoal", 
            "title": "Running"
        }, 
        {
            "location": "/strands_hri/strands_human_aware_navigation/#human-aware-cmd_vel", 
            "text": "A node to adjust the velocity of the robot by taking the /cmd_vel topic and republishing it. In order for this to work \nyou have to remap the /cmd_vel output of your node or navigation stack to the input topic of this node. It relies on \nthe output of the strands_pedestrian_localisation package to provide the actual position of humans in the vicinity of \nthe robot.", 
            "title": "Human aware cmd_vel"
        }, 
        {
            "location": "/strands_hri/strands_human_aware_navigation/#parameters_1", 
            "text": "pedestrian_location   Default: /pedestrian_localisation/localisations : The topic on which the actual pedestrian \nlocations are published by the pedestrian localisation package (bayes_people_tracker/PeopleTracker).  cmd_vel_in   Default: /human_aware_cmd_vel/input/cmd_vel : The topic to which the original /cmd_vel should be \npublished.  cmd_vel_out   Default: /cmd_vel : The modified /cmd_vel.  threshold   Default: 3 : Threshold in seconds to determine which person detections from the cache are too old to use.  max_speed   Default: 0.7 : The maximum speed the robot should achiev.  max_dist   Default: 5.0 : The distance at which the node starts taking detections of humans into account.  min_dist   Default: 1.5 : The cmd_vel will be 0.0 if there is a person closer to the robot than this.", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_hri/strands_human_aware_navigation/#running_1", 
            "text": "rosrun strands_human_aware_velocity human_aware_cmd_vel [_parameter:=value]", 
            "title": "Running"
        }, 
        {
            "location": "/strands_hri/strands_human_following/", 
            "text": "Package Description\n\n\nThis package contains simple human following\n\n\nThe main function of this package is making the robot follow humans based on\nthe information from passive people detection.\n\n\nThe State Machine inside the package contains:\n\n\nStart stage - Wandering stage - Following stage - Local-searching stage\n\n\nGetting Start\n\n\nBefore you can launch this package, you need:\n\n\nroscore\nstrands robot bringup \nstrands robot camera\nstrands ui\nstrands navigation\nperception people tracker\n\n\n\nTo start this module, simply launch:\n\n\nroslaunch strands_human_following simple_follow.launch [wandering_mode, config_file]\nrosrun strands_human_following simple_follow_client.py\n\n\n\nThere are two parameters that can be changed:\n\n wandering_mode [wait, normal] with wait as default\n\n config_file where the configuration of this module is stored. It is stored in /strands_hri/strands_human_following/conf/default.yaml by default\n\n\nConfigurations\n\n\nTo change the settings of this package, find:\n\n\n/strands_hri/strands_human_following/conf/default.yaml\n\n\n\nParameters you can change are listed below:\n\n\nwander_area            : (polygon)\nfollow_area            : (polygon)\nmax_t_frames(tolerence): (int)\nalpha                  : (double)\ndistance(safe distance): (double)\n\n\n\nWander area is the area where the robot will wander around when there is no\nperson detected. Follow area is the area where the robot will follow a person;\nThe robot will not follow outside the follow area even when there are persons\ndetected. Both wander area and follow area are polygon which are defined by\npoints [x,y].\n\n\nMax_t_frames is the maximum frames where the robot searches a person after it\nloses the person. Alpha is the factor of how far the pan tilt unit rotates when the robot follows a person. Distance is the distance between the robot and a person when the robot follows the person. \n\n\nTasks and Routines\n\n\nThere is an example called routine_example.py of how to create a task and add\nit to scheduler. First of all,\n\n\nroslaunch task_executor task-scheduler.launch\n\n\n\nmust be up and running. Create the task as follows:\n\n\ntask = Task(start_node_id=[WayPoint], max_duration=[DurationOfTheTask], action='simple_follow')\n\ntask_utils.add_int_argument(task, [DurationOfHumanFollowing])\n\n\n\n\nTask_utils is needed since SimpleFollow.action has one integer argument to\ndetermine how long the human following runs.\n\n\nProblems\n\n\nCan not catch up with human's normal pace.\n\n\nCan not follow when people walk towards or pass by the robot.", 
            "title": "Strands human following"
        }, 
        {
            "location": "/strands_hri/strands_human_following/#package-description", 
            "text": "This package contains simple human following  The main function of this package is making the robot follow humans based on\nthe information from passive people detection.  The State Machine inside the package contains:  Start stage - Wandering stage - Following stage - Local-searching stage", 
            "title": "Package Description"
        }, 
        {
            "location": "/strands_hri/strands_human_following/#getting-start", 
            "text": "Before you can launch this package, you need:  roscore\nstrands robot bringup \nstrands robot camera\nstrands ui\nstrands navigation\nperception people tracker  To start this module, simply launch:  roslaunch strands_human_following simple_follow.launch [wandering_mode, config_file]\nrosrun strands_human_following simple_follow_client.py  There are two parameters that can be changed:  wandering_mode [wait, normal] with wait as default  config_file where the configuration of this module is stored. It is stored in /strands_hri/strands_human_following/conf/default.yaml by default", 
            "title": "Getting Start"
        }, 
        {
            "location": "/strands_hri/strands_human_following/#configurations", 
            "text": "To change the settings of this package, find:  /strands_hri/strands_human_following/conf/default.yaml  Parameters you can change are listed below:  wander_area            : (polygon)\nfollow_area            : (polygon)\nmax_t_frames(tolerence): (int)\nalpha                  : (double)\ndistance(safe distance): (double)  Wander area is the area where the robot will wander around when there is no\nperson detected. Follow area is the area where the robot will follow a person;\nThe robot will not follow outside the follow area even when there are persons\ndetected. Both wander area and follow area are polygon which are defined by\npoints [x,y].  Max_t_frames is the maximum frames where the robot searches a person after it\nloses the person. Alpha is the factor of how far the pan tilt unit rotates when the robot follows a person. Distance is the distance between the robot and a person when the robot follows the person.", 
            "title": "Configurations"
        }, 
        {
            "location": "/strands_hri/strands_human_following/#tasks-and-routines", 
            "text": "There is an example called routine_example.py of how to create a task and add\nit to scheduler. First of all,  roslaunch task_executor task-scheduler.launch  must be up and running. Create the task as follows:  task = Task(start_node_id=[WayPoint], max_duration=[DurationOfTheTask], action='simple_follow')\n\ntask_utils.add_int_argument(task, [DurationOfHumanFollowing])  Task_utils is needed since SimpleFollow.action has one integer argument to\ndetermine how long the human following runs.", 
            "title": "Tasks and Routines"
        }, 
        {
            "location": "/strands_hri/strands_human_following/#problems", 
            "text": "Can not catch up with human's normal pace.  Can not follow when people walk towards or pass by the robot.", 
            "title": "Problems"
        }, 
        {
            "location": "/strands_hri/strands_simple_follow_me/", 
            "text": "Follow me\n\n\nThis package provides a simple node that uses the pedestrian tracker results from the strands_perception_people repository to follow the closest person in front of the robot.\nIn order to use it you have to run scitos_2d_nav.launch from the scitos_2d_navigation repository with a pre-recorded or empty map. After words run the perception_people_launch people_tracker_no_HOG.launch and this node on the robot.", 
            "title": "Strands simple follow me"
        }, 
        {
            "location": "/strands_hri/strands_simple_follow_me/#follow-me", 
            "text": "This package provides a simple node that uses the pedestrian tracker results from the strands_perception_people repository to follow the closest person in front of the robot.\nIn order to use it you have to run scitos_2d_nav.launch from the scitos_2d_navigation repository with a pre-recorded or empty map. After words run the perception_people_launch people_tracker_no_HOG.launch and this node on the robot.", 
            "title": "Follow me"
        }, 
        {
            "location": "/strands_hri/strands_visualise_speech/", 
            "text": "Visualise speech package\n\n\nThis package provides a control node for the head lights of the robot to compensate for the fact that it does not have a \nmouth for speaker identification. At the current stage the node uses pulseaudio to read the output levels of the \nsoundcard and translates this to control commands for the head lights. The louder the more lights are illuminated.\n\n\nInstallation\n\n\n\n\nmake sure that the submodules are there before running \ncatkin_make\n\n\ncd strands_hri\n\n\ngit submodule init\n\n\ngit submodule update\n\n\n\n\nUsage\n\n\nCannot be run remotely. Needs to access the robots hardware.\n\n\nThis node is a action server and needs a goal to start doing anything. The only parameter of the goal is the runtime of \nthe system:\n* seconds: run time in seconds. 0 for infinite (until preempted)\n\n\nRun with: \nrosrun strands_visualise_speech sound_to_light.py\n\nor better: have a look at the strands_hri_utils package to run together with the convenience action server provided in \nthere.\n\n\nTroubleshooting\n:\n\n The audio grabber assumes that your sound sink is called \nalsa_output.usb-Burr-Brown_from_TI_USB_Audio_CODEC-00-CODEC.analog-stereo\n \nif it is not, the node won't do anything. You can check if the name is correct by looking at the output. The node will \nlist all available sinks when started. Re-run the node with the parameter \n_sink_name:=\nyour sink\n. If the output states: \n\nsetting up peak recording using \nyour sink name\n it is configured correctly.\n\n If the programme states \nConnection failed\n, make sure that no other user is logged in, hogging the sound resource.\n\n\nProblems\n:\n\n The light control is very slow and therefore the lights are a bit delayed.\n\n The publish rate had to be artificially reduced to 4hz because it would otherwise interfer with all the hardware \ncontrol commands.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_hri/strands_visualise_speech/#visualise-speech-package", 
            "text": "This package provides a control node for the head lights of the robot to compensate for the fact that it does not have a \nmouth for speaker identification. At the current stage the node uses pulseaudio to read the output levels of the \nsoundcard and translates this to control commands for the head lights. The louder the more lights are illuminated.", 
            "title": "Visualise speech package"
        }, 
        {
            "location": "/strands_hri/strands_visualise_speech/#installation", 
            "text": "make sure that the submodules are there before running  catkin_make  cd strands_hri  git submodule init  git submodule update", 
            "title": "Installation"
        }, 
        {
            "location": "/strands_hri/strands_visualise_speech/#usage", 
            "text": "Cannot be run remotely. Needs to access the robots hardware.  This node is a action server and needs a goal to start doing anything. The only parameter of the goal is the runtime of \nthe system:\n* seconds: run time in seconds. 0 for infinite (until preempted)  Run with:  rosrun strands_visualise_speech sound_to_light.py \nor better: have a look at the strands_hri_utils package to run together with the convenience action server provided in \nthere.  Troubleshooting :  The audio grabber assumes that your sound sink is called  alsa_output.usb-Burr-Brown_from_TI_USB_Audio_CODEC-00-CODEC.analog-stereo  \nif it is not, the node won't do anything. You can check if the name is correct by looking at the output. The node will \nlist all available sinks when started. Re-run the node with the parameter  _sink_name:= your sink . If the output states:  setting up peak recording using  your sink name  it is configured correctly.  If the programme states  Connection failed , make sure that no other user is logged in, hogging the sound resource.  Problems :  The light control is very slow and therefore the lights are a bit delayed.  The publish rate had to be artificially reduced to 4hz because it would otherwise interfer with all the hardware \ncontrol commands.", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_hri/strands_visualise_speech/pulse/", 
            "text": "These are simple ctypes bindings for pulseaudio.\n\n\nGenerated from pulseaudio 1.1 on 24/11/2011.", 
            "title": "Pulse"
        }, 
        {
            "location": "/strands_morse/", 
            "text": "strands_morse\n\n\nA MORSE-based simulation for the STRANDS project\n\n\nPrerequisites\n\n\nFor running the simulation you would need:\n\n\n\n\nMORSE\n \n\n\nBlender\n\n\nROS\n\n\n\n\nWe recommend to install the morse-blender-bundle from our debian server:\n\n\n\n\nSet-up instructions: \nUsing the STRANDS repository\n\n\nInstall the bundle: \nsudo apt-get install morse-blender-2.65-py-3.3\n\n\nSource the environment: \nsource /opt/morse-blender-2.65-py-3.3/.bashrc\n \nYou might want to add this to your ~/.bashrc\n\n\nCheck if morse has installed correctly: \nmorse check\n\n\n\n\nPlease refer to the respective installation guides to install them on your system. \n\n\nThe current software has been tested under the following configuration:\n\n\n\n\nMORSE (strands-project/morse)\n\n\nBlender 2.63a\n\n\nROS Groovy\n\n\nPython 3.2.3 (3.3.0)\n\n\nUbuntu 12.04 LTS\n\n\n\n\nPlease note: Using a depth camera requires Python 3.3.0 and a corresponding Blender version (\n2.65). \n\n\nGetting Started\n\n\nStart some terminals and run the commands below:\n\n\n\n\n\n\nFire up roscore:\n\n\n$ roscore\n\n\n\n\n\n\n\nRun the MORSE simulation:\n\n\n$ roslaunch strands_morse bham_cs_morse.launch\n\n\n\n\n\n\n\n(optional) Launch the 2D navigation:\n\n\n$ roslaunch strands_morse bham_cs_nav2d.launch\n\n\n\n\n\n\n\n(optional) To visualize the environment model in RVIZ run \n\n\n$ rosrun rviz rviz\n\n$ roslaunch strands_morse bham_cs_rviz.launch\n\n\n\n\n\n\n\nand add a new display of type \nRobotModel\n in RVIZ. Set the robot_description to\n   \n/env/env_description\n and TF prefix to \nenv\n. (requires \nstrands_utils\n)\n\n\nThe commands above use the lower ground floor of the Computer Science building\nby default. For simulating a different level of the building please run:\n\n\n    $ roslaunch strands_morse bham_cs_morse.launch env:=cs_1\n\n\n\nOther available environments are: \ncs_lg\n, \ncs_ug\n, \ncs_1\n, \ncs_2\n, \ncs\n (for the whole building)\n\n\nSimilarly, you have to run the navigation and visualization with an extra argument as follows:\n\n\n    $ roslaunch strands_morse bham_cs_nav2d.launch env:=cs_1\n\n    $ roslaunch strands_morse bham_cs_rviz.launch env:=cs_1\n\n\n\n\n\nTo start run the MORSE simulation of the TUM kitchen with a ScitosA5:\n\n\n   $ roslaunch strands_morse tum_kitchen_morse.launch\n\n\n\nAlternatively:\n\n\n   $ roslaunch strands_morse tum_kitchen_morse.launch env:=tum_kitchen_WITHOUT_CAMERAS\n\n\n\n\n\nThe Scitos robot model is equipped with a video, depth and semantic camera by\ndefault. \n\n\nIf you wish to start the robot without any camera provide\n\nScitosa5.WITHOUT_CAMERAS\n in the robot's constructor (see example below).\nStarting the robot without any camera is useful to run MORSE in\n\nfast_mode\n.\n\n\nIf you only want to disable the depth camera, provide\n\nScitosa5.WITHOUT_DEPTHCAM\n in the constructor. This might be useful if you run\nMORSE on MacOSX, because users have reported problems when using a depth\ncamera.\n\n\nExample usage in the MORSE builder script:\n\n\n\n\n\n\nwith cameras:\n\n\nrobot = Scitosa5()\n# Alterntively\n# robot = Scitosa5(Scitosa5.WITH_CAMERAS)\n\n\n\n\n\n\n\nwithout cameras\n\n\nrobot = Scitosa5(Scitosa5.WITHOUT_CAMERAS)\n\n\n\n\n\n\n\nwithout depth camera\n\n\nrobot = Scitosa5(Scitosa5.WITHOUT_DEPTHCAMS)\n\n\n\n\n\n\n\nwith OpenNI stack generating point clouds and images, be sure to launch \nroslaunch strands_morse generate_camera_topics.launch\n and\n\n\nrobot = Scitosa5(Scitosa5.WITH_OPENNI)", 
            "title": "Home"
        }, 
        {
            "location": "/strands_morse/#strands_morse", 
            "text": "A MORSE-based simulation for the STRANDS project", 
            "title": "strands_morse"
        }, 
        {
            "location": "/strands_morse/#prerequisites", 
            "text": "For running the simulation you would need:   MORSE    Blender  ROS   We recommend to install the morse-blender-bundle from our debian server:   Set-up instructions:  Using the STRANDS repository  Install the bundle:  sudo apt-get install morse-blender-2.65-py-3.3  Source the environment:  source /opt/morse-blender-2.65-py-3.3/.bashrc   You might want to add this to your ~/.bashrc  Check if morse has installed correctly:  morse check   Please refer to the respective installation guides to install them on your system.   The current software has been tested under the following configuration:   MORSE (strands-project/morse)  Blender 2.63a  ROS Groovy  Python 3.2.3 (3.3.0)  Ubuntu 12.04 LTS   Please note: Using a depth camera requires Python 3.3.0 and a corresponding Blender version ( 2.65).", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/strands_morse/#getting-started", 
            "text": "Start some terminals and run the commands below:    Fire up roscore:  $ roscore    Run the MORSE simulation:  $ roslaunch strands_morse bham_cs_morse.launch    (optional) Launch the 2D navigation:  $ roslaunch strands_morse bham_cs_nav2d.launch    (optional) To visualize the environment model in RVIZ run   $ rosrun rviz rviz\n\n$ roslaunch strands_morse bham_cs_rviz.launch    and add a new display of type  RobotModel  in RVIZ. Set the robot_description to\n    /env/env_description  and TF prefix to  env . (requires  strands_utils )  The commands above use the lower ground floor of the Computer Science building\nby default. For simulating a different level of the building please run:      $ roslaunch strands_morse bham_cs_morse.launch env:=cs_1  Other available environments are:  cs_lg ,  cs_ug ,  cs_1 ,  cs_2 ,  cs  (for the whole building)  Similarly, you have to run the navigation and visualization with an extra argument as follows:      $ roslaunch strands_morse bham_cs_nav2d.launch env:=cs_1\n\n    $ roslaunch strands_morse bham_cs_rviz.launch env:=cs_1   To start run the MORSE simulation of the TUM kitchen with a ScitosA5:     $ roslaunch strands_morse tum_kitchen_morse.launch  Alternatively:     $ roslaunch strands_morse tum_kitchen_morse.launch env:=tum_kitchen_WITHOUT_CAMERAS   The Scitos robot model is equipped with a video, depth and semantic camera by\ndefault.   If you wish to start the robot without any camera provide Scitosa5.WITHOUT_CAMERAS  in the robot's constructor (see example below).\nStarting the robot without any camera is useful to run MORSE in fast_mode .  If you only want to disable the depth camera, provide Scitosa5.WITHOUT_DEPTHCAM  in the constructor. This might be useful if you run\nMORSE on MacOSX, because users have reported problems when using a depth\ncamera.  Example usage in the MORSE builder script:    with cameras:  robot = Scitosa5()\n# Alterntively\n# robot = Scitosa5(Scitosa5.WITH_CAMERAS)    without cameras  robot = Scitosa5(Scitosa5.WITHOUT_CAMERAS)    without depth camera  robot = Scitosa5(Scitosa5.WITHOUT_DEPTHCAMS)    with OpenNI stack generating point clouds and images, be sure to launch  roslaunch strands_morse generate_camera_topics.launch  and  robot = Scitosa5(Scitosa5.WITH_OPENNI)", 
            "title": "Getting Started"
        }, 
        {
            "location": "/strands_morse/bham/architecture/", 
            "text": "These are architectural floor plans of the CS building. They are a little out of date, but appear to be to scale.", 
            "title": "Architecture"
        }, 
        {
            "location": "/strands_morse/bham/", 
            "text": "bham\n\n\nThe package provides a simulation of the CS building at Birmingham. This comprises of a Blender model of the building (data/CS_Building.blend) and a couple of Morse actuators to simulate the four-level lift.\n\n\nTo test it out, get bham on your ROS path and run:\n\n\nrosrun bham simulator.sh\n\n\nand after that has loaded up:\n\n\nrosrun bham lift_controller.py.\n\n\nIn the current simulation a B21 is sitting in the centre of floor 1. It can be controlled through ROS using /b21/cmd_vel. \n\n\nThe lift is controlled by publishing a std_msgs/Bool message on  \"/lift_sim/[command|call]floor\", where floor is [B|G|1|2]. These topics correspond to a user pressing the \"call\" button on the outside of the lift, or the \"command\" button on the inside of the lift to goto to \"floor\". The lift travels in the order requested, and waits 8 seconds before closing the door.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_morse/bham/#bham", 
            "text": "The package provides a simulation of the CS building at Birmingham. This comprises of a Blender model of the building (data/CS_Building.blend) and a couple of Morse actuators to simulate the four-level lift.  To test it out, get bham on your ROS path and run:  rosrun bham simulator.sh  and after that has loaded up:  rosrun bham lift_controller.py.  In the current simulation a B21 is sitting in the centre of floor 1. It can be controlled through ROS using /b21/cmd_vel.   The lift is controlled by publishing a std_msgs/Bool message on  \"/lift_sim/[command|call]floor\", where floor is [B|G|1|2]. These topics correspond to a user pressing the \"call\" button on the outside of the lift, or the \"command\" button on the inside of the lift to goto to \"floor\". The lift travels in the order requested, and waits 8 seconds before closing the door.", 
            "title": "bham"
        }, 
        {
            "location": "/strands_morse/bham/real_TO-BE-MOVED/", 
            "text": "These maps come from real data acquired using a B21 robot and a Sick laser \n\n\nThey are pretty raw, and inability of the laser scanner to see glass is very evident, especially in floor 1 where the glass barrier is not seen, but instead the outside world is seen through the building front doors.", 
            "title": "real TO BE MOVED"
        }, 
        {
            "location": "/strands_morse/wiki/BHAM-Morse/", 
            "text": "BHAM CS Building Simulation with Moving Lift\n\n\nSetup\n\n\nThe following additional setup is required in addition to the STRANDS standard Morse install.\n\n\nPySide\n\n\nPySide is needed for the control manager GUI. It must be installed from source for the STRANDS install of Python 3.3:\n\n\n1) Install system depends\n\n\nsudo apt-get install build-essential git cmake libqt4-dev libphonon-dev libxml2-dev libxslt1-dev qtmobility-dev\n\n\n\n\n\n2) Install Python3.3 setuptools:\n\n\ncd /opt/strands/tmp  # OR THE PLACE YOU CHOSE\nwget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py\npython3.3 ez_setup.py\n\n\n\n\n3) Build \n Install PySide (this takes a while)\n\n\n\nwget https://pypi.python.org/packages/source/P/PySide/PySide-1.2.1.tar.gz\ntar -xvzf PySide-1.2.1.tar.gz\ncd PySide-1.2.1\npython3.3 setup.py install --qmake=/usr/bin/qmake-qt4\n\n\n\n\nUsing the Lift\n\n\n1) Start Morse with all floors of the CS building:\n\n\nroslaunch strands_morse bham_cs_morse.launch env:=cs\n\n\n\n\n2) Start the 'Lift Controller' - a small program that maintains a queue of lift calls and commands through ROS, and passes them on to Morse.\n\n\nrosrun strands_morse lift_controller.py\n\n\n\n\n3) Launch the control interface\n\n\nrosrun strands_morse bham_control_gui.py\n\n\n\n\nThe control GUI allows you to\n\n Hide / Show different floors of the building - Top row of buttons\n\n Call the lift to a floor (ie. press the button outside the lift) - Second row of buttons\n* Command the lift to a floor (ie. press a button inside the lift) - Bottom row of buttons.\n\n\nHow the Lift works\n\n\nThe lift acts as a \"robot\" within the environment, with \nmorse.core.actuator.Actuator\ns to control the doors and the lift platform. These actuators provide services to open and close doors, and to move the lift platform between floors, exposed as  Morse RPC functions using the basic morse socketstream 'middleware'.\n\n\nThe 'Lift Controller' uses the morse middleware through the pymorse library to command the lift between floors. It receives floor requests through ROS topics, subscribing to:\n\n\n/lift_sim/[command|call]floor : std_messages/Bool\n\n\n\n\nwhere floor is [B|G|1|2]. For example, \n/lift_sim/callG\n.\n\n\nThe topics correspond to a user pressing the \"call\" button on the outside of the lift on floor, or the \"command\" button on the inside of the lift to got to \"floor\".\n\n\nThe controller maintains a simple queue and moves the lift in the order requested, and waits a minimum of 8 seconds before closing the door between floor visits.", 
            "title": "BHAM Morse"
        }, 
        {
            "location": "/strands_morse/wiki/BHAM-Morse/#bham-cs-building-simulation-with-moving-lift", 
            "text": "", 
            "title": "BHAM CS Building Simulation with Moving Lift"
        }, 
        {
            "location": "/strands_morse/wiki/BHAM-Morse/#setup", 
            "text": "The following additional setup is required in addition to the STRANDS standard Morse install.", 
            "title": "Setup"
        }, 
        {
            "location": "/strands_morse/wiki/BHAM-Morse/#pyside", 
            "text": "PySide is needed for the control manager GUI. It must be installed from source for the STRANDS install of Python 3.3:  1) Install system depends  sudo apt-get install build-essential git cmake libqt4-dev libphonon-dev libxml2-dev libxslt1-dev qtmobility-dev  2) Install Python3.3 setuptools:  cd /opt/strands/tmp  # OR THE PLACE YOU CHOSE\nwget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py\npython3.3 ez_setup.py  3) Build   Install PySide (this takes a while)  \nwget https://pypi.python.org/packages/source/P/PySide/PySide-1.2.1.tar.gz\ntar -xvzf PySide-1.2.1.tar.gz\ncd PySide-1.2.1\npython3.3 setup.py install --qmake=/usr/bin/qmake-qt4", 
            "title": "PySide"
        }, 
        {
            "location": "/strands_morse/wiki/BHAM-Morse/#using-the-lift", 
            "text": "1) Start Morse with all floors of the CS building:  roslaunch strands_morse bham_cs_morse.launch env:=cs  2) Start the 'Lift Controller' - a small program that maintains a queue of lift calls and commands through ROS, and passes them on to Morse.  rosrun strands_morse lift_controller.py  3) Launch the control interface  rosrun strands_morse bham_control_gui.py  The control GUI allows you to  Hide / Show different floors of the building - Top row of buttons  Call the lift to a floor (ie. press the button outside the lift) - Second row of buttons\n* Command the lift to a floor (ie. press a button inside the lift) - Bottom row of buttons.", 
            "title": "Using the Lift"
        }, 
        {
            "location": "/strands_morse/wiki/BHAM-Morse/#how-the-lift-works", 
            "text": "The lift acts as a \"robot\" within the environment, with  morse.core.actuator.Actuator s to control the doors and the lift platform. These actuators provide services to open and close doors, and to move the lift platform between floors, exposed as  Morse RPC functions using the basic morse socketstream 'middleware'.  The 'Lift Controller' uses the morse middleware through the pymorse library to command the lift between floors. It receives floor requests through ROS topics, subscribing to:  /lift_sim/[command|call]floor : std_messages/Bool  where floor is [B|G|1|2]. For example,  /lift_sim/callG .  The topics correspond to a user pressing the \"call\" button on the outside of the lift on floor, or the \"command\" button on the inside of the lift to got to \"floor\".  The controller maintains a simple queue and moves the lift in the order requested, and waits a minimum of 8 seconds before closing the door between floor visits.", 
            "title": "How the Lift works"
        }, 
        {
            "location": "/strands_morse/wiki/Home/", 
            "text": "MORSE on OSX using Homebrew\n\n\n\n\nMORSE on Ubuntu\n\n\n\n\n\n\nBHAM Morse with Lift", 
            "title": "Home"
        }, 
        {
            "location": "/strands_morse/wiki/MORSE-on-OSX-using-Homebrew/", 
            "text": "Installing\n\n\nTonight I installed and ran MORSE (up to the first tutorial at least) under OSX. This was done using Homebrew. The steps I took were as follows\n\n\n\n\n\n\nDownload Blender from the \nBlender download page\n, and copy the apps to /Applications. I used Blender 2.67a 64 bit. \n\n\n\n\n\n\nAs that Blender uses Python 3.3.0 you need to install the exact matching version for your system. I did this using \nHomebrew\n. You have to go through some extra steps as the current version of Python is 3.3.1. The hash for the checkout command comes from \nbrew versions python3\n.\n\n\n\n\n\n\n$ cd /usr/local\n$ git checkout 864e9f1 /usr/local/Library/Formula/python3.rb\n$ brew install python3\n\n\n\n\nThen make sure pkg-config can find this install\n\n\n$ export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/Cellar/python3/3.3.0/Frameworks/Python.framework/Versions/3.3/lib/pkgconfig\n\n\n\n\nI also had to remove my Python 2.7 packages from my environment as cmake failed:\n\n\nunset PYTHONPATH\n\n\n\n\n\n\nI installed MORSE using the \nmanual installation instructions\n. However, I needed a bit of a hack to get this working. First, the compiled Python modules wouldn't link against the Python libs, so I edit line 261 of \nconfig/FindPythonLibs.cmake\n to be the following\n\n\n\n\nTARGET_LINK_LIBRARIES(${_NAME} python3.3m)\n\n\n\n\nThere's probably a more elegant way of fixing this, but I didn't look for it. I then ran \nccmake\n as follows\n\n\n$ mkdir build \n cd build\n$ ccmake -DPYTHON_INCLUDE_DIR=/usr/local/Cellar/python3/3.3.0/Frameworks/Python.framework/Versions/3.3/include/python3.3m -DPYTHON_LIBRARY=/usr/local/Cellar/python3/3.3.0/Frameworks/Python.framework -DBUILD_ROS_SUPPORT=ON -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/opt/morse -DPYTHON_EXECUTABLE=/usr/local/bin/python3.3  -DCMAKE_MODULE_LINKER_FLAGS=`pkg-config --libs python3` ..\n$ sudo make install\n\n\n\n\n\n\nTo allow MORSE to find Blender I set \nMORSE_BLENDER\n as follows:\n\n\n\n\n$ export MORSE_BLENDER=/Applications/blender.app/Contents/MacOS/blender\n\n\n\n\n\n\nAt this point \nmorse check\n should succeed (despite some minor Blender complaints), and you can run the \nbasic tutorial\n:\n\n\n\n\n$ morse run /opt/share/morse/examples/tutorials/tutorial-1-sockets.py \n\n\n\n\nand in another terminal\n\n\n$ telnet localhost 4000\n\n\n\n\nfollowed by\n\n\nid1 atrv.motion set_speed [2, -1]\n\n\n\n\nThis should make your robot go in a circle in the simulation.\n\n\nROS Integration\n\n\nAlthough the above worked for me, I couldn't run the CS sim with ROS integration. This seemed to be because Python 3 couldn't load some of the installed Python 2.7 packages (I installed ROS from source following instructions \nhere\n). To fix my issues I had to do the following\n\n\n$ pip3 install PyYAML rospkg catkin_pkg\n\n\n\n\nI have then been running with the following PYTHONPATH (the first seems necessary as the ROS install doesn't do things quite right).\n\n\n$ export PYTHONPATH=/opt/ros/groovy/lib/python2.7/site-packages:/usr/local/lib/python2.7/site-packages\n\n\n\n\nI also had to change \nstrands_sim/morse_config.py\n so that the first line launches python3 from the location Homebrew installed it, i.e.\n\n\n#!/usr/local/bin/python3\n\n\n\n\nI suspect there's a more elegant way of doing this.\n\n\nCurrent Issues\n\n\nThe rendering of Blender/MORSE seems a bit odd on my 15\" rMBP as I only see the simulation in a quarter of the window. This problem goes away when running on an external display though. Logged here: https://github.com/laas/morse/issues/383\n\n\nProblems\n\n\nIf you see an error like the following\n\n\nTraceback (most recent call last):\n  File \n/opt/bin/morse\n, line 825, in \nmodule\n\n    args.func(args)\n  File \n/opt/bin/morse\n, line 645, in do_check\n    check_setup()\n  File \n/opt/bin/morse\n, line 275, in check_setup\n    blender_prefix = os.path.join(os.path.normpath(prefix), os.path.normpath(\nbin/blender\n))\nNameError: global name 'prefix' is not defined\n\n\n\n\nit means that MORSE can't find Blender. Set \nMORSE_BLENDER\n as above.", 
            "title": "MORSE on OSX using Homebrew"
        }, 
        {
            "location": "/strands_morse/wiki/MORSE-on-OSX-using-Homebrew/#installing", 
            "text": "Tonight I installed and ran MORSE (up to the first tutorial at least) under OSX. This was done using Homebrew. The steps I took were as follows    Download Blender from the  Blender download page , and copy the apps to /Applications. I used Blender 2.67a 64 bit.     As that Blender uses Python 3.3.0 you need to install the exact matching version for your system. I did this using  Homebrew . You have to go through some extra steps as the current version of Python is 3.3.1. The hash for the checkout command comes from  brew versions python3 .    $ cd /usr/local\n$ git checkout 864e9f1 /usr/local/Library/Formula/python3.rb\n$ brew install python3  Then make sure pkg-config can find this install  $ export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/Cellar/python3/3.3.0/Frameworks/Python.framework/Versions/3.3/lib/pkgconfig  I also had to remove my Python 2.7 packages from my environment as cmake failed:  unset PYTHONPATH   I installed MORSE using the  manual installation instructions . However, I needed a bit of a hack to get this working. First, the compiled Python modules wouldn't link against the Python libs, so I edit line 261 of  config/FindPythonLibs.cmake  to be the following   TARGET_LINK_LIBRARIES(${_NAME} python3.3m)  There's probably a more elegant way of fixing this, but I didn't look for it. I then ran  ccmake  as follows  $ mkdir build   cd build\n$ ccmake -DPYTHON_INCLUDE_DIR=/usr/local/Cellar/python3/3.3.0/Frameworks/Python.framework/Versions/3.3/include/python3.3m -DPYTHON_LIBRARY=/usr/local/Cellar/python3/3.3.0/Frameworks/Python.framework -DBUILD_ROS_SUPPORT=ON -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/opt/morse -DPYTHON_EXECUTABLE=/usr/local/bin/python3.3  -DCMAKE_MODULE_LINKER_FLAGS=`pkg-config --libs python3` ..\n$ sudo make install   To allow MORSE to find Blender I set  MORSE_BLENDER  as follows:   $ export MORSE_BLENDER=/Applications/blender.app/Contents/MacOS/blender   At this point  morse check  should succeed (despite some minor Blender complaints), and you can run the  basic tutorial :   $ morse run /opt/share/morse/examples/tutorials/tutorial-1-sockets.py   and in another terminal  $ telnet localhost 4000  followed by  id1 atrv.motion set_speed [2, -1]  This should make your robot go in a circle in the simulation.", 
            "title": "Installing"
        }, 
        {
            "location": "/strands_morse/wiki/MORSE-on-OSX-using-Homebrew/#ros-integration", 
            "text": "Although the above worked for me, I couldn't run the CS sim with ROS integration. This seemed to be because Python 3 couldn't load some of the installed Python 2.7 packages (I installed ROS from source following instructions  here ). To fix my issues I had to do the following  $ pip3 install PyYAML rospkg catkin_pkg  I have then been running with the following PYTHONPATH (the first seems necessary as the ROS install doesn't do things quite right).  $ export PYTHONPATH=/opt/ros/groovy/lib/python2.7/site-packages:/usr/local/lib/python2.7/site-packages  I also had to change  strands_sim/morse_config.py  so that the first line launches python3 from the location Homebrew installed it, i.e.  #!/usr/local/bin/python3  I suspect there's a more elegant way of doing this.", 
            "title": "ROS Integration"
        }, 
        {
            "location": "/strands_morse/wiki/MORSE-on-OSX-using-Homebrew/#current-issues", 
            "text": "The rendering of Blender/MORSE seems a bit odd on my 15\" rMBP as I only see the simulation in a quarter of the window. This problem goes away when running on an external display though. Logged here: https://github.com/laas/morse/issues/383", 
            "title": "Current Issues"
        }, 
        {
            "location": "/strands_morse/wiki/MORSE-on-OSX-using-Homebrew/#problems", 
            "text": "If you see an error like the following  Traceback (most recent call last):\n  File  /opt/bin/morse , line 825, in  module \n    args.func(args)\n  File  /opt/bin/morse , line 645, in do_check\n    check_setup()\n  File  /opt/bin/morse , line 275, in check_setup\n    blender_prefix = os.path.join(os.path.normpath(prefix), os.path.normpath( bin/blender ))\nNameError: global name 'prefix' is not defined  it means that MORSE can't find Blender. Set  MORSE_BLENDER  as above.", 
            "title": "Problems"
        }, 
        {
            "location": "/strands_morse/wiki/MORSE-on-Ubuntu/", 
            "text": "Simple install script for MORSE and ROS for STRANDS\n\n\nThe easiest way to install MORSE is to use the install script \nhere\n. This gets all of the necessary packages, and installs Python 3.3, Blender, and MORSE in a user directory. It also installs ROS Groovy in /opt.\n\n\n\n\nDownload the script \n setup.sh \n\n\nCreate a directory in home under which packages will be installed. The directory will later contain the sub-directories bin/ include/ lib/ opt/ share/ src/ tmp/, following the usual filesystem structure.\n\n\nOpen a terminal, cd to the newly created directory and execute the install script. During install, your password will be requested to install ROS in /opt using the official packages.\nAfter all the components are installed, your directory will have a .bashrc file inside it. Sourcing this file sets the PATH etc in the shell session. The install script sets up automatic sourcing of this file at the end of your ~/.bashrc file.\n\n\n\n\nOpen a new terminal and run\n\n\nmorse check\n\n\n\n\n\n\nThis should tell you that your environment is setup correctly. :-)\n\n\n\n\nget yourself a github account (https://github.com) and send your account name to marc@hanheide.net\n\n\n\n\nThe manual way\n\n\n...\n\n\nIssues and solutions\n\n\n\n\nCannot find a MORSE environment or simulation scene matching \nstrands_sim\n!\n\n\nadd the path to \nstrands_sim\n in \n~/.morse/config\n. This is done by going into the strands_sim directory, \n\n\nTo easily achieve this do:\n\n\nrun \nmorse create test\n: this will create the setupfile you need by creating a dummy morse project.\n\n\nYou can then remove the newly created created dummy project by \nrm -rf test/\n\n\nNow its time to edit the config file that was created to work with your path to the strands morse project, do this by: \ngedit ~/.morse/config\n\n\nchange the path and name to fit your installation in \n.morse/config\n. For me this file looks like this:\n    \n[sites]\n    strands_sim = /home/johane/strands/strands_morse/strands_sim\n\n\n\n\n\n\nRun bham morse sim with ros:\n\n\nCreate a directory like: \nstrands/sim/src\n and cd to that\n\n\nclone the git repository to this location \ngit clone git://github.com/strands-project/strands_morse.git\n and \ncd strands_morse\n\n\ncreate a src dir and move everything into it: \nmkdir src \n mv * src\n\n\ngo to strands/sim/src and run \ncatkin_init_workspace\n\n\ngo to strands/sim and run \ncatkin_make\n\n\nsource the ros environment: \nsource devel/setup.bash\n\n\nrun it: \nrosrun strands_sim simulator.sh", 
            "title": "MORSE on Ubuntu"
        }, 
        {
            "location": "/strands_morse/wiki/MORSE-on-Ubuntu/#simple-install-script-for-morse-and-ros-for-strands", 
            "text": "The easiest way to install MORSE is to use the install script  here . This gets all of the necessary packages, and installs Python 3.3, Blender, and MORSE in a user directory. It also installs ROS Groovy in /opt.   Download the script   setup.sh   Create a directory in home under which packages will be installed. The directory will later contain the sub-directories bin/ include/ lib/ opt/ share/ src/ tmp/, following the usual filesystem structure.  Open a terminal, cd to the newly created directory and execute the install script. During install, your password will be requested to install ROS in /opt using the official packages.\nAfter all the components are installed, your directory will have a .bashrc file inside it. Sourcing this file sets the PATH etc in the shell session. The install script sets up automatic sourcing of this file at the end of your ~/.bashrc file.   Open a new terminal and run  morse check    This should tell you that your environment is setup correctly. :-)   get yourself a github account (https://github.com) and send your account name to marc@hanheide.net", 
            "title": "Simple install script for MORSE and ROS for STRANDS"
        }, 
        {
            "location": "/strands_morse/wiki/MORSE-on-Ubuntu/#the-manual-way", 
            "text": "...", 
            "title": "The manual way"
        }, 
        {
            "location": "/strands_morse/wiki/MORSE-on-Ubuntu/#issues-and-solutions", 
            "text": "Cannot find a MORSE environment or simulation scene matching  strands_sim !  add the path to  strands_sim  in  ~/.morse/config . This is done by going into the strands_sim directory,   To easily achieve this do:  run  morse create test : this will create the setupfile you need by creating a dummy morse project.  You can then remove the newly created created dummy project by  rm -rf test/  Now its time to edit the config file that was created to work with your path to the strands morse project, do this by:  gedit ~/.morse/config  change the path and name to fit your installation in  .morse/config . For me this file looks like this:\n     [sites]\n    strands_sim = /home/johane/strands/strands_morse/strands_sim    Run bham morse sim with ros:  Create a directory like:  strands/sim/src  and cd to that  clone the git repository to this location  git clone git://github.com/strands-project/strands_morse.git  and  cd strands_morse  create a src dir and move everything into it:  mkdir src   mv * src  go to strands/sim/src and run  catkin_init_workspace  go to strands/sim and run  catkin_make  source the ros environment:  source devel/setup.bash  run it:  rosrun strands_sim simulator.sh", 
            "title": "Issues and solutions"
        }, 
        {
            "location": "/strands_movebase/calibrate_chest/", 
            "text": "calibrate_chest\n\n\nThe new way to use this package is through the \ncalibration_server\n action server. To use it, run \nrosrun calibrate_chest calibration_server\n and, in another terminal, \nrosrun actionlib axclient.py /calibrate_chest\n. To compute a new transformation between the camera and the floor, enter command \ncalibrate\n. If you want to publish an already saved calibration (the calibration is saved in mongodb between runs), enter command \npublish\n. When you run the calibrate command, it will check that you are closer than 3 degrees from the desired 46 degrees angle of the chest camera. Otherwise, no calibration will be stored.\n\n\ncalibrate_chest node (legacy)\n\n\n\n\nDo \nrosrun calibrate_chest calibrate_chest\n with the datacentre running if you want to store the parameters there, make sure that the largest visible plane for the chest camera is the floor. Also, notice that you have to have the chest camera running and publishing on topic \nchest_xtion\n.\n\n\nTo use these parameters when you launch the bringup next time, be sure to have the datacentre running.\n\n\nThe urdf can be updated manually by doing \nrosrun calibrate_chest chest_calibration_publisher\n if you don't want to restart the larger system (e.g. \nstrands_movebase.launch\n, which includes this).\n\n\n\n\nExample\n\n\nWhen running \nrosrun calibrate_chest calibrate_chest\n the node tries to find the largest visible plane and determine the angle and height of the chest camera. It will display a window of the current point cloud, with the points belonging to the floor coloured in red. It should look something like the following, you might need a mouse to rotate:\n\n\n\n\nClose the window to save the calibration.", 
            "title": "Calibrate chest"
        }, 
        {
            "location": "/strands_movebase/calibrate_chest/#calibrate_chest", 
            "text": "The new way to use this package is through the  calibration_server  action server. To use it, run  rosrun calibrate_chest calibration_server  and, in another terminal,  rosrun actionlib axclient.py /calibrate_chest . To compute a new transformation between the camera and the floor, enter command  calibrate . If you want to publish an already saved calibration (the calibration is saved in mongodb between runs), enter command  publish . When you run the calibrate command, it will check that you are closer than 3 degrees from the desired 46 degrees angle of the chest camera. Otherwise, no calibration will be stored.", 
            "title": "calibrate_chest"
        }, 
        {
            "location": "/strands_movebase/calibrate_chest/#calibrate_chest-node-legacy", 
            "text": "Do  rosrun calibrate_chest calibrate_chest  with the datacentre running if you want to store the parameters there, make sure that the largest visible plane for the chest camera is the floor. Also, notice that you have to have the chest camera running and publishing on topic  chest_xtion .  To use these parameters when you launch the bringup next time, be sure to have the datacentre running.  The urdf can be updated manually by doing  rosrun calibrate_chest chest_calibration_publisher  if you don't want to restart the larger system (e.g.  strands_movebase.launch , which includes this).", 
            "title": "calibrate_chest node (legacy)"
        }, 
        {
            "location": "/strands_movebase/calibrate_chest/#example", 
            "text": "When running  rosrun calibrate_chest calibrate_chest  the node tries to find the largest visible plane and determine the angle and height of the chest camera. It will display a window of the current point cloud, with the points belonging to the floor coloured in red. It should look something like the following, you might need a mouse to rotate:   Close the window to save the calibration.", 
            "title": "Example"
        }, 
        {
            "location": "/strands_movebase/", 
            "text": "strands_movebase\n\n\nA repository for all the STRANDS-augmented movebase, including 3D obstacle avoidance, etc. Relies on scitos_2d_navigation if it is configured to only use laser scan input for navigation, https://github.com/strands-project/scitos_2d_navigation.\n\n\nUsage\n\n\n\n\nroslaunch strands_movebase movebase.launch map:=/path/to/map.yaml\n\n\nTo be able to use this package you have to create a map with gmapping. This can be run with \nrosrun gmapping slam_gmapping\n, save the map with \nrosrun map_server map_saver\n.\n\n\nEach launch file takes the argument \nmap\n which is the path to the map saved with gmapping.\n\n\nOptionally, provide a \nwith_no_go_map:=true\n and \nno_go_map\n, the path to a map annotated with no-go areas.\n\n\nIf you do not want to launch it with the 3d obstacle avoidance, provide the additional argument \nwith_camera:=false\n.\n\n\nProvide \ncamera:=\ncamera_namespace\n if you have an OpenNI camera publishing on another namespace than the default \nchest_xtion\n.\n\n\nOptionally provide \nz_obstacle_threshold:=\nvalue\n, where \nvalue\n m is the distance from the floor above which we consider points as obstacles. Making this larger than the default 0.1 improves navigation robustness but may lead to missing small obstacles.\n\n\nSame goes for \nz_stair_threshold:=\nvalue\n, the distance below which points are considered negative obstacles. Again, increasing improves robustness, but make sure you don't have any stairs with smaller gaps than this.\n\n\n\n\nIf you run with the camera option, be sure that you have a depth camera publishing on the \ncamera_namespace\n topic. The camera also needs a valid TF transform connecting it to \nbase_link\n. For more details on the Strands solution, see https://github.com/strands-project/strands_movebase/tree/hydro-devel/calibrate_chest and https://github.com/strands-project/strands_movebase/tree/hydro-devel/strands_description.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_movebase/#strands_movebase", 
            "text": "A repository for all the STRANDS-augmented movebase, including 3D obstacle avoidance, etc. Relies on scitos_2d_navigation if it is configured to only use laser scan input for navigation, https://github.com/strands-project/scitos_2d_navigation.", 
            "title": "strands_movebase"
        }, 
        {
            "location": "/strands_movebase/#usage", 
            "text": "roslaunch strands_movebase movebase.launch map:=/path/to/map.yaml  To be able to use this package you have to create a map with gmapping. This can be run with  rosrun gmapping slam_gmapping , save the map with  rosrun map_server map_saver .  Each launch file takes the argument  map  which is the path to the map saved with gmapping.  Optionally, provide a  with_no_go_map:=true  and  no_go_map , the path to a map annotated with no-go areas.  If you do not want to launch it with the 3d obstacle avoidance, provide the additional argument  with_camera:=false .  Provide  camera:= camera_namespace  if you have an OpenNI camera publishing on another namespace than the default  chest_xtion .  Optionally provide  z_obstacle_threshold:= value , where  value  m is the distance from the floor above which we consider points as obstacles. Making this larger than the default 0.1 improves navigation robustness but may lead to missing small obstacles.  Same goes for  z_stair_threshold:= value , the distance below which points are considered negative obstacles. Again, increasing improves robustness, but make sure you don't have any stairs with smaller gaps than this.   If you run with the camera option, be sure that you have a depth camera publishing on the  camera_namespace  topic. The camera also needs a valid TF transform connecting it to  base_link . For more details on the Strands solution, see https://github.com/strands-project/strands_movebase/tree/hydro-devel/calibrate_chest and https://github.com/strands-project/strands_movebase/tree/hydro-devel/strands_description.", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_navigation/", 
            "text": "strands_navigation\n\n\nHigher-level navigation capabilities\n\n\nHere is a diagram of the interactions between the components (\nsource\n):", 
            "title": "Home"
        }, 
        {
            "location": "/strands_navigation/#strands_navigation", 
            "text": "Higher-level navigation capabilities  Here is a diagram of the interactions between the components ( source ):", 
            "title": "strands_navigation"
        }, 
        {
            "location": "/strands_navigation/joy_map_saver/", 
            "text": "joy_map_saver Package\n\n\nSaves current Metric map and also provides Simultaneous creation of Metric and topological maps", 
            "title": "Joy map saver"
        }, 
        {
            "location": "/strands_navigation/message_store_map_switcher/", 
            "text": "Message Store Map Server\n\n\nThis package provides tools for serving OccupancyGrid maps from the mongodb_store. \n\n\nRunning the datacentre\n\n\nAll of the below assumes you have the \nmongodb_store\n nodes running. Do this with:\n\n\nroslaunch mongodb_store datacentre.launch \n\n\n\n\nAdding maps to the datacentre\n\n\nAt the moment maps must be added using the \nmessage_store_map_saver\n executable. This loads a map described by a yaml file (using code from the main \nmap_server\n) and then inserts the map into the message store with the name up to the last \".\" in the yaml file name. Usage is:\n\n\nrosrun message_store_map_switcher message_store_map_saver \nmap.yaml\n\n\n\n\n\nE.g. \n\n\nrosrun message_store_map_switcher message_store_map_saver cs_lg.yaml\n\n\n\n\nResults in a map named \ncs_lg\n being added to the message store. This currently uses the default database and collection (both named \nmessage_store\n).\n\n\nRunning the map server\n\n\nThe \nmessage_store_map_server\n can be started as follows:\n\n\nrosrun message_store_map_switcher message_store_map_server.py\n\n\n\n\nThis will start it without any map being published. If you want to start with a default map you can use the \n-d\n command line parameter:\n\n\nrosrun message_store_map_switcher message_store_map_server.py -d cs_lg\n\n\n\n\nThis can be overridden with the ROS parameter \"default_map\". \n\n\nSwitching maps\n\n\nTo switch to another map from the message store, you must use the \nmessage_store_map_switcher/SwitchMap\n service at \n/switch_map\n. This accepts a string for the map name and returns a boolean if the map was switched. For example,\n\n\nrosservice call /switch_map \ncs_lg\n\nresult: True\n\n\n\n\nrosservice call /switch_map \nnot_cs_lg\n\nresult: False", 
            "title": "Message store map switcher"
        }, 
        {
            "location": "/strands_navigation/message_store_map_switcher/#message-store-map-server", 
            "text": "This package provides tools for serving OccupancyGrid maps from the mongodb_store.", 
            "title": "Message Store Map Server"
        }, 
        {
            "location": "/strands_navigation/message_store_map_switcher/#running-the-datacentre", 
            "text": "All of the below assumes you have the  mongodb_store  nodes running. Do this with:  roslaunch mongodb_store datacentre.launch", 
            "title": "Running the datacentre"
        }, 
        {
            "location": "/strands_navigation/message_store_map_switcher/#adding-maps-to-the-datacentre", 
            "text": "At the moment maps must be added using the  message_store_map_saver  executable. This loads a map described by a yaml file (using code from the main  map_server ) and then inserts the map into the message store with the name up to the last \".\" in the yaml file name. Usage is:  rosrun message_store_map_switcher message_store_map_saver  map.yaml   E.g.   rosrun message_store_map_switcher message_store_map_saver cs_lg.yaml  Results in a map named  cs_lg  being added to the message store. This currently uses the default database and collection (both named  message_store ).", 
            "title": "Adding maps to the datacentre"
        }, 
        {
            "location": "/strands_navigation/message_store_map_switcher/#running-the-map-server", 
            "text": "The  message_store_map_server  can be started as follows:  rosrun message_store_map_switcher message_store_map_server.py  This will start it without any map being published. If you want to start with a default map you can use the  -d  command line parameter:  rosrun message_store_map_switcher message_store_map_server.py -d cs_lg  This can be overridden with the ROS parameter \"default_map\".", 
            "title": "Running the map server"
        }, 
        {
            "location": "/strands_navigation/message_store_map_switcher/#switching-maps", 
            "text": "To switch to another map from the message store, you must use the  message_store_map_switcher/SwitchMap  service at  /switch_map . This accepts a string for the map name and returns a boolean if the map was switched. For example,  rosservice call /switch_map  cs_lg \nresult: True  rosservice call /switch_map  not_cs_lg \nresult: False", 
            "title": "Switching maps"
        }, 
        {
            "location": "/strands_navigation/monitored_navigation/", 
            "text": "The monitored navigation package\n\n\nThis package provides an actionlib server that executes, monitors and recovers from failure (possibly by asking for human help) a continuous navigation actionlib server, e.g., \nmove_base\n. \nIt receives a goal in the form:\n\n\n    string action_server\n\n\n\n\n    geometry_msgs/PoseStamped target_pose\n\n\n\n\nwhere \naction_server\n is an actionlib server that receives a  \ngeometry_msgs/PoseStamped goal\n (as \nmove_base\n).\n\n\nTo run the bare-bone monitored navigation state machine you can do\n\n\nrosrun monitored_navigation monitored_nav.py\n\n\n\n\nRunning monitored_navigation within the STRANDS system\n\n\nTo launch the monitored_navigation with the default STRANDS recovery behaviours and human help interfaces, you need to have \nstrands_recovery_behaviours\n installed and do:\n\n\nroslaunch strands_recovery_behaviours strands_monitored_nav.launch\n\n\n\n\nConfiguring monitors and recovery behaviours, and interfaces for asking help from humans\n\n\nThe \nmonitored_navigation\n package provides a skeleton state machine where one can add smach implementations of monitor and recovery behaviours. Furthermore, it allows the robot to ask for human help \nvia pre-defined interfaces.\n\n\n\n\nThe monitors and recoveries should be subclasses  of the \n\nMonitorState\n and \nRecoverStateMachine\n classes respectively. A monitor runs in parallel with the execution of \nthe continuous navigation action. When it outputs \n'invalid'\n, the continuous navigation, and any other monitors that are running, are preempted, and the state machine jumps to\nthe corresponding recovery behaviour. Examples of STRANDS specific behaviours can be found\n \nhere\n.\n\n\nThe interfaces for asking help should be subclasses of the \nUIHelper\n class. The \nask_help\n, \nbeing_helped\n, \nhelp_finished\n and \nhelp_failed\n methods need\nto be defined for the specific ui being used. Help is asked by a \nRecoverStateMachine\n via a service\ncall to \n'/monitored_navigation/human_help/'\n. Service definition \nhere\n. The  \nask_help\n, \nbeing_helped\n, \nhelp_finished\n and \nhelp_failed\n methods\nreceive datafrom the service request, and do something appropriate, depending on the ui they are implementing (say something, send email, ...). Examples of STRANDS specific helpers can be found\n \nhere\n.\n\n\n\n\nAt initialization\n\n\nA yaml file that defines the monitor and recovery behaviours that should be added to the state machine at startup can be provided by doing\n\n\nrosrun monitored_navigation monitored_nav.py path_to_yaml_file\n\n\n\n\nThe yaml file specifies the \nnav_recovery\n mechanism and the \nmonitor_recovery_pairs\n. \n\n\n\n\nThe \nnav_recovery\n is specified by:\n\n\npackage:\n The package where the recovery behaviour can be found\n\n\nrecovery_file:\n The file where the \nRecoverStateMachine\n subclass implementation can be found (without the .py extenstion)\n\n\nrecovery_class:\n The name of the recovery class. This should be a subclass of \nRecoverStateMachine\n\n\n\n\n\n\nThe \nmonitor_recovery_pairs\n is a list where each element is specified by:\n\n\nname:\n The name to be used as an indentifier of the monitor/recovery pair\n\n\npackage:\n The package where the monitor and recovery behaviour can be found\n\n\nmonitor_file:\n The file where the \nMonitorState\n subclass implementation can be found (without the .py extenstion)\n\n\nmonitor_class:\n The name of the monitor class. This should be a subclass of \nMonitorState\n\n\nrecovery_file:\n The file where the \nRecoverStateMachine\n subclass implementation can be found (without the .py extenstion)\n\n\nrecovery_class:\n The name of the recovery class. This should be a subclass of \nRecoverStateMachine\n\n\n\n\n\n\nThe \nhuman_help\n is a list where each element is specified by:\n\n\npackage:\n The package where the helper implementation can be found\n\n\nhelper_file:\n The file where the \nUIHelper\n subclass implementation can be found (without the .py extenstion)\n\n\nrecovery_class:\n The name of the rhelper class. This should be a subclass of \nUIHelper\n\n\n\n\n\n\n\n\nAn example yaml file for the STRANDS-specific configuration can be found \nhere\n.\n\n\nAt run-time\n\n\nMonitors and recovery behaviours can also be added and removed at runtime (only if the monitored navigation action server is \nnot\n running). To do this, the \nmonitored_navigation\n node provides the \nfollowing services, based on the  \nDynClassLoaderDef\n message. The meaning of the message fields is the same as described above for the yaml file:\n\n\n\n\n/monitored_navigation/add_monitor_recovery_pair\n. Add a monitor/recovery pair. Service definition is \nhere\n\n\n/monitored_navigation/del_monitor_recovery_pair\n. Remove a monitor/recovery pair, by name. Service definition is \nhere\n\n\n/monitored_navigation/set_monitor_recovery_pairs\n. Set a list of monitor/recovery pairs. The ones currently being used are removed. Service definition is \n\nhere\n\n\n/monitored_navigation/get_monitor_recovery_pairs\n. Get current monitor/recovery pairs. Service definition is \n\nhere\n\n\n/monitored_navigation/set_nav_recovery\n. Set the recovery state machine for navigation. The current recovery for navigation is replaced by the new one. Service definition is \nhere\n\n\n/monitored_navigation/get_nav_recovery\n. Get the current recovery state machine for navigation. Service definition is \nhere\n\n\n/monitored_navigation/add_helper\n. Add a human helper interface. Service definition is \nhere\n\n\n/monitored_navigation/del_helper\n. Remove a human helper interface, by name. Service definition is \nhere\n\n\n/monitored_navigation/set_helpers\n. Set a list of human helper interfaces. The ones currently being used are removed. Service definition is \n\nhere\n\n\n/monitored_navigation/get_helpers\n. Gett  list of current human helper interfaces. Service definition is \n\nhere", 
            "title": "Monitored navigation"
        }, 
        {
            "location": "/strands_navigation/monitored_navigation/#the-monitored-navigation-package", 
            "text": "This package provides an actionlib server that executes, monitors and recovers from failure (possibly by asking for human help) a continuous navigation actionlib server, e.g.,  move_base . \nIt receives a goal in the form:      string action_server      geometry_msgs/PoseStamped target_pose  where  action_server  is an actionlib server that receives a   geometry_msgs/PoseStamped goal  (as  move_base ).  To run the bare-bone monitored navigation state machine you can do  rosrun monitored_navigation monitored_nav.py", 
            "title": "The monitored navigation package"
        }, 
        {
            "location": "/strands_navigation/monitored_navigation/#running-monitored_navigation-within-the-strands-system", 
            "text": "To launch the monitored_navigation with the default STRANDS recovery behaviours and human help interfaces, you need to have  strands_recovery_behaviours  installed and do:  roslaunch strands_recovery_behaviours strands_monitored_nav.launch", 
            "title": "Running monitored_navigation within the STRANDS system"
        }, 
        {
            "location": "/strands_navigation/monitored_navigation/#configuring-monitors-and-recovery-behaviours-and-interfaces-for-asking-help-from-humans", 
            "text": "The  monitored_navigation  package provides a skeleton state machine where one can add smach implementations of monitor and recovery behaviours. Furthermore, it allows the robot to ask for human help \nvia pre-defined interfaces.   The monitors and recoveries should be subclasses  of the  MonitorState  and  RecoverStateMachine  classes respectively. A monitor runs in parallel with the execution of \nthe continuous navigation action. When it outputs  'invalid' , the continuous navigation, and any other monitors that are running, are preempted, and the state machine jumps to\nthe corresponding recovery behaviour. Examples of STRANDS specific behaviours can be found\n  here .  The interfaces for asking help should be subclasses of the  UIHelper  class. The  ask_help ,  being_helped ,  help_finished  and  help_failed  methods need\nto be defined for the specific ui being used. Help is asked by a  RecoverStateMachine  via a service\ncall to  '/monitored_navigation/human_help/' . Service definition  here . The   ask_help ,  being_helped ,  help_finished  and  help_failed  methods\nreceive datafrom the service request, and do something appropriate, depending on the ui they are implementing (say something, send email, ...). Examples of STRANDS specific helpers can be found\n  here .", 
            "title": "Configuring monitors and recovery behaviours, and interfaces for asking help from humans"
        }, 
        {
            "location": "/strands_navigation/monitored_navigation/#at-initialization", 
            "text": "A yaml file that defines the monitor and recovery behaviours that should be added to the state machine at startup can be provided by doing  rosrun monitored_navigation monitored_nav.py path_to_yaml_file  The yaml file specifies the  nav_recovery  mechanism and the  monitor_recovery_pairs .    The  nav_recovery  is specified by:  package:  The package where the recovery behaviour can be found  recovery_file:  The file where the  RecoverStateMachine  subclass implementation can be found (without the .py extenstion)  recovery_class:  The name of the recovery class. This should be a subclass of  RecoverStateMachine    The  monitor_recovery_pairs  is a list where each element is specified by:  name:  The name to be used as an indentifier of the monitor/recovery pair  package:  The package where the monitor and recovery behaviour can be found  monitor_file:  The file where the  MonitorState  subclass implementation can be found (without the .py extenstion)  monitor_class:  The name of the monitor class. This should be a subclass of  MonitorState  recovery_file:  The file where the  RecoverStateMachine  subclass implementation can be found (without the .py extenstion)  recovery_class:  The name of the recovery class. This should be a subclass of  RecoverStateMachine    The  human_help  is a list where each element is specified by:  package:  The package where the helper implementation can be found  helper_file:  The file where the  UIHelper  subclass implementation can be found (without the .py extenstion)  recovery_class:  The name of the rhelper class. This should be a subclass of  UIHelper     An example yaml file for the STRANDS-specific configuration can be found  here .", 
            "title": "At initialization"
        }, 
        {
            "location": "/strands_navigation/monitored_navigation/#at-run-time", 
            "text": "Monitors and recovery behaviours can also be added and removed at runtime (only if the monitored navigation action server is  not  running). To do this, the  monitored_navigation  node provides the \nfollowing services, based on the   DynClassLoaderDef  message. The meaning of the message fields is the same as described above for the yaml file:   /monitored_navigation/add_monitor_recovery_pair . Add a monitor/recovery pair. Service definition is  here  /monitored_navigation/del_monitor_recovery_pair . Remove a monitor/recovery pair, by name. Service definition is  here  /monitored_navigation/set_monitor_recovery_pairs . Set a list of monitor/recovery pairs. The ones currently being used are removed. Service definition is  here  /monitored_navigation/get_monitor_recovery_pairs . Get current monitor/recovery pairs. Service definition is  here  /monitored_navigation/set_nav_recovery . Set the recovery state machine for navigation. The current recovery for navigation is replaced by the new one. Service definition is  here  /monitored_navigation/get_nav_recovery . Get the current recovery state machine for navigation. Service definition is  here  /monitored_navigation/add_helper . Add a human helper interface. Service definition is  here  /monitored_navigation/del_helper . Remove a human helper interface, by name. Service definition is  here  /monitored_navigation/set_helpers . Set a list of human helper interfaces. The ones currently being used are removed. Service definition is  here  /monitored_navigation/get_helpers . Gett  list of current human helper interfaces. Service definition is  here", 
            "title": "At run-time"
        }, 
        {
            "location": "/strands_navigation/nav_goals_generator/", 
            "text": "nav_goals_generator\n\n\nROS Service to generate 2D navigation goals with orientation in a specifed\nregion of interest (ROI). The service takes the number of navigation goals\n(\nn\n), an \ninflation radius\n which ressembles the robot's footprint, and a ROI\ndescribed by a polygon as arguments and returns a list of goal poses.\n\n\nUsage\n\n\nFirst make sure that an occupancy grid map (nav_msgs/OccupancyGrid) is\npublished on a topic, e.g. \n/map\n. Second, launch the service as follows:\n\n\nroslaunch nav_goal_generation nav_goal_generation.launch\n\n\n\n\nThe default map is \n/map\n. It can be overwritten by passing an argument as follows:\n\n\nroslaunch nav_goal_generation nav_goal_generation.launch map:=/projected_map\n\n\n\n\nIf the map argument is a costmap, you should also set the flag \nis_costmap\n to \ntrue\n. Then the inflation radius in the service call is ignored (a costmap is already inflated)\n\n\nroslaunch nav_goal_generation nav_goal_generation.launch map:=/move_base/global_costmap/costmap  is_costmap:=true\n\n\n\n\nYou can send a service request as follows:\n\n\nrosservice call /nav_goals '{n: 100, inflation_radius: 0.5, roi: {points: [[0,0,0],[2,-2,0],[-2,-2,0]]}}'\n\n\n\n\nwhereby the first argument is the number of goal loactions to be generated\n(here 100) and and the second argument is the inflation radius of the robot's\nfootprint (here 0.5), and the third argument is a ROI specified as a list of\npoints (at least three).  The result of the pose generation is additionally\npublished on the topic \n/nav_goals\n in order to visualize the result in RVIZ.\n\n\nIf the service is called with an empty ROI, the full map is considered as ROI\nby default. \n\n\nrosservice call /nav_goals '{n: 100, inflation_radius: 0.5, roi: {}}'\n\n\n\n\nIf a specified ROI includes a point that is outside the map, its \nconflicting\n\ncoordinates are automatically adjusted to the map's bounding box.\n\n\nKnown Issues\n\n\nThe service fails if there is no map topic available or no message has been\npublished on this topic after the service has been started. As a workaround,\nthe map server could be started after this service.", 
            "title": "Nav goals generator"
        }, 
        {
            "location": "/strands_navigation/nav_goals_generator/#nav_goals_generator", 
            "text": "ROS Service to generate 2D navigation goals with orientation in a specifed\nregion of interest (ROI). The service takes the number of navigation goals\n( n ), an  inflation radius  which ressembles the robot's footprint, and a ROI\ndescribed by a polygon as arguments and returns a list of goal poses.", 
            "title": "nav_goals_generator"
        }, 
        {
            "location": "/strands_navigation/nav_goals_generator/#usage", 
            "text": "First make sure that an occupancy grid map (nav_msgs/OccupancyGrid) is\npublished on a topic, e.g.  /map . Second, launch the service as follows:  roslaunch nav_goal_generation nav_goal_generation.launch  The default map is  /map . It can be overwritten by passing an argument as follows:  roslaunch nav_goal_generation nav_goal_generation.launch map:=/projected_map  If the map argument is a costmap, you should also set the flag  is_costmap  to  true . Then the inflation radius in the service call is ignored (a costmap is already inflated)  roslaunch nav_goal_generation nav_goal_generation.launch map:=/move_base/global_costmap/costmap  is_costmap:=true  You can send a service request as follows:  rosservice call /nav_goals '{n: 100, inflation_radius: 0.5, roi: {points: [[0,0,0],[2,-2,0],[-2,-2,0]]}}'  whereby the first argument is the number of goal loactions to be generated\n(here 100) and and the second argument is the inflation radius of the robot's\nfootprint (here 0.5), and the third argument is a ROI specified as a list of\npoints (at least three).  The result of the pose generation is additionally\npublished on the topic  /nav_goals  in order to visualize the result in RVIZ.  If the service is called with an empty ROI, the full map is considered as ROI\nby default.   rosservice call /nav_goals '{n: 100, inflation_radius: 0.5, roi: {}}'  If a specified ROI includes a point that is outside the map, its  conflicting \ncoordinates are automatically adjusted to the map's bounding box.", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_navigation/nav_goals_generator/#known-issues", 
            "text": "The service fails if there is no map topic available or no message has been\npublished on this topic after the service has been started. As a workaround,\nthe map server could be started after this service.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/strands_navigation/topological_logging_manager/", 
            "text": "Topological loggind based on a white list of nodes\n\n\nDue to privacy protection we might not be allowed to record certain data in specific\nareas. Like the east wing of AAF for example. This package contains a node, that\nuses the topologocal map information to trigger logging of sensitive data.\n\n\nUsage\n\n\nThe logging is based on a so-called white list of nodes, e.g. this AAF example:\n\n\nnodes:\n    - \nEingang\n\n    - \nRezeption\n\n    - \nWayPoint2\n\n    - \nWayPoint17\n\n    - \nWayPoint35\n\n\n\n\n\nThis file is in YAML format. The node has to be started with \n_white_list_file:=\nfile_name\n.\n\n\nIt monitores the topics for the current edge and current node. Whenever the robot\nis at a node that is in the white list, the script publishes \ntrue\n, \nfalse\n otherwise.\nIn addition, it uses the look-up service of topological navigation, to get the\nedges between these nodes. When the robot is on that edge, it also publishes\n\ntrue\n and \nfalse\n otherwise. Publishing rate is 30hz, can be changed with the \n_publishing_rate\n\nparameter.\n\n\nTo allow logging on all nodes and edges use:\n\n\nnodes:\n    - \nALL\n\n\n\n\n\nwhich you can find in the \nconf\n dir.\n\n\nPublished are a bool and a \"stamped bool\" which is the custom LoggingManager.msg\ncontaining a header and a bool. The stamped bool is useful in combination with\nthe ApproximateTimeSynchronizer. Let's say you want to log camera images:\n\n\nimport rospy\nimport message_filters\nfrom topological_logging_manager.msg import LoggingManager\nfrom sensor_msgs.msg import Image\n...\n\nclass SaveImages():\n    def __init__(self):\n        ...\n\n        subs = [\n            message_filters.Subscriber(rospy.get_param(\n~rgb\n, \n/head_xtion/rgb/image_rect_color\n), Image),\n            message_filters.Subscriber(rospy.get_param(\n~logging_manager\n, \n/logging_manager/log_stamped\n), LoggingManager),\n        ]\n\n        ts = message_filters.ApproximateTimeSynchronizer(subs, queue_size=5, slop=0.1)\n        ts.registerCallback(self.cb)\n...\n\n    def cb(self, img, log):\n        if log:\n            ...\n...\n\n\n\n\nIf the message is not published, the callback will not be triggered, if it publishes,\nyou can check if recording is true or false.\n\n\nThe normal bool can be used as a flag for everything else.\n\n\nParameters\n\n\n\n\nwhite_list_file\n: the yaml file containing the node names\n\n\ncurrent_node_topic\n \ndefault=\"/current_node\n: The topic where the current node is published\n\n\nclosest_node_topic\n \ndefault=\"/closest_node\n: The topic where the closest node is published\n\n\nuse_closest_node\n \ndefault=\"true\"\n: If true uses the closest node to determine if true or false. If false, only being in the influence zone of the node will give true.\n\n\nedge_topic\n \ndefault=\"/current_edge\n: The topic where the current edge is published\n\n\ncheck_topic_rate\n \ndefault=\"1\"\n: The rate at which the edge and node topics are checked if they are still alive.\n\n\npublishing_rate\n \ndefault=\"30\"\n: Rate at which the result is published. 30 to match camera output.\n\n\nbool_publisher_topic\n _default=\"/logging_manager/log: The topic n which the std_msgs/Bool is published\n\n\nbool_stamped_publisher_topic\n \ndefault=\"/logging_manager/log_stamped\n: The topic n which the topological_logging_manager/LoggingManager is published\n\n\n\n\nSecurity\n\n\nTo ensure that this works as reliable as possible, the default should always be not to record.\nAs mentioned above, using the \"stamped bool\" acheives this quite easily. To ensure\nthat the node always publishes the correct values, it has topic monitores for\nthe two used topics that check if they are still alive, every second. The results\nare published in a different thread because the edges and nodes are only published when they change.\nThe monitores are therefore used to detect if the navigation died.", 
            "title": "Topological logging manager"
        }, 
        {
            "location": "/strands_navigation/topological_logging_manager/#topological-loggind-based-on-a-white-list-of-nodes", 
            "text": "Due to privacy protection we might not be allowed to record certain data in specific\nareas. Like the east wing of AAF for example. This package contains a node, that\nuses the topologocal map information to trigger logging of sensitive data.", 
            "title": "Topological loggind based on a white list of nodes"
        }, 
        {
            "location": "/strands_navigation/topological_logging_manager/#usage", 
            "text": "The logging is based on a so-called white list of nodes, e.g. this AAF example:  nodes:\n    -  Eingang \n    -  Rezeption \n    -  WayPoint2 \n    -  WayPoint17 \n    -  WayPoint35   This file is in YAML format. The node has to be started with  _white_list_file:= file_name .  It monitores the topics for the current edge and current node. Whenever the robot\nis at a node that is in the white list, the script publishes  true ,  false  otherwise.\nIn addition, it uses the look-up service of topological navigation, to get the\nedges between these nodes. When the robot is on that edge, it also publishes true  and  false  otherwise. Publishing rate is 30hz, can be changed with the  _publishing_rate \nparameter.  To allow logging on all nodes and edges use:  nodes:\n    -  ALL   which you can find in the  conf  dir.  Published are a bool and a \"stamped bool\" which is the custom LoggingManager.msg\ncontaining a header and a bool. The stamped bool is useful in combination with\nthe ApproximateTimeSynchronizer. Let's say you want to log camera images:  import rospy\nimport message_filters\nfrom topological_logging_manager.msg import LoggingManager\nfrom sensor_msgs.msg import Image\n...\n\nclass SaveImages():\n    def __init__(self):\n        ...\n\n        subs = [\n            message_filters.Subscriber(rospy.get_param( ~rgb ,  /head_xtion/rgb/image_rect_color ), Image),\n            message_filters.Subscriber(rospy.get_param( ~logging_manager ,  /logging_manager/log_stamped ), LoggingManager),\n        ]\n\n        ts = message_filters.ApproximateTimeSynchronizer(subs, queue_size=5, slop=0.1)\n        ts.registerCallback(self.cb)\n...\n\n    def cb(self, img, log):\n        if log:\n            ...\n...  If the message is not published, the callback will not be triggered, if it publishes,\nyou can check if recording is true or false.  The normal bool can be used as a flag for everything else.", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_navigation/topological_logging_manager/#parameters", 
            "text": "white_list_file : the yaml file containing the node names  current_node_topic   default=\"/current_node : The topic where the current node is published  closest_node_topic   default=\"/closest_node : The topic where the closest node is published  use_closest_node   default=\"true\" : If true uses the closest node to determine if true or false. If false, only being in the influence zone of the node will give true.  edge_topic   default=\"/current_edge : The topic where the current edge is published  check_topic_rate   default=\"1\" : The rate at which the edge and node topics are checked if they are still alive.  publishing_rate   default=\"30\" : Rate at which the result is published. 30 to match camera output.  bool_publisher_topic  _default=\"/logging_manager/log: The topic n which the std_msgs/Bool is published  bool_stamped_publisher_topic   default=\"/logging_manager/log_stamped : The topic n which the topological_logging_manager/LoggingManager is published", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_navigation/topological_logging_manager/#security", 
            "text": "To ensure that this works as reliable as possible, the default should always be not to record.\nAs mentioned above, using the \"stamped bool\" acheives this quite easily. To ensure\nthat the node always publishes the correct values, it has topic monitores for\nthe two used topics that check if they are still alive, every second. The results\nare published in a different thread because the edges and nodes are only published when they change.\nThe monitores are therefore used to detect if the navigation died.", 
            "title": "Security"
        }, 
        {
            "location": "/strands_navigation/topological_rviz_tools/", 
            "text": "topological_rviz_tools\n\n\nRviz tool for creating a STRANDS topological map\n\n\nUsage\n\n\nThis rviz toolset can be launched using\n\n\nroslaunch topological_rviz_tools strands_rviz_topmap.launch map:=/path/to/map.yaml topmap:=topmap_pointset db_path:=/path/to/db standalone:=true\n\n\n\n\nmap\n specifies the \nyaml\n file for the map you are using, and \ntopmap\n the\ncorresponding pointset that exists in the database. \ndb_path\n is used to point\nto the database you want to use. If \nstandalone\n is true, everything needed will\nbe run automatically. If false, it is assumed that other parts of the strands\nsystem are running (navigation and mongodb_store in particular), and only run\na few additional things.\n\n\nYou can also add the node tool, edge tool and topological map panel to rviz\nindividually by using the buttons for adding tools or panels. You will need to\nrun the above launch file with \nstandalone=false\n for this to work correctly.\n\n\nWhen you launch with a database which contains a topological map, you should see\nsomething like the following:\n\n\n\n\nYou can move nodes around by clicking the arrow at the centre of topological\nnodes and dragging. The ring around the node allows you to change the\norientation of the node. You can delete edges using the red arrows in the middle\nof edges.\n\n\nThe following sections give a little more detail about to tools and panel\nlabelled in the image above.\n\n\n1. The edge tool\n\n\nUse this tool to create edges between nodes. Left click to select the start point of\nthe edge, then click again to create an edge. The edge will be created between\nthe nodes closest to the two clicks, but only if there are nodes within some\ndistance of the clicks. Left clicking will create a bidirectional edge, whereas\nright clicking will create an edge only from the node closest to the first click\nto the second one. This tool stays on until you press escape.\n\n\nThe shortcut is \ne\n.\n\n\n2. The node tool\n\n\nThis tool allows you to add nodes to the topological map. Click the tool and\nthen click on the map to add a node in that location. Edges will automatically\nbe added between the new node and any nodes in close proximity.\n\n\nThe shortcut is \nn\n.\n\n\n3. Add tag button\n\n\nThis button allows you to add tags to nodes. You can select multiple nodes, and\nthe tag you enter in the dialog box will be added to all of them.\n\n\n4. Remove button\n\n\nWith this button, you can remove edges, tags, and nodes from the topological\nmap. You can select multiple elements and they will all be removed at once.\n\n\n5. Topological map panel\n\n\nYou can see all the elements of the topological map here. You can edit the\nfollowing elements:\n\n\n\n\nNode name\n\n\nNode pose\n\n\nNode tags\n\n\nNode yaw tolerance\n\n\nNode xy tolerance\n\n\nEdge action\n\n\nEdge velocity\n\n\n\n\nCtrl-click allows you to select multiple distinct elements. Shift-click will\nselect elements between the previously selected element and the current one.", 
            "title": "Topological rviz tools"
        }, 
        {
            "location": "/strands_navigation/topological_rviz_tools/#topological_rviz_tools", 
            "text": "Rviz tool for creating a STRANDS topological map", 
            "title": "topological_rviz_tools"
        }, 
        {
            "location": "/strands_navigation/topological_rviz_tools/#usage", 
            "text": "This rviz toolset can be launched using  roslaunch topological_rviz_tools strands_rviz_topmap.launch map:=/path/to/map.yaml topmap:=topmap_pointset db_path:=/path/to/db standalone:=true  map  specifies the  yaml  file for the map you are using, and  topmap  the\ncorresponding pointset that exists in the database.  db_path  is used to point\nto the database you want to use. If  standalone  is true, everything needed will\nbe run automatically. If false, it is assumed that other parts of the strands\nsystem are running (navigation and mongodb_store in particular), and only run\na few additional things.  You can also add the node tool, edge tool and topological map panel to rviz\nindividually by using the buttons for adding tools or panels. You will need to\nrun the above launch file with  standalone=false  for this to work correctly.  When you launch with a database which contains a topological map, you should see\nsomething like the following:   You can move nodes around by clicking the arrow at the centre of topological\nnodes and dragging. The ring around the node allows you to change the\norientation of the node. You can delete edges using the red arrows in the middle\nof edges.  The following sections give a little more detail about to tools and panel\nlabelled in the image above.", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_navigation/topological_rviz_tools/#1-the-edge-tool", 
            "text": "Use this tool to create edges between nodes. Left click to select the start point of\nthe edge, then click again to create an edge. The edge will be created between\nthe nodes closest to the two clicks, but only if there are nodes within some\ndistance of the clicks. Left clicking will create a bidirectional edge, whereas\nright clicking will create an edge only from the node closest to the first click\nto the second one. This tool stays on until you press escape.  The shortcut is  e .", 
            "title": "1. The edge tool"
        }, 
        {
            "location": "/strands_navigation/topological_rviz_tools/#2-the-node-tool", 
            "text": "This tool allows you to add nodes to the topological map. Click the tool and\nthen click on the map to add a node in that location. Edges will automatically\nbe added between the new node and any nodes in close proximity.  The shortcut is  n .", 
            "title": "2. The node tool"
        }, 
        {
            "location": "/strands_navigation/topological_rviz_tools/#3-add-tag-button", 
            "text": "This button allows you to add tags to nodes. You can select multiple nodes, and\nthe tag you enter in the dialog box will be added to all of them.", 
            "title": "3. Add tag button"
        }, 
        {
            "location": "/strands_navigation/topological_rviz_tools/#4-remove-button", 
            "text": "With this button, you can remove edges, tags, and nodes from the topological\nmap. You can select multiple elements and they will all be removed at once.", 
            "title": "4. Remove button"
        }, 
        {
            "location": "/strands_navigation/topological_rviz_tools/#5-topological-map-panel", 
            "text": "You can see all the elements of the topological map here. You can edit the\nfollowing elements:   Node name  Node pose  Node tags  Node yaw tolerance  Node xy tolerance  Edge action  Edge velocity   Ctrl-click allows you to select multiple distinct elements. Shift-click will\nselect elements between the previously selected element and the current one.", 
            "title": "5. Topological map panel"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/", 
            "text": "Topological Navigation\n\n\nThis node provides support for topological navigation in the STRANDS system. \n\n\nThis module requires:\n * move_base\n * strands_datacentre\n * monitored_navigation\n\n\nYou will also need to create a \ntopological map\n\n\nLaunching Topological Localisation and Navigation\n\n\n\n\nOnce your map is inserted in the DB you can launch your topological navigation nodes using:\n  \nroslaunch topological_navigation topological_navigation.launch map:=topological_map_name node_by_node:=false\n\n\n\n\nnote:\n When Node by node navigation is active the robot will cancel the topological navigation when it reaches the influence zone of a node and it will navigate to its waypoint aborting the action.\n\n\nnote:\n Statistics are being recorded in the nav_stats collection within the autonomous_patrolling\n\n\nnote:\n every action server for the actions stored in the topological map have to be running, for example if the ramp_climb action is required you will need the ramp_climb server running, you can run it using:\n\nrosrun ramp_climb ramp_climb\n\n\nNavigate\n\n\n\n\n\n\nYou can test the topological navigation using:\n  \nrosrun topological_navigation nav_client.py Destination\n\n  Where destination is the name of the node you want to reach.\n\n\n\n\n\n\nYou can also run \nRViz\n \nrosrun rviz rviz\n and subscribe to the interactive markers \n/name_of_map_go_to/update\n and clicking on the Green Arrows to move the robot around.\n\n\n\n\n\n\nCreate Topological Maps\n\n\nThere are different ways of creating \ntopological maps\n:\n\n\n\n\n\n\nSimultaneous Metric and Topological Mapping\n\n\n\n\n\n\nFrom WayPoint File\n\n\n\n\n\n\nUsing RViz\n\n\n\n\n\n\nUsing Gamepad\n\n\n\n\n\n\nEdit using MongoDB Client\n\n\n\n\n\n\n*. [Using Localise by Topic] (#Using-Localise-by-Topic)\n\n\nSimultaneous Metric and Topological Mapping\n\n\n@To improve\n\n\nTo follow this process you will need to place the robot in the Charging station and reset the odometry, once it is there you can launch the mapping by running:\n\n\nroslaunch topological_utils mapping.launch met_map:='name_of_metric_map' top_map:='name_of_topological_map'\n\n\nOnce this is done undock the robot using the Visual_charging_server. \n\n\nWhen the robot is undocked you can move it around using the gamepad and when you wish to create a new topological node press button \n'B'\n once you are done save your metric map by pressing \n'A'\n.\n\n\nTo edit the topological map you can use the following methods\n\n\nCreation of the Topological map From WayPoint File\n\n\n\n\n\n\n(\nModified\n) The first step is to create the waypoint file by running:\n \nrosrun topological_utils joy_add_waypoint.py name_of_the_waypoint_file.csv\n\n\n\n\n\n\nNext create a topological map file from the waypoint file using:\n\nrosrun topological_utils tmap_from_waypoints.py input_file.csv output_file.tmap\n this will create a topological tree on which every waypoint in the file is a node and each waypoint is connected to every other waypoint using move_base and an octagonal influence area, for example,\n\n\n\n\n\n\nFrom a normal waypoint file like this:\n  \nSTON\n  -2.3003,0.120533,-4.88295e-05,0,0,-0.0530802,0.99859\n  -7.95369,-3.03503,-4.4791e-05,0,0,0.928319,-0.371784\n  -8.91935,2.94528,0.00139425,0,0,0.473654,0.880711\n  -2.68708,-2.23003,-0.000478224,0,0,-0.759842,0.650108\n\n  It will create a topological map file like this:\n  ```JSON\n  node: \n    ChargingPoint\n    waypoint:\n        -2.3003,0.120533,-4.88295e-05,0,0,-0.0530802,0.99859\n    edges:\n        WayPoint1, move_base\n        WayPoint2, move_base\n        WayPoint3, move_base\n    vertices:\n        1.380000,0.574000\n        0.574000,1.380000\n        -0.574000,1.380000\n        -1.380000,0.574000\n        -1.380000,-0.574000\n        -0.574000,-1.380000\n        0.574000,-1.380000\n        1.380000,-0.574000\n  node: \n    WayPoint1\n    waypoint:\n        -7.95369,-3.03503,-4.4791e-05,0,0,0.928319,-0.371784\n    edges:\n        ChargingPoint, move_base\n        WayPoint2, move_base\n        WayPoint3, move_base\n    vertices:\n        1.380000,0.574000\n        0.574000,1.380000\n        -0.574000,1.380000\n        -1.380000,0.574000\n        -1.380000,-0.574000\n        -0.574000,-1.380000\n        0.574000,-1.380000\n        1.380000,-0.574000\n  node: \n    WayPoint2\n    waypoint:\n        -8.91935,2.94528,0.00139425,0,0,0.473654,0.880711\n    edges:\n        ChargingPoint, move_base\n        WayPoint1, move_base\n        WayPoint3, move_base\n    vertices:\n        1.380000,0.574000\n        0.574000,1.380000\n        -0.574000,1.380000\n        -1.380000,0.574000\n        -1.380000,-0.574000\n        -0.574000,-1.380000\n        0.574000,-1.380000\n        1.380000,-0.574000\n  node: \n    WayPoint3\n    waypoint:\n        -2.68708,-2.23003,-0.000478224,0,0,-0.759842,0.650108\n    edges:\n        ChargingPoint, move_base\n        WayPoint1, move_base\n        WayPoint2, move_base\n    vertices:\n        1.380000,0.574000\n        0.574000,1.380000\n        -0.574000,1.380000\n        -1.380000,0.574000\n        -1.380000,-0.574000\n        -0.574000,-1.380000\n        0.574000,-1.380000\n        1.380000,-0.574000\n\n\n``\n1. *Editing the topological map*, lets say that you want to make sure that to reach a node you get there using a specific action like ramp_climb from another node then you will need to edit the topological map file. You can also edit the vertices of the influence area so it can be defined using different kinds of polygons.\n  For example, lets suppose that in the previous map you want WayPoint3 to be reached only from the Waypoint1 using\nramp_climb` and that you want to define rectangular areas around your waypoints and a triangular area around the ChargingPoint, in such case your map file should look like this:\n\n\nJSON\n  node: \n    ChargingPoint\n    waypoint:\n        -2.3003,0.120533,-4.88295e-05,0,0,-0.0530802,0.99859\n    edges:\n         WayPoint1, move_base\n         WayPoint2, move_base\n    vertices:\n        0.0,1.5\n        -1.5,-1.5\n        1.5,-1.5\n  node: \n    WayPoint1\n    waypoint:\n        -7.95369,-3.03503,-4.4791e-05,0,0,0.928319,-0.371784\n    edges:\n        ChargingPoint, move_base\n        WayPoint2, move_base\n        WayPoint3, ramp_climb\n    vertices:\n        1.380000,0.574000\n        -1.380000,0.574000\n        -1.380000,-0.574000\n        1.380000,-0.574000\n  node: \n    WayPoint2\n    waypoint:\n        -8.91935,2.94528,0.00139425,0,0,0.473654,0.880711\n    edges:\n        ChargingPoint, move_base\n        WayPoint1, move_base\n    vertices:\n        0.574000,1.380000\n        -0.574000,1.380000\n        -0.574000,-1.380000\n        0.574000,-1.380000\n  node: \n    WayPoint3\n    waypoint:\n        -2.68708,-2.23003,-0.000478224,0,0,-0.759842,0.650108\n    edges:\n        WayPoint1, ramp_climb\n    vertices:\n        1.5,1.5\n        -1.5,1.5\n        -1.5,-1.5\n        1.5,-1.5\n\n  In this case the topological navigation node will always call ramp_climb when reaching or leaving WayPoint3, you can also edit the names of the nodes to fit your needs.\n  \nnote:\n At the moment only \nmove_base\n and \nramp_climb\n can be included.\n  \nnote:\n The polygons of the influence area will be created using the convex-hull approach so they may not correspond exactly to the vertices previously defined.\n\n\n\n\nOnce you are happy with your topological map you have to insert it in the strands_datacentre using:\n  \nrosrun topological_utils insert_map.py topological_map.tmap topological_map_name map_name\n\n\n\n\nUsing RViz\n\n\nRViz\n can be used to edit topological maps on system runtime, however there is also the possibility of inserting a basic \ntopological map\n and editing it using \nRViz\n. The Following steps will guide through this process.\n\n\n\n\n\n\nIt is necessary to insert a basic map in the DB and run the topological system based on it for this a launch file is provided and it can be used in the following way:\n\n\n\n\n\n\nLaunch 2d navigation using for example:\n  \nroslaunch strands_movebase movebase.launch map:=/path/to/your/map.yaml with_chest_xtion:=false\n\n\n\n\n\n\nLaunch empty topological map:\n  \nroslaunch topological_navigation topological_navigation_empty_map.launch map:='name_of_topological_map'\n\n\n\n\n\n\nThis will create a basic map and run the topological system with it.\n\n\n\n\n\n\nOnce this is done you can run \nRviz\n, \nrosrun rviz rviz\n and create two \nmarker array\n interfaces \n/topological_node_zones_array\n and \n/topological_edges_array\n optionally an additional \nmarker array\n interface can be created \n/topological_nodes_array\n. This will allow the visualization of the topological map.\n\n\n\n\n\n\nAfter this the map can be edited using interactive markers:\n\n\n\n\n\n\nAdding Nodes:\n for this objective there is one interactive marker topic called \n/name_of_your_map/add_rm_node/update\n that will put a green box on top of the robot. Drive the robot where you want to add a new node and press on the green box that will add a node there connected to all close by nodes by a \nmove_base\n action.\n\n\n\n\n\n\nEditing Node Position:\n with \n/name_of_your_map_markers/update\n  it is possible to move the waypoints around and change their final goal orientation.\n\n\n\n\n\n\nRemoving Unnecessary Edges:\n to remove edges there is another marker \n/name_of_your_map_edges/update\n that will allow you to remove edges by pressing on the red arrows.\n\n\n\n\n\n\nChange the Influence Zones:\n Finally it is possible to change the influence zones with \n/name_of_your_map_zones/update\n and moving the vertices of such zones around.\n\n\n\n\n\n\nUsing Gamepad\n\n\nThis method is basically the same as the previous method (follow steps 1 and 2, also 3 if/when needed) with the addition that in this case it is possible to add nodes at the current robot position by pressing button \n'B'\n on the Gamepad.\n\n\nEdit using MongoDB Client\n\n\n@TODO\n\n\nUsing Localise by Topic\n\n\nLocalise by topic is a \nJSON\n string defined in the topological map which is empty by default \n\"localise_by_topic\" : \"\"\n which means that the robot will use its pose to obtained its location on the topological map. \n\n\nHowever in some specific cases it might be necessary to localise the robot by means of an specific topic in this cases the localise by topic string should be defined with the following \nmandatory\n fields \n\n\n\n\ntopic\n: the name of the topic that will be used for localisation\n\n\nfield\n: the field of the topic that will be compared\n\n\nval\n: the value of \nfield\n that will be used to set the current node (\n/current_node\n)\n\n\n\n\nA typical localise by topic string will look like this:\n\n\n{\ntopic\n:\n/battery_state\n,\nfield\n:\ncharging\n,\nval\n:true}\n\n\n\n\n\nThere are also some \noptional\n fields that can be set:\n\n\n\n\nlocalise_anywhere\n: (default value \ntrue\n) If set to \ntrue\n topological localisation will assume the robot to be at the node whenever localisation by topic corresponds to the values on the string, else this will only happen when the robot is in the influence area of the specific node.\n\n\npersistency\n: (default value \n10\n) is number of consecutive messages with the correct value that must arrive before localisation by topic is determined\n\n\n\n\ntimeout\n: @todo\n\n\n\n\n\n\nPlease note: when localised by topic is active the robot will never assume this node to be \nclosest node\n  unless it is also \ncurrent node", 
            "title": "Home"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/#topological-navigation", 
            "text": "This node provides support for topological navigation in the STRANDS system.   This module requires:\n * move_base\n * strands_datacentre\n * monitored_navigation  You will also need to create a  topological map", 
            "title": "Topological Navigation"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/#launching-topological-localisation-and-navigation", 
            "text": "Once your map is inserted in the DB you can launch your topological navigation nodes using:\n   roslaunch topological_navigation topological_navigation.launch map:=topological_map_name node_by_node:=false   note:  When Node by node navigation is active the robot will cancel the topological navigation when it reaches the influence zone of a node and it will navigate to its waypoint aborting the action.  note:  Statistics are being recorded in the nav_stats collection within the autonomous_patrolling  note:  every action server for the actions stored in the topological map have to be running, for example if the ramp_climb action is required you will need the ramp_climb server running, you can run it using: rosrun ramp_climb ramp_climb", 
            "title": "Launching Topological Localisation and Navigation"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/#navigate", 
            "text": "You can test the topological navigation using:\n   rosrun topological_navigation nav_client.py Destination \n  Where destination is the name of the node you want to reach.    You can also run  RViz   rosrun rviz rviz  and subscribe to the interactive markers  /name_of_map_go_to/update  and clicking on the Green Arrows to move the robot around.", 
            "title": "Navigate"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/#create-topological-maps", 
            "text": "There are different ways of creating  topological maps :    Simultaneous Metric and Topological Mapping    From WayPoint File    Using RViz    Using Gamepad    Edit using MongoDB Client    *. [Using Localise by Topic] (#Using-Localise-by-Topic)", 
            "title": "Create Topological Maps"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/#simultaneous-metric-and-topological-mapping", 
            "text": "@To improve  To follow this process you will need to place the robot in the Charging station and reset the odometry, once it is there you can launch the mapping by running:  roslaunch topological_utils mapping.launch met_map:='name_of_metric_map' top_map:='name_of_topological_map'  Once this is done undock the robot using the Visual_charging_server.   When the robot is undocked you can move it around using the gamepad and when you wish to create a new topological node press button  'B'  once you are done save your metric map by pressing  'A' .  To edit the topological map you can use the following methods", 
            "title": "Simultaneous Metric and Topological Mapping"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/#creation-of-the-topological-map-from-waypoint-file", 
            "text": "( Modified ) The first step is to create the waypoint file by running:\n  rosrun topological_utils joy_add_waypoint.py name_of_the_waypoint_file.csv    Next create a topological map file from the waypoint file using: rosrun topological_utils tmap_from_waypoints.py input_file.csv output_file.tmap  this will create a topological tree on which every waypoint in the file is a node and each waypoint is connected to every other waypoint using move_base and an octagonal influence area, for example,    From a normal waypoint file like this:\n   STON\n  -2.3003,0.120533,-4.88295e-05,0,0,-0.0530802,0.99859\n  -7.95369,-3.03503,-4.4791e-05,0,0,0.928319,-0.371784\n  -8.91935,2.94528,0.00139425,0,0,0.473654,0.880711\n  -2.68708,-2.23003,-0.000478224,0,0,-0.759842,0.650108 \n  It will create a topological map file like this:\n  ```JSON\n  node: \n    ChargingPoint\n    waypoint:\n        -2.3003,0.120533,-4.88295e-05,0,0,-0.0530802,0.99859\n    edges:\n        WayPoint1, move_base\n        WayPoint2, move_base\n        WayPoint3, move_base\n    vertices:\n        1.380000,0.574000\n        0.574000,1.380000\n        -0.574000,1.380000\n        -1.380000,0.574000\n        -1.380000,-0.574000\n        -0.574000,-1.380000\n        0.574000,-1.380000\n        1.380000,-0.574000\n  node: \n    WayPoint1\n    waypoint:\n        -7.95369,-3.03503,-4.4791e-05,0,0,0.928319,-0.371784\n    edges:\n        ChargingPoint, move_base\n        WayPoint2, move_base\n        WayPoint3, move_base\n    vertices:\n        1.380000,0.574000\n        0.574000,1.380000\n        -0.574000,1.380000\n        -1.380000,0.574000\n        -1.380000,-0.574000\n        -0.574000,-1.380000\n        0.574000,-1.380000\n        1.380000,-0.574000\n  node: \n    WayPoint2\n    waypoint:\n        -8.91935,2.94528,0.00139425,0,0,0.473654,0.880711\n    edges:\n        ChargingPoint, move_base\n        WayPoint1, move_base\n        WayPoint3, move_base\n    vertices:\n        1.380000,0.574000\n        0.574000,1.380000\n        -0.574000,1.380000\n        -1.380000,0.574000\n        -1.380000,-0.574000\n        -0.574000,-1.380000\n        0.574000,-1.380000\n        1.380000,-0.574000\n  node: \n    WayPoint3\n    waypoint:\n        -2.68708,-2.23003,-0.000478224,0,0,-0.759842,0.650108\n    edges:\n        ChargingPoint, move_base\n        WayPoint1, move_base\n        WayPoint2, move_base\n    vertices:\n        1.380000,0.574000\n        0.574000,1.380000\n        -0.574000,1.380000\n        -1.380000,0.574000\n        -1.380000,-0.574000\n        -0.574000,-1.380000\n        0.574000,-1.380000\n        1.380000,-0.574000  ``\n1. *Editing the topological map*, lets say that you want to make sure that to reach a node you get there using a specific action like ramp_climb from another node then you will need to edit the topological map file. You can also edit the vertices of the influence area so it can be defined using different kinds of polygons.\n  For example, lets suppose that in the previous map you want WayPoint3 to be reached only from the Waypoint1 using ramp_climb` and that you want to define rectangular areas around your waypoints and a triangular area around the ChargingPoint, in such case your map file should look like this:  JSON\n  node: \n    ChargingPoint\n    waypoint:\n        -2.3003,0.120533,-4.88295e-05,0,0,-0.0530802,0.99859\n    edges:\n         WayPoint1, move_base\n         WayPoint2, move_base\n    vertices:\n        0.0,1.5\n        -1.5,-1.5\n        1.5,-1.5\n  node: \n    WayPoint1\n    waypoint:\n        -7.95369,-3.03503,-4.4791e-05,0,0,0.928319,-0.371784\n    edges:\n        ChargingPoint, move_base\n        WayPoint2, move_base\n        WayPoint3, ramp_climb\n    vertices:\n        1.380000,0.574000\n        -1.380000,0.574000\n        -1.380000,-0.574000\n        1.380000,-0.574000\n  node: \n    WayPoint2\n    waypoint:\n        -8.91935,2.94528,0.00139425,0,0,0.473654,0.880711\n    edges:\n        ChargingPoint, move_base\n        WayPoint1, move_base\n    vertices:\n        0.574000,1.380000\n        -0.574000,1.380000\n        -0.574000,-1.380000\n        0.574000,-1.380000\n  node: \n    WayPoint3\n    waypoint:\n        -2.68708,-2.23003,-0.000478224,0,0,-0.759842,0.650108\n    edges:\n        WayPoint1, ramp_climb\n    vertices:\n        1.5,1.5\n        -1.5,1.5\n        -1.5,-1.5\n        1.5,-1.5 \n  In this case the topological navigation node will always call ramp_climb when reaching or leaving WayPoint3, you can also edit the names of the nodes to fit your needs.\n   note:  At the moment only  move_base  and  ramp_climb  can be included.\n   note:  The polygons of the influence area will be created using the convex-hull approach so they may not correspond exactly to the vertices previously defined.   Once you are happy with your topological map you have to insert it in the strands_datacentre using:\n   rosrun topological_utils insert_map.py topological_map.tmap topological_map_name map_name", 
            "title": "Creation of the Topological map From WayPoint File"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/#using-rviz", 
            "text": "RViz  can be used to edit topological maps on system runtime, however there is also the possibility of inserting a basic  topological map  and editing it using  RViz . The Following steps will guide through this process.    It is necessary to insert a basic map in the DB and run the topological system based on it for this a launch file is provided and it can be used in the following way:    Launch 2d navigation using for example:\n   roslaunch strands_movebase movebase.launch map:=/path/to/your/map.yaml with_chest_xtion:=false    Launch empty topological map:\n   roslaunch topological_navigation topological_navigation_empty_map.launch map:='name_of_topological_map'    This will create a basic map and run the topological system with it.    Once this is done you can run  Rviz ,  rosrun rviz rviz  and create two  marker array  interfaces  /topological_node_zones_array  and  /topological_edges_array  optionally an additional  marker array  interface can be created  /topological_nodes_array . This will allow the visualization of the topological map.    After this the map can be edited using interactive markers:    Adding Nodes:  for this objective there is one interactive marker topic called  /name_of_your_map/add_rm_node/update  that will put a green box on top of the robot. Drive the robot where you want to add a new node and press on the green box that will add a node there connected to all close by nodes by a  move_base  action.    Editing Node Position:  with  /name_of_your_map_markers/update   it is possible to move the waypoints around and change their final goal orientation.    Removing Unnecessary Edges:  to remove edges there is another marker  /name_of_your_map_edges/update  that will allow you to remove edges by pressing on the red arrows.    Change the Influence Zones:  Finally it is possible to change the influence zones with  /name_of_your_map_zones/update  and moving the vertices of such zones around.", 
            "title": "Using RViz"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/#using-gamepad", 
            "text": "This method is basically the same as the previous method (follow steps 1 and 2, also 3 if/when needed) with the addition that in this case it is possible to add nodes at the current robot position by pressing button  'B'  on the Gamepad.", 
            "title": "Using Gamepad"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/#edit-using-mongodb-client", 
            "text": "@TODO", 
            "title": "Edit using MongoDB Client"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/#using-localise-by-topic", 
            "text": "Localise by topic is a  JSON  string defined in the topological map which is empty by default  \"localise_by_topic\" : \"\"  which means that the robot will use its pose to obtained its location on the topological map.   However in some specific cases it might be necessary to localise the robot by means of an specific topic in this cases the localise by topic string should be defined with the following  mandatory  fields    topic : the name of the topic that will be used for localisation  field : the field of the topic that will be compared  val : the value of  field  that will be used to set the current node ( /current_node )   A typical localise by topic string will look like this:  { topic : /battery_state , field : charging , val :true}   There are also some  optional  fields that can be set:   localise_anywhere : (default value  true ) If set to  true  topological localisation will assume the robot to be at the node whenever localisation by topic corresponds to the values on the string, else this will only happen when the robot is in the influence area of the specific node.  persistency : (default value  10 ) is number of consecutive messages with the correct value that must arrive before localisation by topic is determined   timeout : @todo    Please note: when localised by topic is active the robot will never assume this node to be  closest node   unless it is also  current node", 
            "title": "Using Localise by Topic"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/tests/", 
            "text": "Topological Navigation tests\n\n\nThe topological navigation tests are designed to optimise the parameter set of the DWA planner and to check the functionalitites of topological navigation in its execute policy mode. The test are run automatically on our Jenkins server but can also be run locally on your machine to test a new paremeterset. \n\n\nRun test\n\n\nTest for your whole worksapce can be run with \ncatkin_make test\n which assumes that \nstrands_navigation\n is present in your ros workspace. If you only want to run the test described here, use \ncatkin_make test --pkg topological_navigation\n. This will only run the critical topological navigation tests. A set of supplementary tests can be run with:\n\n\nroslaunch topological_navigation navigation_scenarios.test\nrosrun topological_navigation topological_navigation_tester_supplementary.py\n\n\n\n\nTo run the testes from an installed version of topological_navigation do:\n\n\nroslaunch topological_navigation navigation_scenarios.test\n\n\n\n\nand \n\n\nrosrun topological_navigation topological_navigation_tester_critical.py\n\n\n\n\nfor the critical tests or\n\n\nrosrun topological_navigation topological_navigation_tester_supplementary.py\n\n\n\n\nfor the supplementary tests to test parameter sets.\n\n\nThis will start all the tests and report the result to the terminal and a log file.\n\n\nTest Scenarios, and Infrastructure\n\n\nInfrastructure\n\n\nIn order to run the tests, the commands above will start the following nodes:\n\n\n\n\nstrands_morse\n using the move_base_arena environment\n\n\nmongodb_store\n in test mode, creating a new and empty temporary datacentre\n\n\nscitos_2d_navigation\n with the correct slam map for the tests\n\n\nA static transform publisher with the transformation from \nmap\n to \nworld\n\n\ntopological_navigation\n:\n\n\nmonitored_navigation\n with all recovery behaviours disabled\n\n\nmap_manager.py\n\n\nfremenserver\n\n\nlocalisation.py\n\n\nnavigation.py\n\n\nexecute_policy_server.py\n\n\nvisualise_map.py\n\n\ntravel_time_estimator.py\n\n\ntopological_prediction.py\n\n\nget_simple_policy.py\n\n\nThe test scneario server which is responsible for the test control\n\n\nThe ros test file \ntopological_navigation_tester.py\n\n\n\n\nTest Scenarios\n\n\nAll the test scnearios are based on a simple topological map that has to have a Node called \nStart\n and a node called \nEnd\n. The robot will always be teleported (in simulation) or pushed (on the real robot) to this node and then use policy execution to traverse the edges towards the node called \nEnd\n. If the node \nEnd\n cannot be reached, the navigation will fail and the scneario_server will trigger the graceful death behaviour, trying to navigate the robot back to the node \nStart\n.\n\n\nThe topological maps used for the tests presented here can be found in \nstrands_morse/mba/maps\n and are loaded into the datacentre (if they haven't already been inserted) by the scanario_server. Currently run tests are:\n\n\nStatic:\n\n\n\n\nSystem critical:\n\n\nmb_test0\n: Traversing a 2m wide l-shaped corridor.\n\n\nSupplementary:\n\n\nmb_test1\n: The robot starting 10cm away from a wall facing it straight on (0 degrees)\n\n\nmb_test2\n: The robot starting 10cm away from a wall facing it turned to the left (-45 degrees)\n\n\nmb_test3\n: The robot starting 10cm away from a wall facing it turned to the right (+45 degrees)\n\n\nmb_test4\n: Traversing a 1.55m wide straight corridor with .55m wide chairs on one side.\n\n\nmb_test5\n: Traversing a 2.1m wide straight corridor with .55m wide chairs on both sides.\n\n\nmb_test6\n: Cross a 80cm wide door.\n\n\nmb_test7\n: Cross a 70cm wide door.\n\n\nmb_test8\n: The robot is trapped in a corner and has to reach a goal behind it.\n\n\nmb_test9\n: Traverse a 1m wide straight corridor.\n\n\nmb_test10\n: Traverse a 1m wide l-shaped corridor.\n\n\nmb_test11\n: Wheelchair blocking an intermediate node in open space.\n\n\nmb_test12\n: Wheelchair blocking final node in open space. Graceful fail has to be tru to pass this test as the node itself can never be reached precisely.\n\n\nmb_test13\n: Human blocking an intermediate node in open space.\n\n\nmb_test14\n: Human blocking final node in open space. Graceful fail has to be tru to pass this test as the node itself can never be reached precisely.\n\n\nmb_test15\n: Non-SLAM map chairs on one side of 2m wide l-shaped corridor.\n\n\nmb_test16\n: Non-SLAM map wheelchairs on one side of 2m wide l-shaped corridor.\n\n\nmb_test17\n: Non-SLAM map wheelchairs block 2m wide l-shaped corridor after intermediate waypoint. Graceful fail has to be tru to pass this test as the node itself can never be reached.\n\n\nmb_test18\n: Non-SLAM map static humans block 2m wide l-shaped corridor after intermediate waypoint. Graceful fail has to be tru to pass this test as the node itself can never be reached.\n\n\n\n\nDynamic\n\n\nComing soon\n\n\nScenario Server control\n\n\nThe scenario server can also be used without the unit test file for tests on the real robot. Running\n\n\nroslaunch topological_navigation navigation_scenarios.test robot:=true map_dir:=\npath_to_topologicalmaps\n\n\n\n\n\nwill start only the scneario server, the simple policy generation, and the joypad control. This assumes that everything on the robot, up to topological navigation, is running. The \nmap_dir\n argument specifies a directory which holds the topological maps that you want to use for testing. If this is given, the maps will automatically inserted into the datacentre. If the maps have been inserted previously, the server will print a warning and skip the insertion. If the \nmap_dir\n argument is omitted, no maps are inserted. The scneario server then offers 3 simple services:\n\n\n\n\n/scneario_server/load \nmap_name\n expects a string which is the name of the topological map you want to test. Keep in mind that this has to have the node \nStart\n and \nEnd\n.\n\n\n/scenario_server/reset\n is an empty service and is called to reset the data recording and the robot position. The robot position, however, cannot as easily be changed in real life as it can be in simulation, hence the robot has to be pushed to the starting node and then confirmed via the \nA\n button on the joypad. The reasoning behind having to push the robot is that the starting position might not be easily reachable via \nmove_base\n. The server will print \n+++ Please push the robot to 'Start' +++\n where \nStart\n is the name of the starting node, until the node is reached. Once the node is reached, the server will print \n+++ Please confirm correct positioning with A button on joypad: distance 0.65m 180.06deg +++\n where distance represents the distance of the current \n/robot_pose\n to the metric coordinates of the node. In simulation this will just teleport the robot to the correct node.\n\n\n/scneario_server/start\n starts the policy execution. Returns\n\n\n\n\nbool nav_success\nbool graceful_fail\nbool human_success\nfloat64 min_distance_to_human\nfloat64 mean_speed\nfloat64 distance_travelled\nfloat64 travel_time\n\n\nJoypad control\n\n\nFor convenience, a joypad control of the \nreset\n and \nstart\n service is provided. The \nload\n service still has to be called manually to tell the server which map to use. The joypad control then offers an easy way to interact with the scenario server during the tests:\n\n\n\n\nA\n: Toggle between \nstart\n and \nreset\n.\n\n\nB\n: Confirm current selection.\n\n\n\n\nIf in doubt, press \nA\n and look for the output on the terminal.\n\n\nCreating Topological maps for testing\n\n\nThe topological map used for each test has to be a different one and the nodes have to conform to a specific naming scheme. By default, the start node has to be called \nStart\n and the goal has to be called \nEnd\n. This can be changed in \ntopologcial_navigation/tests/conf/scenario_server.yaml\n. The easiest way to create these maps is:\n\n\n\n\nStart topological navigation with \nroslaunch topological_navigation topologocal_navigation_empty_map.launch map:=\nmap_name\n where \nmap_name\n will be the name of your new map and cannot be the name of an existing one.\n\n\nDrive the robot to the positions of the nodes you want to create and use the \nadd_rm_node\n interactive marker in rviz to create a new node.\n\n\nUse the interactive \nedges\n marker in rviz to delete unwanted edges.\n\n\nRename the start and end node to \nStart\n and \nEnd\n using \nrosrun topological_utils rename_node \nold_name\n \nnew_name\n \nmap_name\n\n\nThis map can now be loaded with \n/scenario_server/load \nmap_name\n\n\n\n\nCreating scnearios with obstacles that are not in the SLAM map\n\n\nFor this purpose, there are \nChair\ns, \nOfficeChair\ns, \nWheelChair\ns, and \nStaticHuman\ns present in the current test environment that can be positioned via topological nodes. The exact obstacle types are defined in the \nconf/scenario_server.yaml\n under \nobstacle_types\n; The names used have to be the variable name of the obstacle in the morse environment. Obstacle types have to be lower case, node names can be camel case to enhance readability. Currently, in the test environment, each object has 10 instances so you can only use 10 of any single object. The Objects are positioned according to topological nodes following a naming scheme: \nObstacleStaticHuman1\n for example positions a static human model on this node. \nObstacle\n is the \nobstacle_node_prefix\n  defined in the \nconf/scneario_server.yaml\n, \nStaticHuman\n is the identifier of the obstacle type (if this omitted, an arbitrary object will be used), and \n1\n is just an arbitrary number to make the node name unique. When creating the topoligocal map, make sure that all edges from and to obstacle nodes are removed. Additionally, the nodes need to define a \nlocalise_by_topic\n json string so the robot will never localise itself based on these nodes. In the current simulation, we use the charging topic, because the robot will never charge and hence never localise itself at these nodes. We have to use an existing topic otherwise topological navigation fails. Example node:\n\n\n- meta:\n     map: mb_arena\n     node: ObstacleStaticHuman1\n     pointset: mb_test16\n   node:\n     edges: []\n     localise_by_topic: '{\ntopic\n: \n/battery_state\n, \nfield\n: \ncharging\n, \nval\n: true}'\n     map: mb_arena\n     name: ObstacleStaticHuman1\n     pointset: mb_test16\n     pose:\n       orientation:\n         w: 0.816770076752\n         ...\n       position:\n        x: -4.58532047272\n        ...\n     verts:\n     - x: 0.689999997616\n       y: 0.287000000477\n     ...\n     xy_goal_tolerance: 0.3\n     yaw_goal_tolerance: 0.1\n\n\n\n\nWhere the important bit is the \nlocalise_by_topic: '{\"topic\": \"/battery_state\", \"field\": \"charging\", \"val\": true}'\n entry.\n\n\nAfter the map has been loaded the obstacles will be spawned at (or better moved to) the respective nodes before the robot starts navigating. Before each test the arena is cleared to make sure that no obstacles linger.", 
            "title": "Tests"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/tests/#topological-navigation-tests", 
            "text": "The topological navigation tests are designed to optimise the parameter set of the DWA planner and to check the functionalitites of topological navigation in its execute policy mode. The test are run automatically on our Jenkins server but can also be run locally on your machine to test a new paremeterset.", 
            "title": "Topological Navigation tests"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/tests/#run-test", 
            "text": "Test for your whole worksapce can be run with  catkin_make test  which assumes that  strands_navigation  is present in your ros workspace. If you only want to run the test described here, use  catkin_make test --pkg topological_navigation . This will only run the critical topological navigation tests. A set of supplementary tests can be run with:  roslaunch topological_navigation navigation_scenarios.test\nrosrun topological_navigation topological_navigation_tester_supplementary.py  To run the testes from an installed version of topological_navigation do:  roslaunch topological_navigation navigation_scenarios.test  and   rosrun topological_navigation topological_navigation_tester_critical.py  for the critical tests or  rosrun topological_navigation topological_navigation_tester_supplementary.py  for the supplementary tests to test parameter sets.  This will start all the tests and report the result to the terminal and a log file.", 
            "title": "Run test"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/tests/#test-scenarios-and-infrastructure", 
            "text": "", 
            "title": "Test Scenarios, and Infrastructure"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/tests/#infrastructure", 
            "text": "In order to run the tests, the commands above will start the following nodes:   strands_morse  using the move_base_arena environment  mongodb_store  in test mode, creating a new and empty temporary datacentre  scitos_2d_navigation  with the correct slam map for the tests  A static transform publisher with the transformation from  map  to  world  topological_navigation :  monitored_navigation  with all recovery behaviours disabled  map_manager.py  fremenserver  localisation.py  navigation.py  execute_policy_server.py  visualise_map.py  travel_time_estimator.py  topological_prediction.py  get_simple_policy.py  The test scneario server which is responsible for the test control  The ros test file  topological_navigation_tester.py", 
            "title": "Infrastructure"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/tests/#test-scenarios", 
            "text": "All the test scnearios are based on a simple topological map that has to have a Node called  Start  and a node called  End . The robot will always be teleported (in simulation) or pushed (on the real robot) to this node and then use policy execution to traverse the edges towards the node called  End . If the node  End  cannot be reached, the navigation will fail and the scneario_server will trigger the graceful death behaviour, trying to navigate the robot back to the node  Start .  The topological maps used for the tests presented here can be found in  strands_morse/mba/maps  and are loaded into the datacentre (if they haven't already been inserted) by the scanario_server. Currently run tests are:  Static:   System critical:  mb_test0 : Traversing a 2m wide l-shaped corridor.  Supplementary:  mb_test1 : The robot starting 10cm away from a wall facing it straight on (0 degrees)  mb_test2 : The robot starting 10cm away from a wall facing it turned to the left (-45 degrees)  mb_test3 : The robot starting 10cm away from a wall facing it turned to the right (+45 degrees)  mb_test4 : Traversing a 1.55m wide straight corridor with .55m wide chairs on one side.  mb_test5 : Traversing a 2.1m wide straight corridor with .55m wide chairs on both sides.  mb_test6 : Cross a 80cm wide door.  mb_test7 : Cross a 70cm wide door.  mb_test8 : The robot is trapped in a corner and has to reach a goal behind it.  mb_test9 : Traverse a 1m wide straight corridor.  mb_test10 : Traverse a 1m wide l-shaped corridor.  mb_test11 : Wheelchair blocking an intermediate node in open space.  mb_test12 : Wheelchair blocking final node in open space. Graceful fail has to be tru to pass this test as the node itself can never be reached precisely.  mb_test13 : Human blocking an intermediate node in open space.  mb_test14 : Human blocking final node in open space. Graceful fail has to be tru to pass this test as the node itself can never be reached precisely.  mb_test15 : Non-SLAM map chairs on one side of 2m wide l-shaped corridor.  mb_test16 : Non-SLAM map wheelchairs on one side of 2m wide l-shaped corridor.  mb_test17 : Non-SLAM map wheelchairs block 2m wide l-shaped corridor after intermediate waypoint. Graceful fail has to be tru to pass this test as the node itself can never be reached.  mb_test18 : Non-SLAM map static humans block 2m wide l-shaped corridor after intermediate waypoint. Graceful fail has to be tru to pass this test as the node itself can never be reached.   Dynamic  Coming soon", 
            "title": "Test Scenarios"
        }, 
        {
            "location": "/strands_navigation/topological_navigation/tests/#scenario-server-control", 
            "text": "The scenario server can also be used without the unit test file for tests on the real robot. Running  roslaunch topological_navigation navigation_scenarios.test robot:=true map_dir:= path_to_topologicalmaps   will start only the scneario server, the simple policy generation, and the joypad control. This assumes that everything on the robot, up to topological navigation, is running. The  map_dir  argument specifies a directory which holds the topological maps that you want to use for testing. If this is given, the maps will automatically inserted into the datacentre. If the maps have been inserted previously, the server will print a warning and skip the insertion. If the  map_dir  argument is omitted, no maps are inserted. The scneario server then offers 3 simple services:   /scneario_server/load  map_name  expects a string which is the name of the topological map you want to test. Keep in mind that this has to have the node  Start  and  End .  /scenario_server/reset  is an empty service and is called to reset the data recording and the robot position. The robot position, however, cannot as easily be changed in real life as it can be in simulation, hence the robot has to be pushed to the starting node and then confirmed via the  A  button on the joypad. The reasoning behind having to push the robot is that the starting position might not be easily reachable via  move_base . The server will print  +++ Please push the robot to 'Start' +++  where  Start  is the name of the starting node, until the node is reached. Once the node is reached, the server will print  +++ Please confirm correct positioning with A button on joypad: distance 0.65m 180.06deg +++  where distance represents the distance of the current  /robot_pose  to the metric coordinates of the node. In simulation this will just teleport the robot to the correct node.  /scneario_server/start  starts the policy execution. Returns   bool nav_success\nbool graceful_fail\nbool human_success\nfloat64 min_distance_to_human\nfloat64 mean_speed\nfloat64 distance_travelled\nfloat64 travel_time  Joypad control  For convenience, a joypad control of the  reset  and  start  service is provided. The  load  service still has to be called manually to tell the server which map to use. The joypad control then offers an easy way to interact with the scenario server during the tests:   A : Toggle between  start  and  reset .  B : Confirm current selection.   If in doubt, press  A  and look for the output on the terminal.  Creating Topological maps for testing  The topological map used for each test has to be a different one and the nodes have to conform to a specific naming scheme. By default, the start node has to be called  Start  and the goal has to be called  End . This can be changed in  topologcial_navigation/tests/conf/scenario_server.yaml . The easiest way to create these maps is:   Start topological navigation with  roslaunch topological_navigation topologocal_navigation_empty_map.launch map:= map_name  where  map_name  will be the name of your new map and cannot be the name of an existing one.  Drive the robot to the positions of the nodes you want to create and use the  add_rm_node  interactive marker in rviz to create a new node.  Use the interactive  edges  marker in rviz to delete unwanted edges.  Rename the start and end node to  Start  and  End  using  rosrun topological_utils rename_node  old_name   new_name   map_name  This map can now be loaded with  /scenario_server/load  map_name   Creating scnearios with obstacles that are not in the SLAM map  For this purpose, there are  Chair s,  OfficeChair s,  WheelChair s, and  StaticHuman s present in the current test environment that can be positioned via topological nodes. The exact obstacle types are defined in the  conf/scenario_server.yaml  under  obstacle_types ; The names used have to be the variable name of the obstacle in the morse environment. Obstacle types have to be lower case, node names can be camel case to enhance readability. Currently, in the test environment, each object has 10 instances so you can only use 10 of any single object. The Objects are positioned according to topological nodes following a naming scheme:  ObstacleStaticHuman1  for example positions a static human model on this node.  Obstacle  is the  obstacle_node_prefix   defined in the  conf/scneario_server.yaml ,  StaticHuman  is the identifier of the obstacle type (if this omitted, an arbitrary object will be used), and  1  is just an arbitrary number to make the node name unique. When creating the topoligocal map, make sure that all edges from and to obstacle nodes are removed. Additionally, the nodes need to define a  localise_by_topic  json string so the robot will never localise itself based on these nodes. In the current simulation, we use the charging topic, because the robot will never charge and hence never localise itself at these nodes. We have to use an existing topic otherwise topological navigation fails. Example node:  - meta:\n     map: mb_arena\n     node: ObstacleStaticHuman1\n     pointset: mb_test16\n   node:\n     edges: []\n     localise_by_topic: '{ topic :  /battery_state ,  field :  charging ,  val : true}'\n     map: mb_arena\n     name: ObstacleStaticHuman1\n     pointset: mb_test16\n     pose:\n       orientation:\n         w: 0.816770076752\n         ...\n       position:\n        x: -4.58532047272\n        ...\n     verts:\n     - x: 0.689999997616\n       y: 0.287000000477\n     ...\n     xy_goal_tolerance: 0.3\n     yaw_goal_tolerance: 0.1  Where the important bit is the  localise_by_topic: '{\"topic\": \"/battery_state\", \"field\": \"charging\", \"val\": true}'  entry.  After the map has been loaded the obstacles will be spawned at (or better moved to) the respective nodes before the robot starts navigating. Before each test the arena is cleared to make sure that no obstacles linger.", 
            "title": "Scenario Server control"
        }, 
        {
            "location": "/strands_navigation/wiki/Home/", 
            "text": "Welcome to the strands_navigation wiki!\n\n\n[[Useful Mongodb Queries and Updates]]", 
            "title": "Home"
        }, 
        {
            "location": "/strands_navigation/wiki/Topological-Map-Definition/", 
            "text": "Topological Map\n\n\nA topological map in the strands system is published over the '/topological_map' topic its ros message is type defined the following \nway\n:\n\n\nstring name          # topological map name generally should be the same as in the pointset field\nstring map           # 2D Metric map name (not used currently)\nstring pointset      # The name of the group of nodes that compose the topological map, typically the same as name\nstring last_updated  # Last time the map was modified\nstrands_navigation_msgs/TopologicalNode[] nodes  # The group of nodes that composes the topological map\n\n\n\n\nTopological Nodes\n\n\nThe nodes are the locations to which the robot should navigate on its working environment these location are related to coordinates in the robot map frame, the nodes are defined as a ros message in the following [way] (https://github.com/strands-project/strands_navigation/blob/indigo-devel/strands_navigation_msgs/msg/TopologicalNode.msg)\n\n\nstring name                 # Node name\nstring map                  # 2D Metric map name (not used currently)\nstring pointset             # The name of the group of nodes that compose the topological map\ngeometry_msgs/Pose pose     # Node X,Y,Z,W coordinates in the map frame\nfloat64 yaw_goal_tolerance  # The tolerance in radians for the waypoint position\nfloat64 xy_goal_tolerance   # The tolerance in meters for the waypoint position\nVertex[] verts              # X,Y coordinates for the vertices of the influence zone relative to the node pose\nEdge[] edges                # The group of outward oriented edges connecting this node to others\nstring localise_by_topic    # The configuration for localisation by topic \n\n\n\n\nEdges\n\n\nThe edges are the definitions for the connections between nodes, typically this connections are included in the node message, which means that the edges defined for each node are the ones on which the node they are defined at is the point of origin, they are also defined as a ros message type in the following \nway\n\n\nstring edge_id                        # An identifier for the edge\nstring node                           # Name of destination node\nstring action                         # Name of navigation action, e.g. move_base\nfloat64 top_vel                       # Maximum speed in [m/s]\nstring map_2d                         # Name of the 2D map \nfloat64 inflation_radius              # Obstacle inflation radius on this edge, 0 for default\nstring recovery_behaviours_config     # Not currently used, typically a json string\n\n\n\n\nYAML format\n\n\nthe topological map can also be defined in yaml format, this is used to export and manually modify maps a map on this format would look like this:\n\n\n- meta:\n    map: mb_arena\n    node: Start\n    pointset: mb_test2\n  node:\n    edges:\n    - action: move_base\n      edge_id: Start_End\n      inflation_radius: 0.0\n      map_2d: mb_arena\n      node: End\n      recovery_behaviours_config: ''\n      top_vel: 0.55\n    localise_by_topic: ''\n    map: mb_arena\n    name: Start\n    pointset: mb_test2\n    pose:\n      orientation:\n        w: 0.92388\n        x: 0\n        y: 0\n        z: 0.38268\n      position:\n        x: -1.5\n        y: 2.6\n        z: 0.0\n    verts:\n    - x: 0.689999997616\n      y: 0.287000000477\n    - x: 0.287000000477\n      y: 0.689999997616\n    - x: -0.287000000477\n      y: 0.689999997616\n    - x: -0.689999997616\n      y: 0.287000000477\n    - x: -0.689999997616\n      y: -0.287000000477\n    - x: -0.287000000477\n      y: -0.689999997616\n    - x: 0.287000000477\n      y: -0.689999997616\n    - x: 0.689999997616\n      y: -0.287000000477\n    xy_goal_tolerance: 0.3\n    yaw_goal_tolerance: 0.1\n- meta:\n    map: mb_arena\n    node: End\n    pointset: mb_test2\n  node:\n    edges:\n    - action: move_base\n      edge_id: End_Start\n      inflation_radius: 0.0\n      map_2d: mb_arena\n      node: Start\n      recovery_behaviours_config: ''\n      top_vel: 0.55\n    localise_by_topic: ''\n    map: mb_arena\n    name: End\n    pointset: mb_test2\n    pose:\n      orientation:\n        w: 0.70711\n        x: 0.0\n        y: 0.0\n        z: -0.70711\n      position:\n        x: -1.5\n        y: -0.5\n        z: 0.0\n    verts:\n    - x: 0.689999997616\n      y: 0.287000000477\n    - x: 0.287000000477\n      y: 0.689999997616\n    - x: -0.287000000477\n      y: 0.689999997616\n    - x: -0.689999997616\n      y: 0.287000000477\n    - x: -0.689999997616\n      y: -0.287000000477\n    - x: -0.287000000477\n      y: -0.689999997616\n    - x: 0.287000000477\n      y: -0.689999997616\n    - x: 0.689999997616\n      y: -0.287000000477\n    xy_goal_tolerance: 0.3\n    yaw_goal_tolerance: 0.1\n\n\n\n\nAll the fields correspond to the definitions presented before, please note that each node has a \"meta\" field.\n\n\nNode Meta information\n\n\nEach node is stored in the datacentre with additional meta information, in the YAML version of a topological map this meta information \nMUST\n be included with at lest the three fields shown in the map, however any other meta information can be added as long as it is included in a valid YAML format for example:\n\n\n- meta:\n    map: mb_arena\n    node: End\n    pointset: mb_test2\n    example: 'this is an example field'", 
            "title": "Topological Map Definition"
        }, 
        {
            "location": "/strands_navigation/wiki/Topological-Map-Definition/#topological-map", 
            "text": "A topological map in the strands system is published over the '/topological_map' topic its ros message is type defined the following  way :  string name          # topological map name generally should be the same as in the pointset field\nstring map           # 2D Metric map name (not used currently)\nstring pointset      # The name of the group of nodes that compose the topological map, typically the same as name\nstring last_updated  # Last time the map was modified\nstrands_navigation_msgs/TopologicalNode[] nodes  # The group of nodes that composes the topological map", 
            "title": "Topological Map"
        }, 
        {
            "location": "/strands_navigation/wiki/Topological-Map-Definition/#topological-nodes", 
            "text": "The nodes are the locations to which the robot should navigate on its working environment these location are related to coordinates in the robot map frame, the nodes are defined as a ros message in the following [way] (https://github.com/strands-project/strands_navigation/blob/indigo-devel/strands_navigation_msgs/msg/TopologicalNode.msg)  string name                 # Node name\nstring map                  # 2D Metric map name (not used currently)\nstring pointset             # The name of the group of nodes that compose the topological map\ngeometry_msgs/Pose pose     # Node X,Y,Z,W coordinates in the map frame\nfloat64 yaw_goal_tolerance  # The tolerance in radians for the waypoint position\nfloat64 xy_goal_tolerance   # The tolerance in meters for the waypoint position\nVertex[] verts              # X,Y coordinates for the vertices of the influence zone relative to the node pose\nEdge[] edges                # The group of outward oriented edges connecting this node to others\nstring localise_by_topic    # The configuration for localisation by topic", 
            "title": "Topological Nodes"
        }, 
        {
            "location": "/strands_navigation/wiki/Topological-Map-Definition/#edges", 
            "text": "The edges are the definitions for the connections between nodes, typically this connections are included in the node message, which means that the edges defined for each node are the ones on which the node they are defined at is the point of origin, they are also defined as a ros message type in the following  way  string edge_id                        # An identifier for the edge\nstring node                           # Name of destination node\nstring action                         # Name of navigation action, e.g. move_base\nfloat64 top_vel                       # Maximum speed in [m/s]\nstring map_2d                         # Name of the 2D map \nfloat64 inflation_radius              # Obstacle inflation radius on this edge, 0 for default\nstring recovery_behaviours_config     # Not currently used, typically a json string", 
            "title": "Edges"
        }, 
        {
            "location": "/strands_navigation/wiki/Topological-Map-Definition/#yaml-format", 
            "text": "the topological map can also be defined in yaml format, this is used to export and manually modify maps a map on this format would look like this:  - meta:\n    map: mb_arena\n    node: Start\n    pointset: mb_test2\n  node:\n    edges:\n    - action: move_base\n      edge_id: Start_End\n      inflation_radius: 0.0\n      map_2d: mb_arena\n      node: End\n      recovery_behaviours_config: ''\n      top_vel: 0.55\n    localise_by_topic: ''\n    map: mb_arena\n    name: Start\n    pointset: mb_test2\n    pose:\n      orientation:\n        w: 0.92388\n        x: 0\n        y: 0\n        z: 0.38268\n      position:\n        x: -1.5\n        y: 2.6\n        z: 0.0\n    verts:\n    - x: 0.689999997616\n      y: 0.287000000477\n    - x: 0.287000000477\n      y: 0.689999997616\n    - x: -0.287000000477\n      y: 0.689999997616\n    - x: -0.689999997616\n      y: 0.287000000477\n    - x: -0.689999997616\n      y: -0.287000000477\n    - x: -0.287000000477\n      y: -0.689999997616\n    - x: 0.287000000477\n      y: -0.689999997616\n    - x: 0.689999997616\n      y: -0.287000000477\n    xy_goal_tolerance: 0.3\n    yaw_goal_tolerance: 0.1\n- meta:\n    map: mb_arena\n    node: End\n    pointset: mb_test2\n  node:\n    edges:\n    - action: move_base\n      edge_id: End_Start\n      inflation_radius: 0.0\n      map_2d: mb_arena\n      node: Start\n      recovery_behaviours_config: ''\n      top_vel: 0.55\n    localise_by_topic: ''\n    map: mb_arena\n    name: End\n    pointset: mb_test2\n    pose:\n      orientation:\n        w: 0.70711\n        x: 0.0\n        y: 0.0\n        z: -0.70711\n      position:\n        x: -1.5\n        y: -0.5\n        z: 0.0\n    verts:\n    - x: 0.689999997616\n      y: 0.287000000477\n    - x: 0.287000000477\n      y: 0.689999997616\n    - x: -0.287000000477\n      y: 0.689999997616\n    - x: -0.689999997616\n      y: 0.287000000477\n    - x: -0.689999997616\n      y: -0.287000000477\n    - x: -0.287000000477\n      y: -0.689999997616\n    - x: 0.287000000477\n      y: -0.689999997616\n    - x: 0.689999997616\n      y: -0.287000000477\n    xy_goal_tolerance: 0.3\n    yaw_goal_tolerance: 0.1  All the fields correspond to the definitions presented before, please note that each node has a \"meta\" field.", 
            "title": "YAML format"
        }, 
        {
            "location": "/strands_navigation/wiki/Topological-Map-Definition/#node-meta-information", 
            "text": "Each node is stored in the datacentre with additional meta information, in the YAML version of a topological map this meta information  MUST  be included with at lest the three fields shown in the map, however any other meta information can be added as long as it is included in a valid YAML format for example:  - meta:\n    map: mb_arena\n    node: End\n    pointset: mb_test2\n    example: 'this is an example field'", 
            "title": "Node Meta information"
        }, 
        {
            "location": "/strands_navigation/wiki/Useful-Mongodb-Queries-and-Updates/", 
            "text": "Topological maps are stored in the \ntopological_maps\n collection in the \nmessage_store\n database. Using \nRobomongo\n (or the \nmongodb\n tool) one can easily run mongodb queries and updates to inspect or modify the topological maps. See http://docs.mongodb.org/manual/reference/method/db.collection.update/#db.collection.update for more information about \u00b4update\u00b4.\n\n\nRobomongo\n\n\nTo run queries in robomongo connect to the \nmongodb_store\n database (usually running on port 62345), and then open a shell in the \nmessage_store\n database by right clicking on it.\n\n\nSearching for Tags\n\n\ndb.getCollection('topological_maps').find(\n    {pointset:'aaf_predep', '_meta.tag':'no_go'}\n)\n\n\n\n\nChanging Actions of Edges\n\n\nThis will find all entries of pointset \naaf_predep\n where the action is defined as \nmove_base\n. Then it will set \nthe first\n matching entry in the \nedges\n array to \nhuman_aware_navigation\n. \n\n\nNote that this always only replaces the \nfirst\n matching value in the array, hence this needs to be executed repeatedly until no more changes occur. Mongodb unfortunately doesn't all yet to change all at once (see https://jira.mongodb.org/browse/SERVER-1243)\n\n\ndb.topological_maps.update(\n    {pointset:'aaf_predep', 'edges.action':'move_base'},\n    {$set: {'edges.$.action':'human_aware_navigation'}},\n    {multi:true}\n)", 
            "title": "Useful Mongodb Queries and Updates"
        }, 
        {
            "location": "/strands_navigation/wiki/Useful-Mongodb-Queries-and-Updates/#robomongo", 
            "text": "To run queries in robomongo connect to the  mongodb_store  database (usually running on port 62345), and then open a shell in the  message_store  database by right clicking on it.", 
            "title": "Robomongo"
        }, 
        {
            "location": "/strands_navigation/wiki/Useful-Mongodb-Queries-and-Updates/#searching-for-tags", 
            "text": "db.getCollection('topological_maps').find(\n    {pointset:'aaf_predep', '_meta.tag':'no_go'}\n)", 
            "title": "Searching for Tags"
        }, 
        {
            "location": "/strands_navigation/wiki/Useful-Mongodb-Queries-and-Updates/#changing-actions-of-edges", 
            "text": "This will find all entries of pointset  aaf_predep  where the action is defined as  move_base . Then it will set  the first  matching entry in the  edges  array to  human_aware_navigation .   Note that this always only replaces the  first  matching value in the array, hence this needs to be executed repeatedly until no more changes occur. Mongodb unfortunately doesn't all yet to change all at once (see https://jira.mongodb.org/browse/SERVER-1243)  db.topological_maps.update(\n    {pointset:'aaf_predep', 'edges.action':'move_base'},\n    {$set: {'edges.$.action':'human_aware_navigation'}},\n    {multi:true}\n)", 
            "title": "Changing Actions of Edges"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker/", 
            "text": "People Tracker\n\n\nThis package uses the bayes_tracking library developed by Nicola Bellotto (University of Lincoln), please cite with: \n10.5281/zenodo.15825\n and [1]\n\n\nThe people_tracker uses a single config file to add an arbitrary amount of detectors. The file \nconfig/detectors.yaml\n contains the necessary information for the upper_body_detector and the ROS leg_detector (see \nto_pose_array\n in detector_msg_to_pose_array/README.md):\n\n\nbayes_people_tracker:\n    filter_type: \nUKF\n                                         # The Kalman filter type: EKF = Extended Kalman Filter, UKF = Uncented Kalman Filter\n    cv_noise_params:                                           # The noise for the constant velocity prediction model\n        x: 1.4\n        y: 1.4\n    detectors:                                                 # Add detectors under this namespace\n        upper_body_detector:                                   # Name of detector (used internally to identify them). Has to be unique.\n            topic: \n/upper_body_detector/bounding_box_centres\n # The topic on which the geometry_msgs/PoseArray is published\n            cartesian_noise_params:                            # The noise for the cartesian observation model\n                x: 0.5\n                y: 0.5\n            matching_algorithm: \nNNJPDA\n                       # The algorthim to match different detections. NN = Nearest Neighbour, NNJPDA = NN Joint Probability Data Association\n        leg_detector:                                          # Name of detector (used internally to identify them). Has to be unique.\n            topic: \n/to_pose_array/leg_detector\n               # The topic on which the geometry_msgs/PoseArray is published\n            cartesian_noise_params:                            # The noise for the cartesian observation model\n                x: 0.2\n                y: 0.2\n            matching_algorithm: \nNNJPDA\n                       # The algorthim to match different detections. NN = Nearest Neighbour, NNJPDA = NN Joint Probability Data Association\n\n\n\n\nNew detectors are added under the parameter namespace \nbayes_people_tracker/detectors\n. Let's have a look at the upper body detector as an example:\n\n\nTracker Parameters\n\n\nThe tracker offers two configuration parameters:\n\n \nfilter_type\n: This specefies which variant of the Kalman filter to use. Currently, it implements an Extended and an Unscented Kalman filter which can be chosen via \nEKF\n and \nUKF\n, respectively.\n\n \ncv_noise_params\n: parameter is used for the constant velocity prediction model.\n * specifies the standard deviation of the x and y velocity.\n\n\nDetector Parameters\n\n\n\n\nFor every detector you have to create a new namespace where the name is used as an internal identifier for this detector. Therefore it has to be unique. In this case it is \nupper_body_detector\n\n\nThe \ntopic\n parameter specifies the topic under which the detections are published. The type has to be \ngeometry_msgs/PoseArray\n. See \nto_pose_array\n in detector_msg_to_pose_array/README.md if your detector does not publish a PoseArray.\n\n\nThe \ncartesian_noise_params\n parameter is used for the Cartesian observation model.\n\n\nspecifies the standard deviation of x and y.\n\n\nmatching_algorithm\n specifies the algorithm used to match detections from different sensors/detectors. Currently there are two different algorithms which are based on the Mahalanobis distance of the detections (default being NNJPDA if parameter is misspelled):\n\n\nNN: Nearest Neighbour\n\n\nNNJPDA: Nearest Neighbour Joint Probability Data Association\n\n\n\n\nAll of these are just normal ROS parameters and can be either specified by the parameter server or using the yaml file in the provided launch file.\n\n\nMessage Type:\n\n\nThe tracker publishes the following:\n\n \npose\n: A \ngeometry_msgs/PoseStamped\n for the clostes person.\n\n \npose_array\n: A \ngeometry_msgs/PoseArray\n for all detections.\n\n \npeople\n: A \npeople_msgs/People\n for all detections. Can be used for layerd costmaps.\n\n \nmarker_array\n: A \nvisualization_msgs/MarkerArray\n showing little stick figures for every detection. Figures are orient according to the direction of velocity.\n* \npositions\n: A \nbayes_people_tracker/PeopleTracker\n message. See below. \n\n\nstd_msgs/Header header\nstring[] uuids             # Unique uuid5 (NAMESPACE_DNS) person id as string. Id is based on system time on start-up and tracker id. Array index matches ids array index\ngeometry_msgs/Pose[] poses # The real world poses of the detected people in the given target frame. Default: /map. Array index matches ids/uuids array index\nfloat64[] distances        # The distances of the detected persons to the robot (polar coordinates). Array index matches ids array index.\nfloat64[] angles           # Angles of the detected persons to the coordinate frames z axis (polar coordinates). Array index matches ids array index.\nfloat64 min_distance       # The minimal distance in the distances array.\nfloat64 min_distance_angle # The angle according to the minimal distance.\n\n\n\n\nThe poses will be published in a given \ntarget_frame\n (see below) but the distances and angles will always be relative to the robot in the \n/base_link\n tf frame.\n\n\nRunning\n\n\nParameters:\n\n\n\n\ntarget_frame\n: \nDefault: /base_link\n:the tf frame in which the tracked poses will be published. \n\n\nposition\n: \nDefault: /people_tracker/positions\n: The topic under which the results are published as bayes_people_tracker/PeopleTracker`\n\n\npose\n: \nDefault: /people_tracker/pose\n: The topic under which the closest detected person is published as a geometry_msgs/PoseStamped`\n\n\npose_array\n: \nDefault: /people_tracker/pose_array\n: The topic under which the detections are published as a geometry_msgs/PoseArray`\n\n\npoeple\n: \nDefault: /people_tracker/people\n: The topic under which the results are published as people_msgs/People`\n\n\nmarker\n: \nDefault /people_tracker/marker_array\n: A visualisation marker array.\n\n\n\n\nYou can run the node with:\n\n\nroslaunch bayes_people_tracker people_tracker.launch\n\n\n\n\nThis is the recommended way of launching it since this will also read the config file and set the right parameters for the detectors.\n\n\nUpdating old bag files\n\n\nWith version \n1.1.8 the message type of the people tracker has been changed to include the velocities of humans as a Vector3. To update old rosbag files just run \nrosbag check\n this should tell you that there is a rule file to update this bag. Then run \nrosbag fix\n to update your old bag file and change the message type to the new version. The velocities will all be \n0\n but apart from that everything should work as intended.\n\n\n[1] N. Bellotto and H. Hu, \u201cComputationally efficient solutions for tracking people with a mobile robot: an experimental evaluation of bayesian filters,\u201d Autonomous Robots, vol. 28, no. 4, pp. 425\u2013438, 2010.", 
            "title": "Bayes people tracker"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker/#people-tracker", 
            "text": "This package uses the bayes_tracking library developed by Nicola Bellotto (University of Lincoln), please cite with:  10.5281/zenodo.15825  and [1]  The people_tracker uses a single config file to add an arbitrary amount of detectors. The file  config/detectors.yaml  contains the necessary information for the upper_body_detector and the ROS leg_detector (see  to_pose_array  in detector_msg_to_pose_array/README.md):  bayes_people_tracker:\n    filter_type:  UKF                                          # The Kalman filter type: EKF = Extended Kalman Filter, UKF = Uncented Kalman Filter\n    cv_noise_params:                                           # The noise for the constant velocity prediction model\n        x: 1.4\n        y: 1.4\n    detectors:                                                 # Add detectors under this namespace\n        upper_body_detector:                                   # Name of detector (used internally to identify them). Has to be unique.\n            topic:  /upper_body_detector/bounding_box_centres  # The topic on which the geometry_msgs/PoseArray is published\n            cartesian_noise_params:                            # The noise for the cartesian observation model\n                x: 0.5\n                y: 0.5\n            matching_algorithm:  NNJPDA                        # The algorthim to match different detections. NN = Nearest Neighbour, NNJPDA = NN Joint Probability Data Association\n        leg_detector:                                          # Name of detector (used internally to identify them). Has to be unique.\n            topic:  /to_pose_array/leg_detector                # The topic on which the geometry_msgs/PoseArray is published\n            cartesian_noise_params:                            # The noise for the cartesian observation model\n                x: 0.2\n                y: 0.2\n            matching_algorithm:  NNJPDA                        # The algorthim to match different detections. NN = Nearest Neighbour, NNJPDA = NN Joint Probability Data Association  New detectors are added under the parameter namespace  bayes_people_tracker/detectors . Let's have a look at the upper body detector as an example:", 
            "title": "People Tracker"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker/#tracker-parameters", 
            "text": "The tracker offers two configuration parameters:   filter_type : This specefies which variant of the Kalman filter to use. Currently, it implements an Extended and an Unscented Kalman filter which can be chosen via  EKF  and  UKF , respectively.   cv_noise_params : parameter is used for the constant velocity prediction model.\n * specifies the standard deviation of the x and y velocity.", 
            "title": "Tracker Parameters"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker/#detector-parameters", 
            "text": "For every detector you have to create a new namespace where the name is used as an internal identifier for this detector. Therefore it has to be unique. In this case it is  upper_body_detector  The  topic  parameter specifies the topic under which the detections are published. The type has to be  geometry_msgs/PoseArray . See  to_pose_array  in detector_msg_to_pose_array/README.md if your detector does not publish a PoseArray.  The  cartesian_noise_params  parameter is used for the Cartesian observation model.  specifies the standard deviation of x and y.  matching_algorithm  specifies the algorithm used to match detections from different sensors/detectors. Currently there are two different algorithms which are based on the Mahalanobis distance of the detections (default being NNJPDA if parameter is misspelled):  NN: Nearest Neighbour  NNJPDA: Nearest Neighbour Joint Probability Data Association   All of these are just normal ROS parameters and can be either specified by the parameter server or using the yaml file in the provided launch file.", 
            "title": "Detector Parameters"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker/#message-type", 
            "text": "The tracker publishes the following:   pose : A  geometry_msgs/PoseStamped  for the clostes person.   pose_array : A  geometry_msgs/PoseArray  for all detections.   people : A  people_msgs/People  for all detections. Can be used for layerd costmaps.   marker_array : A  visualization_msgs/MarkerArray  showing little stick figures for every detection. Figures are orient according to the direction of velocity.\n*  positions : A  bayes_people_tracker/PeopleTracker  message. See below.   std_msgs/Header header\nstring[] uuids             # Unique uuid5 (NAMESPACE_DNS) person id as string. Id is based on system time on start-up and tracker id. Array index matches ids array index\ngeometry_msgs/Pose[] poses # The real world poses of the detected people in the given target frame. Default: /map. Array index matches ids/uuids array index\nfloat64[] distances        # The distances of the detected persons to the robot (polar coordinates). Array index matches ids array index.\nfloat64[] angles           # Angles of the detected persons to the coordinate frames z axis (polar coordinates). Array index matches ids array index.\nfloat64 min_distance       # The minimal distance in the distances array.\nfloat64 min_distance_angle # The angle according to the minimal distance.  The poses will be published in a given  target_frame  (see below) but the distances and angles will always be relative to the robot in the  /base_link  tf frame.", 
            "title": "Message Type:"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker/#running", 
            "text": "Parameters:   target_frame :  Default: /base_link :the tf frame in which the tracked poses will be published.   position :  Default: /people_tracker/positions : The topic under which the results are published as bayes_people_tracker/PeopleTracker`  pose :  Default: /people_tracker/pose : The topic under which the closest detected person is published as a geometry_msgs/PoseStamped`  pose_array :  Default: /people_tracker/pose_array : The topic under which the detections are published as a geometry_msgs/PoseArray`  poeple :  Default: /people_tracker/people : The topic under which the results are published as people_msgs/People`  marker :  Default /people_tracker/marker_array : A visualisation marker array.   You can run the node with:  roslaunch bayes_people_tracker people_tracker.launch  This is the recommended way of launching it since this will also read the config file and set the right parameters for the detectors.", 
            "title": "Running"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker/#updating-old-bag-files", 
            "text": "With version  1.1.8 the message type of the people tracker has been changed to include the velocities of humans as a Vector3. To update old rosbag files just run  rosbag check  this should tell you that there is a rule file to update this bag. Then run  rosbag fix  to update your old bag file and change the message type to the new version. The velocities will all be  0  but apart from that everything should work as intended.  [1] N. Bellotto and H. Hu, \u201cComputationally efficient solutions for tracking people with a mobile robot: an experimental evaluation of bayesian filters,\u201d Autonomous Robots, vol. 28, no. 4, pp. 425\u2013438, 2010.", 
            "title": "Updating old bag files"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker_logging/", 
            "text": "Logging package\n\n\nThis packge contains a logging node to save the detections to the message_store.\n\n\nAll the information given on how to run the nodes should only be used if you need to run them seperately. In normal cases please refer to the \nperception_people_launch\n package to start the whole perception pipeline.\n\n\nLogging\n\n\nThis node uses the \nLogging.msg\n to save the detected people together with their realworld position, the robots pose, the upper body detector and people tracker results, and the tf transform used to create the real world coordinates in the message store.\n\n\nRun with:\n\n\nroslaunch bayes_people_tracker_logging logging.launch\n\n\nParameters:\n* \nlog\n: \nDefault: true\n This convenience parameter allows to start the whole system without logging the data\n\n\nUpdating old database entries\n\n\nThis assumes that your \nmongodb_store\n is running.\n\n\nWith version \n1.1.8 the message type of the people tracker has been changed to include the velocities of humans as a Vector3. To update old database entries just run \nrosrun bayes_people_tracker_logging migrate.py\n which will update entries in place. Please be careful and create a copy of the \npeople_perception\n collection in the message store before running this.", 
            "title": "Bayes people tracker logging"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker_logging/#logging-package", 
            "text": "This packge contains a logging node to save the detections to the message_store.  All the information given on how to run the nodes should only be used if you need to run them seperately. In normal cases please refer to the  perception_people_launch  package to start the whole perception pipeline.", 
            "title": "Logging package"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker_logging/#logging", 
            "text": "This node uses the  Logging.msg  to save the detected people together with their realworld position, the robots pose, the upper body detector and people tracker results, and the tf transform used to create the real world coordinates in the message store.  Run with:  roslaunch bayes_people_tracker_logging logging.launch  Parameters:\n*  log :  Default: true  This convenience parameter allows to start the whole system without logging the data", 
            "title": "Logging"
        }, 
        {
            "location": "/strands_perception_people/bayes_people_tracker_logging/#updating-old-database-entries", 
            "text": "This assumes that your  mongodb_store  is running.  With version  1.1.8 the message type of the people tracker has been changed to include the velocities of humans as a Vector3. To update old database entries just run  rosrun bayes_people_tracker_logging migrate.py  which will update entries in place. Please be careful and create a copy of the  people_perception  collection in the message store before running this.", 
            "title": "Updating old database entries"
        }, 
        {
            "location": "/strands_perception_people/detector_msg_to_pose_array/", 
            "text": "Message conversion package\n\n\nThis package contains a node to convert messages of people detectors to a pose_array which is naturally understood by the bayes_people_tracker. Some detectors use custom message types which makes it hard to use them otherwise.\n\n\nAll the information given on how to run the nodes should only be used if you need to run them seperately. In normal cases please refer to the \nperception_people_launch\n package to start the whole perception pipeline.\n\n\nto_pose_array\n\n\nSmall node that takes in an arbitrary message from a topic and extracts a pose according to a given identifier. The found poses are published as a geometry_msgs/PoseArray. The node is used to transform the output of any people detector to a pose array for the people_tracker. The node is configured using the detectors.yaml in the config directory:\n\n\nto_pose_array:\n    detectors:                                  # Add detectors under this namespace\n        leg_detector:                           # Name of detector (used internally to identify them. Has to be unique.\n            topic: \n/people_tracker_measurements\n  # The topic on which the geometry_msgs/PoseArray is published\n            point_name: \npos\n                   # The name of the point containing the coordinates in question\n\n\n\n\nThe parameter namespace \nto_pose_array/detectors\n can contain as many sub namespaces as needed. The above example shows the namespace \nto_pose_array/detectors/leg_detector\n which contains the information to parse the messages generated by the ros hydro package \nleg_detector\n. \n\n\n\n\ntopic\n: this string is the topic name under which the messages containing the positions are published.\n\n\npoint_name\n: this string specifies the identifier for the detected positions. In this case the leg_detector publishes a message which caontains data like:\n\n\n\n\npos.x\npos.y\npos.z\n\n\n\n\nThe message is parsed for all occurences of the \npos\n identifier and the result is published as a \nPoseArray\n.\n\n\nRun with\n\n\nroslaunch detector_msg_to_pose_array to_pose_array.launch", 
            "title": "Detector msg to pose array"
        }, 
        {
            "location": "/strands_perception_people/detector_msg_to_pose_array/#message-conversion-package", 
            "text": "This package contains a node to convert messages of people detectors to a pose_array which is naturally understood by the bayes_people_tracker. Some detectors use custom message types which makes it hard to use them otherwise.  All the information given on how to run the nodes should only be used if you need to run them seperately. In normal cases please refer to the  perception_people_launch  package to start the whole perception pipeline.", 
            "title": "Message conversion package"
        }, 
        {
            "location": "/strands_perception_people/detector_msg_to_pose_array/#to_pose_array", 
            "text": "Small node that takes in an arbitrary message from a topic and extracts a pose according to a given identifier. The found poses are published as a geometry_msgs/PoseArray. The node is used to transform the output of any people detector to a pose array for the people_tracker. The node is configured using the detectors.yaml in the config directory:  to_pose_array:\n    detectors:                                  # Add detectors under this namespace\n        leg_detector:                           # Name of detector (used internally to identify them. Has to be unique.\n            topic:  /people_tracker_measurements   # The topic on which the geometry_msgs/PoseArray is published\n            point_name:  pos                    # The name of the point containing the coordinates in question  The parameter namespace  to_pose_array/detectors  can contain as many sub namespaces as needed. The above example shows the namespace  to_pose_array/detectors/leg_detector  which contains the information to parse the messages generated by the ros hydro package  leg_detector .    topic : this string is the topic name under which the messages containing the positions are published.  point_name : this string specifies the identifier for the detected positions. In this case the leg_detector publishes a message which caontains data like:   pos.x\npos.y\npos.z  The message is parsed for all occurences of the  pos  identifier and the result is published as a  PoseArray .  Run with  roslaunch detector_msg_to_pose_array to_pose_array.launch", 
            "title": "to_pose_array"
        }, 
        {
            "location": "/strands_perception_people/ground_plane_estimation/", 
            "text": "Ground Plane\n\n\nThis package estimates the ground plane using depth images. \n\n\nIt can also be used with a fixed ground plane which is just rotated according to the ptu tilt angle. In this case the optimal ground plane is assumed and represented by the normal vector [0,-1,0] and the distance of 1.7 (Distance of the origin of the camera coordinated system to the plane in meters). These values can be changed in the config/fixed_gp.yaml file. As you can see this assumes to be used with the head_xtion and is therefore exclusive to the robot but it also prevents failurs due to wrongly estimated planes in cases where the camera can't see the ground. All this was necessary because the camera on the head of the robot is rather high and therefore has a high risk of not seeing the actual ground plane. At the current state of development it is advised to use the fixed plane set-up when running it on the robot.\n\n\nRun\n\n\nParameters for estimation:\n\n \nload_params_from_file\n \ndefault = true\n: \nfalse\n tries to read parameters from datacentre, \ntrue\n reads parameters from YAML file specified by \nparam_file\n\n\n \nparam_file\n \ndefault = $(find ground_plane_estimation)/config/estimated_gp.yaml\n: The config file containing all the essential parameters. Only used if \nload_params_from_file == true\n.\n\n \nmachine\n \ndefault = localhost\n: Determines on which machine this node should run.\n\n \nuser\n \ndefault = \"\"\n: The user used for the ssh connection if machine is not localhost.\n\n \nqueue_size\n \ndefault = 5\n: The synchronisation queue size\n\n \nconfig_file\n \ndefault = \"\"\n: The global config file. Can be found in ground_plane_estimation/config\n\n \ncamera_namespace\n \ndefault = /head_xtion\n: The camera namespace.\n\n \ndepth_image\n \ndefault = /depth/image_rect\n: \ncamera_namespace\n + \ndepth_image\n = depth image topic\n\n \ncamera_info_rgb\n \ndefault = /rgb/camera_info\n: \ncamera_namespace\n + \ncamera_info_rgb\n = rgb camera info topic\n\n \nground_plane\n \ndefault = /ground_plane\n: The estimated ground plane\n\n\nParameters for the fixed ground plane:\n\n \nload_params_from_file\n \ndefault = true\n: \nfalse\n tries to read parameters from datacentre, \ntrue\n reads parameters from YAML file specified by \nparam_file\n\n\n \nparam_file\n \ndefault = $(find ground_plane_estimation)/config/fixed_gp.yaml\n: The config file containing all the essential parameters. Only used if \nload_params_from_file == true\n.\n\n \nmachine\n \ndefault = localhost\n: Determines on which machine this node should run.\n\n \nuser\n \ndefault = \"\"\n: The user used for the ssh connection if machine is not localhost.\n\n \nptu_state\n \ndefault = /ptu/state\n: The current angles of the ptu\n\n \nground_plane\n \ndefault = /ground_plane\n: The rotated ground plane\n\n\nrosrun:\n\n\nrosrun ground_plane_estimation ground_plane_estimated [_parameter_name:=value]\n\n\n\n\nor\n\n\nrosrun ground_plane_estimation ground_plane_fixed [_parameter_name:=value]\n\n\n\n\nroslaunch:\n\n\nroslaunch ground_plane_estimation ground_plane_estimated.launch [parameter_name:=value]\n\n\n\n\nor\n\n\nroslaunch ground_plane_estimation ground_plane_fixed.launch [parameter_name:=value]", 
            "title": "Ground plane estimation"
        }, 
        {
            "location": "/strands_perception_people/ground_plane_estimation/#ground-plane", 
            "text": "This package estimates the ground plane using depth images.   It can also be used with a fixed ground plane which is just rotated according to the ptu tilt angle. In this case the optimal ground plane is assumed and represented by the normal vector [0,-1,0] and the distance of 1.7 (Distance of the origin of the camera coordinated system to the plane in meters). These values can be changed in the config/fixed_gp.yaml file. As you can see this assumes to be used with the head_xtion and is therefore exclusive to the robot but it also prevents failurs due to wrongly estimated planes in cases where the camera can't see the ground. All this was necessary because the camera on the head of the robot is rather high and therefore has a high risk of not seeing the actual ground plane. At the current state of development it is advised to use the fixed plane set-up when running it on the robot.", 
            "title": "Ground Plane"
        }, 
        {
            "location": "/strands_perception_people/ground_plane_estimation/#run", 
            "text": "Parameters for estimation:   load_params_from_file   default = true :  false  tries to read parameters from datacentre,  true  reads parameters from YAML file specified by  param_file    param_file   default = $(find ground_plane_estimation)/config/estimated_gp.yaml : The config file containing all the essential parameters. Only used if  load_params_from_file == true .   machine   default = localhost : Determines on which machine this node should run.   user   default = \"\" : The user used for the ssh connection if machine is not localhost.   queue_size   default = 5 : The synchronisation queue size   config_file   default = \"\" : The global config file. Can be found in ground_plane_estimation/config   camera_namespace   default = /head_xtion : The camera namespace.   depth_image   default = /depth/image_rect :  camera_namespace  +  depth_image  = depth image topic   camera_info_rgb   default = /rgb/camera_info :  camera_namespace  +  camera_info_rgb  = rgb camera info topic   ground_plane   default = /ground_plane : The estimated ground plane  Parameters for the fixed ground plane:   load_params_from_file   default = true :  false  tries to read parameters from datacentre,  true  reads parameters from YAML file specified by  param_file    param_file   default = $(find ground_plane_estimation)/config/fixed_gp.yaml : The config file containing all the essential parameters. Only used if  load_params_from_file == true .   machine   default = localhost : Determines on which machine this node should run.   user   default = \"\" : The user used for the ssh connection if machine is not localhost.   ptu_state   default = /ptu/state : The current angles of the ptu   ground_plane   default = /ground_plane : The rotated ground plane  rosrun:  rosrun ground_plane_estimation ground_plane_estimated [_parameter_name:=value]  or  rosrun ground_plane_estimation ground_plane_fixed [_parameter_name:=value]  roslaunch:  roslaunch ground_plane_estimation ground_plane_estimated.launch [parameter_name:=value]  or  roslaunch ground_plane_estimation ground_plane_fixed.launch [parameter_name:=value]", 
            "title": "Run"
        }, 
        {
            "location": "/strands_perception_people/human_trajectory/", 
            "text": "Human Trajectory\n\n\nThis is a ROS package that extracts human poses information from people tracker and stitches each pose for each human together. This package can be run online by subscribing to /people_tracker/positions or offline by retrieving human poses from either perception_people collection or people_trajectory collection.\n\n\nRun this package by typing \n\n\nroslaunch human_trajectory human_trajectory.launch\n\n\n\n\nThe online stitching provides both mini batch trajectories messages where trajectories are split into chunked and complete batch trajectories which only publishes complete trajectory messages when the persons are not detected anymore. \n\n\nParameters\n\n\n\n\nwith_logging_manager\n: the option (true/false) to subscribe to logging manager and get\n  permission to store obtained data\n\n\npath_visualisation\n:  the option to visualise each detected trajectory in rviz using Path. However, it only works for online construction. \n\n\nonline_construction\n: the option (true/false) to stitch human poses online from other packages (bayes_people_tracker) or offline from perception_people or people_trajectory collection in mongodb.\n\n\ntracker_topic\n: the name of the people tracker topic, default is /people_tracker/positions\n\n\nlogging_manager_topic\n: the name of the logging manager topic, default is /logging_manager/log_stamped\n\n\n\n\nNote\n\n\nThe offline retrieval limits the number of trajectories obtained from mongodb to 10000 trajectories. To work around the limitation, OfflineTrajectories class provides a query that can be passed to mongodb to obtain specific trajectories.", 
            "title": "Human trajectory"
        }, 
        {
            "location": "/strands_perception_people/human_trajectory/#human-trajectory", 
            "text": "This is a ROS package that extracts human poses information from people tracker and stitches each pose for each human together. This package can be run online by subscribing to /people_tracker/positions or offline by retrieving human poses from either perception_people collection or people_trajectory collection.  Run this package by typing   roslaunch human_trajectory human_trajectory.launch  The online stitching provides both mini batch trajectories messages where trajectories are split into chunked and complete batch trajectories which only publishes complete trajectory messages when the persons are not detected anymore.", 
            "title": "Human Trajectory"
        }, 
        {
            "location": "/strands_perception_people/human_trajectory/#parameters", 
            "text": "with_logging_manager : the option (true/false) to subscribe to logging manager and get\n  permission to store obtained data  path_visualisation :  the option to visualise each detected trajectory in rviz using Path. However, it only works for online construction.   online_construction : the option (true/false) to stitch human poses online from other packages (bayes_people_tracker) or offline from perception_people or people_trajectory collection in mongodb.  tracker_topic : the name of the people tracker topic, default is /people_tracker/positions  logging_manager_topic : the name of the logging manager topic, default is /logging_manager/log_stamped", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_perception_people/human_trajectory/#note", 
            "text": "The offline retrieval limits the number of trajectories obtained from mongodb to 10000 trajectories. To work around the limitation, OfflineTrajectories class provides a query that can be passed to mongodb to obtain specific trajectories.", 
            "title": "Note"
        }, 
        {
            "location": "/strands_perception_people/", 
            "text": "strands_perception_people\n\n\nPlease see perception_people_launch/README.md for start-up information.\n\n\nWhen using the default STRANDS perception pipeline, please cite:\n\n\n@inproceedings{dondrup2015tracking,\n  title={Real-time multisensor people tracking for human-robot spatial interaction},\n  author={Dondrup, Christian and Bellotto, Nicola and Jovan, Ferdian and Hanheide, Marc},\n  publisher={ICRA/IEEE},\n  booktitle={Workshop on Machine Learning for Social Robotics at International Conference on Robotics and Automation (ICRA)},\n  year={2015}\n}\n\n\n\n\nThis package contains the people perception pipeline. It is comprised of two detectors:\n\n Upper body detector\n\n Leg Detector: http://wiki.ros.org/leg_detector\n\n\nDepricated and moved to attic branch:\n* Ground HOG feature detector\n\n\nTwo trackers:\n\n Bayesian People Tracker\n\n Pedestrian Tracker (currently depricated)\n\n\nAnd a lot of utility and helper nodes. See https://www.youtube.com/watch?v=zdnvhQU1YNo for a concise explanation. \n\n\nPlease refere to the READMEs in the specific packages.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_perception_people/#strands_perception_people", 
            "text": "Please see perception_people_launch/README.md for start-up information.  When using the default STRANDS perception pipeline, please cite:  @inproceedings{dondrup2015tracking,\n  title={Real-time multisensor people tracking for human-robot spatial interaction},\n  author={Dondrup, Christian and Bellotto, Nicola and Jovan, Ferdian and Hanheide, Marc},\n  publisher={ICRA/IEEE},\n  booktitle={Workshop on Machine Learning for Social Robotics at International Conference on Robotics and Automation (ICRA)},\n  year={2015}\n}  This package contains the people perception pipeline. It is comprised of two detectors:  Upper body detector  Leg Detector: http://wiki.ros.org/leg_detector  Depricated and moved to attic branch:\n* Ground HOG feature detector  Two trackers:  Bayesian People Tracker  Pedestrian Tracker (currently depricated)  And a lot of utility and helper nodes. See https://www.youtube.com/watch?v=zdnvhQU1YNo for a concise explanation.   Please refere to the READMEs in the specific packages.", 
            "title": "strands_perception_people"
        }, 
        {
            "location": "/strands_perception_people/mdl_people_tracker/", 
            "text": "Pedestrian Tracking\n\n\nThis package uses the data generated by ground_hog, visual_odometry und upper_body_detector to track people. It is also possible to omit the ground_hog detections.\n\n\nRun\n\n\nParameters:\n\n \nload_params_from_file\n \ndefault = true\n: \nfalse\n tries to read parameters from datacentre, \ntrue\n reads parameters from YAML file specified by \nconfig_file\n\n\n \nconfig_file\n \ndefault = $(find mdl_people_tracker)/config/mdl_people_tracker.yaml\n: The config file containing all the essential parameters. Only used if \nload_params_from_file == true\n.\n\n \nmachine\n \ndefault = localhost\n: Determines on which machine this node should run.\n\n \nuser\n \ndefault = \"\"\n: The user used for the ssh connection if machine is not localhost.\n\n \nqueue_size\n \ndefault = 20\n: The synchronisation queue size\n\n \ncamera_namespace\n \ndefault = /head_xtion\n: The camera namespace.\n\n \nrgb_image\n \ndefault = /rgb/image_rect_color\n: \ncamera_namespace\n + \nrgb_image\n = rgb image topic\n\n \ncamera_info_rgb\n \ndefault = /rgb/camera_info\n: \ncamera_namespace\n + \ncamera_info_rgb\n = rgb camera info topic\n\n \nground_plane\n \ndefault = /ground_plane\n: The ground plane published by the upper_body_detector\n\n \nground_hog\n \ndefault = /groundHOG/detections\n: The ground_hog detections\n\n \nupper_body_detections\n \ndefault = /upper_body_detector/detections\n: The upper_body_detector_detections\n\n \nvisual_odometry\n \ndefault = /visual_odometry/motion_matrix\n: The visual_odometry data\n\n \npedestrain_array\n \ndefault = /mdl_people_tracker/people_array\n: The resulting tracking data.\n\n \npeople_markers\" default=\"/mdl_people_tracker/marker_array_: A visualisation array for rviz\n*\npeople_poses\" default = /mdl_people_tracker/pose_array_: A PoseArray of the detected people\n\n\nrosrun:\n\n\nrosrun mdl_people_tracker mdl_people_tracker [_parameter_name:=value]\n\n\n\n\nroslaunch:\n\n\nroslaunch mdl_people_tracker mdl_people_tracker_with_HOG.launch [parameter_name:=value]\n\n\n\n\nTo run the tracker without the groundHOG feture extraction running use:\n\n\nrosrun mdl_people_tracker mdl_people_tracker _ground_hog:=\n\n\n\n\n\nor\n\n\nroslaunch mdl_people_tracker mdl_people_tracker.launch [parameter_name:=value]", 
            "title": "Mdl people tracker"
        }, 
        {
            "location": "/strands_perception_people/mdl_people_tracker/#pedestrian-tracking", 
            "text": "This package uses the data generated by ground_hog, visual_odometry und upper_body_detector to track people. It is also possible to omit the ground_hog detections.", 
            "title": "Pedestrian Tracking"
        }, 
        {
            "location": "/strands_perception_people/mdl_people_tracker/#run", 
            "text": "Parameters:   load_params_from_file   default = true :  false  tries to read parameters from datacentre,  true  reads parameters from YAML file specified by  config_file    config_file   default = $(find mdl_people_tracker)/config/mdl_people_tracker.yaml : The config file containing all the essential parameters. Only used if  load_params_from_file == true .   machine   default = localhost : Determines on which machine this node should run.   user   default = \"\" : The user used for the ssh connection if machine is not localhost.   queue_size   default = 20 : The synchronisation queue size   camera_namespace   default = /head_xtion : The camera namespace.   rgb_image   default = /rgb/image_rect_color :  camera_namespace  +  rgb_image  = rgb image topic   camera_info_rgb   default = /rgb/camera_info :  camera_namespace  +  camera_info_rgb  = rgb camera info topic   ground_plane   default = /ground_plane : The ground plane published by the upper_body_detector   ground_hog   default = /groundHOG/detections : The ground_hog detections   upper_body_detections   default = /upper_body_detector/detections : The upper_body_detector_detections   visual_odometry   default = /visual_odometry/motion_matrix : The visual_odometry data   pedestrain_array   default = /mdl_people_tracker/people_array : The resulting tracking data.   people_markers\" default=\"/mdl_people_tracker/marker_array_: A visualisation array for rviz\n* people_poses\" default = /mdl_people_tracker/pose_array_: A PoseArray of the detected people  rosrun:  rosrun mdl_people_tracker mdl_people_tracker [_parameter_name:=value]  roslaunch:  roslaunch mdl_people_tracker mdl_people_tracker_with_HOG.launch [parameter_name:=value]  To run the tracker without the groundHOG feture extraction running use:  rosrun mdl_people_tracker mdl_people_tracker _ground_hog:=   or  roslaunch mdl_people_tracker mdl_people_tracker.launch [parameter_name:=value]", 
            "title": "Run"
        }, 
        {
            "location": "/strands_perception_people/odometry_to_motion_matrix/", 
            "text": "Odometry to motion_matrix package\n\n\nThis packge contains a tool for the conversion of the robots odometry to a motion matrix to substitude the visual odometry.\n\n\nAll the information given on how to run the nodes should only be used if you need to run them seperately. In normal cases please refer to the \nperception_people_launch\n package to start the whole perception pipeline.\n\n\nodom2visual\n\n\nThis node creates a motion matrix from the robots odometry using the Eigen library to substitude the visual odometry.\n\n\nRun with:\n\n\nroslaunch odometry_to_motion_matrix odom2visual.launch\n\n\nParameters:\n\n \nodom\n: \nDefault: /odom\n The topic on which the robots odometry is published\n\n \nmotion_parameters\n: \nDefault: /visual_odometry/motion_matrix\n The topic on which the resulting motion matrix is published", 
            "title": "Odometry to motion matrix"
        }, 
        {
            "location": "/strands_perception_people/odometry_to_motion_matrix/#odometry-to-motion_matrix-package", 
            "text": "This packge contains a tool for the conversion of the robots odometry to a motion matrix to substitude the visual odometry.  All the information given on how to run the nodes should only be used if you need to run them seperately. In normal cases please refer to the  perception_people_launch  package to start the whole perception pipeline.", 
            "title": "Odometry to motion_matrix package"
        }, 
        {
            "location": "/strands_perception_people/odometry_to_motion_matrix/#odom2visual", 
            "text": "This node creates a motion matrix from the robots odometry using the Eigen library to substitude the visual odometry.  Run with:  roslaunch odometry_to_motion_matrix odom2visual.launch  Parameters:   odom :  Default: /odom  The topic on which the robots odometry is published   motion_parameters :  Default: /visual_odometry/motion_matrix  The topic on which the resulting motion matrix is published", 
            "title": "odom2visual"
        }, 
        {
            "location": "/strands_perception_people/rwth_upper_body_skeleton_random_walk/", 
            "text": "Upper body skeleton estimator\n\n\nThis package estimates the 3d positions of the 9 upper body skeleton joints.\n\n\nLaunching\n\n\nUse the following launchfile for launching:\n\n\nroslaunch rwth_upper_body_skeleton_random_walk fixed.launch \n\n\n\n\n\nThe skeleton detector requires the output of the upper body detector as a starting point for rough person segmentation from background and will not work if upper bodies are not detected.\n\n\nRun\n\n\nDependencies\n\n\nThis node needs \nupper_body_detector/upper_body_detector.launch\n to run,\nwhich in turn needs \nground_plane_estimation/ground_plane_fixed.launch\n.\n\n\nParameters\n\n\n\n\ndepth_image_msg\n *default = /head_xtion/depth/image_rect_meters: Depth Image Frame.\n\n\nupper_body_msg\n *default = /upper_body_detector/detections: The deteced upper bodies\n\n\nrgb_image_msg\n *default = /head_xtion/rgb/image_rect_color: RGB Image Frame.\n\n\n\n\nroslaunch\n\n\nroslaunch rwth_upper_body_skeleton_random_walk fixed.launch  [parameter_name:=value]", 
            "title": "Rwth upper body skeleton random walk"
        }, 
        {
            "location": "/strands_perception_people/rwth_upper_body_skeleton_random_walk/#upper-body-skeleton-estimator", 
            "text": "This package estimates the 3d positions of the 9 upper body skeleton joints.", 
            "title": "Upper body skeleton estimator"
        }, 
        {
            "location": "/strands_perception_people/rwth_upper_body_skeleton_random_walk/#launching", 
            "text": "Use the following launchfile for launching:  roslaunch rwth_upper_body_skeleton_random_walk fixed.launch   The skeleton detector requires the output of the upper body detector as a starting point for rough person segmentation from background and will not work if upper bodies are not detected.", 
            "title": "Launching"
        }, 
        {
            "location": "/strands_perception_people/rwth_upper_body_skeleton_random_walk/#run", 
            "text": "", 
            "title": "Run"
        }, 
        {
            "location": "/strands_perception_people/rwth_upper_body_skeleton_random_walk/#dependencies", 
            "text": "This node needs  upper_body_detector/upper_body_detector.launch  to run,\nwhich in turn needs  ground_plane_estimation/ground_plane_fixed.launch .", 
            "title": "Dependencies"
        }, 
        {
            "location": "/strands_perception_people/rwth_upper_body_skeleton_random_walk/#parameters", 
            "text": "depth_image_msg  *default = /head_xtion/depth/image_rect_meters: Depth Image Frame.  upper_body_msg  *default = /upper_body_detector/detections: The deteced upper bodies  rgb_image_msg  *default = /head_xtion/rgb/image_rect_color: RGB Image Frame.", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_perception_people/rwth_upper_body_skeleton_random_walk/#roslaunch", 
            "text": "roslaunch rwth_upper_body_skeleton_random_walk fixed.launch  [parameter_name:=value]", 
            "title": "roslaunch"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/", 
            "text": "Head orientation estimator\n\n\nThis package estimates the orientation of the head of people detected by the upper body detector.\n\n\nFor the G4S Y1 deployment\n\n\nFor the G4S Y1 deployment, we won't run the actual estimation, we'll only run\nlogging code and that'll run 24/7, but can be paused/resumed by service calls.\n\n\nEverything is logged into the \nheads\n collection.\n\n\nLaunching\n\n\nUse the g5s launchfile:\n\n\nroslaunch strands_head_orientation g4s.launch\n\n\n\n\nDirectly edit the parameters in the launchfile if they need adapting.\nEspecially, please check \nexpected_runtime\n and modify it if the expected\nruntime is \nnot\n 3 weeks.\n\n\nThere is a hardcoded (by intent) limit of detection and image count to be logged\n(\ng_maxStored*\n in \nstore_detections.cpp\n) which\namounts to roughly 40Gb and 10Gb, respectively.\n\n\nPausing and resuming\n\n\nIn total, ~1 CPU core is used by this node and its dependencies. It can be paused\nand later on resumed if all available power is needed by someone else.\n\n\nPausing\n\n\nSend a message of type \nstrands_head_orientation/StopHeadAnalysis\n to the\n\nstop_head_analysis\n service and all activity will be paused.\n\n\nThe \nstop\n executable in this package does exactly that, see its \nsource\n\nfor an example or just execute it with \nrecording\n as parameter:\n\n\nrosrun strands_head_orientation stop recording\n\n\n\n\nResuming\n\n\nFor resuming from the paused state, send a message of type\n\nstrands_head_orientation/StartHeadAnalysis\n to the \nstart_head_analysis\n\nservice.\n\n\nAgain, the \nstart\n executable and its \nsource\n can be helpful:\n\n\nrosrun strands_head_orientation start recording\n\n\n\n\nPoking\n\n\nYou can also check for the current state by sending a message of type\n\nstrands_head_orientation/IsHeadAnalysisRunning\n to the \nstatus_head_analysis\n\nservice.\n\n\nYet again, the \nstatus\n executable and its \nsource\n help you:\n\n\nrosrun strands_head_orientation status recording\n\n\n\n\nAnything below this line can be ignored for the g4s scenario.\n\n\n\n\nInstall\n\n\nPlease download and extract one of the \navailable model files\n into the \nmodels\n folder. The larger models will have better prediction but will run slower. By default:\n\n\ncd strands_head_orientation/models\nwget https://omnomnom.vision.rwth-aachen.de/strands/data/ghmodels-l/model-small.tar.bz2\ntar xjv \n model-small.tar.bz2\nln -s default-0.1 default\n\n\n\n\nThis should create the \nmodels/default\n link pointing to the \nmodels/default-0.1\n folder with a bunch of files.\n\n\nRun\n\n\nDependencies\n\n\nThis node needs \nupper_body_detector/upper_body_detector.launch\n to run,\nwhich in turn needs \nground_plane_estimation/ground_plane_fixed.launch\n.\nOr just run the people tracker which starts both of the above.\n\n\nParameters\n\n\n\n\nqueue_size\n \ndefault = 10\n: The synchronisation queue size\n\n\ncamera_namespace\n \ndefault = /head_xtion\n: The camera namespace.\n\n\nupper_body_detections\n \ndefault = /upper_body_detector/detections\n: The deteced upper bodies\n\n\nhead_ori\n \ndefault = /head_orientation/head_ori\n: Publishing an orientation array for each detected upper body.\n\n\nmodel_dir\n \ndefault = \"\"\n: The learned model to use to do predictions. Models can be found in \nstrands_head_orientation/models/\n.\n\n\nautostart\n \ndefault = true\n: Whether to start analyzing detections right away or wait for a signal.\n\n\n\n\nrosrun\n\n\nrosrun strands_head_orientation strands_head_orientation [_parameter_name:=value]\n\n\n\n\nroslaunch\n\n\nroslaunch strands_head_orientation head_orientation.launch [parameter_name:=value]\n\n\n\n\nStart/stop\n\n\nThis node should be permanently running. While running, it can be started and\nstopped and when stopped it should hardly use any resources, except for some\nmemory.\n\n\nIts activity can be controlled on-demand through calls to the following services:\n  - \nstart_head_analysis\n: starts the analysis of all incoming detections.\n  - \nstop_head_analysis\n: stops the analysis of any detections.\n  - \nstatus_head_analysis\n: returns whether the analysis is currently started.\n\n\nFor manual usage, you can use the \nstart\n, \nstop\n and \nstatus\n executables:\n\n\nrosrun strands_head_orientation start\nrosrun strands_head_orientation status\nrosrun strands_head_orientation stop", 
            "title": "Strands head orientation"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#head-orientation-estimator", 
            "text": "This package estimates the orientation of the head of people detected by the upper body detector.", 
            "title": "Head orientation estimator"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#for-the-g4s-y1-deployment", 
            "text": "For the G4S Y1 deployment, we won't run the actual estimation, we'll only run\nlogging code and that'll run 24/7, but can be paused/resumed by service calls.  Everything is logged into the  heads  collection.", 
            "title": "For the G4S Y1 deployment"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#launching", 
            "text": "Use the g5s launchfile:  roslaunch strands_head_orientation g4s.launch  Directly edit the parameters in the launchfile if they need adapting.\nEspecially, please check  expected_runtime  and modify it if the expected\nruntime is  not  3 weeks.  There is a hardcoded (by intent) limit of detection and image count to be logged\n( g_maxStored*  in  store_detections.cpp ) which\namounts to roughly 40Gb and 10Gb, respectively.", 
            "title": "Launching"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#pausing-and-resuming", 
            "text": "In total, ~1 CPU core is used by this node and its dependencies. It can be paused\nand later on resumed if all available power is needed by someone else.", 
            "title": "Pausing and resuming"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#pausing", 
            "text": "Send a message of type  strands_head_orientation/StopHeadAnalysis  to the stop_head_analysis  service and all activity will be paused.  The  stop  executable in this package does exactly that, see its  source \nfor an example or just execute it with  recording  as parameter:  rosrun strands_head_orientation stop recording", 
            "title": "Pausing"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#resuming", 
            "text": "For resuming from the paused state, send a message of type strands_head_orientation/StartHeadAnalysis  to the  start_head_analysis \nservice.  Again, the  start  executable and its  source  can be helpful:  rosrun strands_head_orientation start recording", 
            "title": "Resuming"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#poking", 
            "text": "You can also check for the current state by sending a message of type strands_head_orientation/IsHeadAnalysisRunning  to the  status_head_analysis \nservice.  Yet again, the  status  executable and its  source  help you:  rosrun strands_head_orientation status recording  Anything below this line can be ignored for the g4s scenario.", 
            "title": "Poking"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#install", 
            "text": "Please download and extract one of the  available model files  into the  models  folder. The larger models will have better prediction but will run slower. By default:  cd strands_head_orientation/models\nwget https://omnomnom.vision.rwth-aachen.de/strands/data/ghmodels-l/model-small.tar.bz2\ntar xjv   model-small.tar.bz2\nln -s default-0.1 default  This should create the  models/default  link pointing to the  models/default-0.1  folder with a bunch of files.", 
            "title": "Install"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#run", 
            "text": "", 
            "title": "Run"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#dependencies", 
            "text": "This node needs  upper_body_detector/upper_body_detector.launch  to run,\nwhich in turn needs  ground_plane_estimation/ground_plane_fixed.launch .\nOr just run the people tracker which starts both of the above.", 
            "title": "Dependencies"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#parameters", 
            "text": "queue_size   default = 10 : The synchronisation queue size  camera_namespace   default = /head_xtion : The camera namespace.  upper_body_detections   default = /upper_body_detector/detections : The deteced upper bodies  head_ori   default = /head_orientation/head_ori : Publishing an orientation array for each detected upper body.  model_dir   default = \"\" : The learned model to use to do predictions. Models can be found in  strands_head_orientation/models/ .  autostart   default = true : Whether to start analyzing detections right away or wait for a signal.", 
            "title": "Parameters"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#rosrun", 
            "text": "rosrun strands_head_orientation strands_head_orientation [_parameter_name:=value]", 
            "title": "rosrun"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#roslaunch", 
            "text": "roslaunch strands_head_orientation head_orientation.launch [parameter_name:=value]", 
            "title": "roslaunch"
        }, 
        {
            "location": "/strands_perception_people/strands_head_orientation/#startstop", 
            "text": "This node should be permanently running. While running, it can be started and\nstopped and when stopped it should hardly use any resources, except for some\nmemory.  Its activity can be controlled on-demand through calls to the following services:\n  -  start_head_analysis : starts the analysis of all incoming detections.\n  -  stop_head_analysis : stops the analysis of any detections.\n  -  status_head_analysis : returns whether the analysis is currently started.  For manual usage, you can use the  start ,  stop  and  status  executables:  rosrun strands_head_orientation start\nrosrun strands_head_orientation status\nrosrun strands_head_orientation stop", 
            "title": "Start/stop"
        }, 
        {
            "location": "/strands_perception_people/upper_body_detector/", 
            "text": "Upper Body Detector\n\n\nThis package detects the upper bodies of persons using the depth image.\n\n\nRun\n\n\nParameters:\n\n \nload_params_from_file\n \ndefault = true\n: \nfalse\n tries to read parameters from datacentre, \ntrue\n reads parameters from YAML file specified by \nconfig_file\n\n\n \nconfig_file\n \ndefault = $(find upper_body_detector)/config/upper_body_detector.yaml\n: The config file containing all the essential parameters. Only used if \nload_params_from_file == true\n.\n\n \ntemplate_file\n \ndefault = $(find upper_body_detector)/config/upper_body_template.yaml\n: The upper body template file. Read from the database if \nload_params_from_file == true\n.\n\n \nmachine\n \ndefault = localhost\n: Determines on which machine this node should run.\n\n \nuser\n \ndefault = \"\"\n: The user used for the ssh connection if machine is not localhost.\n\n \nqueue_size\n \ndefault = 20\n: The synchronisation queue size\n\n \nconfig_file\n \ndefault = \"\"\n: The global config file. Can be found in strands_upper_bodydetector/config\n\n \ntemplate_file\n \ndefault = \"\"\n: The template file. Can be found in config.\n\n \ncamera_namespace\n \ndefault = /head_xtion\n: The camera namespace.\n\n \ndepth_image\n \ndefault = /depth/image_rect\n: \ncamera_namespace\n + \ndepth_image\n = depth image topic\n\n \nrgb_image\n \ndefault = /rgb/image_rect_color\n: \ncamera_namespace\n + \nrgb_image\n = rgb image topic\n\n \ncamera_info_depth\n \ndefault = /depth/camera_info\n: \ncamera_namespace\n + \ncamera_info_depth\n = depth camera info topic\n\n \nground_plane\n \ndefault = /ground_plane\n: The estimated/fixed ground plane\n\n \nupper_body_detections\n \ndefault = /upper_body_detector/detections\n: The deteced upper bodies\n\n \nupper_body_bb_centres\n \ndefault = /upper_body_detector/bounding_box_centres\n: Publishing a pose array of the centres of the bounding boxes\n\n \nupper_body_image\n \ndefault = /upper_body_detector/image\n: The resulting image showing the detections as a boundingbox\n* `upper_body_markers default = /upper_body_detector/marker_array_: A visualisation array for rviz\n\n\nrosrun:\n\n\nrosrun upper_body_detector upper_body_detector [_parameter_name:=value]\n\n\n\n\nroslaunch:\n\n\nroslaunch upper_body_detector upper_body_detector.launch [parameter_name:=value]", 
            "title": "Upper body detector"
        }, 
        {
            "location": "/strands_perception_people/upper_body_detector/#upper-body-detector", 
            "text": "This package detects the upper bodies of persons using the depth image.", 
            "title": "Upper Body Detector"
        }, 
        {
            "location": "/strands_perception_people/upper_body_detector/#run", 
            "text": "Parameters:   load_params_from_file   default = true :  false  tries to read parameters from datacentre,  true  reads parameters from YAML file specified by  config_file    config_file   default = $(find upper_body_detector)/config/upper_body_detector.yaml : The config file containing all the essential parameters. Only used if  load_params_from_file == true .   template_file   default = $(find upper_body_detector)/config/upper_body_template.yaml : The upper body template file. Read from the database if  load_params_from_file == true .   machine   default = localhost : Determines on which machine this node should run.   user   default = \"\" : The user used for the ssh connection if machine is not localhost.   queue_size   default = 20 : The synchronisation queue size   config_file   default = \"\" : The global config file. Can be found in strands_upper_bodydetector/config   template_file   default = \"\" : The template file. Can be found in config.   camera_namespace   default = /head_xtion : The camera namespace.   depth_image   default = /depth/image_rect :  camera_namespace  +  depth_image  = depth image topic   rgb_image   default = /rgb/image_rect_color :  camera_namespace  +  rgb_image  = rgb image topic   camera_info_depth   default = /depth/camera_info :  camera_namespace  +  camera_info_depth  = depth camera info topic   ground_plane   default = /ground_plane : The estimated/fixed ground plane   upper_body_detections   default = /upper_body_detector/detections : The deteced upper bodies   upper_body_bb_centres   default = /upper_body_detector/bounding_box_centres : Publishing a pose array of the centres of the bounding boxes   upper_body_image   default = /upper_body_detector/image : The resulting image showing the detections as a boundingbox\n* `upper_body_markers default = /upper_body_detector/marker_array_: A visualisation array for rviz  rosrun:  rosrun upper_body_detector upper_body_detector [_parameter_name:=value]  roslaunch:  roslaunch upper_body_detector upper_body_detector.launch [parameter_name:=value]", 
            "title": "Run"
        }, 
        {
            "location": "/strands_perception_people/vision_people_logging/", 
            "text": "Vision/People Logging package\n\n\nThis packge contains a logging node to save the detections to the message_store.\n\n\nAll the information given on how to run the nodes should only be used if you need to run them seperately. In normal cases please refer to the \nperception_people_launch\n package to start the whole perception pipeline.\n\n\nLogging\n\n\nThis node uses the \nLoggingUBD.msg\n to save the detected people together with the robots pose and the tf transform which can be used to create the real world coordinates in the message store.\n\n\nRun with:\n\n\nroslaunch vision_people_logging logging_ubd.launch\n\n\nParameters:\n* \nlog\n: \nDefault: true\n This convenience parameter allows to start the whole system without logging the data", 
            "title": "Vision people logging"
        }, 
        {
            "location": "/strands_perception_people/vision_people_logging/#visionpeople-logging-package", 
            "text": "This packge contains a logging node to save the detections to the message_store.  All the information given on how to run the nodes should only be used if you need to run them seperately. In normal cases please refer to the  perception_people_launch  package to start the whole perception pipeline.", 
            "title": "Vision/People Logging package"
        }, 
        {
            "location": "/strands_perception_people/vision_people_logging/#logging", 
            "text": "This node uses the  LoggingUBD.msg  to save the detected people together with the robots pose and the tf transform which can be used to create the real world coordinates in the message store.  Run with:  roslaunch vision_people_logging logging_ubd.launch  Parameters:\n*  log :  Default: true  This convenience parameter allows to start the whole system without logging the data", 
            "title": "Logging"
        }, 
        {
            "location": "/strands_perception_people/wheelchair_detector/", 
            "text": "DROW detector\n\n\nThis is the DROW detector package (http://arxiv.org/abs/1603.02636). Given a laser scan it will detect two types of mobility aids and publish these as a pose array.\n\n\nAs input only a laser scan is needed (of type \nLaserScan\n). Currently there are three output topics (all of type \nPoseArray\n), two class specific ones for wheelchairs and walkers and one class agnostic one, that summarizes both topics. Our pretrained model was trained based on a Laser scanner with a field of view of 225 degrees and an angular resolution of 0.5 degrees at a height of ~37cm. The model will generalize to different field of views and should be robust to slight height deviations. In order to apply it to different angular resolutions or completely different heights you will need to train it on your own data though. See the paper for further info.\n\n\nParams\n\n\n\n\nlaser_topic\n default: \n/scan\n the input laser scan.\n\n\nwheelchair_detection_topic\n default: \n/wheelchair_detections\n the topics wheelchair detections will be published on.\n\n\nwalker_detection_topic\n default: \n/walker_detections\n the topics walker detections will be published on.\n\n\nclass_agnostic_detection_topic\n default: \n/mobility_aid_detections\n the topic where both types of mobility aid detections will be published to jointly.\n\n\nthreshold\n default: \n0.5\n the detection threshold. This will increase the precision at the cost of recall and vice versa.\n\n\nuse_cudnn\n, default: \nfalse\n determines if we force cudnn for the convolutions. With is faster, but harder to setup. (see dependencies)\n\n\nnetwork_param_file\n no default, the path to the network parameter file.\n\n\n\n\nDependencies\n\n\n\n\nA decent GPU (the ones on our side pcs will do :D)\n\n\nCUDA (tested with 7.5 download here https://developer.nvidia.com/cuda-downloads)\n\n\nCUDNN (HIGHLY RECOMMENDED. Without this the detector will not go beyond 4fps, while with it, it has no problems keeping up with the laser frequency. You need to register for this, download the archive, put it somewhere and export the correct paths for theano to find http://deeplearning.net/software/theano/library/sandbox/cuda/dnn.html)\n\n\n\n\nRunning the detector\n\n\nIn order to run the detector, make sure all dependencies are installed and download the network model from our server.\n\n \n$ roscd wheelchair_detector\n\n\n \n$ cd scripts\n\n* \n$ sh get_network_parameter_file.sh\n\nThe launch file is configured in such a way that it will look for the network in the resources folder, but you can put it anywhere you like it to be as long as you change the launch file accordingly.\n\n\nIn order to launch the node, you will need to provide the theano flags:\n* \n$ THEANO_FLAGS='cuda.root=/usr/local/cuda/,floatX=float32,device=gpu0' roslaunch wheelchair_detector drow.launch\n\nIf you have cuda somewhere else, make sure to adapt the flag. All of these can also be set in a \ntheano config\n once instead.\n\n\nTODO\n\n\n\n\nMake the voting parameters public. This is especially important for lasers with a larger FoV.\n\n\nAdd a project page with training code, data, etc.\n\n\nChange the name to DROW?", 
            "title": "Wheelchair detector"
        }, 
        {
            "location": "/strands_perception_people/wheelchair_detector/#drow-detector", 
            "text": "This is the DROW detector package (http://arxiv.org/abs/1603.02636). Given a laser scan it will detect two types of mobility aids and publish these as a pose array.  As input only a laser scan is needed (of type  LaserScan ). Currently there are three output topics (all of type  PoseArray ), two class specific ones for wheelchairs and walkers and one class agnostic one, that summarizes both topics. Our pretrained model was trained based on a Laser scanner with a field of view of 225 degrees and an angular resolution of 0.5 degrees at a height of ~37cm. The model will generalize to different field of views and should be robust to slight height deviations. In order to apply it to different angular resolutions or completely different heights you will need to train it on your own data though. See the paper for further info.", 
            "title": "DROW detector"
        }, 
        {
            "location": "/strands_perception_people/wheelchair_detector/#params", 
            "text": "laser_topic  default:  /scan  the input laser scan.  wheelchair_detection_topic  default:  /wheelchair_detections  the topics wheelchair detections will be published on.  walker_detection_topic  default:  /walker_detections  the topics walker detections will be published on.  class_agnostic_detection_topic  default:  /mobility_aid_detections  the topic where both types of mobility aid detections will be published to jointly.  threshold  default:  0.5  the detection threshold. This will increase the precision at the cost of recall and vice versa.  use_cudnn , default:  false  determines if we force cudnn for the convolutions. With is faster, but harder to setup. (see dependencies)  network_param_file  no default, the path to the network parameter file.", 
            "title": "Params"
        }, 
        {
            "location": "/strands_perception_people/wheelchair_detector/#dependencies", 
            "text": "A decent GPU (the ones on our side pcs will do :D)  CUDA (tested with 7.5 download here https://developer.nvidia.com/cuda-downloads)  CUDNN (HIGHLY RECOMMENDED. Without this the detector will not go beyond 4fps, while with it, it has no problems keeping up with the laser frequency. You need to register for this, download the archive, put it somewhere and export the correct paths for theano to find http://deeplearning.net/software/theano/library/sandbox/cuda/dnn.html)", 
            "title": "Dependencies"
        }, 
        {
            "location": "/strands_perception_people/wheelchair_detector/#running-the-detector", 
            "text": "In order to run the detector, make sure all dependencies are installed and download the network model from our server.   $ roscd wheelchair_detector    $ cd scripts \n*  $ sh get_network_parameter_file.sh \nThe launch file is configured in such a way that it will look for the network in the resources folder, but you can put it anywhere you like it to be as long as you change the launch file accordingly.  In order to launch the node, you will need to provide the theano flags:\n*  $ THEANO_FLAGS='cuda.root=/usr/local/cuda/,floatX=float32,device=gpu0' roslaunch wheelchair_detector drow.launch \nIf you have cuda somewhere else, make sure to adapt the flag. All of these can also be set in a  theano config  once instead.", 
            "title": "Running the detector"
        }, 
        {
            "location": "/strands_perception_people/wheelchair_detector/#todo", 
            "text": "Make the voting parameters public. This is especially important for lasers with a larger FoV.  Add a project page with training code, data, etc.  Change the name to DROW?", 
            "title": "TODO"
        }, 
        {
            "location": "/strands_perception_people/opencv_warco/libsvm/", 
            "text": "This code uses dense vectors to store data instances. If most feature\nvalues are non-zeros, the training/testing time is faster than the\nstandard libsvm, which implement sparse vectors.\n\n\nExperimental Setting:\n\n\nWe select four concepts, animal, people, sky, and weather, from\nMediaMill Challenge Problem for this experiment. All instances are\n120-dimension dense vectors. We compare the standard libsvm and this\nmodification.\n\n\nThe experimental procedure performs parameter selection (30 parameter\nsets), model training (best parameters), test data predictions. There\nare 30,993 and 12,914 records in training (used in parameter selection\nand model training stages) and testing (used in prediction stage)\nsets, respectively. The following table shows the results.\n\n\n(1) parameter-selection execution time (sec.):\n          original    dense        change \n          libsvm      repr.        ratio\nanimal     2483.23     1483.02     -40.3%\npeople    22844.26    13893.21     -39.2%\nsky       13765.39     8460.06     -38.5%\nweather    2240.01     1325.32     -40.1%\nAVERAGE   10083.17     6540.46     -39.1%\n\n\n(2) model-training execution time (sec.):\n          original    dense        change \n          libsvm      repr.        ratio\nanimal     113.238      70.085     -38.1% \npeople     725.995     451.244     -37.8%\nsky       1234.881     784.071     -36.5%\nweather    123.179      76.532     -37.9%\nAVERAGE    549.323     345.483     -37.1%\n\n\n(3) prediction execution time (sec.):\n          original    dense       change\n          libsvm      repr..      ratio\nanimal      12.226        6.895    -43.6%\npeople      99.268       54.855    -44.7%\nsky         78.069       42.417    -45.7%\nweather     10.843        6.495    -44.9%\nAVERAGE     50.102       27.666    -44.8% \n\n\nOverall, dense-representation libsvm saves on average 39.1%, 37.1%,\nand 44.8% of the execution time in parameter selection, model training\nand prediction stages, respectively. This modification is thus useful\nfor dense data sets.", 
            "title": "Libsvm"
        }, 
        {
            "location": "/strands_perception_people/opencv_warco/opencv_warco/", 
            "text": "opencv_warco\n\n\nA C++, OpenCV-based reimplementation of Tosato's WARCO image classification\nalgorithm.\n\n\nThis is an \nunfinished experiment\n, still highly in flux. It will not work.\nIt will break. It might microwave your cat. It might classify pictures of heads\naccording to their gaze direction.\n\n\nLicense: MIT\n\n\nFor ease of use, this bundles \njsoncpp\n which\nis MIT/Public Domain.\n\n\nCopyright (c) 2013 Lucas Beyer\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", 
            "title": "Opencv warco"
        }, 
        {
            "location": "/strands_perception_people/opencv_warco/opencv_warco/#opencv_warco", 
            "text": "A C++, OpenCV-based reimplementation of Tosato's WARCO image classification\nalgorithm.  This is an  unfinished experiment , still highly in flux. It will not work.\nIt will break. It might microwave your cat. It might classify pictures of heads\naccording to their gaze direction.", 
            "title": "opencv_warco"
        }, 
        {
            "location": "/strands_perception_people/opencv_warco/opencv_warco/#license-mit", 
            "text": "For ease of use, this bundles  jsoncpp  which\nis MIT/Public Domain.  Copyright (c) 2013 Lucas Beyer  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", 
            "title": "License: MIT"
        }, 
        {
            "location": "/strands_perception_people/perception_people_launch/attic/", 
            "text": "Attic directory containing depricated launch files\n\n\nthese launch files are used to start the ppl perception using HOG featur detection. This is currently deprecated and not supported. The code for this can be found in the \nattic\n branch. This directory is not installed and therefore just to preserve the files themselves. Following are the instructions on how to use these files:\n\n\npeople_tracker_robot_with_HOG.launch\n\n\nThis version of the tracking does rely on the ground_hog feature extraction and is therefore only usable on PCs with an NVIDIA graphics card. It also relys on the fixed ground plane assumption made for the robot. To use this you have to run it remotely on a machine talking to the rosmaster on the robot, e.g. a laptop inside the robot.\n\n\nParameters:\n\n \nload_params_from_file\n \ndefault = true\n: \nfalse\n tries to read parameters from datacentre, \ntrue\n reads parameters from YAML file specified by \nparam_file\n\n\n \nmachine\n \ndefault = localhost\n: Determines on which machine this node should run.\n\n \nuser\n \ndefault = \"\"\n: The user used for the ssh connection if machine is not localhost.\n\n \ngp_queue_size\n \ndefault = 5\n: The ground plane sync queue size\n\n \ngh_queue_size\n \ndefault = 20\n: The ground plane sync queue size\n\n \nubd_queue_size\n \ndefault = 5\n: The upper body detector sync queue size\n\n \npt_queue_size\n \ndefault = 10\n: The people tracking sync queue size\n\n \nptu_state\n \ndefault = /ptu/state\n: The ptu state topic\n\n \ncamera_namespace\n \ndefault = /head_xtion\n: The camera namespace.\n\n \nrgb_image\n \ndefault = /rgb/image_rect_color\n: \ncamera_namespace\n + \nrgb_image\n = rgb image topic\n\n \ndepth_image\n \ndefault = /depth/image_rect\n: \ncamera_namespace\n + \ndepth_image\n = depth image topic\n\n \nmono_image\n \ndefault = /rgb/image_mono\n: \ncamera_namespace\n + \nmono_image\n = mono image topic\n\n \ncamera_info_rgb\n \ndefault = /rgb/camera_info\n: \ncamera_namespace\n + \ncamera_info_rgb\n = rgb camera info topic\n\n \ncamera_info_depth\n \ndefault = /depth/camera_info\n: \ncamera_namespace\n + \ncamera_info_depth\n = depth camera info topic\n\n \nground_plane\n \ndefault = /ground_plane\n: The fixed ground plane\n\n \nupper_body_detections\n \ndefault = /upper_body_detector/detections\n: The detected upper body\n\n \nupper_body_bb_centres\n \ndefault = /upper_body_detector/bounding_box_centres\n: Publishing a pose array of the centres of the bounding boxes\n\n \nupper_body_markers default = /upper_body_detector/marker_array_: A visualisation array for rviz\n*\nupper_body_image\n_default = /upper_body_detector/image_: The detected upper body image\n*\nvisual_odometry\n_default = /visual_odometry/motion_matrix_: The odometry. This takes the real odometry and only follows naming conventions for the ease of use.\n*\npedestrain_array\n_default = /mdl_people_tracker/people_array_: The detected and tracked people\n*\npeople_markers\" default=\"/mdl_people_tracker/marker_array_: A visualisation array for rviz\n* \npeople_poses\" default = /mdl_people_tracker/pose_array_: A PoseArray of the detected people\n*\ntf_target_frame\n_default = /map: The coordinate system into which the localisations should be transformed\n*\npd_positions\n_default = /people_tracker/positions_: The poses of the tracked people\n*\npd_marker\n_default = /people_tracker/marker_array_: A marker arry to visualise found people in rviz\n*\nlog` \ndefault = false\n: Log people and robot locations together with tracking and detection results to message_store database into people_perception collection. Disabled by default because if it is enabled the perception is running continuously.\n\n\nRunning:\n\n\nroslaunch perception_people_launch people_tracker_robot_with_HOG.launch [parameter_name:=value]\n\n\n\n\npeople_tracker_standalone_with_HOG.launch\n\n\nThis depends on the strands_ground_hog package which has to be built with the libcudaHOG. See README file of 3rd_party directory. It also uses the \n/camera\n namespace as a default and estimates the groundplane because it is not supposed to be run on the robot but on an external PC with a different set-up.\n\n\nParameters:\n\n \nload_params_from_file\n \ndefault = true\n: \nfalse\n tries to read parameters from datacentre, \ntrue\n reads parameters from YAML file specified by \nparam_file\n\n\n \nmachine\n \ndefault = localhost\n: Determines on which machine this node should run.\n\n \nuser\n \ndefault = \"\"\n: The user used for the ssh connection if machine is not localhost.\n\n \ngh_queue_size\n \ndefault = 10\n: The ground hog sync queue size\n\n \ngp_queue_size\n \ndefault = 5\n: The ground plane sync queue size\n\n \nvo_queue_size\n \ndefault = 5\n: The visual odometry sync queue size\n\n \nubd_queue_size\n \ndefault = 5\n: The upper body detector sync queue size\n\n \npt_queue_size\n \ndefault = 10\n: The people tracking sync queue size\n\n \ncamera_namespace\n \ndefault = /camera\n: The camera namespace.\n\n \nrgb_image\n \ndefault = /rgb/image_rect_color\n: \ncamera_namespace\n + \nrgb_image\n = rgb image topic\n\n \ndepth_image\n \ndefault = /depth/image_rect\n: \ncamera_namespace\n + \ndepth_image\n = depth image topic\n\n \nmono_image\n \ndefault = /rgb/image_mono\n: \ncamera_namespace\n + \nmono_image\n = mono image topic\n\n \ncamera_info_rgb\n \ndefault = /rgb/camera_info\n: \ncamera_namespace\n + \ncamera_info_rgb\n = rgb camera info topic\n\n \ncamera_info_depth\n \ndefault = /depth/camera_info\n: \ncamera_namespace\n + \ncamera_info_depth\n = depth camera info topic\n\n \nground_plane\n \ndefault = /ground_plane\n: The estimated ground plane\n\n \nground_hog_detections\n \ndefault = /groundHOG/detections\n: The ground HOG detections\n\n \nupper_body_detections\n \ndefault = /upper_body_detector/detections\n: The detected upper body\n\n \nupper_body_bb_centres\n \ndefault = /upper_body_detector/bounding_box_centres\n: Publishing a pose array of the centres of the bounding boxes\n\n \nupper_body_markers default = /upper_body_detector/marker_array_: A visualisation array for rviz\n*\nground_hog_image\n_default = /groundHOG/image_: The ground HOG image\n*\nupper_body_image\n_default = /upper_body_detector/image_: The detected upper body image\n*\nvisual_odometry\n_default = /visual_odometry/motion_matrix_: The visual odometry\n*\npeople_markers\" default=\"/mdl_people_tracker/marker_array_: A visualisation array for rviz\n\n \npeople_poses\" default = /mdl_people_tracker/pose_array_: A PoseArray of the detected people\n*\npeople_markers\" default=\"/mdl_people_tracker/marker_array\n: A visualisation array for rviz\n*\ntf_target_frame` \ndefault = \"\"\n: The coordinate system into which the localisations should be transformed. As this might not run on a robot and therefore no tf is available this is an empty string.\n\n\nRunning:\n\n\nroslaunch perception_people_launch people_tracker_standalone_with_HOG.launch [parameter_name:=value]", 
            "title": "Attic"
        }, 
        {
            "location": "/strands_perception_people/perception_people_launch/attic/#attic-directory-containing-depricated-launch-files", 
            "text": "these launch files are used to start the ppl perception using HOG featur detection. This is currently deprecated and not supported. The code for this can be found in the  attic  branch. This directory is not installed and therefore just to preserve the files themselves. Following are the instructions on how to use these files:", 
            "title": "Attic directory containing depricated launch files"
        }, 
        {
            "location": "/strands_perception_people/perception_people_launch/attic/#people_tracker_robot_with_hoglaunch", 
            "text": "This version of the tracking does rely on the ground_hog feature extraction and is therefore only usable on PCs with an NVIDIA graphics card. It also relys on the fixed ground plane assumption made for the robot. To use this you have to run it remotely on a machine talking to the rosmaster on the robot, e.g. a laptop inside the robot.  Parameters:   load_params_from_file   default = true :  false  tries to read parameters from datacentre,  true  reads parameters from YAML file specified by  param_file    machine   default = localhost : Determines on which machine this node should run.   user   default = \"\" : The user used for the ssh connection if machine is not localhost.   gp_queue_size   default = 5 : The ground plane sync queue size   gh_queue_size   default = 20 : The ground plane sync queue size   ubd_queue_size   default = 5 : The upper body detector sync queue size   pt_queue_size   default = 10 : The people tracking sync queue size   ptu_state   default = /ptu/state : The ptu state topic   camera_namespace   default = /head_xtion : The camera namespace.   rgb_image   default = /rgb/image_rect_color :  camera_namespace  +  rgb_image  = rgb image topic   depth_image   default = /depth/image_rect :  camera_namespace  +  depth_image  = depth image topic   mono_image   default = /rgb/image_mono :  camera_namespace  +  mono_image  = mono image topic   camera_info_rgb   default = /rgb/camera_info :  camera_namespace  +  camera_info_rgb  = rgb camera info topic   camera_info_depth   default = /depth/camera_info :  camera_namespace  +  camera_info_depth  = depth camera info topic   ground_plane   default = /ground_plane : The fixed ground plane   upper_body_detections   default = /upper_body_detector/detections : The detected upper body   upper_body_bb_centres   default = /upper_body_detector/bounding_box_centres : Publishing a pose array of the centres of the bounding boxes   upper_body_markers default = /upper_body_detector/marker_array_: A visualisation array for rviz\n* upper_body_image _default = /upper_body_detector/image_: The detected upper body image\n* visual_odometry _default = /visual_odometry/motion_matrix_: The odometry. This takes the real odometry and only follows naming conventions for the ease of use.\n* pedestrain_array _default = /mdl_people_tracker/people_array_: The detected and tracked people\n* people_markers\" default=\"/mdl_people_tracker/marker_array_: A visualisation array for rviz\n*  people_poses\" default = /mdl_people_tracker/pose_array_: A PoseArray of the detected people\n* tf_target_frame _default = /map: The coordinate system into which the localisations should be transformed\n* pd_positions _default = /people_tracker/positions_: The poses of the tracked people\n* pd_marker _default = /people_tracker/marker_array_: A marker arry to visualise found people in rviz\n* log`  default = false : Log people and robot locations together with tracking and detection results to message_store database into people_perception collection. Disabled by default because if it is enabled the perception is running continuously.  Running:  roslaunch perception_people_launch people_tracker_robot_with_HOG.launch [parameter_name:=value]", 
            "title": "people_tracker_robot_with_HOG.launch"
        }, 
        {
            "location": "/strands_perception_people/perception_people_launch/attic/#people_tracker_standalone_with_hoglaunch", 
            "text": "This depends on the strands_ground_hog package which has to be built with the libcudaHOG. See README file of 3rd_party directory. It also uses the  /camera  namespace as a default and estimates the groundplane because it is not supposed to be run on the robot but on an external PC with a different set-up.  Parameters:   load_params_from_file   default = true :  false  tries to read parameters from datacentre,  true  reads parameters from YAML file specified by  param_file    machine   default = localhost : Determines on which machine this node should run.   user   default = \"\" : The user used for the ssh connection if machine is not localhost.   gh_queue_size   default = 10 : The ground hog sync queue size   gp_queue_size   default = 5 : The ground plane sync queue size   vo_queue_size   default = 5 : The visual odometry sync queue size   ubd_queue_size   default = 5 : The upper body detector sync queue size   pt_queue_size   default = 10 : The people tracking sync queue size   camera_namespace   default = /camera : The camera namespace.   rgb_image   default = /rgb/image_rect_color :  camera_namespace  +  rgb_image  = rgb image topic   depth_image   default = /depth/image_rect :  camera_namespace  +  depth_image  = depth image topic   mono_image   default = /rgb/image_mono :  camera_namespace  +  mono_image  = mono image topic   camera_info_rgb   default = /rgb/camera_info :  camera_namespace  +  camera_info_rgb  = rgb camera info topic   camera_info_depth   default = /depth/camera_info :  camera_namespace  +  camera_info_depth  = depth camera info topic   ground_plane   default = /ground_plane : The estimated ground plane   ground_hog_detections   default = /groundHOG/detections : The ground HOG detections   upper_body_detections   default = /upper_body_detector/detections : The detected upper body   upper_body_bb_centres   default = /upper_body_detector/bounding_box_centres : Publishing a pose array of the centres of the bounding boxes   upper_body_markers default = /upper_body_detector/marker_array_: A visualisation array for rviz\n* ground_hog_image _default = /groundHOG/image_: The ground HOG image\n* upper_body_image _default = /upper_body_detector/image_: The detected upper body image\n* visual_odometry _default = /visual_odometry/motion_matrix_: The visual odometry\n* people_markers\" default=\"/mdl_people_tracker/marker_array_: A visualisation array for rviz   people_poses\" default = /mdl_people_tracker/pose_array_: A PoseArray of the detected people\n* people_markers\" default=\"/mdl_people_tracker/marker_array : A visualisation array for rviz\n* tf_target_frame`  default = \"\" : The coordinate system into which the localisations should be transformed. As this might not run on a robot and therefore no tf is available this is an empty string.  Running:  roslaunch perception_people_launch people_tracker_standalone_with_HOG.launch [parameter_name:=value]", 
            "title": "people_tracker_standalone_with_HOG.launch"
        }, 
        {
            "location": "/strands_perception_people/perception_people_launch/", 
            "text": "Launch package\n\n\nThis convenience package contains launch files to start-up the whole people tracker system.\n\n\nGeneral remarks\n\n\nAll the the packages rely heavily on the synchronisation of rgb and depth images and the generated data of the other nodes. The synchronization is realised using a queue which saves a predefined number of messages on which the synchronisation is performed. \nAs a rule of thumb: the faster your machine the shorter the queue to prevent unnecessary use of memory.\n You can set queue sizes using:\n\n\nroslaunch perception_people_launch file.launch gh_queue_size:=11 vo_queue_size:=22 ubd_queue_size:=33 pt_queue_size:=44\n\n\n\n\nThis will overwrite the default values.  \ngh = ground_hog, vo = visual_odemetry, ubd = upper_body_detector, pt = mdl_people_tracker\n\n\nThe whole pipeline is desinged to unsuscribe from everything if there is no subscriber to the published topics. This causes the nodes to not use any CPU when there is no one listening on the published topics. This might result in a 1-2 second dealy after subscribing to one of the topics before the first data is published. Also, setting \nlog\n to true when starting the perception pipeline will cause it to always run and log data.\n\n\nRunning on robot\n\n\nThese launch files will make use of the fixed ground plane which is just rotated according to the PTU tilt and the robot odometry instead of the visual odometry. Additionally, the necessary parameters are assumed to be provided by the parameter server (see mongodb_store confog_manager on default parameters) therefore the \nload_params_from_file\n parameter is set to \nfalse\n and the nodes will querry the config parameters from the parameter server. The standalone version on the other hand uses the provided config files. If parameters are not present in the paremeter server on the robot but you want to launch the ppl perception, run with \nload_params_from_file:=true\n.\n\n\npeople_tracker_robot.launch\n\n\nThis version of the tracking does not rely on the ground_hog feature extraction and is therefore usable on PCs with no NVIDIA graphics card (like the embedded robot PC). However, this has the drawback that the system only relies on depth data to detect people which limits the distance at which persons can be detected to approx. 5 meters. Where possible the ground_hog detection should be used to enhance tracking results. It also uses the fixed ground plane assumption because it is ment to be executed on the robots head xtion camera.\n\n\nParameters:\n\n \nload_params_from_file\n \ndefault = true\n: \nfalse\n tries to read parameters from datacentre, \ntrue\n reads parameters from YAML file specified by \nparam_file\n\n\n \nmachine\n \ndefault = localhost\n: Determines on which machine this node should run.\n\n \nuser\n \ndefault = \"\"\n: The user used for the ssh connection if machine is not localhost.\n\n \ngp_queue_size\n \ndefault = 5\n: The ground plane sync queue size\n\n \nubd_queue_size\n \ndefault = 5\n: The upper body detector sync queue size\n\n \npt_queue_size\n \ndefault = 10\n: The people tracking sync queue size\n\n \nptu_state\n \ndefault = /ptu/state\n: The ptu state topic\n\n \ncamera_namespace\n \ndefault = /head_xtion\n: The camera namespace.\n\n \nrgb_image\n \ndefault = /rgb/image_rect_color\n: \ncamera_namespace\n + \nrgb_image\n = rgb image topic\n\n \ndepth_image\n \ndefault = /depth/image_rect\n: \ncamera_namespace\n + \ndepth_image\n = depth image topic\n\n \nmono_image\n \ndefault = /rgb/image_mono\n: \ncamera_namespace\n + \nmono_image\n = mono image topic\n\n \ncamera_info_rgb\n \ndefault = /rgb/camera_info\n: \ncamera_namespace\n + \ncamera_info_rgb\n = rgb camera info topic\n\n \ncamera_info_depth\n \ndefault = /depth/camera_info\n: \ncamera_namespace\n + \ncamera_info_depth\n = depth camera info topic\n\n \nground_plane\n \ndefault = /ground_plane\n: The fixed ground plane\n\n \nupper_body_detections\n \ndefault = /upper_body_detector/detections\n: The detected upper body\n\n \nupper_body_bb_centres\n \ndefault = /upper_body_detector/bounding_box_centres\n: Publishing a pose array of the centres of the bounding boxes\n\n \nupper_body_markers default = /upper_body_detector/marker_array_: A visualisation array for rviz\n*\nupper_body_image\n_default = /upper_body_detector/image_: The detected upper body image\n*\nvisual_odometry\n_default = /visual_odometry/motion_matrix_: The odometry. This takes the real odometry and only follows naming conventions for the ease of use.\n*\nmdl_people_array\n_default = /mdl_people_tracker/people_array_: The detected and tracked people\n*\nmdl_people_markers\" default=\"/mdl_people_tracker/marker_array_: A visualisation array for rviz\n\n \nmdl_people_poses\" default = /mdl_people_tracker/pose_array_: A PoseArray of the detected people\n*\ntf_target_frame\n_default = /map: The coordinate system into which the localisations should be transformed\n*\nbayes_people_positions\n_default = /people_tracker/positions_: The poses of the tracked people\n*\nbayes_people_pose\n: _Default: /people_tracker/pose_: The topic under which the closest detected person is published as a geometry_msgs/PoseStamped\n\n\n \nbayes_people_pose_array\n: \nDefault: /people_tracker/pose_array\n: The topic under which the detections are published as a geometry_msgs/PoseArray\n*\nbayes_people_poeple\n: _Default: /people_tracker/people_: The topic under which the results are published as people_msgs/People\n\n\n \npd_marker\n \ndefault = /people_tracker/marker_array\n: A marker arry to visualise found people in rviz\n\n \nlog\n \ndefault = false\n: Log people and robot locations together with tracking and detection results to message_store database into people_perception collection. Disabled by default because if it is enabled the perception is running continuously.\n\n \nwith_mdl_tracker\n \ndefault = false\n: Starts the mdl people tracker in addition to the bayes tracker\n\n \nwith_laser_filter\n \ndefault = true\n: Starts the laser filter to reduce false positives from the leg detector\n\n \nwith_tracker_filter_map\n \ndefault = false\n: Use a special map to filter the tracker results instead of just the map used for navigation.\n\n \ntracker_filter_map\n: The map to use instead of the navigation map to filter the tracker results.\n\n \ntracker_filter_positions\n \ndefault = /people_tracker_filter/positions\n: The filtered tracker results.\n\n \ntracker_filter_pose\n \ndefault = /people_tracker_filter/pose\n: The filtered pose for the closest person.\n\n \ntracker_filter_pose_array\n \ndefault = /people_tracker_filter/pose_array\n: The filetered pose array.\n\n \ntracker_filter_people\n \ndefault = /people_tracker_filter/people\n: The filetered people message.\n\n \ntracker_filter_marker\n \ndefault = /people_tracker_filter/marker_array\n: The filetered marker array.\n\n\nRunning:\n\n\nroslaunch perception_people_launch people_tracker_robot.launch [parameter_name:=value]\n\n\n\n\npeople_tracker_standalone.launch\n\n\nThis version of the tracking does not rely on the ground_hog feature extraction and is therefore usable on PCs with no NVIDIA graphics card. However, this has the drawback that the system only relies on depth data to detect people which limits the distance at which persons can be detected to approx. 5 meters. Where possible the ground_hog detection should be used to enhance tracking results. \n\n\nIt also uses the \n/camera\n namespace as a default and estimates the groundplane because it is not supposed to be run on the robot but on an external PC with a different set-up.\n\n\nParameters:\n\n \nload_params_from_file\n \ndefault = true\n: \nfalse\n tries to read parameters from datacentre, \ntrue\n reads parameters from YAML file specified by \nparam_file\n\n\n \nmachine\n \ndefault = localhost\n: Determines on which machine this node should run.\n\n \nuser\n \ndefault = \"\"\n: The user used for the ssh connection if machine is not localhost.\n\n \ngh_queue_size\n \ndefault = 20\n: The ground plane sync queue size\n\n \nvo_queue_size\n \ndefault = 5\n: The visual odometry sync queue size\n\n \nubd_queue_size\n \ndefault = 5\n: The upper body detector sync queue size\n\n \npt_queue_size\n \ndefault = 10\n: The people tracking sync queue size\n\n \ncamera_namespace\n \ndefault = /camera\n: The camera namespace.\n\n \nrgb_image\n \ndefault = /rgb/image_rect_color\n: \ncamera_namespace\n + \nrgb_image\n = rgb image topic\n\n \ndepth_image\n \ndefault = /depth/image_rect\n: \ncamera_namespace\n + \ndepth_image\n = depth image topic\n\n \nmono_image\n \ndefault = /rgb/image_mono\n: \ncamera_namespace\n + \nmono_image\n = mono image topic\n\n \ncamera_info_rgb\n \ndefault = /rgb/camera_info\n: \ncamera_namespace\n + \ncamera_info_rgb\n = rgb camera info topic\n\n \ncamera_info_depth\n \ndefault = /depth/camera_info\n: \ncamera_namespace\n + \ncamera_info_depth\n = depth camera info topic\n\n \nground_plane\n \ndefault = /ground_plane\n: The estimated ground plane\n\n \nupper_body_detections\n \ndefault = /upper_body_detector/detections\n: The detected upper body\n\n \nupper_body_bb_centres\n \ndefault = /upper_body_detector/bounding_box_centres\n: Publishing a pose array of the centres of the bounding boxes\n\n \nupper_body_markers default = /upper_body_detector/marker_array_: A visualisation array for rviz\n*\nupper_body_image\n_default = /upper_body_detector/image_: The detected upper body image\n*\nvisual_odometry\n_default = /visual_odometry/motion_matrix_: The visual odometry\n*\npedestrain_array\n_default = /mdl_people_tracker/people_array_: The detected and tracked people\n*\npeople_markers\" default=\"/mdl_people_tracker/marker_array_: A visualisation array for rviz\n\n \npeople_poses\" default = /mdl_people_tracker/pose_array_: A PoseArray of the detected people\n*\ntf_target_frame` \ndefault = \"\"\n: The coordinate system into which the localisations should be transformed. As this might not run on a robot and therefore no tf is available this is an empty string.\n\n\nRunning:\n\n\nroslaunch perception_people_launch people_tracker_standalone.launch [parameter_name:=value]", 
            "title": "Home"
        }, 
        {
            "location": "/strands_perception_people/perception_people_launch/#launch-package", 
            "text": "This convenience package contains launch files to start-up the whole people tracker system.", 
            "title": "Launch package"
        }, 
        {
            "location": "/strands_perception_people/perception_people_launch/#general-remarks", 
            "text": "All the the packages rely heavily on the synchronisation of rgb and depth images and the generated data of the other nodes. The synchronization is realised using a queue which saves a predefined number of messages on which the synchronisation is performed.  As a rule of thumb: the faster your machine the shorter the queue to prevent unnecessary use of memory.  You can set queue sizes using:  roslaunch perception_people_launch file.launch gh_queue_size:=11 vo_queue_size:=22 ubd_queue_size:=33 pt_queue_size:=44  This will overwrite the default values.   gh = ground_hog, vo = visual_odemetry, ubd = upper_body_detector, pt = mdl_people_tracker  The whole pipeline is desinged to unsuscribe from everything if there is no subscriber to the published topics. This causes the nodes to not use any CPU when there is no one listening on the published topics. This might result in a 1-2 second dealy after subscribing to one of the topics before the first data is published. Also, setting  log  to true when starting the perception pipeline will cause it to always run and log data.", 
            "title": "General remarks"
        }, 
        {
            "location": "/strands_perception_people/perception_people_launch/#running-on-robot", 
            "text": "These launch files will make use of the fixed ground plane which is just rotated according to the PTU tilt and the robot odometry instead of the visual odometry. Additionally, the necessary parameters are assumed to be provided by the parameter server (see mongodb_store confog_manager on default parameters) therefore the  load_params_from_file  parameter is set to  false  and the nodes will querry the config parameters from the parameter server. The standalone version on the other hand uses the provided config files. If parameters are not present in the paremeter server on the robot but you want to launch the ppl perception, run with  load_params_from_file:=true .", 
            "title": "Running on robot"
        }, 
        {
            "location": "/strands_perception_people/perception_people_launch/#people_tracker_robotlaunch", 
            "text": "This version of the tracking does not rely on the ground_hog feature extraction and is therefore usable on PCs with no NVIDIA graphics card (like the embedded robot PC). However, this has the drawback that the system only relies on depth data to detect people which limits the distance at which persons can be detected to approx. 5 meters. Where possible the ground_hog detection should be used to enhance tracking results. It also uses the fixed ground plane assumption because it is ment to be executed on the robots head xtion camera.  Parameters:   load_params_from_file   default = true :  false  tries to read parameters from datacentre,  true  reads parameters from YAML file specified by  param_file    machine   default = localhost : Determines on which machine this node should run.   user   default = \"\" : The user used for the ssh connection if machine is not localhost.   gp_queue_size   default = 5 : The ground plane sync queue size   ubd_queue_size   default = 5 : The upper body detector sync queue size   pt_queue_size   default = 10 : The people tracking sync queue size   ptu_state   default = /ptu/state : The ptu state topic   camera_namespace   default = /head_xtion : The camera namespace.   rgb_image   default = /rgb/image_rect_color :  camera_namespace  +  rgb_image  = rgb image topic   depth_image   default = /depth/image_rect :  camera_namespace  +  depth_image  = depth image topic   mono_image   default = /rgb/image_mono :  camera_namespace  +  mono_image  = mono image topic   camera_info_rgb   default = /rgb/camera_info :  camera_namespace  +  camera_info_rgb  = rgb camera info topic   camera_info_depth   default = /depth/camera_info :  camera_namespace  +  camera_info_depth  = depth camera info topic   ground_plane   default = /ground_plane : The fixed ground plane   upper_body_detections   default = /upper_body_detector/detections : The detected upper body   upper_body_bb_centres   default = /upper_body_detector/bounding_box_centres : Publishing a pose array of the centres of the bounding boxes   upper_body_markers default = /upper_body_detector/marker_array_: A visualisation array for rviz\n* upper_body_image _default = /upper_body_detector/image_: The detected upper body image\n* visual_odometry _default = /visual_odometry/motion_matrix_: The odometry. This takes the real odometry and only follows naming conventions for the ease of use.\n* mdl_people_array _default = /mdl_people_tracker/people_array_: The detected and tracked people\n* mdl_people_markers\" default=\"/mdl_people_tracker/marker_array_: A visualisation array for rviz   mdl_people_poses\" default = /mdl_people_tracker/pose_array_: A PoseArray of the detected people\n* tf_target_frame _default = /map: The coordinate system into which the localisations should be transformed\n* bayes_people_positions _default = /people_tracker/positions_: The poses of the tracked people\n* bayes_people_pose : _Default: /people_tracker/pose_: The topic under which the closest detected person is published as a geometry_msgs/PoseStamped    bayes_people_pose_array :  Default: /people_tracker/pose_array : The topic under which the detections are published as a geometry_msgs/PoseArray * bayes_people_poeple : _Default: /people_tracker/people_: The topic under which the results are published as people_msgs/People    pd_marker   default = /people_tracker/marker_array : A marker arry to visualise found people in rviz   log   default = false : Log people and robot locations together with tracking and detection results to message_store database into people_perception collection. Disabled by default because if it is enabled the perception is running continuously.   with_mdl_tracker   default = false : Starts the mdl people tracker in addition to the bayes tracker   with_laser_filter   default = true : Starts the laser filter to reduce false positives from the leg detector   with_tracker_filter_map   default = false : Use a special map to filter the tracker results instead of just the map used for navigation.   tracker_filter_map : The map to use instead of the navigation map to filter the tracker results.   tracker_filter_positions   default = /people_tracker_filter/positions : The filtered tracker results.   tracker_filter_pose   default = /people_tracker_filter/pose : The filtered pose for the closest person.   tracker_filter_pose_array   default = /people_tracker_filter/pose_array : The filetered pose array.   tracker_filter_people   default = /people_tracker_filter/people : The filetered people message.   tracker_filter_marker   default = /people_tracker_filter/marker_array : The filetered marker array.  Running:  roslaunch perception_people_launch people_tracker_robot.launch [parameter_name:=value]", 
            "title": "people_tracker_robot.launch"
        }, 
        {
            "location": "/strands_perception_people/perception_people_launch/#people_tracker_standalonelaunch", 
            "text": "This version of the tracking does not rely on the ground_hog feature extraction and is therefore usable on PCs with no NVIDIA graphics card. However, this has the drawback that the system only relies on depth data to detect people which limits the distance at which persons can be detected to approx. 5 meters. Where possible the ground_hog detection should be used to enhance tracking results.   It also uses the  /camera  namespace as a default and estimates the groundplane because it is not supposed to be run on the robot but on an external PC with a different set-up.  Parameters:   load_params_from_file   default = true :  false  tries to read parameters from datacentre,  true  reads parameters from YAML file specified by  param_file    machine   default = localhost : Determines on which machine this node should run.   user   default = \"\" : The user used for the ssh connection if machine is not localhost.   gh_queue_size   default = 20 : The ground plane sync queue size   vo_queue_size   default = 5 : The visual odometry sync queue size   ubd_queue_size   default = 5 : The upper body detector sync queue size   pt_queue_size   default = 10 : The people tracking sync queue size   camera_namespace   default = /camera : The camera namespace.   rgb_image   default = /rgb/image_rect_color :  camera_namespace  +  rgb_image  = rgb image topic   depth_image   default = /depth/image_rect :  camera_namespace  +  depth_image  = depth image topic   mono_image   default = /rgb/image_mono :  camera_namespace  +  mono_image  = mono image topic   camera_info_rgb   default = /rgb/camera_info :  camera_namespace  +  camera_info_rgb  = rgb camera info topic   camera_info_depth   default = /depth/camera_info :  camera_namespace  +  camera_info_depth  = depth camera info topic   ground_plane   default = /ground_plane : The estimated ground plane   upper_body_detections   default = /upper_body_detector/detections : The detected upper body   upper_body_bb_centres   default = /upper_body_detector/bounding_box_centres : Publishing a pose array of the centres of the bounding boxes   upper_body_markers default = /upper_body_detector/marker_array_: A visualisation array for rviz\n* upper_body_image _default = /upper_body_detector/image_: The detected upper body image\n* visual_odometry _default = /visual_odometry/motion_matrix_: The visual odometry\n* pedestrain_array _default = /mdl_people_tracker/people_array_: The detected and tracked people\n* people_markers\" default=\"/mdl_people_tracker/marker_array_: A visualisation array for rviz   people_poses\" default = /mdl_people_tracker/pose_array_: A PoseArray of the detected people\n* tf_target_frame`  default = \"\" : The coordinate system into which the localisations should be transformed. As this might not run on a robot and therefore no tf is available this is an empty string.  Running:  roslaunch perception_people_launch people_tracker_standalone.launch [parameter_name:=value]", 
            "title": "people_tracker_standalone.launch"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/fovis/", 
            "text": "This software is constructed according to the Pods software policies and\ntemplates.  The policies and templates can be found at:\n\n\nhttp://sourceforge.net/projects/pods\n\n\nAbout this project\n\n\nName:         libfovis\nAuthors:      Albert Huang \n\n              Abraham Bachrach \n\n              Daniel Maturana \n\n\nSummary:      Visual odometry with a depth camera (e.g., Kinect/Primesense) or\n              stereo camera.\n\n\nRequirements:\n\n\nCMake       (http://www.cmake.org)\n  Eigen 3     (http://eigen.tuxfamily.org)\n  Intel SSE2\n\n\nFovis was developed on Ubuntu, but may work with other platforms.\n\n\nBuild Instructions\n\n\nFor system-wide installation:\n\n\n$ mkdir build\n  $ cd build\n  $ cmake ..\n  $ make\n  $ sudo make install\n  $ sudo ldconfig\n\n\nThis usually installs Fovis to /usr/local or something like that.\n\n\nFor use in the source directory:\n\n\n$ mkdir build\n  $ make\n\n\nDocumentation\n\n\nFovis is documented using Doxygen (http://www.doxygen.org).  To build the documentation:\n\n\n$ cd doc\n  $ doxygen\n\n\nFollowing that, open up doc/html/index.html in your web browser.\n\n\nLicense\n\n\nfovis is free software: you can redistribute it and/or modify\n  it under the terms of the GNU General Public License as published\n  by the Free Software Foundation, either version 3 of the License, or\n  (at your option) any later version.\n\n\nfovis is distributed in the hope that it will be useful,\n  but WITHOUT ANY WARRANTY; without even the implied warranty of\n  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n  GNU General Public License for more details.\n\n\nYou should have received a copy of the GNU General Public License\n  along with fovis.  If not, see \nhttp://www.gnu.org/licenses/\n.", 
            "title": "Fovis"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/fovis/#about-this-project", 
            "text": "Name:         libfovis\nAuthors:      Albert Huang  \n              Abraham Bachrach  \n              Daniel Maturana   Summary:      Visual odometry with a depth camera (e.g., Kinect/Primesense) or\n              stereo camera.  Requirements:  CMake       (http://www.cmake.org)\n  Eigen 3     (http://eigen.tuxfamily.org)\n  Intel SSE2  Fovis was developed on Ubuntu, but may work with other platforms.", 
            "title": "About this project"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/fovis/#build-instructions", 
            "text": "For system-wide installation:  $ mkdir build\n  $ cd build\n  $ cmake ..\n  $ make\n  $ sudo make install\n  $ sudo ldconfig  This usually installs Fovis to /usr/local or something like that.  For use in the source directory:  $ mkdir build\n  $ make", 
            "title": "Build Instructions"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/fovis/#documentation", 
            "text": "Fovis is documented using Doxygen (http://www.doxygen.org).  To build the documentation:  $ cd doc\n  $ doxygen  Following that, open up doc/html/index.html in your web browser.", 
            "title": "Documentation"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/fovis/#license", 
            "text": "fovis is free software: you can redistribute it and/or modify\n  it under the terms of the GNU General Public License as published\n  by the Free Software Foundation, either version 3 of the License, or\n  (at your option) any later version.  fovis is distributed in the hope that it will be useful,\n  but WITHOUT ANY WARRANTY; without even the implied warranty of\n  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n  GNU General Public License for more details.  You should have received a copy of the GNU General Public License\n  along with fovis.  If not, see  http://www.gnu.org/licenses/ .", 
            "title": "License"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/fv-example-freenect/", 
            "text": "This example produces visual odometry estimates for a Microsoft Kinect using\nlibfreenect.\n\n\nYou must have libfreenect installed.  It is strongly recommended that you\ninstall libfreenect from source to get the latest version possible.  For more\ninformation on libfreenect, see here:\n  http://openkinect.org/wiki/Getting_Started", 
            "title": "Fv example freenect"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/fv-example-openni/", 
            "text": "This example obtains depth images using the OpenNI API.  It's designed to work\nwith either a Microsoft Kinect, or PrimeSense sensor, although I haven't\nactually tested it on a PrimeSense.\n\n\nYou must have OpenNI installed, and either the Kinect or PrimeSense.\n\n\nOpenNI:\n  http://www.openni.org\n\n\nKinect modules:\n  As of Apr 21, 2011, there's no \"official\" repository of Kinect modules for\n  OpenNI, but there are community-developed forks of the PrimeSense module.  One\n  promising such fork is here: \n\n\nhttps://github.com/avin2/SensorKinect\n\n\nPrimeSense modules:\n  http://www.openni.org", 
            "title": "Fv example openni"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/", 
            "text": "Visual Odometry\n\n\nThis package calculates the visual odometry using depth and mono images.\n\n\nRun\n\n\nParameters:\n\n \nqueue_size\n \ndefault = 20\n: The synchronisation queue size\n\n \ncamera_namespace\n \ndefault = /head_xtion\n: The camera namespace.\n\n \ndepth_image\n \ndefault = /depth/image_rect\n: \ncamera_namespace\n + \ndepth_image\n = depth image topic\n\n \nmono_image\n \ndefault = /rgb/image_mono\n: \ncamera_namespace\n + \nmono_image\n = mono image topic\n\n \ncamera_info_depth\n \ndefault = /depth/camera_info\n: \ncamera_namespace\n + \ncamera_info_depth\n = depth camera info topic\n\n \nmotion_parameters\n \ndefault = /visual_odometry/motion_matrix\n: The visual odometry\n\n\nrosrun:\n\n\nrosrun visual_odometry visual_odometry [_parameter_name:=value]\n\n\n\n\nroslaunch:\n\n\nroslaunch visual_odometry visual_odometry.launch [parameter_name:=value]\n\n\n\n\nTroubleshooting\n\n\nIf you get an error message that states: \n\n\n/usr/lib/gcc/i686-linux-gnu/4.6/include/emmintrin.h:32:3: error: #error \nSSE2 instruction set not enabled\n\n\n\n\n\nor similar, you have to add \nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -msse -msse2 -msse3\")\n to the CMakeLists.txt file after the\n\n\nif(CMAKE_COMPILER_IS_GNUCXX)\n    set(CMAKE_CXX_FLAGS \n-O3\n)        ## Optimize\nendif()\n\n\n\n\nstatement so that it looks like:\n\n\nif(CMAKE_COMPILER_IS_GNUCXX)\n    set(CMAKE_CXX_FLAGS \n-O3\n)        ## Optimize\nendif()\n\nset(CMAKE_CXX_FLAGS \n${CMAKE_CXX_FLAGS} -msse -msse2 -msse3\n)", 
            "title": "Home"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/#visual-odometry", 
            "text": "This package calculates the visual odometry using depth and mono images.", 
            "title": "Visual Odometry"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/#run", 
            "text": "Parameters:   queue_size   default = 20 : The synchronisation queue size   camera_namespace   default = /head_xtion : The camera namespace.   depth_image   default = /depth/image_rect :  camera_namespace  +  depth_image  = depth image topic   mono_image   default = /rgb/image_mono :  camera_namespace  +  mono_image  = mono image topic   camera_info_depth   default = /depth/camera_info :  camera_namespace  +  camera_info_depth  = depth camera info topic   motion_parameters   default = /visual_odometry/motion_matrix : The visual odometry  rosrun:  rosrun visual_odometry visual_odometry [_parameter_name:=value]  roslaunch:  roslaunch visual_odometry visual_odometry.launch [parameter_name:=value]", 
            "title": "Run"
        }, 
        {
            "location": "/strands_perception_people/visual_odometry/#troubleshooting", 
            "text": "If you get an error message that states:   /usr/lib/gcc/i686-linux-gnu/4.6/include/emmintrin.h:32:3: error: #error  SSE2 instruction set not enabled   or similar, you have to add  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -msse -msse2 -msse3\")  to the CMakeLists.txt file after the  if(CMAKE_COMPILER_IS_GNUCXX)\n    set(CMAKE_CXX_FLAGS  -O3 )        ## Optimize\nendif()  statement so that it looks like:  if(CMAKE_COMPILER_IS_GNUCXX)\n    set(CMAKE_CXX_FLAGS  -O3 )        ## Optimize\nendif()\n\nset(CMAKE_CXX_FLAGS  ${CMAKE_CXX_FLAGS} -msse -msse2 -msse3 )", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/strands_qsr_lib/docs/", 
            "text": "Generate documentation\n\n\nFrom within the \ndocs\n folder run the following:\n\n\nTo autogenerate the packages and modules documentation from their docstrings:\n\n\nsphinx-apidoc ../qsr_lib/src/ -o rsts/api/ -e\n\n\n\n\nNOTE:\n \nsphinx-apidoc\n does not overwrite any files, so if you want to regenerate the documentation for a package or a \nmodule then delete first its rst file in the \ndocs/rsts/api\n folder.\n\n\nThen simply run:\n\n\nmake html\n\n\n\n\nqsrs.rst\n\n\nThe table in \nqsrs.rst\n is generated from \nqsrs_table.md\n as writing tables in RsT is a pain.\nmd files can be converted to rst ones using  \npandoc\n. \nThe actual command run from within the \ndocs/rsts/handwritten/qsrs\n folder is:\n\n\npandoc --from=markdown --to=rst --output=qsrs_table.rst qsrs_table.md\n\n\n\n\nIf pandoc converts \"`\" to \"``\" then do two string replacements:\n\n\n\n\n\"``\" to \"`\"\n\n\n\"`  \" to \"`     \", i.e 'backtilt+2 spaces' to 'backtilt+6 spaces'", 
            "title": "Docs"
        }, 
        {
            "location": "/strands_qsr_lib/docs/#generate-documentation", 
            "text": "From within the  docs  folder run the following:  To autogenerate the packages and modules documentation from their docstrings:  sphinx-apidoc ../qsr_lib/src/ -o rsts/api/ -e  NOTE:   sphinx-apidoc  does not overwrite any files, so if you want to regenerate the documentation for a package or a \nmodule then delete first its rst file in the  docs/rsts/api  folder.  Then simply run:  make html", 
            "title": "Generate documentation"
        }, 
        {
            "location": "/strands_qsr_lib/docs/#qsrsrst", 
            "text": "The table in  qsrs.rst  is generated from  qsrs_table.md  as writing tables in RsT is a pain.\nmd files can be converted to rst ones using   pandoc . \nThe actual command run from within the  docs/rsts/handwritten/qsrs  folder is:  pandoc --from=markdown --to=rst --output=qsrs_table.rst qsrs_table.md  If pandoc converts \"`\" to \"``\" then do two string replacements:   \"``\" to \"`\"  \"`  \" to \"`     \", i.e 'backtilt+2 spaces' to 'backtilt+6 spaces'", 
            "title": "qsrs.rst"
        }, 
        {
            "location": "/strands_qsr_lib/", 
            "text": "QSRlib\n\n\nA library for Qualitative Spatial Relations and Reasoning.\n\n\nRead the docs.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_qsr_lib/#qsrlib", 
            "text": "A library for Qualitative Spatial Relations and Reasoning.  Read the docs.", 
            "title": "QSRlib"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_prob_rep/", 
            "text": "The QSR Probabilistic Representation library\n\n\nThis library provides functionalities to create probablistic models of QSR state chains, e.g. produced by the qsr_lib. This can for example be used for classification and sampling of new state chains. Currently only HMMs are supported.\n\n\nUsage\n\n\nThe recommended language is python 2.7 using ROS Indigo. Eventhough the library uses ROS only for communication, the provided infrastructure currently relies on ROS services to be used. Python is recommended due to the fact that the provided ros_client is implemented in python and using the services directly might be a bit confusing at first since they rely on json parsable strings.\n\n\nFor a python usage exmple, please see the example_ros_client.py in scripts. Try \nrosrun qsr_prob_rep example_hmm_client.py -h\n or \nrosrun qsr_prob_rep example_pf_client.py -h\n for usage information depending on if you want to create a Hidden Markov Model or a Particle Filter.\n\n\nCurrently implemented functionality\n\n\nHMM\n\n\n\n\ncreate\n: This takes the desired qsr_type and a json parsable list of lists of QSR state chains and returns the xml representation of the trained HMM as a string. This function is easiest to use when reading the state chains from files as it's done in the example client. The resulting xml can either be written to disk, kept in memory, or stored in a datacentre. The xml string is used in all other functionalities to load the HMM.\n\n\nsample\n: Given a HMM as an XML string, the desired number and length of samples, and the qsr the HMM models, this function produces sample state chains and returns them as numpy arrays.\n\n\nloglikelihood\n: Given an HMM and a (list of) state chain(s), this function calculates the accumulative loglikelihood for the state chain(s) to be produced by the given HMM. Might produce \n-inf\n if production is impossible.\n\n\n\n\nParticle Filter\n\n\n\n\ncreate\n: This takes a model consisting or several transition probability and observation probability matrices in a dictionary and a state look up table to create a particle filter. Create an instance of the \nPfRepRequestCreate\n class filling all the necessary information. The required model can be built with a helper class \nqsrrep_pf.pf_model.PfModel\n that includes tests for the model sanity. Have a look at the example client on how to use this. The look up table has to have as many states as the matrices have rows and coloumns, and should be a simple list or numpy array of the states that will be observed. The index of the state in the look up table has to correspond to the index for this state in the given matrices. Have a look at the \nqsrrep_utils.qtc_model_generation\n class for inspiration on how to create one. This returns a uuid identifying your particle filter.\n\n\nThe necessary files can be create directly from the HMM by exporting the emission matrix as the observation probability matrix and the transitions as the transition probability matrix. This however, requires that you have enough training data to learn sensible emissians and transitions at the same time. If not, create your own observation probability matrix like it is done in \nqsrrep_utils.qtc_model_generation\n. The \ncreate_pf_models.py\n script can help to generate the files from the HMM.\n\n\npredict\n: Takes the uuid of an existing particle filter and the number of sample generations to predict and returns the most likely next states and model including their probabilities.\n\n\nupdate\n: Runs the baysian update for the particle filter identified by uuid id using an observation provided.\n\n\nlist_filters\n: Lists all currently active filters.\n\n\nremove\n: Removes the particle filter with the given uuid from memory.\n\n\n\n\nGeneral usage advice\n\n\nThe ros_client for python hides a lot of the complexity from the user. Just create an instance of the correct request class and call \nROSClient().call_service\n and the right service will be called for you. All the json parsing happens in the background and you don't have to worry about this. Have a look at the example clients to see how to do this.\n\n\nCurrently implemented QSRs\n\n\n\n\nQTCB\n: The basic variant of the Qtalitative Trajectory Calculus. For more information on the implementation see [1]\n\n\nQTCC\n: The double-cross variant of the Qtalitative Trajectory Calculus. For more information on the implementation see [1]\n\n\nQTCBC\n: The mixture representation of the basic and double-cross variant of the Qtalitative Trajectory Calculus. For more information on the implementation see [1]\n\n\nRCC3\n: A very simple rcc3 representation using uniform transition and emission matrices.\n\n\n\n\nFor Developers\n\n\nThe following is currently hevily outdated and needs updating!!!\n\n\nAdding a new QSR\n\n\nAdding a new QSR based on the simple RCC3 example:\n\n\n\n\nCreate a new file in \nsrc/qsrrep_hmms\n or copy the \nrcc3_hmm.py\n\n\nImport: \nfrom qsrrep_hmms.hmm_abstractclass import HMMAbstractclass\n\n\nCreate a new class that inherits from \nHMMAbstractclass\n\n\nIn the \n__init__(self)\n function:\n\n\nCall the super calss constructor: \nsuper(self.__class__, self).__init__()\n\n\nSet \nself.num_possible_states\n to the number of possible states your QSR has\n\n\nOverwrite the two functions \n_qsr_to_symbol\n and \n_symbol_to_qsr\n. See rcc3 example for inspiration.\n\n\nAdd your new QSR to \nsrc/qsrrep_lib/rep_lib.py\n:\n\n\nImport your new file from \nqsrrep_hmms\n\n\nAdd your new QSR to the dictionary of available QSRs \nhmm_types_available\n following the examples given\n\n\nEnjoy your new QSR HMM!\n\n\n\n\nThe example client is agnostic of the QSR implemented, a simple restart of the eserver and allows to use your QSR with the example client immediately.\n\n\nAdding a more specialised QSR\n\n\nSometimes a uniformly distributed transition and emission matrix (as they are used in above example by default) is just not precise enough like in the case of QTC. Please have a look at \nsrc/qsrrep_hmms/qtc_hmm_abstractclass.py\n on how to deal with that.\n\n\nBasic instructions:\n\n\n\n\nFollow above instructions to create a new QSR\n\n\nOverwrite the \n_create_transition_matrix\n and \n_create_emission_matrix\n\n\n\n\nAdding a new functionality\n\n\n\n\nAdd your new functionality to \nqsrrep_hmms.hmm_abstractclass.py\n following the examples of \n_create\n, \n_sample\n, and \n_log_likelihood\n.\n\n\nAdd a getter for your new functionality in \nqsrrep_hmms.hmm_abstractclass.py\n following the examples of \nget_hmm\n, \nget_samples\n, and \nget_log_likelihood\n.\n\n\nCreate a request class following the naming scheme: \nHMMRepRequestFunctionality\n in \nqsrrep_lib.rep_io.py\n where \nFunctionality\n is replaced by your new functionality name.\n\n\nInherit from \nHMMRepRequestAbstractclass\n\n\nDefine the \n_const_function_pointer\n to use your function in \nrep_lib.py\n.\n\n\nMake the pointer look like the one in \nHMMRepRequestAbstractclass\n and replace \nmy_function\n with the function name in \nrep_lib.py\n (implemented later on)\n\n\nOverride \n__init__\n definig a custom function header and adding the variables to the variable \nself.kwargs\n, following the example of the other classes in this file.\n\n\n\n\n\n\nCreate a response class in \nqsrrep_lib.rep_io.py\n following the naming scheme \nHMMReqResponseFunctionality\n where \nFuntionality\n should be the same as for the request class.\n\n\nOverride the \nget\n function to make sure it returns a string (str or json dump)\n\n\nAdd your new functionality to \navailable_services\n in the bottom of \nqsrrep_lib.rep_io.py\n.\n\n\nThe string key will be used to create the service name\n\n\nThe value should be a list where the first entry is your request class and the second the response class.\n\n\n\n\n\n\nAdd a new function in \nqsrrep_lib.rep_lib.py\n that calls your getter function from \nqsrrep_hmms.hmm_abstractclass.py\n and returns your response from \nqsrrep_lib.rep_io.py\n\n\nThe ros server will automatically create a service for your new functionality and the ros_client will know how to deal with it given your request class is used as an input for it.\n\n\nAdd thee new functionality to the \nexample_ros_client.py\n using proper argument parsing like done for the other functions.\n\n\n\n\n[1] Dondrup, C.; Bellotto, N.; Hanheide, M.; Eder, K.; Leonards, U. A Computational Model of Human-Robot Spatial Interactions Based on a Qualitative Trajectory Calculus. In: Robotics 2015, 4, 63-102.", 
            "title": "Qsr prob rep"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_prob_rep/#the-qsr-probabilistic-representation-library", 
            "text": "This library provides functionalities to create probablistic models of QSR state chains, e.g. produced by the qsr_lib. This can for example be used for classification and sampling of new state chains. Currently only HMMs are supported.", 
            "title": "The QSR Probabilistic Representation library"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_prob_rep/#usage", 
            "text": "The recommended language is python 2.7 using ROS Indigo. Eventhough the library uses ROS only for communication, the provided infrastructure currently relies on ROS services to be used. Python is recommended due to the fact that the provided ros_client is implemented in python and using the services directly might be a bit confusing at first since they rely on json parsable strings.  For a python usage exmple, please see the example_ros_client.py in scripts. Try  rosrun qsr_prob_rep example_hmm_client.py -h  or  rosrun qsr_prob_rep example_pf_client.py -h  for usage information depending on if you want to create a Hidden Markov Model or a Particle Filter.", 
            "title": "Usage"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_prob_rep/#currently-implemented-functionality", 
            "text": "HMM   create : This takes the desired qsr_type and a json parsable list of lists of QSR state chains and returns the xml representation of the trained HMM as a string. This function is easiest to use when reading the state chains from files as it's done in the example client. The resulting xml can either be written to disk, kept in memory, or stored in a datacentre. The xml string is used in all other functionalities to load the HMM.  sample : Given a HMM as an XML string, the desired number and length of samples, and the qsr the HMM models, this function produces sample state chains and returns them as numpy arrays.  loglikelihood : Given an HMM and a (list of) state chain(s), this function calculates the accumulative loglikelihood for the state chain(s) to be produced by the given HMM. Might produce  -inf  if production is impossible.   Particle Filter   create : This takes a model consisting or several transition probability and observation probability matrices in a dictionary and a state look up table to create a particle filter. Create an instance of the  PfRepRequestCreate  class filling all the necessary information. The required model can be built with a helper class  qsrrep_pf.pf_model.PfModel  that includes tests for the model sanity. Have a look at the example client on how to use this. The look up table has to have as many states as the matrices have rows and coloumns, and should be a simple list or numpy array of the states that will be observed. The index of the state in the look up table has to correspond to the index for this state in the given matrices. Have a look at the  qsrrep_utils.qtc_model_generation  class for inspiration on how to create one. This returns a uuid identifying your particle filter.  The necessary files can be create directly from the HMM by exporting the emission matrix as the observation probability matrix and the transitions as the transition probability matrix. This however, requires that you have enough training data to learn sensible emissians and transitions at the same time. If not, create your own observation probability matrix like it is done in  qsrrep_utils.qtc_model_generation . The  create_pf_models.py  script can help to generate the files from the HMM.  predict : Takes the uuid of an existing particle filter and the number of sample generations to predict and returns the most likely next states and model including their probabilities.  update : Runs the baysian update for the particle filter identified by uuid id using an observation provided.  list_filters : Lists all currently active filters.  remove : Removes the particle filter with the given uuid from memory.   General usage advice  The ros_client for python hides a lot of the complexity from the user. Just create an instance of the correct request class and call  ROSClient().call_service  and the right service will be called for you. All the json parsing happens in the background and you don't have to worry about this. Have a look at the example clients to see how to do this.", 
            "title": "Currently implemented functionality"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_prob_rep/#currently-implemented-qsrs", 
            "text": "QTCB : The basic variant of the Qtalitative Trajectory Calculus. For more information on the implementation see [1]  QTCC : The double-cross variant of the Qtalitative Trajectory Calculus. For more information on the implementation see [1]  QTCBC : The mixture representation of the basic and double-cross variant of the Qtalitative Trajectory Calculus. For more information on the implementation see [1]  RCC3 : A very simple rcc3 representation using uniform transition and emission matrices.", 
            "title": "Currently implemented QSRs"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_prob_rep/#for-developers", 
            "text": "The following is currently hevily outdated and needs updating!!!", 
            "title": "For Developers"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_prob_rep/#adding-a-new-qsr", 
            "text": "Adding a new QSR based on the simple RCC3 example:   Create a new file in  src/qsrrep_hmms  or copy the  rcc3_hmm.py  Import:  from qsrrep_hmms.hmm_abstractclass import HMMAbstractclass  Create a new class that inherits from  HMMAbstractclass  In the  __init__(self)  function:  Call the super calss constructor:  super(self.__class__, self).__init__()  Set  self.num_possible_states  to the number of possible states your QSR has  Overwrite the two functions  _qsr_to_symbol  and  _symbol_to_qsr . See rcc3 example for inspiration.  Add your new QSR to  src/qsrrep_lib/rep_lib.py :  Import your new file from  qsrrep_hmms  Add your new QSR to the dictionary of available QSRs  hmm_types_available  following the examples given  Enjoy your new QSR HMM!   The example client is agnostic of the QSR implemented, a simple restart of the eserver and allows to use your QSR with the example client immediately.", 
            "title": "Adding a new QSR"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_prob_rep/#adding-a-more-specialised-qsr", 
            "text": "Sometimes a uniformly distributed transition and emission matrix (as they are used in above example by default) is just not precise enough like in the case of QTC. Please have a look at  src/qsrrep_hmms/qtc_hmm_abstractclass.py  on how to deal with that.  Basic instructions:   Follow above instructions to create a new QSR  Overwrite the  _create_transition_matrix  and  _create_emission_matrix", 
            "title": "Adding a more specialised QSR"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_prob_rep/#adding-a-new-functionality", 
            "text": "Add your new functionality to  qsrrep_hmms.hmm_abstractclass.py  following the examples of  _create ,  _sample , and  _log_likelihood .  Add a getter for your new functionality in  qsrrep_hmms.hmm_abstractclass.py  following the examples of  get_hmm ,  get_samples , and  get_log_likelihood .  Create a request class following the naming scheme:  HMMRepRequestFunctionality  in  qsrrep_lib.rep_io.py  where  Functionality  is replaced by your new functionality name.  Inherit from  HMMRepRequestAbstractclass  Define the  _const_function_pointer  to use your function in  rep_lib.py .  Make the pointer look like the one in  HMMRepRequestAbstractclass  and replace  my_function  with the function name in  rep_lib.py  (implemented later on)  Override  __init__  definig a custom function header and adding the variables to the variable  self.kwargs , following the example of the other classes in this file.    Create a response class in  qsrrep_lib.rep_io.py  following the naming scheme  HMMReqResponseFunctionality  where  Funtionality  should be the same as for the request class.  Override the  get  function to make sure it returns a string (str or json dump)  Add your new functionality to  available_services  in the bottom of  qsrrep_lib.rep_io.py .  The string key will be used to create the service name  The value should be a list where the first entry is your request class and the second the response class.    Add a new function in  qsrrep_lib.rep_lib.py  that calls your getter function from  qsrrep_hmms.hmm_abstractclass.py  and returns your response from  qsrrep_lib.rep_io.py  The ros server will automatically create a service for your new functionality and the ros_client will know how to deal with it given your request class is used as an input for it.  Add thee new functionality to the  example_ros_client.py  using proper argument parsing like done for the other functions.   [1] Dondrup, C.; Bellotto, N.; Hanheide, M.; Eder, K.; Leonards, U. A Computational Model of Human-Robot Spatial Interactions Based on a Qualitative Trajectory Calculus. In: Robotics 2015, 4, 63-102.", 
            "title": "Adding a new functionality"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_lib/data/", 
            "text": "Datasets description\n\n\n\n\ndata1.csv\n: A set of hand-drawn int 2D-points for 2 objects (\"o1\", \"o2\") with fixed width and length\n(i.e. bounding boxes are provided). Tried to cover all possible cardir and RCC states.\n\n\ndata2.csv\n: A set of 10K random integer 2D-points (\nrandom.randint(0, 50)\n) for 3 objects (\"o1\", \"o2\", \"o3\")\nand random float width and length (\nrandom.randint(3, 6)\n) (i.e. bounding boxes are provided).\n\n\ndata3.csv\n: A set of 10K random integer 2D-points (\nrandom.randint(0, 50)\n) for 3 objects (\"o1\", \"o2\", \"o3\"),\n but no width and length are given (i.e. no bounding boxes are provided).\n\n\ndata4.csv\n: A set of 10K random float 2D-points (\nrandom.uniform(0, 50)\n) for 3 objects (\"o1\", \"o2\", \"o3\")\nand random float width and length (\nrandom.randint(3, 6)\n) (i.e. bounding boxes are provided).\n\n\ndata2_first100.csv\n: First 100 instances of data2.\n\n\ndata3_first100.csv\n: First 100 instances of data3.\n\n\ndata4_first100.csv\n: First 100 instances of data4.\n\n\n\n\nRunning the tests\n\n\nrosrun qsr_lib unittests_generate_ground_truth.py -i \nworld name\n -o \noutput filename\n \nqsr\n\n\n\n\n\nE.g.\n\n\nrosrun qsr_lib unittests_generate_ground_truth.py -i data1 -o qsr_lib/tests/data/data1_rcc4_defaults.txt rcc4\n\n\n\n\n-i\n options: data1 | data2 | data3 | data4\n\n\nTypes of generated data\n\n\n\n\nDefaults. E.g.\n\n\n\n\nrosrun qsr_lib unittests_generate_ground_truth.py -i data1 -o qsr_lib/tests/data/data1_rcc4_defaults.txt rcc4", 
            "title": "Data"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_lib/data/#datasets-description", 
            "text": "data1.csv : A set of hand-drawn int 2D-points for 2 objects (\"o1\", \"o2\") with fixed width and length\n(i.e. bounding boxes are provided). Tried to cover all possible cardir and RCC states.  data2.csv : A set of 10K random integer 2D-points ( random.randint(0, 50) ) for 3 objects (\"o1\", \"o2\", \"o3\")\nand random float width and length ( random.randint(3, 6) ) (i.e. bounding boxes are provided).  data3.csv : A set of 10K random integer 2D-points ( random.randint(0, 50) ) for 3 objects (\"o1\", \"o2\", \"o3\"),\n but no width and length are given (i.e. no bounding boxes are provided).  data4.csv : A set of 10K random float 2D-points ( random.uniform(0, 50) ) for 3 objects (\"o1\", \"o2\", \"o3\")\nand random float width and length ( random.randint(3, 6) ) (i.e. bounding boxes are provided).  data2_first100.csv : First 100 instances of data2.  data3_first100.csv : First 100 instances of data3.  data4_first100.csv : First 100 instances of data4.", 
            "title": "Datasets description"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_lib/data/#running-the-tests", 
            "text": "rosrun qsr_lib unittests_generate_ground_truth.py -i  world name  -o  output filename   qsr   E.g.  rosrun qsr_lib unittests_generate_ground_truth.py -i data1 -o qsr_lib/tests/data/data1_rcc4_defaults.txt rcc4  -i  options: data1 | data2 | data3 | data4", 
            "title": "Running the tests"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_lib/data/#types-of-generated-data", 
            "text": "Defaults. E.g.   rosrun qsr_lib unittests_generate_ground_truth.py -i data1 -o qsr_lib/tests/data/data1_rcc4_defaults.txt rcc4", 
            "title": "Types of generated data"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_lib/", 
            "text": "QSR_Lib\n\n\nCan be run as python standalone, as well as a ROS server is provided with an example client.\n\n\nNotes for users\n\n\n1) Running it as python standalone\nJust run the example.py in the $STRANDS_QSR_LIB/qsr_lib/scripts/standalone\n\n\n2) Running it via ROS\n\n\n$ rosrun qsr_lib qsrlib_ros_server.py\n\n\n\n\nexample client:\n\n\n$ rosrun qsr_lib example_ros_client.py\n\n\n\n\nsee qsrlib_ros_client.py for more details\n\n\nQTC\n\n\nQTC descirbes the movement of 2 agents in 2d space. The implemented version is described in [1,2].\nBoth agents need to have their x,y coordinates described for at least two timesteps. Please see \nqsr_lib/share/qtc_data\n or the \nexample_ros_client.py\n for a very short example.\n\n\nTo run the ros example client uisng qtc:\n\n\nrosrun qsr_lib example_ros_client.py \nqtc-varaient\n\n\n\n\n\nThis has a few options\n\n\n\n\nMendatory: \n\n\nqtc-varaient\n: Can be either \nqtcb\n, \nqtcc\n, or \nqtcbc\n. Please refer to [1,2] for additional infromation about the variants\n\n\nOptional:\n\n\n-h|--help\n: to display usage information\n\n\n-i|--input FILE\n: read x,y coordinates from a csv file. See \nqsr_lib/share/qtc_data/qtc_example.csv\n\n\n--quantisation_factor FLOAT\n: Determines the minimum distance one of the agents have to diverge from the double cross to be counted as movement. Please refer to [1] to read more about smoothing and quantisation of QTC.\n\n\n--validate\n: Creates a QTC representation that is valid according to the CND [3]\n\n\n--no_collapse\n: Does not collapse similar adjacent states as it the default in QTC since it does not represent discrete time but a sequence of states\n\n\n--distance_threshold\n: Sets a threshold distance for the transition from QTCB to QTCC and vice-versa. Only for QTCBC [2].\n\n\n\n\nTo create a valid QTCC state chain with a quantisation factor of 1 (in the same unit as x,y) one would run:\n\n\n  rosrun qsr_lib example_ros_client.py qtcc --validate --quantisation_factor 1 -i my_file.csv\n\n\n\n\nTo create a QTCB state for each transition from one timestep to the other without validating them, one would run:\n\n\n  rosrun qsr_lib example_ros_client.py qtcb -i my_file.csv --no_collapse\n\n\n\n\nTo create a QTCBC state chain, one would run:\n\n\n  rosrun qsr_lib example_ros_client.py qtcbc -i my_file.csv --distance_threshold 1.5 --validate\n\n\n\n\nNotes for QSR developers\n\n\nYou need to change two files:\n\n\n1) Make a copy of\n$STRANDS_QSR_LIB/qsr_lib/scripts/standalone/makers/maker_qsr_rcc3_rectangle_bounding_boxes_2d.py\nrename it to something suitable (preferably keepung maker_qsr prefix) and edit that file.\n\n\nChange the 3 attribute variables:\n- self.qsr_type : is a string\n- self.required_fields : list of strings, what type and order of input data the QSR requires\n- self.all_possible_relations : list of string, what QSRs it provides\n\n\nChange the method:\n- make : see in the code for where to start and finish your code\n\n\nOptionally change the following also:\n- custom_help\n- custom_checks\n\n\n2) Edit qsrlib.py in $STRANDS_QSR_LIB/qsr_lib/scripts/standalone/\na) import your new QSR class\nb) Append to the dictionary (self.__const_qsrs_available) of the QSRlib class an appropriate entry for your new QSR using as key the same string you have assigned to YOUR_QSR_CLASS.qsr_type and as value the class name WITHOUT parentheses in the end\n\n\nThat's it!\nFor any trouble preferably  a ticket via the github system or alternatively if you are unable to do so contact me directly in the email address y.gatsoulis@leeds.ac.uk\n\n\nCheers!\nYianni\n\n\n[1] Christian Dondrup, Nicola Bellotto, and Marc Hanheide. \"A probabilistic model of human-robot spatial interaction using a qualitative trajectory calculus.\" 2014 AAAI Spring Symposium Series. 2014.\n\n\n[2] Christian Dondrup, Nicola Bellotto, and Marc Hanheide. \"Social distance augmented qualitative trajectory calculus for Human-Robot Spatial Interaction.\" Robot and Human Interactive Communication, 2014 RO-MAN: The 23rd IEEE International Symposium on. IEEE, 2014.\n\n\n[3] Matthias Delafontaine. \"Modelling and analysing moving objects and travelling subjects: Bridging theory and practice\". Diss. Ghent University, 2011.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_lib/#qsr_lib", 
            "text": "Can be run as python standalone, as well as a ROS server is provided with an example client.", 
            "title": "QSR_Lib"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_lib/#notes-for-users", 
            "text": "1) Running it as python standalone\nJust run the example.py in the $STRANDS_QSR_LIB/qsr_lib/scripts/standalone  2) Running it via ROS  $ rosrun qsr_lib qsrlib_ros_server.py  example client:  $ rosrun qsr_lib example_ros_client.py  see qsrlib_ros_client.py for more details", 
            "title": "Notes for users"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_lib/#qtc", 
            "text": "QTC descirbes the movement of 2 agents in 2d space. The implemented version is described in [1,2].\nBoth agents need to have their x,y coordinates described for at least two timesteps. Please see  qsr_lib/share/qtc_data  or the  example_ros_client.py  for a very short example.  To run the ros example client uisng qtc:  rosrun qsr_lib example_ros_client.py  qtc-varaient   This has a few options   Mendatory:   qtc-varaient : Can be either  qtcb ,  qtcc , or  qtcbc . Please refer to [1,2] for additional infromation about the variants  Optional:  -h|--help : to display usage information  -i|--input FILE : read x,y coordinates from a csv file. See  qsr_lib/share/qtc_data/qtc_example.csv  --quantisation_factor FLOAT : Determines the minimum distance one of the agents have to diverge from the double cross to be counted as movement. Please refer to [1] to read more about smoothing and quantisation of QTC.  --validate : Creates a QTC representation that is valid according to the CND [3]  --no_collapse : Does not collapse similar adjacent states as it the default in QTC since it does not represent discrete time but a sequence of states  --distance_threshold : Sets a threshold distance for the transition from QTCB to QTCC and vice-versa. Only for QTCBC [2].   To create a valid QTCC state chain with a quantisation factor of 1 (in the same unit as x,y) one would run:    rosrun qsr_lib example_ros_client.py qtcc --validate --quantisation_factor 1 -i my_file.csv  To create a QTCB state for each transition from one timestep to the other without validating them, one would run:    rosrun qsr_lib example_ros_client.py qtcb -i my_file.csv --no_collapse  To create a QTCBC state chain, one would run:    rosrun qsr_lib example_ros_client.py qtcbc -i my_file.csv --distance_threshold 1.5 --validate", 
            "title": "QTC"
        }, 
        {
            "location": "/strands_qsr_lib/qsr_lib/#notes-for-qsr-developers", 
            "text": "You need to change two files:  1) Make a copy of\n$STRANDS_QSR_LIB/qsr_lib/scripts/standalone/makers/maker_qsr_rcc3_rectangle_bounding_boxes_2d.py\nrename it to something suitable (preferably keepung maker_qsr prefix) and edit that file.  Change the 3 attribute variables:\n- self.qsr_type : is a string\n- self.required_fields : list of strings, what type and order of input data the QSR requires\n- self.all_possible_relations : list of string, what QSRs it provides  Change the method:\n- make : see in the code for where to start and finish your code  Optionally change the following also:\n- custom_help\n- custom_checks  2) Edit qsrlib.py in $STRANDS_QSR_LIB/qsr_lib/scripts/standalone/\na) import your new QSR class\nb) Append to the dictionary (self.__const_qsrs_available) of the QSRlib class an appropriate entry for your new QSR using as key the same string you have assigned to YOUR_QSR_CLASS.qsr_type and as value the class name WITHOUT parentheses in the end  That's it!\nFor any trouble preferably  a ticket via the github system or alternatively if you are unable to do so contact me directly in the email address y.gatsoulis@leeds.ac.uk  Cheers!\nYianni  [1] Christian Dondrup, Nicola Bellotto, and Marc Hanheide. \"A probabilistic model of human-robot spatial interaction using a qualitative trajectory calculus.\" 2014 AAAI Spring Symposium Series. 2014.  [2] Christian Dondrup, Nicola Bellotto, and Marc Hanheide. \"Social distance augmented qualitative trajectory calculus for Human-Robot Spatial Interaction.\" Robot and Human Interactive Communication, 2014 RO-MAN: The 23rd IEEE International Symposium on. IEEE, 2014.  [3] Matthias Delafontaine. \"Modelling and analysing moving objects and travelling subjects: Bridging theory and practice\". Diss. Ghent University, 2011.", 
            "title": "Notes for QSR developers"
        }, 
        {
            "location": "/strands_recovery_behaviours/backoff_behaviour/", 
            "text": "Overview\n\n\nSimple behaviour to get away from obstacles.", 
            "title": "Backoff behaviour"
        }, 
        {
            "location": "/strands_recovery_behaviours/backoff_behaviour/#overview", 
            "text": "Simple behaviour to get away from obstacles.", 
            "title": "Overview"
        }, 
        {
            "location": "/strands_recovery_behaviours/", 
            "text": "strands_recovery_behaviours\n\n\nRecovery behaviours for strands_navigation", 
            "title": "Home"
        }, 
        {
            "location": "/strands_recovery_behaviours/#strands_recovery_behaviours", 
            "text": "Recovery behaviours for strands_navigation", 
            "title": "strands_recovery_behaviours"
        }, 
        {
            "location": "/strands_social/fake_camera_effects/", 
            "text": "Fake Camera Effects\n\n\nThis action server fakes a normal photo camera by playing a shutter sound. Flashing the robots LEDs and closing its eyes.\n\n\nThis was used for the tweeting during the NHM deployment.", 
            "title": "Fake camera effects"
        }, 
        {
            "location": "/strands_social/fake_camera_effects/#fake-camera-effects", 
            "text": "This action server fakes a normal photo camera by playing a shutter sound. Flashing the robots LEDs and closing its eyes.  This was used for the tweeting during the NHM deployment.", 
            "title": "Fake Camera Effects"
        }, 
        {
            "location": "/strands_social/", 
            "text": "strands_social\n\n\nSocial media apps for e.g. twitter", 
            "title": "Home"
        }, 
        {
            "location": "/strands_social/#strands_social", 
            "text": "Social media apps for e.g. twitter", 
            "title": "strands_social"
        }, 
        {
            "location": "/strands_social/social_card_reader/", 
            "text": "Overview\n\n\nThe package allows to issue commands to the robot by presenting it specially designed cards.\nWhile the first card allows to issue only one type of command (i.e. take a photo), the second one allows to issue four different commands.\n\n\nYou will need to print (no rescaling!) the \ncards.pdf\n file, cut out the cards and fold the 'operator' one.\nPresenting the pattern to the robot's camera causes it to publish the command on \n/socialCardReader/command\n topic.", 
            "title": "Social card reader"
        }, 
        {
            "location": "/strands_social/social_card_reader/#overview", 
            "text": "The package allows to issue commands to the robot by presenting it specially designed cards.\nWhile the first card allows to issue only one type of command (i.e. take a photo), the second one allows to issue four different commands.  You will need to print (no rescaling!) the  cards.pdf  file, cut out the cards and fold the 'operator' one.\nPresenting the pattern to the robot's camera causes it to publish the command on  /socialCardReader/command  topic.", 
            "title": "Overview"
        }, 
        {
            "location": "/strands_social/strands_tweets/", 
            "text": "strands_tweets\n\n\nUtility package containing a node that can be used to manage twitter accounts from ROS\n\n\nInstallation\n\n\nInstall Twython via pip\n\n\n    $ pip install twython\n\n\n\n\nor, with easy_install\n\n\n    $ easy_install twython\n\n\n\n\nStarting Out\n\n\n\n\nGo to \nhttps://dev.twitter.com/apps\n and register an application\n\n\nIf the application is registered, hit the \nTest OAuth\n. Skip the two following steps.\n\n\nGo to the settings tab and chage permitions to \nRead, Write and Access direct messages\n\n\nGo back to the Details tab and at the botton hit the \nCreate Access Token Button\n\n\n\n\nGo to OAuth tool tab and get the \nConsumer key\n, \nConsumer secret\n, \nAccess token\n and \nAccess token secret\n and save them on \n/opt/strands/strands_catkin_ws/src/strands_deployment/strands_parameters/defaults/twitter_params.yaml\n with the format as follows:\n    ``` \n    twitter: \n        appKey: '\n'\n        appSecret: '\n'\n        oauthToken: '\n'\n        oauthTokenSecret: '\n'\n\n\n* Launch the mongodb_store:\n roslaunch mongodb_store mongodb_store.launch```\n  * Save the parameters on your locals collection:\n\n\n\n\n\n\nrosservice call /config_manager/save_param /twitter/appKey\n\n\nrosservice call /config_manager/save_param /twitter/appSecret\n\n\nrosservice call /config_manager/save_param /twitter/oauthToken\n\n\nrosservice call /config_manager/save_param /twitter/oauthTokenSecret\n\n  * Now you are ready to go!\n\n\nTweeting\n\n\nText Only\n\n\nRun the Strands_tweets node\n\nrosrun strands_tweets tweet.py\n\n\nYou can send a tweet by calling the \n/strands_tweets/Tweet\n service like this:\n\n\nrosservice call /strands_tweets/Tweet 'Whatever you want to say' false\n\n\nYou can also send a tweet using the \ntweet_test\n client like this:\n\n\nrosrun strands_tweets tweet_test.py 'Whatever you want to say'\n\n\nYou can tweet using actions too try running:\n\n\nrosrun actionlib axclient.py /strands_tweets\n\n\nFill the text field with the text of your tweet and press SEND GOAL\n\n\nTweeting Images\n\n\nYou can tweet Images using an script especially provided for it:\n\n\nrosrun strands_tweets tweet_image_test.py 'text for the tweet less than a 140 characters' /path/to/an/image.png\n\n\nUsing strands_tweets in your code\n\n\nGoal definition\n\n\nstrands_tweets is an action server with the following goal\n\n\n#goal definition\nstring text\nbool force\nbool with_photo\nsensor_msgs/Image photo\n---\n#result definition\nbool success\n---\n#feedback\nstring tweet\n\n\n\n\n\n\n\n\ntext\n: is a field that contains the text to be tweeted\n\n\n\n\n\n\nforce\n: is a boolean to tell the action server to tweet the text even if is longer than a 140 characters, on that texted it will send as many tweets as needed to send the whole string. \nNote\n: this doesn't work when tweeting images\n\n\n\n\n\n\nwith_photo\n: is a boolean to tell the action server that the tweet contains an image\n\n\n\n\n\n\nphoto\n: is the image to be tweeted with the text when 'with_photo' is set to true\n\n\n\n\n\n\nUsing the action server (python)\n\n\n\n\nImport the necessary modules\n\n\n\n\nimport actionlib\nimport strands_tweets.msg\n\n\n\n\nif using images you'll need to import this too:\n\n\nfrom sensor_msgs.msg import Image\n\n\n\n\n\n\nCreate action server client\n\n\n\n\nclient = actionlib.SimpleActionClient('strands_tweets', strands_tweets.msg.SendTweetAction)\nclient.wait_for_server()\n\n\n\n\nstrands_tweets\n is the action server name\n\n\n\n\nCreate goal definition (text only)\n\n\n\n\ntweetgoal = strands_tweets.msg.SendTweetGoal()\ntweetgoal.text = text\ntweetgoal.force = false\ntweetgoal.with_photo = False\n\n\n\n\n\n\nCreate goal definition (with image)\n\n\n\n\ntweetgoal = strands_tweets.msg.SendTweetGoal()\ntweetgoal.text = text\ntweetgoal.with_photo = True\ntweetgoal.photo = image_from_ros_msg\n\n\n\n\nNote\n: the image HAS to be a 'sensor_msgs.msg.Image' ROS message\n\n\n\n\nSend goal\n\n\n\n\nclient.send_goal(tweetgoal)\n# Waits for the server to finish performing the action.\nclient.wait_for_result()\n# Prints out the result of executing the action\nps = client.get_result()\nprint ps\n\n\n\n\n\n\nWhere can I see an example of this?\n\n\n\n\nYou can check the test nodes for this code in :\n\n\nhttps://raw.githubusercontent.com/strands-project/strands_social/hydro-devel/strands_tweets/scripts/tweet_image_test.py\n\n\nand \n\n\nhttps://raw.githubusercontent.com/strands-project/strands_social/hydro-devel/strands_tweets/scripts/tweet_test.py\n\n\nUsing the action server (C++)\n\n\nThere are no examples of this in C++ but you can use it following the steps on this Readme using the action lib API for C++ should do, following this tutorial should help http://wiki.ros.org/actionlib_tutorials/Tutorials/SimpleActionClient", 
            "title": "Strands tweets"
        }, 
        {
            "location": "/strands_social/strands_tweets/#strands_tweets", 
            "text": "Utility package containing a node that can be used to manage twitter accounts from ROS", 
            "title": "strands_tweets"
        }, 
        {
            "location": "/strands_social/strands_tweets/#installation", 
            "text": "Install Twython via pip      $ pip install twython  or, with easy_install      $ easy_install twython", 
            "title": "Installation"
        }, 
        {
            "location": "/strands_social/strands_tweets/#starting-out", 
            "text": "Go to  https://dev.twitter.com/apps  and register an application  If the application is registered, hit the  Test OAuth . Skip the two following steps.  Go to the settings tab and chage permitions to  Read, Write and Access direct messages  Go back to the Details tab and at the botton hit the  Create Access Token Button   Go to OAuth tool tab and get the  Consumer key ,  Consumer secret ,  Access token  and  Access token secret  and save them on  /opt/strands/strands_catkin_ws/src/strands_deployment/strands_parameters/defaults/twitter_params.yaml  with the format as follows:\n    ``` \n    twitter: \n        appKey: ' '\n        appSecret: ' '\n        oauthToken: ' '\n        oauthTokenSecret: ' '  * Launch the mongodb_store:  roslaunch mongodb_store mongodb_store.launch```\n  * Save the parameters on your locals collection:    rosservice call /config_manager/save_param /twitter/appKey  rosservice call /config_manager/save_param /twitter/appSecret  rosservice call /config_manager/save_param /twitter/oauthToken  rosservice call /config_manager/save_param /twitter/oauthTokenSecret \n  * Now you are ready to go!", 
            "title": "Starting Out"
        }, 
        {
            "location": "/strands_social/strands_tweets/#tweeting", 
            "text": "", 
            "title": "Tweeting"
        }, 
        {
            "location": "/strands_social/strands_tweets/#text-only", 
            "text": "Run the Strands_tweets node rosrun strands_tweets tweet.py  You can send a tweet by calling the  /strands_tweets/Tweet  service like this:  rosservice call /strands_tweets/Tweet 'Whatever you want to say' false  You can also send a tweet using the  tweet_test  client like this:  rosrun strands_tweets tweet_test.py 'Whatever you want to say'  You can tweet using actions too try running:  rosrun actionlib axclient.py /strands_tweets  Fill the text field with the text of your tweet and press SEND GOAL", 
            "title": "Text Only"
        }, 
        {
            "location": "/strands_social/strands_tweets/#tweeting-images", 
            "text": "You can tweet Images using an script especially provided for it:  rosrun strands_tweets tweet_image_test.py 'text for the tweet less than a 140 characters' /path/to/an/image.png", 
            "title": "Tweeting Images"
        }, 
        {
            "location": "/strands_social/strands_tweets/#using-strands_tweets-in-your-code", 
            "text": "", 
            "title": "Using strands_tweets in your code"
        }, 
        {
            "location": "/strands_social/strands_tweets/#goal-definition", 
            "text": "strands_tweets is an action server with the following goal  #goal definition\nstring text\nbool force\nbool with_photo\nsensor_msgs/Image photo\n---\n#result definition\nbool success\n---\n#feedback\nstring tweet    text : is a field that contains the text to be tweeted    force : is a boolean to tell the action server to tweet the text even if is longer than a 140 characters, on that texted it will send as many tweets as needed to send the whole string.  Note : this doesn't work when tweeting images    with_photo : is a boolean to tell the action server that the tweet contains an image    photo : is the image to be tweeted with the text when 'with_photo' is set to true", 
            "title": "Goal definition"
        }, 
        {
            "location": "/strands_social/strands_tweets/#using-the-action-server-python", 
            "text": "Import the necessary modules   import actionlib\nimport strands_tweets.msg  if using images you'll need to import this too:  from sensor_msgs.msg import Image   Create action server client   client = actionlib.SimpleActionClient('strands_tweets', strands_tweets.msg.SendTweetAction)\nclient.wait_for_server()  strands_tweets  is the action server name   Create goal definition (text only)   tweetgoal = strands_tweets.msg.SendTweetGoal()\ntweetgoal.text = text\ntweetgoal.force = false\ntweetgoal.with_photo = False   Create goal definition (with image)   tweetgoal = strands_tweets.msg.SendTweetGoal()\ntweetgoal.text = text\ntweetgoal.with_photo = True\ntweetgoal.photo = image_from_ros_msg  Note : the image HAS to be a 'sensor_msgs.msg.Image' ROS message   Send goal   client.send_goal(tweetgoal)\n# Waits for the server to finish performing the action.\nclient.wait_for_result()\n# Prints out the result of executing the action\nps = client.get_result()\nprint ps   Where can I see an example of this?   You can check the test nodes for this code in :  https://raw.githubusercontent.com/strands-project/strands_social/hydro-devel/strands_tweets/scripts/tweet_image_test.py  and   https://raw.githubusercontent.com/strands-project/strands_social/hydro-devel/strands_tweets/scripts/tweet_test.py", 
            "title": "Using the action server (python)"
        }, 
        {
            "location": "/strands_social/strands_tweets/#using-the-action-server-c", 
            "text": "There are no examples of this in C++ but you can use it following the steps on this Readme using the action lib API for C++ should do, following this tutorial should help http://wiki.ros.org/actionlib_tutorials/Tutorials/SimpleActionClient", 
            "title": "Using the action server (C++)"
        }, 
        {
            "location": "/strands_ui/", 
            "text": "STRANDS User Interfaces\n\n\nUser interfaces for the robots, displayed using a web browser.\n\n\nInstallation\n\n\nYou can add the STRANDS UI source to your catkin workspace (if it's not there already) using \nwstool\n:\n\n\nwstool merge https://raw.github.com/hawesie/strands_ui/master/strands_ui_rosinstall.yaml\nwstool update strands_ui twitter_bootstrap\n\n\n\n\nrosdep\n should give you most requirements as usual (e.g. \nrosdep install --from-paths src --ignore-src --rosdistro groovy -y -r\n), but we don't yet have a rosdep for \nweb.py\n which is required by \nstrands_webserver\n. This can be installed using, e.g., \n\n\nsudo apt-get install python-webpy\n\n\n\n\nor\n\n\nsudo pip install web.py\n\n\n\n\nYou can then run \ncatkin_make\n as usual.\n\n\nstrands_webserver\n\n\nThe \nstrands_webserver\n is a node which both acts as a webserver and a ROS node which can receive input from other nodes. This allows these other nodes to control what the webserver displays and receive feedback from user interaction with the pages.\n\n\nRunning\n\n\nTo run the server, first run rosbrige if you don't have it running already.\n\n\nroslaunch rosbridge_server rosbridge_websocket.launch\n\n\n\n\nThen run the server\n\n\nrosrun strands_webserver strands_webserver \n\n\n\n\nThis will launch the webserver on localhost port 8090, so browse to \nhttp://localhost:8090\n and you should see something like\n\n\nReady; display = 1\n\n\n\n\nDisplaying content\n\n\nThe webserver manages individual \ndisplays\n separately, one for each request to its URL. These can be published to individual by specifying their number, or you can just send 0 to publish to all of them.\n\n\nInteraction with \nstrands_webserver\n typically happens by publishing instructions to it for display. These can either be URLs or bits of html which will be injected into its main page. The URLs are loaded in a full size iframe. The bits of html are injected as the \ninnerHTML\n of the \ncontent\n node in the main page (\nstrands_webserver/data/main.html\n).\n\n\nThe example below shows how to publish a simple html page to the webserver, both a normal URL (given that it is happy to appear in an iframe) then a page defined locally.\n\n\n#! /usr/bin/env python\n\nimport sys\nimport os\nimport roslib\nimport rospy\n\nimport strands_webserver.client_utils\n\nif __name__ == '__main__':\n    rospy.init_node(\nstrands_webserver_demo\n)\n    # The display to publish on, defaulting to all displays\n    display_no = rospy.get_param(\n~display\n, 0) \n\n    # display a start-up page\n    strands_webserver.client_utils.display_url(display_no, 'http://strands-project.eu')\n\n    # sleep for 2 seconds\n    rospy.sleep(5.)\n\n    # tell the webserver where it should look for web files to serve\n    http_root = os.path.join(roslib.packages.get_pkg_dir(\nstrands_webserver\n), \ndata\n)\n    strands_webserver.client_utils.set_http_root(http_root)\n\n    # start with a basic page pretending things are going normally\n    strands_webserver.client_utils.display_relative_page(display_no, 'example-page.html')\n\n\n\n\nTo display arbitrary HTML within the the \ncontent\n node in the main page (\nstrands_webserver/data/main.html\n) you can use calls like the following\n\n\nstrands_webserver.client_utils.display_content(display_no, '\np\nHello world\n/p\n') \n\n\n\n\nNote that any \nscript\n nodes will not be evaluated in this injected html. \n\n\nUsing the Webloader action server\n\n\nThe webloader action server offers 4 ways of manipultating webpages shown by the webserver (all of this of course only works if you exclusivly use the webloader server to display pages):\n\n Loading relative or contant generated pages\n\n Reloading the current page\n\n Going back to the previous page\n\n Showing a temporary page for a specific number of seconds\n\n\nUsage still requires your main programm to set\n\n\n    http_root = os.path.join(roslib.packages.get_pkg_dir(\nmy_package\n), \ndirectory\n)\n\n\n\n\nCurrently all these pages are sent to all displays. Possible future extensions could therefore include a history for each display.\n\n\nEvery of the above mentioned actions has their own action file. Reloading and going back are just empty actions. Loading a page reuires to specify \ngoal.relative=BOOL\n to toggle between relative and content generated pages and \ngoal.page=STRING\n is either the name of the relative page or the content for the generated page. For the temporary page you also have to specify \ngoal.timeout=INT\n after which the previous page will be shown again.\n\n\nThis action server was developed to manage the user interface for the NHM deployment where people could trigger to tweet an image. This was displayed on a temporary page and afterwards the display was reset to the previous page. This had the advantage of showing arbitrary pages without care about what should be displayed afterwards.\n\n\nAuto-generated button pages\n\n\nThe \nstrands_webserver\n package also provides functionality for automatically generating pages with buttons which can call services. Such a call might look like this\n\n\n    notice = 'Help me, I am \nem\nstuck\n/em\n'\n    buttons = [('No', 'trigger_pleading'), ('Sure', 'party_time')]\n    service_prefix = '/caller_services'\n    content = strands_webserver.page_utils.generate_alert_button_page(notice, buttons, service_prefix)  \n    strands_webserver.client_utils.display_content(display_no, content) \n\n\n\n\nIn this \nnotice\n is the used to generate a large banner notice and \nbuttons\n maps between a button label (e.g. No) and the service call which will be triggered when the button is pressed, e.g. (\n'/caller_services/trigger_pleading'\n). This service is of type \nstd_srvs/Empty\n. Instead of \ngenerate_alert_button_page\n you can use \ngenerate_button_page\n which accepts arbitrary html instead of a banner notice.\n\n\nThe full example is as follows (also available as \nstrands_webserver/scripts/strands_webserver_demo.py\n).\n\n\n#! /usr/bin/env python\n\nimport sys\nimport os\nimport roslib\nimport rospy\n\nimport strands_webserver.page_utils\nimport strands_webserver.client_utils\nimport std_srvs.srv \n\n\ndef trigger_pleading(req):\n      print 'please, please help'\n\n\ndef party_time(req):\n      print 'woo, off I go baby'\n\n\nif __name__ == '__main__':\n    rospy.init_node(\nstrands_webserver_demo\n)\n    # The display to publish on, defaulting to all displays\n    display_no = rospy.get_param(\n~display\n, 0) \n\n    # display a start-up page\n    strands_webserver.client_utils.display_url(display_no, 'http://strands-project.eu')\n\n    # sleep for 5 seconds\n    rospy.sleep(5.)\n\n    # tell the webserver where it should look for web files to serve\n    http_root = os.path.join(roslib.packages.get_pkg_dir(\nstrands_webserver\n), \ndata\n)\n    strands_webserver.client_utils.set_http_root(http_root)\n\n    # start with a basic page pretending things are going normally\n    strands_webserver.client_utils.display_relative_page(display_no, 'example-page.html')\n\n    # sleep for 5 seconds\n    rospy.sleep(5.)\n\n    # now ask for help\n    name = 'Help me, I am \nem\nstuck\n/em\n'\n    buttons = [('No', 'trigger_pleading'), ('Sure', 'party_time')]\n    service_prefix = '/caller_services'\n    content = strands_webserver.page_utils.generate_alert_button_page(name, buttons, service_prefix)    \n    strands_webserver.client_utils.display_content(display_no, content) \n    rospy.Service('/caller_services/trigger_pleading', std_srvs.srv.Empty, trigger_pleading) \n    rospy.Service('/caller_services/party_time', std_srvs.srv.Empty, party_time) \n    rospy.spin()\n\n\n\n\nIncludes\n\n\nNeed to describe where the webserver fetches javascript and css from, and the standard includes available.\n\n\nmarathon_touch_gui\n\n\nThis package uses the \nstrands_webserver\n to create an interface for the patroller during the marathon event. It's not very pretty, but it's a start. There is a main page (map, pause button) which is displayed using \nstrands_webserver.client_utils.display_relative_page\n and two pages for recovery methods which are generated using \nstrands_webserver.page_utils.generate_alert_button_page\n. These are wrapped up in \nmarathon_touch_gui.client\n for ease of use. They can be called as follows:\n\n\n    # Setup -- must be done before other marathon_touch_gui calls\n    marathon_touch_gui.client.init_marathon_gui()\n\n    # Show the main page of the GUI\n    marathon_touch_gui.client.display_main_page(displayNo)\n\n    rospy.sleep(2)\n\n    # All callback services should be under this prefix\n    service_prefix = '/patroller'\n\n    # Do something like this on bumper collision\n\n    # when the human gives the 'ok' then /patroller/bumper_recovered is called\n    # note that this service must be provided by some other node\n    on_completion = 'bumper_recovered'\n    marathon_touch_gui.client.bumper_stuck(displayNo, service_prefix, on_completion)\n\n    rospy.sleep(2)\n\n    # Do something like this on move_base failure\n\n    # when the human gives the 'ok' then /patroller/robot_moved is called\n    # note that this service must be provided by some other node\n    on_completion = 'robot_moved'\n    marathon_touch_gui.client.nav_fail(displayNo, service_prefix, on_completion)\n\n    rospy.sleep(2)\n\n    # Return to main page\n    marathon_touch_gui.client.display_main_page(displayNo)\n\n\n\n\nThe full example is in \nmarathon_touch_gui/scripts/demo.py\n\n\nRunning\n\n\nTo run the marathon GUI, first launch \nstrands_webserver\n plus rosbridge and the necessary additional publishers (with HOST_IP set to an external ip for your machine if you want this to be externally accessible, else leave blank for 127.0.0.1):\n\n\nHOST_IP=10.0.11.158 roslaunch marathon_touch_gui marathon_gui_dependencies.launch\n\n\n\n\nThis will launch the webserver on localhost port 8090, so browse to \nhttp://localhost:8090\n.\n\n\nThen you can call the \nmarathon_touch_gui\n functions. To test you can cycle through them with \n\n\nrosrun marathon_touch_gui demo.py \n\n\n\n\nModal Dialog\n\n\nBy publishing a \nstrands_webserver/ModalDlg\n message on the \n/strands_webserver/modal_dialog\n topic a modal dialog can be triggered hovering above any window display. This is useful for system notifications. The \nstrands_webserver/ModalDlg\n is defined as follows:\n\n\nstring title\nstring content\nbool show\n\n\n\n\nThe modal dialog can be closed by the user using a little close item in the top right corner, it can remotely be hidden, by setting \nshow\n to false. Here is an example to display the dialog box:\n\n\nrostopic pub /strands_webserver/modal_dialog strands_webserver/ModalDlg \ntitle: 'Nice Title'\ncontent: '\nb\ntest\n/b\n \na href=\nhttps://github.com/strands-project/aaf_deployment\nAAF\n/a\n'\nshow: true\n\n\n\n\n\nTo hide it again, simply publish\n\n\nrostopic pub /strands_webserver/modal_dialog strands_webserver/ModalDlg \ntitle: ''\ncontent: ''\nshow: false\n\n\n\n\n\nBoth, title and content can contain valid HTML.", 
            "title": "Home"
        }, 
        {
            "location": "/strands_ui/#strands-user-interfaces", 
            "text": "User interfaces for the robots, displayed using a web browser.", 
            "title": "STRANDS User Interfaces"
        }, 
        {
            "location": "/strands_ui/#installation", 
            "text": "You can add the STRANDS UI source to your catkin workspace (if it's not there already) using  wstool :  wstool merge https://raw.github.com/hawesie/strands_ui/master/strands_ui_rosinstall.yaml\nwstool update strands_ui twitter_bootstrap  rosdep  should give you most requirements as usual (e.g.  rosdep install --from-paths src --ignore-src --rosdistro groovy -y -r ), but we don't yet have a rosdep for  web.py  which is required by  strands_webserver . This can be installed using, e.g.,   sudo apt-get install python-webpy  or  sudo pip install web.py  You can then run  catkin_make  as usual.", 
            "title": "Installation"
        }, 
        {
            "location": "/strands_ui/#strands_webserver", 
            "text": "The  strands_webserver  is a node which both acts as a webserver and a ROS node which can receive input from other nodes. This allows these other nodes to control what the webserver displays and receive feedback from user interaction with the pages.", 
            "title": "strands_webserver"
        }, 
        {
            "location": "/strands_ui/#running", 
            "text": "To run the server, first run rosbrige if you don't have it running already.  roslaunch rosbridge_server rosbridge_websocket.launch  Then run the server  rosrun strands_webserver strands_webserver   This will launch the webserver on localhost port 8090, so browse to  http://localhost:8090  and you should see something like  Ready; display = 1", 
            "title": "Running"
        }, 
        {
            "location": "/strands_ui/#displaying-content", 
            "text": "The webserver manages individual  displays  separately, one for each request to its URL. These can be published to individual by specifying their number, or you can just send 0 to publish to all of them.  Interaction with  strands_webserver  typically happens by publishing instructions to it for display. These can either be URLs or bits of html which will be injected into its main page. The URLs are loaded in a full size iframe. The bits of html are injected as the  innerHTML  of the  content  node in the main page ( strands_webserver/data/main.html ).  The example below shows how to publish a simple html page to the webserver, both a normal URL (given that it is happy to appear in an iframe) then a page defined locally.  #! /usr/bin/env python\n\nimport sys\nimport os\nimport roslib\nimport rospy\n\nimport strands_webserver.client_utils\n\nif __name__ == '__main__':\n    rospy.init_node( strands_webserver_demo )\n    # The display to publish on, defaulting to all displays\n    display_no = rospy.get_param( ~display , 0) \n\n    # display a start-up page\n    strands_webserver.client_utils.display_url(display_no, 'http://strands-project.eu')\n\n    # sleep for 2 seconds\n    rospy.sleep(5.)\n\n    # tell the webserver where it should look for web files to serve\n    http_root = os.path.join(roslib.packages.get_pkg_dir( strands_webserver ),  data )\n    strands_webserver.client_utils.set_http_root(http_root)\n\n    # start with a basic page pretending things are going normally\n    strands_webserver.client_utils.display_relative_page(display_no, 'example-page.html')  To display arbitrary HTML within the the  content  node in the main page ( strands_webserver/data/main.html ) you can use calls like the following  strands_webserver.client_utils.display_content(display_no, ' p Hello world /p ')   Note that any  script  nodes will not be evaluated in this injected html.", 
            "title": "Displaying content"
        }, 
        {
            "location": "/strands_ui/#using-the-webloader-action-server", 
            "text": "The webloader action server offers 4 ways of manipultating webpages shown by the webserver (all of this of course only works if you exclusivly use the webloader server to display pages):  Loading relative or contant generated pages  Reloading the current page  Going back to the previous page  Showing a temporary page for a specific number of seconds  Usage still requires your main programm to set      http_root = os.path.join(roslib.packages.get_pkg_dir( my_package ),  directory )  Currently all these pages are sent to all displays. Possible future extensions could therefore include a history for each display.  Every of the above mentioned actions has their own action file. Reloading and going back are just empty actions. Loading a page reuires to specify  goal.relative=BOOL  to toggle between relative and content generated pages and  goal.page=STRING  is either the name of the relative page or the content for the generated page. For the temporary page you also have to specify  goal.timeout=INT  after which the previous page will be shown again.  This action server was developed to manage the user interface for the NHM deployment where people could trigger to tweet an image. This was displayed on a temporary page and afterwards the display was reset to the previous page. This had the advantage of showing arbitrary pages without care about what should be displayed afterwards.", 
            "title": "Using the Webloader action server"
        }, 
        {
            "location": "/strands_ui/#auto-generated-button-pages", 
            "text": "The  strands_webserver  package also provides functionality for automatically generating pages with buttons which can call services. Such a call might look like this      notice = 'Help me, I am  em stuck /em '\n    buttons = [('No', 'trigger_pleading'), ('Sure', 'party_time')]\n    service_prefix = '/caller_services'\n    content = strands_webserver.page_utils.generate_alert_button_page(notice, buttons, service_prefix)  \n    strands_webserver.client_utils.display_content(display_no, content)   In this  notice  is the used to generate a large banner notice and  buttons  maps between a button label (e.g. No) and the service call which will be triggered when the button is pressed, e.g. ( '/caller_services/trigger_pleading' ). This service is of type  std_srvs/Empty . Instead of  generate_alert_button_page  you can use  generate_button_page  which accepts arbitrary html instead of a banner notice.  The full example is as follows (also available as  strands_webserver/scripts/strands_webserver_demo.py ).  #! /usr/bin/env python\n\nimport sys\nimport os\nimport roslib\nimport rospy\n\nimport strands_webserver.page_utils\nimport strands_webserver.client_utils\nimport std_srvs.srv \n\n\ndef trigger_pleading(req):\n      print 'please, please help'\n\n\ndef party_time(req):\n      print 'woo, off I go baby'\n\n\nif __name__ == '__main__':\n    rospy.init_node( strands_webserver_demo )\n    # The display to publish on, defaulting to all displays\n    display_no = rospy.get_param( ~display , 0) \n\n    # display a start-up page\n    strands_webserver.client_utils.display_url(display_no, 'http://strands-project.eu')\n\n    # sleep for 5 seconds\n    rospy.sleep(5.)\n\n    # tell the webserver where it should look for web files to serve\n    http_root = os.path.join(roslib.packages.get_pkg_dir( strands_webserver ),  data )\n    strands_webserver.client_utils.set_http_root(http_root)\n\n    # start with a basic page pretending things are going normally\n    strands_webserver.client_utils.display_relative_page(display_no, 'example-page.html')\n\n    # sleep for 5 seconds\n    rospy.sleep(5.)\n\n    # now ask for help\n    name = 'Help me, I am  em stuck /em '\n    buttons = [('No', 'trigger_pleading'), ('Sure', 'party_time')]\n    service_prefix = '/caller_services'\n    content = strands_webserver.page_utils.generate_alert_button_page(name, buttons, service_prefix)    \n    strands_webserver.client_utils.display_content(display_no, content) \n    rospy.Service('/caller_services/trigger_pleading', std_srvs.srv.Empty, trigger_pleading) \n    rospy.Service('/caller_services/party_time', std_srvs.srv.Empty, party_time) \n    rospy.spin()", 
            "title": "Auto-generated button pages"
        }, 
        {
            "location": "/strands_ui/#includes", 
            "text": "Need to describe where the webserver fetches javascript and css from, and the standard includes available.", 
            "title": "Includes"
        }, 
        {
            "location": "/strands_ui/#marathon_touch_gui", 
            "text": "This package uses the  strands_webserver  to create an interface for the patroller during the marathon event. It's not very pretty, but it's a start. There is a main page (map, pause button) which is displayed using  strands_webserver.client_utils.display_relative_page  and two pages for recovery methods which are generated using  strands_webserver.page_utils.generate_alert_button_page . These are wrapped up in  marathon_touch_gui.client  for ease of use. They can be called as follows:      # Setup -- must be done before other marathon_touch_gui calls\n    marathon_touch_gui.client.init_marathon_gui()\n\n    # Show the main page of the GUI\n    marathon_touch_gui.client.display_main_page(displayNo)\n\n    rospy.sleep(2)\n\n    # All callback services should be under this prefix\n    service_prefix = '/patroller'\n\n    # Do something like this on bumper collision\n\n    # when the human gives the 'ok' then /patroller/bumper_recovered is called\n    # note that this service must be provided by some other node\n    on_completion = 'bumper_recovered'\n    marathon_touch_gui.client.bumper_stuck(displayNo, service_prefix, on_completion)\n\n    rospy.sleep(2)\n\n    # Do something like this on move_base failure\n\n    # when the human gives the 'ok' then /patroller/robot_moved is called\n    # note that this service must be provided by some other node\n    on_completion = 'robot_moved'\n    marathon_touch_gui.client.nav_fail(displayNo, service_prefix, on_completion)\n\n    rospy.sleep(2)\n\n    # Return to main page\n    marathon_touch_gui.client.display_main_page(displayNo)  The full example is in  marathon_touch_gui/scripts/demo.py", 
            "title": "marathon_touch_gui"
        }, 
        {
            "location": "/strands_ui/#running_1", 
            "text": "To run the marathon GUI, first launch  strands_webserver  plus rosbridge and the necessary additional publishers (with HOST_IP set to an external ip for your machine if you want this to be externally accessible, else leave blank for 127.0.0.1):  HOST_IP=10.0.11.158 roslaunch marathon_touch_gui marathon_gui_dependencies.launch  This will launch the webserver on localhost port 8090, so browse to  http://localhost:8090 .  Then you can call the  marathon_touch_gui  functions. To test you can cycle through them with   rosrun marathon_touch_gui demo.py", 
            "title": "Running"
        }, 
        {
            "location": "/strands_ui/#modal-dialog", 
            "text": "By publishing a  strands_webserver/ModalDlg  message on the  /strands_webserver/modal_dialog  topic a modal dialog can be triggered hovering above any window display. This is useful for system notifications. The  strands_webserver/ModalDlg  is defined as follows:  string title\nstring content\nbool show  The modal dialog can be closed by the user using a little close item in the top right corner, it can remotely be hidden, by setting  show  to false. Here is an example to display the dialog box:  rostopic pub /strands_webserver/modal_dialog strands_webserver/ModalDlg  title: 'Nice Title'\ncontent: ' b test /b   a href= https://github.com/strands-project/aaf_deployment AAF /a '\nshow: true   To hide it again, simply publish  rostopic pub /strands_webserver/modal_dialog strands_webserver/ModalDlg  title: ''\ncontent: ''\nshow: false   Both, title and content can contain valid HTML.", 
            "title": "Modal Dialog"
        }, 
        {
            "location": "/strands_ui/mary_tts/", 
            "text": "A simple interface for the \nMARY TTS system\n. Offers a service \n/ros_mary\n and an actionlib interface \n/speak\n that accepts a simple text argument and plays the audio using PulseAudio.\n\n\n\n\nlaunch it: \nroslaunch mary_tts ros_mary.launch\n\n\nmake the robot speak: \nrosservice call /ros_mary 'Welcome to the school of computer science. Please follow me!'\n\n\nin order to use the actionlib cleint you can run \nrosrun actionlib axclient.py /speak\n\n\nswitching voices:\n\nrosservice call /ros_mary/set_voice \"dfki-obadiah-hsmm\"\n\n\nswitching languages:\n\nrosservice call /ros_mary/set_locale \"en_US\"\n\n\n\n\nAvailable languages and voices:\n\n it\n * None\n\n te\n * None\n\n en_US\n * cmu-slt-hsmm (female)\n * dfki-obadiah-hsmm (male)\n * dfki-prudence-hsmm (female)\n * dfki-poppy-hsmm (female)\n * dfki-spike-hsmm (male)\n\n tr\n * None\n\n ru\n * None\n\n de\n * bits1-hsmm (female)\n * bits3-hsmm (male)\n * dfki-pavoque-neutral-hsmm (male)\n* sv\n * None\n\n\nInstalling new voices: Use \nstrands_ui/mary_tts/marytts-5.0/bin/marytts-component-installer.sh\n\n\nTrouble shooting\n\n\nIf you experience errors like this:\n\n\nTraceback (most recent call last):\n  File \n/opt/ros/hydro/lib/mary_tts/marybridge.py\n, line 363, in \nmodule\n\n    player.play(the_sound)\n  File \n/opt/ros/hydro/lib/mary_tts/marybridge.py\n, line 284, in play\n    pa.strerror(ctypes.byref(error))))\nException: Could not create pulse audio stream: 30873552!\n[WARN] [WallTime: 1416493711.323501] mary speach action failed; maybe took too long (more than 10 seconds), maybe pulse is broke.\n\n\n\n\nIt means that mary was started when it could not determine which pulse resource to use. This could have multiple reasons:\n\n mary was started remotely without logging in as the same user on the PC or robot. Only one user can access pulse on a PC. Who that user is decided by who is currently logged in. If no one is logged in then pulse is not running, therefore you have to log in to the PC before starting mary remotely.\n\n mary was started as a different user than the one that is logged in. If \nuser1\n is logged in and \nuser2\n logs in remotely starting mary, mary won't work because pulse is held by \nuser1\n.\n* If you are using \ntmux\n, as done by most of the STRANDS systems, not only the two points above apply but if you start \ntmux\n via ssh with activated X forwarding, mary will try to access the pulse resource on the remote machine. try to always start the tmux session on the robot or PC that is supposed to run mary as the user that is supposed to run mary and is currently logged in. If you want to start it remotely, make sure not to use X forwarding.\n\n\nIf MARY server is only binding to IP6, you can force it to bind to IP4 (from http://superuser.com/a/453329):\n\n\nexport _JAVA_OPTIONS=\n-Djava.net.preferIPv4Stack=true", 
            "title": "Mary tts"
        }, 
        {
            "location": "/strands_ui/mary_tts/#trouble-shooting", 
            "text": "If you experience errors like this:  Traceback (most recent call last):\n  File  /opt/ros/hydro/lib/mary_tts/marybridge.py , line 363, in  module \n    player.play(the_sound)\n  File  /opt/ros/hydro/lib/mary_tts/marybridge.py , line 284, in play\n    pa.strerror(ctypes.byref(error))))\nException: Could not create pulse audio stream: 30873552!\n[WARN] [WallTime: 1416493711.323501] mary speach action failed; maybe took too long (more than 10 seconds), maybe pulse is broke.  It means that mary was started when it could not determine which pulse resource to use. This could have multiple reasons:  mary was started remotely without logging in as the same user on the PC or robot. Only one user can access pulse on a PC. Who that user is decided by who is currently logged in. If no one is logged in then pulse is not running, therefore you have to log in to the PC before starting mary remotely.  mary was started as a different user than the one that is logged in. If  user1  is logged in and  user2  logs in remotely starting mary, mary won't work because pulse is held by  user1 .\n* If you are using  tmux , as done by most of the STRANDS systems, not only the two points above apply but if you start  tmux  via ssh with activated X forwarding, mary will try to access the pulse resource on the remote machine. try to always start the tmux session on the robot or PC that is supposed to run mary as the user that is supposed to run mary and is currently logged in. If you want to start it remotely, make sure not to use X forwarding.  If MARY server is only binding to IP6, you can force it to bind to IP4 (from http://superuser.com/a/453329):  export _JAVA_OPTIONS= -Djava.net.preferIPv4Stack=true", 
            "title": "Trouble shooting"
        }, 
        {
            "location": "/strands_ui/mongodb_media_server/", 
            "text": "Overview\n\n\nThis package provides a web server for serving media files (photos, music,\nvideos) to robot applications. The content is uploaded and managed through a \nweb interface, and is stored in the mongodb database using GridFS.\n\n\nRunning\n\n\nrosrun mongodb_media_server server.py\n\n\nEditing the available content\n\n\nGoto to \nhttp://localhost:8027\n\n\nThe 'File Manager' tab allows you to upload files of any type. The 'BLAH Sets' \ntabs allow you to edit the data groupings. For example, the 'Photo Sets' can\nbe though of a albums of images to be shown together, likewise with Music and \nVideo.\n\n\nAccessing the content\n\n\nEvery file is accessible through:\n\n\nhttp://localhost:8027/get_media/MEDIAKEY\n\n\nwhere MEDIAKEY is the id of the file, or\n\n\nhttp://localhost:8027/get_media_by_name/name\n\n\nwhere name is the filename of the media file.\n\n\nThe IDs of the files are retrievable\nvia the Python api. For example, to retrieve the items in a photo album called\n'AAF-Deploy-InfoTerm':\n\n\nfrom mongodb_media_server import MediaClient\n\nmc = MediaClient('localhost', 62345) # or rospy.get_param('db_host') etc\nfile_set = mc.get_set(\nPhoto/AAF-Deploy-InfoTerm\n)\nfor f in file_set:\n    print \nMedia name:\n, i[0]\n    print \nMedia type:\n, i[1]\n    print \nMedia key:\n, i[2]\n    print \nMedia URL: http://localhost:8027/get_media/\n+i[2]\n\n\n\n\nIn this way it is straight forward to include managed media in a web.py \napplication . \n\n\nAlternatively, the file can also be read in python rather than through the \nweb server interface.\n\n\nPlease see \nsrc/mongodb_media_server/media_client.py\n for the API.\n\n\nTo Do\n\n\nA more complete API for both the web interface (to provide file sets to webpages \nnot working on web.py), and for the python API (to provide file set creation etc.)", 
            "title": "Mongodb media server"
        }, 
        {
            "location": "/strands_ui/mongodb_media_server/#overview", 
            "text": "This package provides a web server for serving media files (photos, music,\nvideos) to robot applications. The content is uploaded and managed through a \nweb interface, and is stored in the mongodb database using GridFS.", 
            "title": "Overview"
        }, 
        {
            "location": "/strands_ui/mongodb_media_server/#running", 
            "text": "rosrun mongodb_media_server server.py", 
            "title": "Running"
        }, 
        {
            "location": "/strands_ui/mongodb_media_server/#editing-the-available-content", 
            "text": "Goto to  http://localhost:8027  The 'File Manager' tab allows you to upload files of any type. The 'BLAH Sets' \ntabs allow you to edit the data groupings. For example, the 'Photo Sets' can\nbe though of a albums of images to be shown together, likewise with Music and \nVideo.", 
            "title": "Editing the available content"
        }, 
        {
            "location": "/strands_ui/mongodb_media_server/#accessing-the-content", 
            "text": "Every file is accessible through:  http://localhost:8027/get_media/MEDIAKEY  where MEDIAKEY is the id of the file, or  http://localhost:8027/get_media_by_name/name  where name is the filename of the media file.  The IDs of the files are retrievable\nvia the Python api. For example, to retrieve the items in a photo album called\n'AAF-Deploy-InfoTerm':  from mongodb_media_server import MediaClient\n\nmc = MediaClient('localhost', 62345) # or rospy.get_param('db_host') etc\nfile_set = mc.get_set( Photo/AAF-Deploy-InfoTerm )\nfor f in file_set:\n    print  Media name: , i[0]\n    print  Media type: , i[1]\n    print  Media key: , i[2]\n    print  Media URL: http://localhost:8027/get_media/ +i[2]  In this way it is straight forward to include managed media in a web.py \napplication .   Alternatively, the file can also be read in python rather than through the \nweb server interface.  Please see  src/mongodb_media_server/media_client.py  for the API.", 
            "title": "Accessing the content"
        }, 
        {
            "location": "/strands_ui/mongodb_media_server/#to-do", 
            "text": "A more complete API for both the web interface (to provide file sets to webpages \nnot working on web.py), and for the python API (to provide file set creation etc.)", 
            "title": "To Do"
        }, 
        {
            "location": "/strands_ui/robot_talk/", 
            "text": "Robot talk\n\n\nA simple interface for managing short \ntext phrases\n a robot might use in a conversation via text and/or text-to-speech output. \nThe text phrases are grouped wrt topics and have associated weights. \nThis allows a user to sample a random text phrase for a given topic according to the weights. \nAll information are stored in MongoDB. \nThe commandline tool \nrtalk.py\n can be used for managing the contents of the MongoDB collection. \nThe RobotTalkProxy provides a Python API for interacting with the collection. \n\n\nCommandline interface \nrtalk.py\n\n\nPlease run the following command for help:\n\n\n$ rosrun robot_talk rtalk.py -h\nusage: rtalk.py [-h]\n                {add,remove,update,list,search,play,play_random,topics} ...\n\npositional arguments:\n  {add,remove,update,list,search,play,play_random,topics}\n                        sub-command -h|--help\n    add                 add -h|--help\n    remove              remove -h|--help\n    update              update -h|--help\n    list                list -h|--help\n    search              search -h|--help\n    play                play -h|--help\n    play_random         play_random -h|--help\n    topics              topics -h|--help\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\nRobotTalkProxy (Python API)\n\n\nPlease confer to the file \nrun_example.py\n for an example for using the API.\n\n\n$ rosrun robot_talk run_example.py \n\nAdding entry: ID: 1 - Topic: Greeting - Text: Hi! - Weight 1.0\nAdding entry: ID: 2 - Topic: Greeting - Text: Hello! - Weight 2.0\nAdding entry: ID: 3 - Topic: Greeting - Text: Hey! - Weight 0.5\n\nListing entries for topic: Greeting\n====  ========  ======  ========\n  ID  Topic     Text      Weight\n====  ========  ======  ========\n   1  Greeting  Hi!          1\n   2  Greeting  Hello!       2\n   3  Greeting  Hey!         0.5\n====  ========  ======  ========\nTotal number: 3\n\nEXAMPLE 1: Get random text for a given topic. Here 'Greeting'\nChosen ID:  1 - Probability: 0.285714285714\nText: Hi!\n\nChosen ID:  2 - Probability: 0.571428571429\nText: Hello!\n\nChosen ID:  1 - Probability: 0.285714285714\nText: Hi!\n\nChosen ID:  3 - Probability: 0.142857142857\nText: Hey!\n\nChosen ID:  1 - Probability: 0.285714285714\nText: Hi!\n\nChosen ID:  2 - Probability: 0.571428571429\nText: Hello!\n\nChosen ID:  1 - Probability: 0.285714285714\nText: Hi!\n\nChosen ID:  1 - Probability: 0.285714285714\nText: Hi!\n\nChosen ID:  2 - Probability: 0.571428571429\nText: Hello!\n\nChosen ID:  2 - Probability: 0.571428571429\nText: Hello!\n\nEXAMPLE 2: Playing random text (via marytts) for a given topic\nChosen ID:  2 - Probability: 0.571428571429\nNow playing: Hello!\n\nRemoving entry ID: 1\nRemoval successful.\nRemoving entry ID: 2\nRemoval successful.\nRemoving entry ID: 3\nRemoval successful.", 
            "title": "Robot talk"
        }, 
        {
            "location": "/strands_ui/robot_talk/#robot-talk", 
            "text": "A simple interface for managing short  text phrases  a robot might use in a conversation via text and/or text-to-speech output. \nThe text phrases are grouped wrt topics and have associated weights. \nThis allows a user to sample a random text phrase for a given topic according to the weights. \nAll information are stored in MongoDB. \nThe commandline tool  rtalk.py  can be used for managing the contents of the MongoDB collection. \nThe RobotTalkProxy provides a Python API for interacting with the collection.", 
            "title": "Robot talk"
        }, 
        {
            "location": "/strands_ui/robot_talk/#commandline-interface-rtalkpy", 
            "text": "Please run the following command for help:  $ rosrun robot_talk rtalk.py -h\nusage: rtalk.py [-h]\n                {add,remove,update,list,search,play,play_random,topics} ...\n\npositional arguments:\n  {add,remove,update,list,search,play,play_random,topics}\n                        sub-command -h|--help\n    add                 add -h|--help\n    remove              remove -h|--help\n    update              update -h|--help\n    list                list -h|--help\n    search              search -h|--help\n    play                play -h|--help\n    play_random         play_random -h|--help\n    topics              topics -h|--help\n\noptional arguments:\n  -h, --help            show this help message and exit", 
            "title": "Commandline interface rtalk.py"
        }, 
        {
            "location": "/strands_ui/robot_talk/#robottalkproxy-python-api", 
            "text": "Please confer to the file  run_example.py  for an example for using the API.  $ rosrun robot_talk run_example.py \n\nAdding entry: ID: 1 - Topic: Greeting - Text: Hi! - Weight 1.0\nAdding entry: ID: 2 - Topic: Greeting - Text: Hello! - Weight 2.0\nAdding entry: ID: 3 - Topic: Greeting - Text: Hey! - Weight 0.5\n\nListing entries for topic: Greeting\n====  ========  ======  ========\n  ID  Topic     Text      Weight\n====  ========  ======  ========\n   1  Greeting  Hi!          1\n   2  Greeting  Hello!       2\n   3  Greeting  Hey!         0.5\n====  ========  ======  ========\nTotal number: 3\n\nEXAMPLE 1: Get random text for a given topic. Here 'Greeting'\nChosen ID:  1 - Probability: 0.285714285714\nText: Hi!\n\nChosen ID:  2 - Probability: 0.571428571429\nText: Hello!\n\nChosen ID:  1 - Probability: 0.285714285714\nText: Hi!\n\nChosen ID:  3 - Probability: 0.142857142857\nText: Hey!\n\nChosen ID:  1 - Probability: 0.285714285714\nText: Hi!\n\nChosen ID:  2 - Probability: 0.571428571429\nText: Hello!\n\nChosen ID:  1 - Probability: 0.285714285714\nText: Hi!\n\nChosen ID:  1 - Probability: 0.285714285714\nText: Hi!\n\nChosen ID:  2 - Probability: 0.571428571429\nText: Hello!\n\nChosen ID:  2 - Probability: 0.571428571429\nText: Hello!\n\nEXAMPLE 2: Playing random text (via marytts) for a given topic\nChosen ID:  2 - Probability: 0.571428571429\nNow playing: Hello!\n\nRemoving entry ID: 1\nRemoval successful.\nRemoving entry ID: 2\nRemoval successful.\nRemoving entry ID: 3\nRemoval successful.", 
            "title": "RobotTalkProxy (Python API)"
        }, 
        {
            "location": "/strands_ui/strands_webserver/", 
            "text": "The STRANDS webserver package\n\n\nQuite a lot information is missing here...\n\n\nModal Dialog\n\n\nThe webserver offers and \nactionlib\n server to display a modal dialog. Here is an example goal definition, the action interface is \nstrands_webserver/modal_dlg\n:\n\n\ntitle: 'This is a test'\ntext: 'We can use full HTML in here, e.g. \nb\nBOLD\n/b\n'\nbuttons: ['OK', 'cancel']\nbuttons_class: ['btn-success']\n\n\n\n\n\nbuttons_class\n can be used to set \nBootstrap button classes\n\n\nThe full action is defined as \nModalDialogSrv.action\n:\n\n\n# the title of the dialog (can be full HTML)\nstring title\n# the main body of the dialog (HTML)\nstring text\n# the HTML definition for the individual buttons (usually just a text)\nstring[] buttons\n# option bootstrap button class, e.g. 'btn-default' or 'btn-success'\nstring[] buttons_class\n---\n# returns the HTML definition of the selected button\nstring button\n# returns the index of the selected button\nuint32 button_id\n---\nstring feedback\n\n\n\n\nThe dialog is displayed as a bootstrap modal dialog, overlaying any information currently displayed. Preempting the actionlib goal will hide the dialog, otherwise it is dismissed as soon as the user clicks on of the defined buttons and the actions returns success in this case. The action server returns \"feedback\" once the dialog has been rendered on the user's screen. The feedback is the full generated HTML definition of the dialog displayed (not very useful, but hey, at least you know when it is displayed).", 
            "title": "Strands webserver"
        }, 
        {
            "location": "/strands_ui/strands_webserver/#the-strands-webserver-package", 
            "text": "Quite a lot information is missing here...", 
            "title": "The STRANDS webserver package"
        }, 
        {
            "location": "/strands_ui/strands_webserver/#modal-dialog", 
            "text": "The webserver offers and  actionlib  server to display a modal dialog. Here is an example goal definition, the action interface is  strands_webserver/modal_dlg :  title: 'This is a test'\ntext: 'We can use full HTML in here, e.g.  b BOLD /b '\nbuttons: ['OK', 'cancel']\nbuttons_class: ['btn-success']  buttons_class  can be used to set  Bootstrap button classes  The full action is defined as  ModalDialogSrv.action :  # the title of the dialog (can be full HTML)\nstring title\n# the main body of the dialog (HTML)\nstring text\n# the HTML definition for the individual buttons (usually just a text)\nstring[] buttons\n# option bootstrap button class, e.g. 'btn-default' or 'btn-success'\nstring[] buttons_class\n---\n# returns the HTML definition of the selected button\nstring button\n# returns the index of the selected button\nuint32 button_id\n---\nstring feedback  The dialog is displayed as a bootstrap modal dialog, overlaying any information currently displayed. Preempting the actionlib goal will hide the dialog, otherwise it is dismissed as soon as the user clicks on of the defined buttons and the actions returns success in this case. The action server returns \"feedback\" once the dialog has been rendered on the user's screen. The feedback is the full generated HTML definition of the dialog displayed (not very useful, but hey, at least you know when it is displayed).", 
            "title": "Modal Dialog"
        }, 
        {
            "location": "/trajectory_behaviours/human_trajectory_classifier/", 
            "text": "Human Trajectory\n\n\nThis is a ROS package that classifies human poses from bayes_people_tracker_logging in mongodb into human movements or non-human movements. The classification is based on poses and velocity of poses which are applied to KNN classifier. \nThis package can only be run offline retrieving human poses from perception_people \n\n\nRun this package by typing \n\n\nrosrun human_movement_identifier classifier.py [train_ratio] [accuracy (1/0)]\n\n\n\n\nwhere \n[train_ratio]\n is the training data ratio between 0.0 and 1.0.\nSetting 0.9 makes 0.1 of all data will be used as test data.\n\n[accuracy]\n is the option to choose between knowing the accuracy of this\nclassifier (1) or just wanting to test data from test data.", 
            "title": "Human trajectory classifier"
        }, 
        {
            "location": "/trajectory_behaviours/human_trajectory_classifier/#human-trajectory", 
            "text": "This is a ROS package that classifies human poses from bayes_people_tracker_logging in mongodb into human movements or non-human movements. The classification is based on poses and velocity of poses which are applied to KNN classifier. \nThis package can only be run offline retrieving human poses from perception_people   Run this package by typing   rosrun human_movement_identifier classifier.py [train_ratio] [accuracy (1/0)]  where  [train_ratio]  is the training data ratio between 0.0 and 1.0.\nSetting 0.9 makes 0.1 of all data will be used as test data. [accuracy]  is the option to choose between knowing the accuracy of this\nclassifier (1) or just wanting to test data from test data.", 
            "title": "Human Trajectory"
        }, 
        {
            "location": "/trajectory_behaviours/relational_learner/docs/", 
            "text": "specify run options as a string of integers. i.e. \"123\"\n\n\nrun_options = {'1': \"Get Objects \n Trajectories\",\n               '2': \"Apply QSR Lib\",\n               '3': \"Generate Episodes\",\n               '4': \"Activity Graphs/Graphlets\",\n               '5': \"Generate Code Book and Histograms\",\n               '6': \"Learn unsupervised behaviours\"}\n\n\nPaul\nUniversity of Leeds", 
            "title": "Docs"
        }, 
        {
            "location": "/trajectory_behaviours/relational_learner/", 
            "text": "Relational Learner\n\n\nA ROS package that uses qualitative spatio-temporal relations to learn and classify human trajectories as statistically novel or not. \n\n\nPrerequisites\n\n\n\n\nroscore\n\n\nmongodb_store\n\n\nstrands_perception_people\n\n\nsoma_trajectory\n\n\nqsrlib\n\n\n\n\nGetting started\n\n\n\n\nRun mongodb datacentres:\n    \n$ roslaunch mongodb_store mongodb_store.launch db_path:= \npath\n\n    $ roslaunch mongodb_store mongodb_store.launch db_path:=/home/strands/mongodb_store/bham_trajectory_store/\n\n  where \npath\n specifies the path to your mongodb_store\n\n\n\n\nAlso run the mongodb robot pose logging tool:\n    \n$ rosrun mongodb_log mongodb_log.py /robot_pose\n\n\n\n\n\n\nMake sure people perception is running to publish detected trajectories:\n    \n$ roslaunch perception_people_launch people_tracker_robot.launch\n\n\n\n\n\n\nAlso run the online-trajectory stiching tool:\n    \n$ rosrun human_trajectory trajectory_publisher.py [publish_interval][online(1)/offline(0)]\n\n  see \nhere\n for more details.\n\n\n\n\n\n\nAlternatively to step 2 and 3, you can run \nsoma_trajectory\n and obtain test trajectories from mongodb store:\n    \n$ rosrun soma_trajectory trajectory_query_service.py\n\n\n\n\n\n\nMake sure the QSRLib Service is running:\n    \n$ rosrun qsr_lib qsrlib_ros_server.py\n\n\n\n\n\n\nRun the Episodes Service:\n    \n$ rosrun relational_learner episode_server.py\n\n\n\n\n\n\nRun the Novelty Service:\n\n\n$ rosrun relational_learner novelty_server.py\n\n\n\n\n\n\nRun the Episodes node: \n    \n$ rosrun relational_learner episodes_client.py\n\n\n\n\n\n\nRun Novelty node: \n    \n$ rosrun relational_learner novelty_client.py\n\n\n\n\n\n\nNote:\n\n\nThis package can be run offline by running \nsoma_trajectory\n in step 2 and 3 instead of \npeople_perception\n. In this case, step 7 becomes:\n    \n$ rosrun relational_learner episodes_client_OT.py\n\n\nwhich queries one region's trajectories from mongodb_store instead of subscribing to published trajectories.", 
            "title": "Home"
        }, 
        {
            "location": "/trajectory_behaviours/relational_learner/#relational-learner", 
            "text": "A ROS package that uses qualitative spatio-temporal relations to learn and classify human trajectories as statistically novel or not.", 
            "title": "Relational Learner"
        }, 
        {
            "location": "/trajectory_behaviours/relational_learner/#prerequisites", 
            "text": "roscore  mongodb_store  strands_perception_people  soma_trajectory  qsrlib", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/trajectory_behaviours/relational_learner/#getting-started", 
            "text": "Run mongodb datacentres:\n     $ roslaunch mongodb_store mongodb_store.launch db_path:=  path \n    $ roslaunch mongodb_store mongodb_store.launch db_path:=/home/strands/mongodb_store/bham_trajectory_store/ \n  where  path  specifies the path to your mongodb_store   Also run the mongodb robot pose logging tool:\n     $ rosrun mongodb_log mongodb_log.py /robot_pose    Make sure people perception is running to publish detected trajectories:\n     $ roslaunch perception_people_launch people_tracker_robot.launch    Also run the online-trajectory stiching tool:\n     $ rosrun human_trajectory trajectory_publisher.py [publish_interval][online(1)/offline(0)] \n  see  here  for more details.    Alternatively to step 2 and 3, you can run  soma_trajectory  and obtain test trajectories from mongodb store:\n     $ rosrun soma_trajectory trajectory_query_service.py    Make sure the QSRLib Service is running:\n     $ rosrun qsr_lib qsrlib_ros_server.py    Run the Episodes Service:\n     $ rosrun relational_learner episode_server.py    Run the Novelty Service:  $ rosrun relational_learner novelty_server.py    Run the Episodes node: \n     $ rosrun relational_learner episodes_client.py    Run Novelty node: \n     $ rosrun relational_learner novelty_client.py", 
            "title": "Getting started"
        }, 
        {
            "location": "/trajectory_behaviours/relational_learner/#note", 
            "text": "This package can be run offline by running  soma_trajectory  in step 2 and 3 instead of  people_perception . In this case, step 7 becomes:\n     $ rosrun relational_learner episodes_client_OT.py  which queries one region's trajectories from mongodb_store instead of subscribing to published trajectories.", 
            "title": "Note:"
        }, 
        {
            "location": "/v4r/ObjectRecognizer/", 
            "text": "Multi-modal RGB-D Object Instance Detector\n\n\nIn this tutorial you will learn how to detect objects in 2.5 RGB-D point clouds. The proposed multi-pipeline recognizer will detect 3D object models and estimate their 6DoF pose in the camera's field of view.\n\n\nObject Model Database\n\n\nThe model folder structure is structured as: \n\n\nobject-model-database  \n\u2502\n\u2514\u2500\u2500\u2500object-name-1\n\u2502   \u2502    [3D_model.pcd]\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 views\n\u2502        \u2502   cloud_000000.pcd\n\u2502        \u2502   cloud_00000x.pcd\n\u2502        \u2502   ...\n\u2502        \u2502   object_indices_000000.txt\n\u2502        \u2502   object_indices_00000x.txt\n\u2502        \u2502   ...\n\u2502        \u2502   pose_000000.txt\n\u2502        \u2502   pose_00000x.txt\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502        \u2502   ...\n\u2502   \n\u2514\u2500\u2500\u2500object-name-2\n\u2502   \u2502    [3D_model.pcd]\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 views\n\u2502        \u2502   cloud_000000.pcd\n\u2502        \u2502   cloud_00000x.pcd\n\u2502        \u2502   ...\n\u2502        \u2502   object_indices_000000.txt\n\u2502        \u2502   object_indices_00000x.txt\n\u2502        \u2502   ...\n\u2502        \u2502   pose_000000.txt\n\u2502        \u2502   pose_00000x.txt\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502        \u2502   ...\n\u2502        \u2502   ...\n\u2502 ...\n\n\n\n\n* this data / folder will be generated\n\n\nObjects are trained from several training views. Each training view is represented as an organized point cloud (\ncloud_xyz.pcd\n), the segmentation mask of the object (\nobject indices_xyz.txt\n) and a camera pose ( \npose_xyz.pcd\n) that aligns the point cloud into a common coordinate system when multiplied by the given 4x4 homogenous transform. The training views for each object are stored inside a \nview\nfolder that is part of the object model folder. The object model folder is named after the object and contains all information about the object, i.e. initially contains the \nviews\n and a complete 3D model ( \n3D_model.pcd\n ) of the object that is in the same coordinate system as the one in views. The complete model is used for hypotheses verification and visualization purposes. \n\n\nAt the first run of the recognizer (or whenever retraining is desired), the object model will be trained with the features of the selected pipeline. The training might take some time and extracted features and keypoints are stored in a subfolder called after its feature descriptor (e.g. sift keypoints for object-name-1 will be stored in \nmodel-database/object-name-1/sift/keypoints.pcd\n). If these files already exist, they will be loaded directly from disk and skip the training stage. Please note that the trained directory needs to be deleted whenever you update your object model (e.g. by adding/removing training views), or the argument \n--retrain\n set when calling the program.\n\n\nIf you have not obtained appropriate data yet, you can download example model files together with test clouds  by running\n\n\n./scripts/get_TUW.sh\n\n\n\n\nfrom your v4r root directory. The files (2.43GB) will be extracted into \ndata/TUW\n.\n\n\nUsage\n\n\nAssuming you built the ObjectRecognizer app, you can now run the recognizer. If you run it for the first time, it will automatically extract the descriptors of choice from your model data (\n-m\n). \nThe test directory can be specified by the argument \n-t\n. The program accepts either a specific file or a folder with test clouds. To use 2D features, these test clouds need to be organized. The \n-z\n argument defines the cut-off distance in meter for the object detection (note that the sensor noise increases with the distance to the camera). Objects further away than this value, will be neglected.\n\n\nComputed feature matches will be grouped into geometric consistent clusters from which a pose can be estimated using Singular Value Decomposition. The cluster size \n-c\n needs to be equal or greater than 3 with a higher number giving more reliable pose estimates (since in general there will be more keypoints extracted from high resolution images, also consider increasing this threshold to avoid false positives and long runtimes). \n\n\nParameters for the recognition pipelines and the hypotheses verification are loaded from the \ncfg/\nfolder by default. For a full list of available parameters, you can use \n-h\n. Example command:\n\n\n./build/bin/ObjectRecognizer -m data/TUW/TUW_models -t data/TUW/test_set -z 2.5 --do_sift true --do_shot false --do_esf false --do_alexnet false -c 5 -v\n\n\n\n\nRecognized results will be stored in a single text file, where each detected object is one line starting with name (same as folder name) of the found object model followed by the confidence (disabled at the moment and always set to -1), and the object pose as a 4x4 homogenous transformation matrix in row-major order aligning the object represented in the model coordinate system with the current camera view. Example output:\n\n\nobject_08 (-1.): 0.508105 -0.243221 0.826241 0.176167 -0.363111 -0.930372 -0.0505756 0.0303915 0.781012 -0.274319 -0.561043 1.0472 0 0 0 1 \nobject_10 (-1.): 0.509662 0.196173 0.837712 0.197087 0.388411 -0.921257 -0.0205712 -0.171647 0.767712 0.335861 -0.545726 1.07244 0 0 0 1 \nobject_29 (-1.): -0.544767 -0.148158 0.825396 0.0723312 -0.332103 0.941911 -0.0501179 0.0478761 -0.770024 -0.301419 -0.562326 0.906379 0 0 0 1 \nobject_34 (-1.): 0.22115 0.501125 0.83664 0.0674237 0.947448 -0.313743 -0.0625169 -0.245826 0.231161 0.806498 -0.544174 0.900966 0 0 0 1 \nobject_35 (-1.): 0.494968 0.0565292 0.86707 0.105458 0.160923 -0.986582 -0.0275425 -0.104025 0.85388 0.153165 -0.497424 0.954036 0 0 0 1 \nobject_35 (-1.): -0.196294 -0.374459 0.906228 0.1488 -0.787666 0.610659 0.0817152 -0.331075 -0.583996 -0.697765 -0.414817 1.01101 0 0 0 1 \n\n\n\n\nTo visualize results, add argument \n-v\n. This will visualize the input scene, the generated hypotheses and the verified ones (from bottom to top). \n\nFor further parameter information, call the program with \n-h\n or have a look at the doxygen documents.  \n\n\nReferences\n\n\n\n\nhttps://repo.acin.tuwien.ac.at/tmp/permanent/dataset_index.php\n\n\nThomas F\u00e4ulhammer, Michael Zillich, Johann Prankl, Markus Vincze, \"A Multi-Modal RGB-D Object Recognizer\", IAPR International Conf. on Pattern Recognition (ICPR), Cancun, Mexico, 2016", 
            "title": "ObjectRecognizer"
        }, 
        {
            "location": "/v4r/ObjectRecognizer/#multi-modal-rgb-d-object-instance-detector", 
            "text": "In this tutorial you will learn how to detect objects in 2.5 RGB-D point clouds. The proposed multi-pipeline recognizer will detect 3D object models and estimate their 6DoF pose in the camera's field of view.", 
            "title": "Multi-modal RGB-D Object Instance Detector"
        }, 
        {
            "location": "/v4r/ObjectRecognizer/#object-model-database", 
            "text": "The model folder structure is structured as:   object-model-database  \n\u2502\n\u2514\u2500\u2500\u2500object-name-1\n\u2502   \u2502    [3D_model.pcd]\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 views\n\u2502        \u2502   cloud_000000.pcd\n\u2502        \u2502   cloud_00000x.pcd\n\u2502        \u2502   ...\n\u2502        \u2502   object_indices_000000.txt\n\u2502        \u2502   object_indices_00000x.txt\n\u2502        \u2502   ...\n\u2502        \u2502   pose_000000.txt\n\u2502        \u2502   pose_00000x.txt\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502        \u2502   ...\n\u2502   \n\u2514\u2500\u2500\u2500object-name-2\n\u2502   \u2502    [3D_model.pcd]\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 views\n\u2502        \u2502   cloud_000000.pcd\n\u2502        \u2502   cloud_00000x.pcd\n\u2502        \u2502   ...\n\u2502        \u2502   object_indices_000000.txt\n\u2502        \u2502   object_indices_00000x.txt\n\u2502        \u2502   ...\n\u2502        \u2502   pose_000000.txt\n\u2502        \u2502   pose_00000x.txt\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502        \u2502   ...\n\u2502        \u2502   ...\n\u2502 ...  * this data / folder will be generated  Objects are trained from several training views. Each training view is represented as an organized point cloud ( cloud_xyz.pcd ), the segmentation mask of the object ( object indices_xyz.txt ) and a camera pose (  pose_xyz.pcd ) that aligns the point cloud into a common coordinate system when multiplied by the given 4x4 homogenous transform. The training views for each object are stored inside a  view folder that is part of the object model folder. The object model folder is named after the object and contains all information about the object, i.e. initially contains the  views  and a complete 3D model (  3D_model.pcd  ) of the object that is in the same coordinate system as the one in views. The complete model is used for hypotheses verification and visualization purposes.   At the first run of the recognizer (or whenever retraining is desired), the object model will be trained with the features of the selected pipeline. The training might take some time and extracted features and keypoints are stored in a subfolder called after its feature descriptor (e.g. sift keypoints for object-name-1 will be stored in  model-database/object-name-1/sift/keypoints.pcd ). If these files already exist, they will be loaded directly from disk and skip the training stage. Please note that the trained directory needs to be deleted whenever you update your object model (e.g. by adding/removing training views), or the argument  --retrain  set when calling the program.  If you have not obtained appropriate data yet, you can download example model files together with test clouds  by running  ./scripts/get_TUW.sh  from your v4r root directory. The files (2.43GB) will be extracted into  data/TUW .", 
            "title": "Object Model Database"
        }, 
        {
            "location": "/v4r/ObjectRecognizer/#usage", 
            "text": "Assuming you built the ObjectRecognizer app, you can now run the recognizer. If you run it for the first time, it will automatically extract the descriptors of choice from your model data ( -m ). \nThe test directory can be specified by the argument  -t . The program accepts either a specific file or a folder with test clouds. To use 2D features, these test clouds need to be organized. The  -z  argument defines the cut-off distance in meter for the object detection (note that the sensor noise increases with the distance to the camera). Objects further away than this value, will be neglected.  Computed feature matches will be grouped into geometric consistent clusters from which a pose can be estimated using Singular Value Decomposition. The cluster size  -c  needs to be equal or greater than 3 with a higher number giving more reliable pose estimates (since in general there will be more keypoints extracted from high resolution images, also consider increasing this threshold to avoid false positives and long runtimes).   Parameters for the recognition pipelines and the hypotheses verification are loaded from the  cfg/ folder by default. For a full list of available parameters, you can use  -h . Example command:  ./build/bin/ObjectRecognizer -m data/TUW/TUW_models -t data/TUW/test_set -z 2.5 --do_sift true --do_shot false --do_esf false --do_alexnet false -c 5 -v  Recognized results will be stored in a single text file, where each detected object is one line starting with name (same as folder name) of the found object model followed by the confidence (disabled at the moment and always set to -1), and the object pose as a 4x4 homogenous transformation matrix in row-major order aligning the object represented in the model coordinate system with the current camera view. Example output:  object_08 (-1.): 0.508105 -0.243221 0.826241 0.176167 -0.363111 -0.930372 -0.0505756 0.0303915 0.781012 -0.274319 -0.561043 1.0472 0 0 0 1 \nobject_10 (-1.): 0.509662 0.196173 0.837712 0.197087 0.388411 -0.921257 -0.0205712 -0.171647 0.767712 0.335861 -0.545726 1.07244 0 0 0 1 \nobject_29 (-1.): -0.544767 -0.148158 0.825396 0.0723312 -0.332103 0.941911 -0.0501179 0.0478761 -0.770024 -0.301419 -0.562326 0.906379 0 0 0 1 \nobject_34 (-1.): 0.22115 0.501125 0.83664 0.0674237 0.947448 -0.313743 -0.0625169 -0.245826 0.231161 0.806498 -0.544174 0.900966 0 0 0 1 \nobject_35 (-1.): 0.494968 0.0565292 0.86707 0.105458 0.160923 -0.986582 -0.0275425 -0.104025 0.85388 0.153165 -0.497424 0.954036 0 0 0 1 \nobject_35 (-1.): -0.196294 -0.374459 0.906228 0.1488 -0.787666 0.610659 0.0817152 -0.331075 -0.583996 -0.697765 -0.414817 1.01101 0 0 0 1   To visualize results, add argument  -v . This will visualize the input scene, the generated hypotheses and the verified ones (from bottom to top).  \nFor further parameter information, call the program with  -h  or have a look at the doxygen documents.", 
            "title": "Usage"
        }, 
        {
            "location": "/v4r/ObjectRecognizer/#references", 
            "text": "https://repo.acin.tuwien.ac.at/tmp/permanent/dataset_index.php  Thomas F\u00e4ulhammer, Michael Zillich, Johann Prankl, Markus Vincze, \"A Multi-Modal RGB-D Object Recognizer\", IAPR International Conf. on Pattern Recognition (ICPR), Cancun, Mexico, 2016", 
            "title": "References"
        }, 
        {
            "location": "/v4r/citation/", 
            "text": "If you use V4R in your work, please cite the appropriate moduls:\n\n\nRTM toolbox\n\n\nPrankl et al., RGB-D Object Modelling for Object Recognition and Tracking. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015  \n\n\n```\n @inproceedings{prankl2015rgb,\n\n  title={RGB-D object modelling for object recognition and tracking},\n\n  author={Prankl, Johann and Aldoma, Aitor and Svejda, Alexander and Vincze, Markus},\n\n  booktitle={Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on},\n\n  pages={96--103},\n\n  year={2015},\n\n  organization={IEEE}\n\n}\n\n\n\n## Object Instance Recognition  \n\nThomas F\u00e4ulhammer, Michael Zillich, Johann Prankl, Markus Vincze, \nA Multi-Modal RGB-D Object Recognizer\n, IAPR Internation Conference on Pattern Recognition (ICPR), 2016,\n\n\n\n\n\n@incproceedings{faeulhammer2016_ICPR,\n\n  title={A Multi-Modal RGB-D Object Recognizer},\n\n  author={F{\\\"a}ulhammer, Thomas and Zillich, Michael and Prankl, Johann and Vincze, Markus},\n\n  booktitle={Pattern Recognition (ICPR), International Conference on}, \n  year={2016},\n\n  organization={IAPR}\n}\n\n\n\n\n## Multi-View Object Instance Recognition\n\n* if parameter `transfer_feature_matches` is set to `true`\n\n F\u00e4ulhammer et al, \nTemporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered And Dynamic Environments\n, IEEE Int. Conf. on Robotics and Automation (ICRA), 2015\n\n ```\n @inproceedings{faeulhammer2015_featInt,  \n  title={Temporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered   And Dynamic Environments},  \n  author={F{\\\na}ulhammer, Thomas and Aldoma, Aitor and Zillich, Michael and Vincze, Markus},  \n  booktitle={Proc.\\ of the International Conference on Robotics and Automation (ICRA)},  \n  year={2015},  \n  organization={IEEE}  \n}\n\n\n\n\n\n\nelse (default)\n\n\n\n\nF\u00e4ulhammer et al., \"Multi-View Hypotheses Transfer for Enhanced Object Recognition in Clutter\", IAPR Conference on Machine Vision Applications (MVA), 2015\n\n\n```\n @inproceedings{faulhammer2015multi, \n\n  title={Multi-View Hypotheses Transfer for Enhanced Object Recognition in Clutter},\n\n  author={F{\\\"a}ulhammer, Thomas and Zillich, Michael and Vincze, Markus},\n\n  booktitle={IAPR Conference on Machine Vision Applications (MVA)},\n\n  year={2015}\n\n}\n\n\n\n\n## Incremental object learning\n\n F\u00e4ulhammer et al., \nAutonomous Learning of Object Models on a Mobile Robot\n, IEEE IEEE Robotics and Automation Letters, 2016\n\n ```\n @ARTICLE{faeulhammer_object_learning_ral,  \n     author={Thomas F\\\n{a}ulhammer and Rare\\cb{s} Ambru\\cb{s} and Chris Burbridge and Michael Zillich and John Folkesson and Nick Hawes and Patric Jensfelt and Markus Vincze},  \n     journal={IEEE Robotics and Automation Letters},  \n     title={Autonomous Learning of Object Models on a Mobile Robot},  \n     year={2016},  \n     volume={2},  \n     number={1},  \n     pages={26-33},  \n     keywords={ Autonomous Agents;RGB-D Perception;Object detection;segmentation;categorization;Visual Learning;Motion and Path Planning},  \n     doi={10.1109/LRA.2016.2522086},  \n     ISSN={2377-3766},  \n     month={1}  \n }\n ```\n\n## ESF Object Classification\n\nWalter Wohlkinger and Markus Vincze, \nEnsemble of Shape Functions for 3D Object Classification\n, IEEE International Conference on Robotics and Biomimetics (IEEE-ROBIO), 2011\n\n\n\n\n\n@inproceedings{wohlkinger2011ensemble,\n\n  title={Ensemble of shape functions for 3d object classification},\n\n  author={Wohlkinger, Walter and Vincze, Markus},\n\n  booktitle={Robotics and Biomimetics (ROBIO), 2011 IEEE International Conference on},\n\n  pages={2987--2992},\n\n  year={2011},\n\n  organization={IEEE}\n\n}\n```", 
            "title": "Citation"
        }, 
        {
            "location": "/v4r/citation/#rtm-toolbox", 
            "text": "Prankl et al., RGB-D Object Modelling for Object Recognition and Tracking. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015    ```\n @inproceedings{prankl2015rgb, \n  title={RGB-D object modelling for object recognition and tracking}, \n  author={Prankl, Johann and Aldoma, Aitor and Svejda, Alexander and Vincze, Markus}, \n  booktitle={Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on}, \n  pages={96--103}, \n  year={2015}, \n  organization={IEEE} \n}  \n## Object Instance Recognition  \n\nThomas F\u00e4ulhammer, Michael Zillich, Johann Prankl, Markus Vincze,  A Multi-Modal RGB-D Object Recognizer , IAPR Internation Conference on Pattern Recognition (ICPR), 2016,  @incproceedings{faeulhammer2016_ICPR, \n  title={A Multi-Modal RGB-D Object Recognizer}, \n  author={F{\\\"a}ulhammer, Thomas and Zillich, Michael and Prankl, Johann and Vincze, Markus}, \n  booktitle={Pattern Recognition (ICPR), International Conference on}, \n  year={2016}, \n  organization={IAPR}\n}  \n\n## Multi-View Object Instance Recognition\n\n* if parameter `transfer_feature_matches` is set to `true`\n\n F\u00e4ulhammer et al,  Temporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered And Dynamic Environments , IEEE Int. Conf. on Robotics and Automation (ICRA), 2015\n\n ```\n @inproceedings{faeulhammer2015_featInt,  \n  title={Temporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered   And Dynamic Environments},  \n  author={F{\\ a}ulhammer, Thomas and Aldoma, Aitor and Zillich, Michael and Vincze, Markus},  \n  booktitle={Proc.\\ of the International Conference on Robotics and Automation (ICRA)},  \n  year={2015},  \n  organization={IEEE}  \n}   else (default)   F\u00e4ulhammer et al., \"Multi-View Hypotheses Transfer for Enhanced Object Recognition in Clutter\", IAPR Conference on Machine Vision Applications (MVA), 2015  ```\n @inproceedings{faulhammer2015multi,  \n  title={Multi-View Hypotheses Transfer for Enhanced Object Recognition in Clutter}, \n  author={F{\\\"a}ulhammer, Thomas and Zillich, Michael and Vincze, Markus}, \n  booktitle={IAPR Conference on Machine Vision Applications (MVA)}, \n  year={2015} \n}  \n\n## Incremental object learning\n\n F\u00e4ulhammer et al.,  Autonomous Learning of Object Models on a Mobile Robot , IEEE IEEE Robotics and Automation Letters, 2016\n\n ```\n @ARTICLE{faeulhammer_object_learning_ral,  \n     author={Thomas F\\ {a}ulhammer and Rare\\cb{s} Ambru\\cb{s} and Chris Burbridge and Michael Zillich and John Folkesson and Nick Hawes and Patric Jensfelt and Markus Vincze},  \n     journal={IEEE Robotics and Automation Letters},  \n     title={Autonomous Learning of Object Models on a Mobile Robot},  \n     year={2016},  \n     volume={2},  \n     number={1},  \n     pages={26-33},  \n     keywords={ Autonomous Agents;RGB-D Perception;Object detection;segmentation;categorization;Visual Learning;Motion and Path Planning},  \n     doi={10.1109/LRA.2016.2522086},  \n     ISSN={2377-3766},  \n     month={1}  \n }\n ```\n\n## ESF Object Classification\n\nWalter Wohlkinger and Markus Vincze,  Ensemble of Shape Functions for 3D Object Classification , IEEE International Conference on Robotics and Biomimetics (IEEE-ROBIO), 2011  @inproceedings{wohlkinger2011ensemble, \n  title={Ensemble of shape functions for 3d object classification}, \n  author={Wohlkinger, Walter and Vincze, Markus}, \n  booktitle={Robotics and Biomimetics (ROBIO), 2011 IEEE International Conference on}, \n  pages={2987--2992}, \n  year={2011}, \n  organization={IEEE} \n}\n```", 
            "title": "RTM toolbox"
        }, 
        {
            "location": "/v4r/contributing/", 
            "text": "Contributing to V4R\n\n\nPlease take a moment to review this document in order to make the contribution\nprocess easy and effective for everyone involved.\n\n\nFollowing these guidelines helps to communicate that you respect the time of\nthe developers managing and developing this open source project. In return,\nthey should reciprocate that respect in addressing your issue or assessing\npatches and features.\n\n\nDependencies\n\n\nV4R is an open-source project with the goal to be easily installed on different platforms by providing released Debian packages for Ubuntu systems. To allow packaging the V4R library, all dependencies need to be defined in \npackage.xml\n. The required names for specific packages can be found \nhere\n or \nhere\n. Packages not included in this list need to be added as \n3rdparty libraries\n to V4R. Whenever possible, try to depend on packaged libraries. This especially applies to PCL and OpenCV. Currently this means contribute your code such that it is compatible to PCL 1.7.2 and OpenCV 2.4.9.\n\nAlso, even though V4R stands for Vision for Robotics, our library is independent of ROS. If you need a ROS component, put your core algorithms into this V4R library and create wrapper interfaces in the seperate \nv4r_ros_wrappers repository\n.\n\n\nDependency Installation (Ubuntu 14.04 \n Ubuntu 16.04)\n\n\nIf you are running a freshly installed Ubuntu 14.04 or Ubuntu 16.04, you can use 'setup.sh' to install all necessary dependencies. Under the hood './setup.sh' installs a phyton script which associates the dependencies listed in the package.xml file with corresponding debian packages and installs them right away.\n\n\nUbuntu 14.04, 'Trusty'\n\n\ncd /wherever_v4r_is_located/v4r\n./setup.sh\n\n\n\n\nUbuntu 16.04, 'Xenial'\n\n\ncd /wherever_v4r_is_located/v4r\n./setup.sh xenial kinetic\n\n\n\n\nAfter executing ./setup.sh within your v4r root directory you should be able to compile v4r on your machine with\n\n\ncd /wherever_v4r_is_located/v4r\nmkdir build \n cd build\ncmake ..\nmake -j8\n\n\n\n\nIf you don't want to use 'setup.sh' for any wired reason, click \nhere\n to jump to \n\"How to Build V4R without using './setup.sh'? (Ubuntu 14.04)\"\n.\n\n\nUsing the issue tracker\n\n\nThe issue tracker for \ninternal\n or \nSTRANDS\n are\nthe preferred channel for submitting \npull requests\n and\n\nbug reports\n, but please respect the following\nrestrictions:\n\n\n\n\nPlease \ndo not\n derail or troll issues. Keep the discussion on topic and\n  respect the opinions of others.\n\n\n\n\n\n\nPull requests\n\n\nPull requests let you tell others about changes you've pushed to a repository on GitHub. Once a pull request is sent, interested parties can review the set of changes, discuss potential modifications, and even push follow-up commits if necessary. Therefore, this is the preferred way of pushing your changes - \ndo not\n push your changes directly onto the master branch!\nAlso, keep your pull requests small and focussed on a specific issue/feature. Do not accumulate months of changes into a single pull request! Such a pull request can not be reviewed!\n\n\nGood pull requests - patches, improvements, new features - are a fantastic\nhelp. They should remain focused in scope and avoid containing unrelated\ncommits.\n\n\n\n\nChecklist\n\n\nPlease use the following checklist to make sure that your contribution is well\nprepared for merging into V4R:\n\n\n\n\n\n\nSource code adheres to the coding conventions described in \nV4R Style Guide\n.\n   But if you modify existing code, do not change/fix style in the lines that\n   are not related to your contribution.\n\n\n\n\n\n\nCommit history is tidy (no merge commits, commits are \nsquashed\n\n   into logical units).\n\n\n\n\n\n\nEach contributed file has a \nlicense\n text on top.\n\n\n\n\n\n\n\n\nBug reports\n\n\nA bug is a \ndemonstrable problem\n that is caused by the code in the repository.\nGood bug reports are extremely helpful - thank you!\n\n\nGuidelines for bug reports:\n\n\n\n\n\n\nCheck if the issue has been reported\n \n use GitHub issue search.\n\n\n\n\n\n\nCheck if the issue has been fixed\n \n try to reproduce it using the\n   latest \nmaster\n branch in the repository.\n\n\n\n\n\n\nIsolate the problem\n \n ideally create a reduced test\n   case.\n\n\n\n\n\n\nA good bug report shouldn't leave others needing to chase you up for more\ninformation. Please try to be as detailed as possible in your report. What is\nyour environment? What steps will reproduce the issue? What would you expect to\nbe the outcome? All these details will help people to fix any potential bugs.\nAfter the report of a bug, a responsible person will be selected and informed to solve the issue.\n\n\n\n\nLicense\n\n\nV4R is \nMIT licensed\n, and by submitting a patch, you agree to\nallow V4R to license your work under the terms of the MIT\nLicense. The corpus of the license should be inserted as a C++ comment on top\nof each \n.h\n and \n.cpp\n file:\n\n\n/******************************************************************************\n * Copyright (c) 2016 Firstname Lastname\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \nSoftware\n), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \nAS IS\n, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n ******************************************************************************/\n\n\n\n\nPlease note that if the academic institution or company you are affiliated with\ndoes not allow to give up the rights, you may insert an additional copyright\nline.\n\n\n\n\nStructure\n\n\nThe repostiory consists of several folders and files containing specific parts of the library. This section gives a short introduction to the most important ones.\n\n\n./3rdparty\n\n\nSee Dependencies.\n\n\n./apps\n\n\nBigger code examples and tools (=more than only one file) such as RTMT.\nApps depend on modules.\n\n\n./cmake\n\n\nSeveral cmake macros.\n\n\n./docs\n\n\nTutorials and further documentations.\n\n\n./modules\n\n\nContains all core components of the library and is organized in logical sub folders which are further called 'packages'.\nA package holds the source files which are located in './src'. \nThe corresponding header files are located in './include/v4r/package_name/'\n\n\nBy following this structure, new modules can be easily added to the corresponding CMakeLists.txt with\n\n\nv4r_define_module(package_name REQUIRED components)\n\n\n\n\ni.e. \n\n\nv4r_define_module(change_detection REQUIRED v4r_common pcl opencv)\n\n\n\n\n\n\n\n\n./modules/common \n anything that can be reused by other packages\n\n\n\n\n\n\n./modules/core \n core is used by every module and does only include macros -\n To make your modules visible to other modules you have to add the macros and 'V4R_EXPORTS' to your header files. \n\n\n\n\n\n\n#include \nv4r/core/macros.h\n\n...\nclass V4R_EXPORTS ...\n\n\n\n\nsamples\n\n\n./samples/exsamples: short code pieces that demonsrate how to use a module.\n\n./samples/tools: small tools with only one file\n\n\nOther Files\n\n\nCITATION.md\n \n This files includes bibTex encoded references.\nThey can be used to cite the appropriate modules if you use V4R in your work.\n\n\nCONTRIBUTING.md\n \n The file you read at the moment.\n\n\n\n\nDocumentation\n\n\nALWAYS document your code. We use Doxygen. A nice introduction to Doxygen can be found \nhere\n.\n\n\nThe Doxygen documentation has to be compiled localy on your system for the moment.\nHowever, it will be available \nonline\n on gitlab quiet soon.\nBajo will find a nice solution for that using the CI system.\n\n\nHow to Build V4R without using './setup.sh'? (Ubuntu 14.04)\n\n\nAs mentioned allready, V4R is using a package.xml file and rosdep to install all necessary dependencies. This can be easily done using ./setup.sh.\nHowever, if you don't want to use 'setup.sh' for any reason follow the instructions below.\n\n\n\n\nDo all the necessary git magic to download v4r on your machine, then open a console.\n\n\nInstall and initialize rosdep, cmake and build-essential\n\n\n\n\nsudo apt-get update\nsudo sh -c 'echo \ndeb http://packages.ros.org/ros/ubuntu trusty main\n \n /etc/apt/sources.list.d/ros-latest.list'\nwget http://packages.ros.org/ros.key -O - | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install -y python-rosdep build-essential cmake\nsudo rosdep init\nrosdep update\n\n\n\n\n\n\nInstall dependencies and compile v4r\n\n\n\n\ncd /wherever_v4r_is_located/v4r\nrosdep install --from-paths . -i -y -r --rosdistro indigo\nmkdir build \n cd build\ncmake ..\nmake -j8", 
            "title": "Contributing"
        }, 
        {
            "location": "/v4r/contributing/#contributing-to-v4r", 
            "text": "Please take a moment to review this document in order to make the contribution\nprocess easy and effective for everyone involved.  Following these guidelines helps to communicate that you respect the time of\nthe developers managing and developing this open source project. In return,\nthey should reciprocate that respect in addressing your issue or assessing\npatches and features.", 
            "title": "Contributing to V4R"
        }, 
        {
            "location": "/v4r/contributing/#dependencies", 
            "text": "V4R is an open-source project with the goal to be easily installed on different platforms by providing released Debian packages for Ubuntu systems. To allow packaging the V4R library, all dependencies need to be defined in  package.xml . The required names for specific packages can be found  here  or  here . Packages not included in this list need to be added as  3rdparty libraries  to V4R. Whenever possible, try to depend on packaged libraries. This especially applies to PCL and OpenCV. Currently this means contribute your code such that it is compatible to PCL 1.7.2 and OpenCV 2.4.9. \nAlso, even though V4R stands for Vision for Robotics, our library is independent of ROS. If you need a ROS component, put your core algorithms into this V4R library and create wrapper interfaces in the seperate  v4r_ros_wrappers repository .", 
            "title": "Dependencies"
        }, 
        {
            "location": "/v4r/contributing/#dependency-installation-ubuntu-1404-ubuntu-1604", 
            "text": "If you are running a freshly installed Ubuntu 14.04 or Ubuntu 16.04, you can use 'setup.sh' to install all necessary dependencies. Under the hood './setup.sh' installs a phyton script which associates the dependencies listed in the package.xml file with corresponding debian packages and installs them right away.  Ubuntu 14.04, 'Trusty'  cd /wherever_v4r_is_located/v4r\n./setup.sh  Ubuntu 16.04, 'Xenial'  cd /wherever_v4r_is_located/v4r\n./setup.sh xenial kinetic  After executing ./setup.sh within your v4r root directory you should be able to compile v4r on your machine with  cd /wherever_v4r_is_located/v4r\nmkdir build   cd build\ncmake ..\nmake -j8  If you don't want to use 'setup.sh' for any wired reason, click  here  to jump to  \"How to Build V4R without using './setup.sh'? (Ubuntu 14.04)\" .", 
            "title": "Dependency Installation (Ubuntu 14.04 &amp; Ubuntu 16.04)"
        }, 
        {
            "location": "/v4r/contributing/#using-the-issue-tracker", 
            "text": "The issue tracker for  internal  or  STRANDS  are\nthe preferred channel for submitting  pull requests  and bug reports , but please respect the following\nrestrictions:   Please  do not  derail or troll issues. Keep the discussion on topic and\n  respect the opinions of others.", 
            "title": "Using the issue tracker"
        }, 
        {
            "location": "/v4r/contributing/#pull-requests", 
            "text": "Pull requests let you tell others about changes you've pushed to a repository on GitHub. Once a pull request is sent, interested parties can review the set of changes, discuss potential modifications, and even push follow-up commits if necessary. Therefore, this is the preferred way of pushing your changes -  do not  push your changes directly onto the master branch!\nAlso, keep your pull requests small and focussed on a specific issue/feature. Do not accumulate months of changes into a single pull request! Such a pull request can not be reviewed!  Good pull requests - patches, improvements, new features - are a fantastic\nhelp. They should remain focused in scope and avoid containing unrelated\ncommits.", 
            "title": "Pull requests"
        }, 
        {
            "location": "/v4r/contributing/#checklist", 
            "text": "Please use the following checklist to make sure that your contribution is well\nprepared for merging into V4R:    Source code adheres to the coding conventions described in  V4R Style Guide .\n   But if you modify existing code, do not change/fix style in the lines that\n   are not related to your contribution.    Commit history is tidy (no merge commits, commits are  squashed \n   into logical units).    Each contributed file has a  license  text on top.", 
            "title": "Checklist"
        }, 
        {
            "location": "/v4r/contributing/#bug-reports", 
            "text": "A bug is a  demonstrable problem  that is caused by the code in the repository.\nGood bug reports are extremely helpful - thank you!  Guidelines for bug reports:    Check if the issue has been reported    use GitHub issue search.    Check if the issue has been fixed    try to reproduce it using the\n   latest  master  branch in the repository.    Isolate the problem    ideally create a reduced test\n   case.    A good bug report shouldn't leave others needing to chase you up for more\ninformation. Please try to be as detailed as possible in your report. What is\nyour environment? What steps will reproduce the issue? What would you expect to\nbe the outcome? All these details will help people to fix any potential bugs.\nAfter the report of a bug, a responsible person will be selected and informed to solve the issue.", 
            "title": "Bug reports"
        }, 
        {
            "location": "/v4r/contributing/#license", 
            "text": "V4R is  MIT licensed , and by submitting a patch, you agree to\nallow V4R to license your work under the terms of the MIT\nLicense. The corpus of the license should be inserted as a C++ comment on top\nof each  .h  and  .cpp  file:  /******************************************************************************\n * Copyright (c) 2016 Firstname Lastname\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the  Software ), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED  AS IS , WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n ******************************************************************************/  Please note that if the academic institution or company you are affiliated with\ndoes not allow to give up the rights, you may insert an additional copyright\nline.", 
            "title": "License"
        }, 
        {
            "location": "/v4r/contributing/#structure", 
            "text": "The repostiory consists of several folders and files containing specific parts of the library. This section gives a short introduction to the most important ones.", 
            "title": "Structure"
        }, 
        {
            "location": "/v4r/contributing/#3rdparty", 
            "text": "See Dependencies.", 
            "title": "./3rdparty"
        }, 
        {
            "location": "/v4r/contributing/#apps", 
            "text": "Bigger code examples and tools (=more than only one file) such as RTMT.\nApps depend on modules.", 
            "title": "./apps"
        }, 
        {
            "location": "/v4r/contributing/#cmake", 
            "text": "Several cmake macros.", 
            "title": "./cmake"
        }, 
        {
            "location": "/v4r/contributing/#docs", 
            "text": "Tutorials and further documentations.", 
            "title": "./docs"
        }, 
        {
            "location": "/v4r/contributing/#modules", 
            "text": "Contains all core components of the library and is organized in logical sub folders which are further called 'packages'.\nA package holds the source files which are located in './src'. \nThe corresponding header files are located in './include/v4r/package_name/'  By following this structure, new modules can be easily added to the corresponding CMakeLists.txt with  v4r_define_module(package_name REQUIRED components)  i.e.   v4r_define_module(change_detection REQUIRED v4r_common pcl opencv)    ./modules/common   anything that can be reused by other packages    ./modules/core   core is used by every module and does only include macros -  To make your modules visible to other modules you have to add the macros and 'V4R_EXPORTS' to your header files.     #include  v4r/core/macros.h \n...\nclass V4R_EXPORTS ...", 
            "title": "./modules"
        }, 
        {
            "location": "/v4r/contributing/#samples", 
            "text": "./samples/exsamples: short code pieces that demonsrate how to use a module. ./samples/tools: small tools with only one file", 
            "title": "samples"
        }, 
        {
            "location": "/v4r/contributing/#other-files", 
            "text": "CITATION.md    This files includes bibTex encoded references.\nThey can be used to cite the appropriate modules if you use V4R in your work.  CONTRIBUTING.md    The file you read at the moment.", 
            "title": "Other Files"
        }, 
        {
            "location": "/v4r/contributing/#documentation", 
            "text": "ALWAYS document your code. We use Doxygen. A nice introduction to Doxygen can be found  here .  The Doxygen documentation has to be compiled localy on your system for the moment.\nHowever, it will be available  online  on gitlab quiet soon.\nBajo will find a nice solution for that using the CI system.", 
            "title": "Documentation"
        }, 
        {
            "location": "/v4r/", 
            "text": "The library itself is independent of ROS, so it is built outside ROS catkin. There are wrappers for ROS (https://github.com/strands-project/v4r_ros_wrappers), which can then be placed inside the normal catkin workspace.\n\n\nDependencies:\n\n\nstated in \npackage.xml\n\nThere are two options to use the SIFT recognizer:\n - Use V4R third party library SIFT GPU (this requires a decent GPU - see www.cs.unc.edu/~ccwu/siftgpu) [default]\n - Use OpenCV non-free SIFT implementation (this requires the non-free module of OpenCV - can be installed from source). This option is enabled if BUILD_SIFTGPU is disabled in cmake.\n\n\nInstallation:\n\n\nIn order to use V4R in ROS, use the \nv4r_ros_wrappers\n.\n\n\nFrom Ubuntu Package\n\n\nsimply install \nsudo apt-get install ros-indigo-v4r\n after enabling the \nSTRANDS repositories\n.\n\n\nFrom Source\n\n\ncd ~/somewhere\ngit clone 'https://rgit.acin.tuwien.ac.at/root/v4r.git'\ncd v4r\n./setup.sh\nmkdir build\ncd build\ncmake ..\nmake\nsudo make install (optional)", 
            "title": "Home"
        }, 
        {
            "location": "/v4r/#dependencies", 
            "text": "stated in  package.xml \nThere are two options to use the SIFT recognizer:\n - Use V4R third party library SIFT GPU (this requires a decent GPU - see www.cs.unc.edu/~ccwu/siftgpu) [default]\n - Use OpenCV non-free SIFT implementation (this requires the non-free module of OpenCV - can be installed from source). This option is enabled if BUILD_SIFTGPU is disabled in cmake.", 
            "title": "Dependencies:"
        }, 
        {
            "location": "/v4r/#installation", 
            "text": "In order to use V4R in ROS, use the  v4r_ros_wrappers .", 
            "title": "Installation:"
        }, 
        {
            "location": "/v4r/#from-ubuntu-package", 
            "text": "simply install  sudo apt-get install ros-indigo-v4r  after enabling the  STRANDS repositories .", 
            "title": "From Ubuntu Package"
        }, 
        {
            "location": "/v4r/#from-source", 
            "text": "cd ~/somewhere\ngit clone 'https://rgit.acin.tuwien.ac.at/root/v4r.git'\ncd v4r\n./setup.sh\nmkdir build\ncd build\ncmake ..\nmake\nsudo make install (optional)", 
            "title": "From Source"
        }, 
        {
            "location": "/v4r/ml/", 
            "text": "Random Forest Training:\n\n\n// define labels to be trained\nstd::vector\n labels;    \n\n\n// for NYU depth dataset v2 this could be...\nlabels.push_back(21);   // wall\n\nlabels.push_back(11);   // floor\nlabels.push_back(3);    // cabinet\nlabels.push_back(5);    // chair\nlabels.push_back(19);   // table\nlabels.push_back(4);    // ceiling\n\n\n// load training data from files\nClassificationData trainingData;\ntrainingData.LoadFromDirectory(\"myTrainingDataDirectory\", labels);\n\n\n// define Random Forest\n//   parameters:\n//   int nTrees\n//   int maxDepth (-1... find depth automatically)\n//   float baggingRatio\n//   int nFeaturesToTryAtEachNode\n//   float minInformationGain\n//   int nMinNumberOfPointsToSplit\nForest rf(10, 20 , 0.1, 200, 0.02, 5);\n\n\n// train forest\n//   parameters:\n//   ClassificationData data\n//   bool refineWithAllDataAfterwards\n//   int verbosityLevel (0 - quiet, 3 - most detail)\nrf.TrainLarge(trainingData, false, 3);\n\n\n// save after training\nrf.SaveToFile(\"myforest\");\n\n\n// Hard classification, returning label ID:\nstd::vector\n featureVector;\n// assuming featureVector contains values...\nint ID = rf.ClassifyPoint(featureVector);\n\n\n// Soft classification, returning probabilities for each label\nstd::vector\n labelProbabilities = rf.SoftClassify(featureVector);\n\n\n\n\nDirectory structure of training data:\n\n\nmytrainingdirectory\n - 0001.data\n - 0002.data\n - 0003.data\n   ...\n - 1242.data\n - categories.txt\n\n\n-\n for every label (the example above has 1242 labels) there is a corresponding data file containing all feature vectors of this label.\n   such a file looks like this:\n\n\n2.46917e-05  0.000273396  0.000204452     0.823049     0.170685     0.988113     0.993125\n 3.20674e-05  0.000280648  0.000229576     0.829844     0.207543     0.987969     0.992765\n 3.73145e-05  0.000279801  0.000257583     0.831597     0.235013     0.987825       0.9925\n ...........  ...........  ...........  ...........  ...........  ...........  ........... \n\n\nEach row: One feature vector (here 7 dimensional)\n\n\nIMPORTANT: Values separated by spaces, vectors separated by newline.\n           COLUMNS MUST ALL HAVE THE SAME CONSTANT WIDTH!!! CODE IS ASSUMING THAT FOR SPEED UP!\n           (here width is 12)\n\n\nThe file categories.txt is necessary to define the label names and has a simple format like this:\n1   floor\n2   wall\n3   ceiling\n4   table\n5   chair\n6   furniture\n7   object", 
            "title": "Ml"
        }, 
        {
            "location": "/v4r/ml/#random-forest-training", 
            "text": "// define labels to be trained\nstd::vector  labels;      // for NYU depth dataset v2 this could be...\nlabels.push_back(21);   // wall \nlabels.push_back(11);   // floor\nlabels.push_back(3);    // cabinet\nlabels.push_back(5);    // chair\nlabels.push_back(19);   // table\nlabels.push_back(4);    // ceiling  // load training data from files\nClassificationData trainingData;\ntrainingData.LoadFromDirectory(\"myTrainingDataDirectory\", labels);  // define Random Forest\n//   parameters:\n//   int nTrees\n//   int maxDepth (-1... find depth automatically)\n//   float baggingRatio\n//   int nFeaturesToTryAtEachNode\n//   float minInformationGain\n//   int nMinNumberOfPointsToSplit\nForest rf(10, 20 , 0.1, 200, 0.02, 5);  // train forest\n//   parameters:\n//   ClassificationData data\n//   bool refineWithAllDataAfterwards\n//   int verbosityLevel (0 - quiet, 3 - most detail)\nrf.TrainLarge(trainingData, false, 3);  // save after training\nrf.SaveToFile(\"myforest\");  // Hard classification, returning label ID:\nstd::vector  featureVector;\n// assuming featureVector contains values...\nint ID = rf.ClassifyPoint(featureVector);  // Soft classification, returning probabilities for each label\nstd::vector  labelProbabilities = rf.SoftClassify(featureVector);   Directory structure of training data:  mytrainingdirectory\n - 0001.data\n - 0002.data\n - 0003.data\n   ...\n - 1242.data\n - categories.txt  -  for every label (the example above has 1242 labels) there is a corresponding data file containing all feature vectors of this label.\n   such a file looks like this:  2.46917e-05  0.000273396  0.000204452     0.823049     0.170685     0.988113     0.993125\n 3.20674e-05  0.000280648  0.000229576     0.829844     0.207543     0.987969     0.992765\n 3.73145e-05  0.000279801  0.000257583     0.831597     0.235013     0.987825       0.9925\n ...........  ...........  ...........  ...........  ...........  ...........  ...........   Each row: One feature vector (here 7 dimensional)  IMPORTANT: Values separated by spaces, vectors separated by newline.\n           COLUMNS MUST ALL HAVE THE SAME CONSTANT WIDTH!!! CODE IS ASSUMING THAT FOR SPEED UP!\n           (here width is 12)  The file categories.txt is necessary to define the label names and has a simple format like this:\n1   floor\n2   wall\n3   ceiling\n4   table\n5   chair\n6   furniture\n7   object", 
            "title": "Random Forest Training:"
        }, 
        {
            "location": "/v4r/opennurbs/", 
            "text": "/\n $Header: $ \n/\n/\n $NoKeywords: $ \n/\n\n\nMore Information:\n  Please see \n\n\nhttp://en.wiki.mcneel.com/default.aspx/McNeel/OpenNURBS.html\n\n\n\nfor information about opennurbs including supported compilers, build\n  instructions, and a description of the examples.\n\n\nLegal Stuff:\n\n\nThe openNURBS Initiative provides CAD, CAM, CAE, and computer\n  graphics software developers the tools to accurately transfer\n  3-D geometry between applications.\n\n\nThe tools provided by openNURBS include:\n\n\n* C++ source code libraries to read and write the file format.\n\n* Quality assurance and revision control.\n\n* Various supporting libraries and utilities.\n\n* Technical support.\n\n\n\nUnlike other open development initiatives, alliances, or\n  consortia:\n\n\n* Commercial use is encouraged.\n\n* The tools, support, and membership are free.\n\n* There are no restrictions. Neither copyright nor copyleft\n  restrictions apply.\n\n* No contribution of effort or technology is required from\n  the members, although it is encouraged.\n\n\n\nFor more information, please see \nhttp://www.openNURBS.org\n.\n\n\nThe openNURBS toolkit uses zlib for mesh and bitmap compression.\n  The zlib source code distributed with openNURBS is a subset of what\n  is available from zlib.  The zlib code itself has not been modified.\n  See ftp://ftp.freesoftware.com/pub/infozip/zlib/zlib.html for more \n  details.\n\n\nZlib has a generous license that is similar to the one for openNURBS.\n  The zlib license shown below was copied from the zlib web page\n  ftp://ftp.freesoftware.com/pub/infozip/zlib/zlib_license.html\n  on 20 March 2000.\n\n\n  zlib.h -- interface of the 'zlib' general purpose compression library\n  version 1.1.3, July 9th, 1998\n\n  Copyright (C) 1995-1998 Jean-loup Gailly and Mark Adler\n\n  This software is provided 'as-is', without any express or implied\n  warranty.  In no event will the authors be held liable for any damages\n  arising from the use of this software.\n\n  Permission is granted to anyone to use this software for any purpose,\n  including commercial applications, and to alter it and redistribute it\n  freely, subject to the following restrictions:\n\n  1. The origin of this software must not be misrepresented; you must not\n     claim that you wrote the original software. If you use this software\n     in a product, an acknowledgment in the product documentation would be\n     appreciated but is not required.\n  2. Altered source versions must be plainly marked as such, and must not be\n     misrepresented as being the original software.\n  3. This notice may not be removed or altered from any source distribution.\n\n  Jean-loup Gailly        Mark Adler\n  jloup@gzip.org          madler@alumni.caltech.edu\n\n\n  The data format used by the zlib library is described by RFCs (Request for\n  Comments) 1950 to 1952 in the files ftp://ds.internic.net/rfc/rfc1950.txt\n  (zlib format), rfc1951.txt (deflate format) and rfc1952.txt (gzip format).\n\n\n\nCopyright (c) 1993-2011 Robert McNeel \n Associates. All Rights Reserved.\n  Rhinoceros is a registered trademark of Robert McNeel \n Associates.\n\n\nTHIS SOFTWARE IS PROVIDED \"AS IS\" WITHOUT EXPRESS OR IMPLIED\n  WARRANTY.  ALL IMPLIED WARRANTIES OF FITNESS FOR ANY PARTICULAR\n  PURPOSE AND OF MERCHANTABILITY ARE HEREBY DISCLAIMED.", 
            "title": "Opennurbs"
        }, 
        {
            "location": "/v4r/docs/imkrecognizer/", 
            "text": "Keypoint based Object Recognition in Monocular Images\n\n\nIMKRecognizer is able to recognize objects in monocular images and to estimate the pose using a pnp-method. Object models, i.e. a sequence of RGBD-keyframes, the corresponding poses and the an object mask, can be created with RTMT, which supports OpenNI-Cameras (ASUS, Kinect).      \n\n\nUsage\n\n\nFor object recognition the model directory with the object-data stored in sub-directories need to be provided. For a RGBD-keyframe this can look like: \u201cdata/models/objectname1/views/frame_xxxx.pcd\u201d. \nIf the recognizer is started the first time the keyframes are loaded, interest points are detected, clustered and stored in a concatenated 'imk_objectname1_objectname2.bin' file. In case any recognition parameter is changed this file must be deleted and it will be created newly next time.\n\n\nbin/example-imkRecognizeObject-file --help\nbin/example-imkRecognizeObject-file -d data/models/ -n objectname1 objectname2 ... -f ~/testimages/image_%04d.jpg -s 0 -e 5 -t 0.5\n\n\n\n\nReferences\n\n\n\n\nJ. Prankl, T. M\u00f6rwald, M. Zillich, M. Vincze: \"Probabilistic Cue Integration for Real-time Object Pose Tracking\"; in: Proceedings of the 9th International Conference on Computer Vision Systems, (2013), 10 S.", 
            "title": "Imkrecognizer"
        }, 
        {
            "location": "/v4r/docs/imkrecognizer/#keypoint-based-object-recognition-in-monocular-images", 
            "text": "IMKRecognizer is able to recognize objects in monocular images and to estimate the pose using a pnp-method. Object models, i.e. a sequence of RGBD-keyframes, the corresponding poses and the an object mask, can be created with RTMT, which supports OpenNI-Cameras (ASUS, Kinect).", 
            "title": "Keypoint based Object Recognition in Monocular Images"
        }, 
        {
            "location": "/v4r/docs/imkrecognizer/#usage", 
            "text": "For object recognition the model directory with the object-data stored in sub-directories need to be provided. For a RGBD-keyframe this can look like: \u201cdata/models/objectname1/views/frame_xxxx.pcd\u201d. \nIf the recognizer is started the first time the keyframes are loaded, interest points are detected, clustered and stored in a concatenated 'imk_objectname1_objectname2.bin' file. In case any recognition parameter is changed this file must be deleted and it will be created newly next time.  bin/example-imkRecognizeObject-file --help\nbin/example-imkRecognizeObject-file -d data/models/ -n objectname1 objectname2 ... -f ~/testimages/image_%04d.jpg -s 0 -e 5 -t 0.5", 
            "title": "Usage"
        }, 
        {
            "location": "/v4r/docs/imkrecognizer/#references", 
            "text": "J. Prankl, T. M\u00f6rwald, M. Zillich, M. Vincze: \"Probabilistic Cue Integration for Real-time Object Pose Tracking\"; in: Proceedings of the 9th International Conference on Computer Vision Systems, (2013), 10 S.", 
            "title": "References"
        }, 
        {
            "location": "/v4r/docs/objectclassification/", 
            "text": "Object Classification by ESF\n\n\nIn this tutorial you will learn how to classify objects using the ESF descriptor. In our example, we will render views from mesh files and then classify point clouds by classifying extracted segments. We use the Cat10 models from 3dNet for training and the test clouds of 3dNet for testing.\n\n\nRendering training views from mesh files\n\n\nIn the first step, we will render training views ( point clouds \n*.pcd\n ) from mesh files ( \n*.ply\n). The example program \nexample_depth_map_renderer\n produces a set of training views (in the output folder given by argument \n-o\n) from a mesh input file (provided by argument \n-i\n). The training views are rendered by placing virtual cameras on a 3D sphere with radius \n-r\n around the mesh file; sampled on an icosahedron with subdivision \n-s\n. Camera parameters are provided by parameters \n--fx, --fy\n(focal length in x and y direction), \n--cx, --cy\n(central point of projection in x and y direction), and \n--width, --height\n(image width and height). To visualize, add argument \n-v\n, for help \n-h\n.\n\n\nNote that the program above renders single \n.ply\n files only. To render a bunch of \n.ply\n files, there is a tool called \ntool_create_classification_db_from_ply_files\n in the V4R library. \n\n\nIf you have not obtained data yet, you can download them using the script file. Go to the v4r root directory and run\n\n\n./scripts/get_3dNet_Cat10.sh\n\n\n\n\nThis will download and extract the Cat10 models (42MB).\n\n\nobject-model-database  \n\u2502\n\u2514\u2500\u2500\u2500class-name-1\n        |\n        \u2514\u2500\u2500\u2500 instance-name-1\n\u2502            \u2502\n\u2502            \u2514\u2500\u2500\u2500 views\n\u2502                 \u2502   cloud_000000.pcd\n\u2502                 \u2502    cloud_00000x.pcd\n\u2502                 \u2502   ...\n\u2502                 \u2502   object_indices_000000.txt\n\u2502                 \u2502   object_indices_00000x.txt\n\u2502                 \u2502   ...\n\u2502                 \u2502   pose_000000.txt\n\u2502                 \u2502   pose_00000x.txt\n\u2502                 \u2502   ...\n\u2502          \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502                 \u2502   ...\n\u2502                 \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502                 \u2502   ...\n        \u2514\u2500\u2500\u2500 instance-name-x\n\u2502            \u2502\n\u2502            \u2514\u2500\u2500\u2500 views\n\u2502                 \u2502   cloud_000000.pcd\n\u2502                 \u2502    cloud_00000x.pcd\n\u2502                 \u2502   ...\n\u2502                 \u2502   object_indices_000000.txt\n\u2502                 \u2502   object_indices_00000x.txt\n\u2502                 \u2502   ...\n\u2502                 \u2502   pose_000000.txt\n\u2502                 \u2502   pose_00000x.txt\n\u2502                 \u2502   ...\n\u2502          \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502                 \u2502   ...\n\u2502                 \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502                 \u2502   ...\n\u2502   \n\u2502\n\u2514\u2500\u2500\u2500class-name-2\n        |\n        \u2514\u2500\u2500\u2500 instance-name-1\n\u2502            \u2502\n\u2502            \u2514\u2500\u2500\u2500 views\n\u2502                 \u2502   cloud_000000.pcd\n\u2502                 \u2502    cloud_00000x.pcd\n\u2502                 \u2502   ...\n\u2502                 \u2502   object_indices_000000.txt\n\u2502                 \u2502   object_indices_00000x.txt\n\u2502                 \u2502   ...\n\u2502                 \u2502   pose_000000.txt\n\u2502                 \u2502   pose_00000x.txt\n\u2502                 \u2502   ...\n\u2502          \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502                 \u2502   ...\n\u2502                 \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502                 \u2502   ...\n        \u2514\u2500\u2500\u2500 instance-name-x\n\u2502            \u2502\n\u2502            \u2514\u2500\u2500\u2500 views\n\u2502                 \u2502   cloud_000000.pcd\n\u2502                 \u2502    cloud_00000x.pcd\n\u2502                 \u2502   ...\n\u2502                 \u2502   object_indices_000000.txt\n\u2502                 \u2502   object_indices_00000x.txt\n\u2502                 \u2502   ...\n\u2502                 \u2502   pose_000000.txt\n\u2502                 \u2502   pose_00000x.txt\n\u2502                 \u2502   ...\n\u2502          \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502                 \u2502   ...\n\u2502                 \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502                 \u2502   ...\n\u2502 ...\n\n\n\n\n* this data / folder will be generated\n\n\nTraining a classifier\n\n\nIf you also want some annotated example point clouds, you can obtain the test set from 3dNet (3.6GB) by running\n\n\n./scripts/get_3dNet_test_data.sh\n\n\n\n\nThe files will be extracted in the \ndata/3dNet\n directory.\n\n\nUsage\n\n\nAssuming you built the examples samples, you can now run the classifier. If you run it for the first time, it will automatically render views by placing a virtual camera on an artificial sphere around the mesh models in \n-i\n and store them in the directory given by the \n-m\n argument. These views are then used for training the classifier, in our case by extracting ESF descriptors. For testing, it will segment the point cloud given by the argument \n-t\n by your method of choice (default: searching for a dominant plane and running Euclidean clustering for the points above). Each segment is then described by ESF and matched by nearest neighbor search to one of your learned object classes. The results will be stored in a text file which has the same name as the input cloud, just replacing the suffix from \n.pcd\n to \n.anno_test\n in the output directory specified by \n-o\n.  \n\n\n./build/bin/example-esf_object_classifier -i data/3dNet/Cat10_ModelDatabase -m data/3dNet/Cat10_Views -t data/3dNet/Cat10_TestDatabase/pcd_binary/ -o /tmp/3dNet_ESF_results\n\n\n\n\nReferences\n\n\n\n\n\n\nhttps://repo.acin.tuwien.ac.at/tmp/permanent/3d-net.org/\n\n\n\n\n\n\nWalter Wohlkinger, Aitor Aldoma Buchaca, Radu Rusu, Markus Vincze. \"3DNet: Large-Scale Object Class Recognition from CAD Models\". In IEEE International Conference on Robotics and Automation (ICRA), 2012.", 
            "title": "Objectclassification"
        }, 
        {
            "location": "/v4r/docs/objectclassification/#object-classification-by-esf", 
            "text": "In this tutorial you will learn how to classify objects using the ESF descriptor. In our example, we will render views from mesh files and then classify point clouds by classifying extracted segments. We use the Cat10 models from 3dNet for training and the test clouds of 3dNet for testing.", 
            "title": "Object Classification by ESF"
        }, 
        {
            "location": "/v4r/docs/objectclassification/#rendering-training-views-from-mesh-files", 
            "text": "In the first step, we will render training views ( point clouds  *.pcd  ) from mesh files (  *.ply ). The example program  example_depth_map_renderer  produces a set of training views (in the output folder given by argument  -o ) from a mesh input file (provided by argument  -i ). The training views are rendered by placing virtual cameras on a 3D sphere with radius  -r  around the mesh file; sampled on an icosahedron with subdivision  -s . Camera parameters are provided by parameters  --fx, --fy (focal length in x and y direction),  --cx, --cy (central point of projection in x and y direction), and  --width, --height (image width and height). To visualize, add argument  -v , for help  -h .  Note that the program above renders single  .ply  files only. To render a bunch of  .ply  files, there is a tool called  tool_create_classification_db_from_ply_files  in the V4R library.   If you have not obtained data yet, you can download them using the script file. Go to the v4r root directory and run  ./scripts/get_3dNet_Cat10.sh  This will download and extract the Cat10 models (42MB).  object-model-database  \n\u2502\n\u2514\u2500\u2500\u2500class-name-1\n        |\n        \u2514\u2500\u2500\u2500 instance-name-1\n\u2502            \u2502\n\u2502            \u2514\u2500\u2500\u2500 views\n\u2502                 \u2502   cloud_000000.pcd\n\u2502                 \u2502    cloud_00000x.pcd\n\u2502                 \u2502   ...\n\u2502                 \u2502   object_indices_000000.txt\n\u2502                 \u2502   object_indices_00000x.txt\n\u2502                 \u2502   ...\n\u2502                 \u2502   pose_000000.txt\n\u2502                 \u2502   pose_00000x.txt\n\u2502                 \u2502   ...\n\u2502          \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502                 \u2502   ...\n\u2502                 \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502                 \u2502   ...\n        \u2514\u2500\u2500\u2500 instance-name-x\n\u2502            \u2502\n\u2502            \u2514\u2500\u2500\u2500 views\n\u2502                 \u2502   cloud_000000.pcd\n\u2502                 \u2502    cloud_00000x.pcd\n\u2502                 \u2502   ...\n\u2502                 \u2502   object_indices_000000.txt\n\u2502                 \u2502   object_indices_00000x.txt\n\u2502                 \u2502   ...\n\u2502                 \u2502   pose_000000.txt\n\u2502                 \u2502   pose_00000x.txt\n\u2502                 \u2502   ...\n\u2502          \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502                 \u2502   ...\n\u2502                 \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502                 \u2502   ...\n\u2502   \n\u2502\n\u2514\u2500\u2500\u2500class-name-2\n        |\n        \u2514\u2500\u2500\u2500 instance-name-1\n\u2502            \u2502\n\u2502            \u2514\u2500\u2500\u2500 views\n\u2502                 \u2502   cloud_000000.pcd\n\u2502                 \u2502    cloud_00000x.pcd\n\u2502                 \u2502   ...\n\u2502                 \u2502   object_indices_000000.txt\n\u2502                 \u2502   object_indices_00000x.txt\n\u2502                 \u2502   ...\n\u2502                 \u2502   pose_000000.txt\n\u2502                 \u2502   pose_00000x.txt\n\u2502                 \u2502   ...\n\u2502          \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502                 \u2502   ...\n\u2502                 \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502                 \u2502   ...\n        \u2514\u2500\u2500\u2500 instance-name-x\n\u2502            \u2502\n\u2502            \u2514\u2500\u2500\u2500 views\n\u2502                 \u2502   cloud_000000.pcd\n\u2502                 \u2502    cloud_00000x.pcd\n\u2502                 \u2502   ...\n\u2502                 \u2502   object_indices_000000.txt\n\u2502                 \u2502   object_indices_00000x.txt\n\u2502                 \u2502   ...\n\u2502                 \u2502   pose_000000.txt\n\u2502                 \u2502   pose_00000x.txt\n\u2502                 \u2502   ...\n\u2502          \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502                 \u2502   ...\n\u2502                 \u2502\n\u2502          \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502                 \u2502   ...\n\u2502 ...  * this data / folder will be generated", 
            "title": "Rendering training views from mesh files"
        }, 
        {
            "location": "/v4r/docs/objectclassification/#training-a-classifier", 
            "text": "If you also want some annotated example point clouds, you can obtain the test set from 3dNet (3.6GB) by running  ./scripts/get_3dNet_test_data.sh  The files will be extracted in the  data/3dNet  directory.", 
            "title": "Training a classifier"
        }, 
        {
            "location": "/v4r/docs/objectclassification/#usage", 
            "text": "Assuming you built the examples samples, you can now run the classifier. If you run it for the first time, it will automatically render views by placing a virtual camera on an artificial sphere around the mesh models in  -i  and store them in the directory given by the  -m  argument. These views are then used for training the classifier, in our case by extracting ESF descriptors. For testing, it will segment the point cloud given by the argument  -t  by your method of choice (default: searching for a dominant plane and running Euclidean clustering for the points above). Each segment is then described by ESF and matched by nearest neighbor search to one of your learned object classes. The results will be stored in a text file which has the same name as the input cloud, just replacing the suffix from  .pcd  to  .anno_test  in the output directory specified by  -o .    ./build/bin/example-esf_object_classifier -i data/3dNet/Cat10_ModelDatabase -m data/3dNet/Cat10_Views -t data/3dNet/Cat10_TestDatabase/pcd_binary/ -o /tmp/3dNet_ESF_results", 
            "title": "Usage"
        }, 
        {
            "location": "/v4r/docs/objectclassification/#references", 
            "text": "https://repo.acin.tuwien.ac.at/tmp/permanent/3d-net.org/    Walter Wohlkinger, Aitor Aldoma Buchaca, Radu Rusu, Markus Vincze. \"3DNet: Large-Scale Object Class Recognition from CAD Models\". In IEEE International Conference on Robotics and Automation (ICRA), 2012.", 
            "title": "References"
        }, 
        {
            "location": "/v4r/docs/objectdetection/", 
            "text": "Multi-modal RGB-D Object Instance Recognizer\n\n\nIn this tutorial you will learn how to detect objects in 2.5 RGB-D point clouds. The proposed multi-pipeline recognizer will detect 3D object models and estimate their 6DoF pose in the camera's field of view.\n\n\nObject Model Database\n\n\nThe model folder structure is structured as: \n\n\nobject-model-database  \n\u2502\n\u2514\u2500\u2500\u2500object-name-1\n\u2502   \u2502    [3D_model.pcd]\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 views\n\u2502        \u2502   cloud_000000.pcd\n\u2502        \u2502   cloud_00000x.pcd\n\u2502        \u2502   ...\n\u2502        \u2502   object_indices_000000.txt\n\u2502        \u2502   object_indices_00000x.txt\n\u2502        \u2502   ...\n\u2502        \u2502   pose_000000.txt\n\u2502        \u2502   pose_00000x.txt\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502        \u2502   ...\n\u2502   \n\u2514\u2500\u2500\u2500object-name-2\n\u2502   \u2502    [3D_model.pcd]\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 views\n\u2502        \u2502   cloud_000000.pcd\n\u2502        \u2502   cloud_00000x.pcd\n\u2502        \u2502   ...\n\u2502        \u2502   object_indices_000000.txt\n\u2502        \u2502   object_indices_00000x.txt\n\u2502        \u2502   ...\n\u2502        \u2502   pose_000000.txt\n\u2502        \u2502   pose_00000x.txt\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502        \u2502   ...\n\u2502        \u2502   ...\n\u2502 ...\n\n\n\n\n* this data / folder will be generated\n\n\nObjects are trained from several training views. Each training view is represented as an organized point cloud (\ncloud_xyz.pcd\n), the segmentation mask of the object (\nobject indices_xyz.txt\n) and a camera pose ( \npose_xyz.pcd\n) that aligns the point cloud into a common coordinate system when multiplied by the given 4x4 homogenous transform. The training views for each object are stored inside a \nview\nfolder that is part of the object model folder. The object model folder is named after the object and contains all information about the object, i.e. initially contains the \nviews\n and a complete 3D model ( \n3D_model.pcd\n ) of the object that is in the same coordinate system as the one in views. The complete model is used for hypotheses verification and visualization purposes. \n\n\nAt the first run of the recognizer (or whenever retraining is desired), the object model will be trained with the features of the selected pipeline. The training might take some time and extracted features and keypoints are stored in a subfolder called after its feature descriptor (e.g. sift keypoints for object-name-1 will be stored in \nmodel-database/object-name-1/sift/keypoints.pcd\n). If these files already exist, they will be loaded directly from disk and skip the training stage. Please note that the trained directory needs to be deleted whenever you update your object model (e.g. by adding/removing training views), or the argument \n--retrain\n set when calling the program.\n\n\nIf you have not obtained appropriate data yet, you can download example model files together with test clouds  by running\n\n\n./scripts/get_TUW.sh\n\n\n\n\nfrom your v4r root directory. The files (2.43GB) will be extracted into \ndata/TUW\n.\n\n\nUsage\n\n\nAssuming you built the ObjectRecognizer app, you can now run the recognizer. If you run it for the first time, it will automatically extract the descriptors of choice from your model data (\n-m\n). \nThe test directory can be specified by the argument \n-t\n. The program accepts either a specific file or a folder with test clouds. To use 2D features, these test clouds need to be organized. \n\n\nExample command:\n\n\n./build/bin/ObjectRecognizer -m data/TUW/TUW_models -t data/TUW/test_set -z 2.5 --or_do_sift true --or_do_shot false --or_remove_planes 1 -v\n\n\n\n\nParameters:\n\n\nParameters for the recognition pipelines and the hypotheses verification are loaded from the \ncfg/\nfolder by default but most of them can also be set from the command line. For a list of available command line parameters, you can use \n-h\n. Note that all parameters will be initialized by the \ncfg/*.xml\n files first, and then updated with the given command line arguments. Therefore, command line arguments will always overwrite the parameters from the XML file!\n\n\n\n Let us now describe some of the parameters:\n\n\nThe \n-z\n argument defines the \ncut-off distance\n in meter for the object detection (note that the sensor noise increases with the distance to the camera). Objects further away than this value, will be neglected.\n\n\nUse \n--or_do_sift true --or_do_shot false\n to enable or disable the local recognition pipelines of SIFT and SHOT, respectively.\n\n\nComputed feature matches will be grouped into geometric consistent clusters from which a pose can be estimated using Singular Value Decomposition. The \ncluster size\n \n--or_cg_thresh\n needs to be equal or greater than 3 with a higher number giving more reliable pose estimates (since in general there will be more keypoints extracted from high resolution images, also consider increasing this threshold to avoid false positives and long runtimes). \n\n\nIf the objects in your test configuration are always standing on a clearly visible surface plane (e.g. ground floor or table top), we recommend to remove points on a plane by setting the parameter \n--or_remove_planes 1\n.\n\n\nTo \nvisualize\n the results, use command line argument \n-v\n. \n\n\nTo \nvisualize intermediate hypotheses verification\n results, you can use \n--hv_vis_model_cues\n.\n\n\n./build/bin/ObjectRecognizer -m data/TUW/TUW_models -t data/TUW/test_set -z 2.5 --or_do_sift true --or_do_shot false --or_remove_planes 1 -v\n\n\n\n\nMultiview Recognizer\n\n\nIf you want to use multiview recognition, enable the entry\n\n\nuse_multiview_\n1\n/use_multiview_\n\n\n\n\n\nin the \ncfg/multipipeline_config.xml\n file. \n\n\nAdditionally, you can also enable \n\n\nuse_multiview_hv_\n1\n/use_multiview_hv_\n\n\nuse_multiview_with_kp_correspondence_transfer_\n1\n/use_multiview_with_kp_correspondence_transfer_\n\n\n\n\n\nIn most settings, we however recommend to leave \nuse_multiview_with_kp_correspondence_transfer_\n disabled to save computation time. \n\n\nThe object recognizer will then treat all \n*.pcd\n files within a folder as observations belonging to the same multi-view sequence. An additional requirement of the multi-view recognizer is that all observations need to be aligned into a common reference frame. Therefore, each \n*.pcd\nfile needs to be associated with a relative camera pose (coarse point cloud registration is not done inside the recognizer!!). We use the header fields \nsensor_origin\n and \nsensor_orientation\n within the point cloud structure to read this pose. The registration can so be checked by viewing all clouds with the PCL tool \npcd_viewer /path/to/my/mv_observations/*.pcd\n. \nPlease check if the registration of your clouds is correct befor using the multi-view recognizer!\n\n\nOutput\n\n\nRecognized results will be stored in a single text file, where each detected object is one line starting with name (same as folder name) of the found object model followed by the confidence (between 0 for poor object hypotheses and 1 for very high confidence -- value in brackets), and the object pose as a 4x4 homogenous transformation matrix in row-major order aligning the object represented in the model coordinate system with the current camera view. Example output:\n\n\nobject_08 (0.251965): 0.508105 -0.243221 0.826241 0.176167 -0.363111 -0.930372 -0.0505756 0.0303915 0.781012 -0.274319 -0.561043 1.0472 0 0 0 1 \nobject_10 (0.109282): 0.509662 0.196173 0.837712 0.197087 0.388411 -0.921257 -0.0205712 -0.171647 0.767712 0.335861 -0.545726 1.07244 0 0 0 1 \nobject_29 (0.616981): -0.544767 -0.148158 0.825396 0.0723312 -0.332103 0.941911 -0.0501179 0.0478761 -0.770024 -0.301419 -0.562326 0.906379 0 0 0 1 \nobject_34 (0.565967): 0.22115 0.501125 0.83664 0.0674237 0.947448 -0.313743 -0.0625169 -0.245826 0.231161 0.806498 -0.544174 0.900966 0 0 0 1 \nobject_35 (0.60515): 0.494968 0.0565292 0.86707 0.105458 0.160923 -0.986582 -0.0275425 -0.104025 0.85388 0.153165 -0.497424 0.954036 0 0 0 1 \nobject_35 (0.589806): -0.196294 -0.374459 0.906228 0.1488 -0.787666 0.610659 0.0817152 -0.331075 -0.583996 -0.697765 -0.414817 1.01101 0 0 0 1 \n\n\n\n\nTo visualize results, add argument \n-v\n. This will visualize the input scene, the generated hypotheses and the verified ones (from bottom to top). \n\nFor further parameter information, call the program with \n-h\n or have a look at the doxygen documents.  \n\n\nReferences\n\n\n\n\nhttps://repo.acin.tuwien.ac.at/tmp/permanent/dataset_index.php\n\n\nThomas F\u00e4ulhammer, Michael Zillich, Johann Prankl, Markus Vincze, \"A Multi-Modal RGB-D Object Recognizer\", IAPR International Conf. on Pattern Recognition (ICPR), Cancun, Mexico, 2016", 
            "title": "Objectdetection"
        }, 
        {
            "location": "/v4r/docs/objectdetection/#multi-modal-rgb-d-object-instance-recognizer", 
            "text": "In this tutorial you will learn how to detect objects in 2.5 RGB-D point clouds. The proposed multi-pipeline recognizer will detect 3D object models and estimate their 6DoF pose in the camera's field of view.", 
            "title": "Multi-modal RGB-D Object Instance Recognizer"
        }, 
        {
            "location": "/v4r/docs/objectdetection/#object-model-database", 
            "text": "The model folder structure is structured as:   object-model-database  \n\u2502\n\u2514\u2500\u2500\u2500object-name-1\n\u2502   \u2502    [3D_model.pcd]\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 views\n\u2502        \u2502   cloud_000000.pcd\n\u2502        \u2502   cloud_00000x.pcd\n\u2502        \u2502   ...\n\u2502        \u2502   object_indices_000000.txt\n\u2502        \u2502   object_indices_00000x.txt\n\u2502        \u2502   ...\n\u2502        \u2502   pose_000000.txt\n\u2502        \u2502   pose_00000x.txt\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502        \u2502   ...\n\u2502   \n\u2514\u2500\u2500\u2500object-name-2\n\u2502   \u2502    [3D_model.pcd]\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 views\n\u2502        \u2502   cloud_000000.pcd\n\u2502        \u2502   cloud_00000x.pcd\n\u2502        \u2502   ...\n\u2502        \u2502   object_indices_000000.txt\n\u2502        \u2502   object_indices_00000x.txt\n\u2502        \u2502   ...\n\u2502        \u2502   pose_000000.txt\n\u2502        \u2502   pose_00000x.txt\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-1]*\n\u2502        \u2502   ...\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 [trained-pipeline-x]*\n\u2502        \u2502   ...\n\u2502        \u2502   ...\n\u2502 ...  * this data / folder will be generated  Objects are trained from several training views. Each training view is represented as an organized point cloud ( cloud_xyz.pcd ), the segmentation mask of the object ( object indices_xyz.txt ) and a camera pose (  pose_xyz.pcd ) that aligns the point cloud into a common coordinate system when multiplied by the given 4x4 homogenous transform. The training views for each object are stored inside a  view folder that is part of the object model folder. The object model folder is named after the object and contains all information about the object, i.e. initially contains the  views  and a complete 3D model (  3D_model.pcd  ) of the object that is in the same coordinate system as the one in views. The complete model is used for hypotheses verification and visualization purposes.   At the first run of the recognizer (or whenever retraining is desired), the object model will be trained with the features of the selected pipeline. The training might take some time and extracted features and keypoints are stored in a subfolder called after its feature descriptor (e.g. sift keypoints for object-name-1 will be stored in  model-database/object-name-1/sift/keypoints.pcd ). If these files already exist, they will be loaded directly from disk and skip the training stage. Please note that the trained directory needs to be deleted whenever you update your object model (e.g. by adding/removing training views), or the argument  --retrain  set when calling the program.  If you have not obtained appropriate data yet, you can download example model files together with test clouds  by running  ./scripts/get_TUW.sh  from your v4r root directory. The files (2.43GB) will be extracted into  data/TUW .", 
            "title": "Object Model Database"
        }, 
        {
            "location": "/v4r/docs/objectdetection/#usage", 
            "text": "Assuming you built the ObjectRecognizer app, you can now run the recognizer. If you run it for the first time, it will automatically extract the descriptors of choice from your model data ( -m ). \nThe test directory can be specified by the argument  -t . The program accepts either a specific file or a folder with test clouds. To use 2D features, these test clouds need to be organized.   Example command:  ./build/bin/ObjectRecognizer -m data/TUW/TUW_models -t data/TUW/test_set -z 2.5 --or_do_sift true --or_do_shot false --or_remove_planes 1 -v", 
            "title": "Usage"
        }, 
        {
            "location": "/v4r/docs/objectdetection/#parameters", 
            "text": "Parameters for the recognition pipelines and the hypotheses verification are loaded from the  cfg/ folder by default but most of them can also be set from the command line. For a list of available command line parameters, you can use  -h . Note that all parameters will be initialized by the  cfg/*.xml  files first, and then updated with the given command line arguments. Therefore, command line arguments will always overwrite the parameters from the XML file!  \n Let us now describe some of the parameters:  The  -z  argument defines the  cut-off distance  in meter for the object detection (note that the sensor noise increases with the distance to the camera). Objects further away than this value, will be neglected.  Use  --or_do_sift true --or_do_shot false  to enable or disable the local recognition pipelines of SIFT and SHOT, respectively.  Computed feature matches will be grouped into geometric consistent clusters from which a pose can be estimated using Singular Value Decomposition. The  cluster size   --or_cg_thresh  needs to be equal or greater than 3 with a higher number giving more reliable pose estimates (since in general there will be more keypoints extracted from high resolution images, also consider increasing this threshold to avoid false positives and long runtimes).   If the objects in your test configuration are always standing on a clearly visible surface plane (e.g. ground floor or table top), we recommend to remove points on a plane by setting the parameter  --or_remove_planes 1 .  To  visualize  the results, use command line argument  -v .   To  visualize intermediate hypotheses verification  results, you can use  --hv_vis_model_cues .  ./build/bin/ObjectRecognizer -m data/TUW/TUW_models -t data/TUW/test_set -z 2.5 --or_do_sift true --or_do_shot false --or_remove_planes 1 -v", 
            "title": "Parameters:"
        }, 
        {
            "location": "/v4r/docs/objectdetection/#multiview-recognizer", 
            "text": "If you want to use multiview recognition, enable the entry  use_multiview_ 1 /use_multiview_   in the  cfg/multipipeline_config.xml  file.   Additionally, you can also enable   use_multiview_hv_ 1 /use_multiview_hv_  use_multiview_with_kp_correspondence_transfer_ 1 /use_multiview_with_kp_correspondence_transfer_   In most settings, we however recommend to leave  use_multiview_with_kp_correspondence_transfer_  disabled to save computation time.   The object recognizer will then treat all  *.pcd  files within a folder as observations belonging to the same multi-view sequence. An additional requirement of the multi-view recognizer is that all observations need to be aligned into a common reference frame. Therefore, each  *.pcd file needs to be associated with a relative camera pose (coarse point cloud registration is not done inside the recognizer!!). We use the header fields  sensor_origin  and  sensor_orientation  within the point cloud structure to read this pose. The registration can so be checked by viewing all clouds with the PCL tool  pcd_viewer /path/to/my/mv_observations/*.pcd . \nPlease check if the registration of your clouds is correct befor using the multi-view recognizer!", 
            "title": "Multiview Recognizer"
        }, 
        {
            "location": "/v4r/docs/objectdetection/#output", 
            "text": "Recognized results will be stored in a single text file, where each detected object is one line starting with name (same as folder name) of the found object model followed by the confidence (between 0 for poor object hypotheses and 1 for very high confidence -- value in brackets), and the object pose as a 4x4 homogenous transformation matrix in row-major order aligning the object represented in the model coordinate system with the current camera view. Example output:  object_08 (0.251965): 0.508105 -0.243221 0.826241 0.176167 -0.363111 -0.930372 -0.0505756 0.0303915 0.781012 -0.274319 -0.561043 1.0472 0 0 0 1 \nobject_10 (0.109282): 0.509662 0.196173 0.837712 0.197087 0.388411 -0.921257 -0.0205712 -0.171647 0.767712 0.335861 -0.545726 1.07244 0 0 0 1 \nobject_29 (0.616981): -0.544767 -0.148158 0.825396 0.0723312 -0.332103 0.941911 -0.0501179 0.0478761 -0.770024 -0.301419 -0.562326 0.906379 0 0 0 1 \nobject_34 (0.565967): 0.22115 0.501125 0.83664 0.0674237 0.947448 -0.313743 -0.0625169 -0.245826 0.231161 0.806498 -0.544174 0.900966 0 0 0 1 \nobject_35 (0.60515): 0.494968 0.0565292 0.86707 0.105458 0.160923 -0.986582 -0.0275425 -0.104025 0.85388 0.153165 -0.497424 0.954036 0 0 0 1 \nobject_35 (0.589806): -0.196294 -0.374459 0.906228 0.1488 -0.787666 0.610659 0.0817152 -0.331075 -0.583996 -0.697765 -0.414817 1.01101 0 0 0 1   To visualize results, add argument  -v . This will visualize the input scene, the generated hypotheses and the verified ones (from bottom to top).  \nFor further parameter information, call the program with  -h  or have a look at the doxygen documents.", 
            "title": "Output"
        }, 
        {
            "location": "/v4r/docs/objectdetection/#references", 
            "text": "https://repo.acin.tuwien.ac.at/tmp/permanent/dataset_index.php  Thomas F\u00e4ulhammer, Michael Zillich, Johann Prankl, Markus Vincze, \"A Multi-Modal RGB-D Object Recognizer\", IAPR International Conf. on Pattern Recognition (ICPR), Cancun, Mexico, 2016", 
            "title": "References"
        }, 
        {
            "location": "/v4r/docs/objectmodeling/", 
            "text": "RTMT Recognition Modeling \n Tracking Toolbox\n\n\nIn this tutorial you will learn how to model objects with an RGB-D camera. These 3D models can used for training an object recognizer or used for object tracking\n\n\nObject modelling\n\n\n\n\nPlace the object on a flat surface on a newspaper or something similar. This allows you to rotate the object without touching it. The texture on the newspaper also helps view registration. The pictures below were taken with the object on a turn table, which is the most convenient way of rotating the object. You can also model objects without a turntable by putting the object on the floor for instance and moving your camera. In this case just skip the step definining the ROI.\n\n\nStart the modelling tool: ~/somewhere/v4r/bin/RTMT\n\n\nPress \"Camera Start\": You should now see the camera image\n\n\n\n\nIf you use a turntable like shown in the image, select the region of interest by pressing \"ROI Set\" and clicking on the flat surface next to the object. If you model the object without a turntable, skip this step.\n\n\n\n\nPress Tracker Start: you now see the tracking quality bar top left\n\n\n\n\nRotate 360 degrees, the program will generate a number of keyframes.\n\n\nIMPORTANT:\n Do not touch the object itself while moving it.  Also, if you selected a ROI, try not to move your fingers above the turntable, otherwise it might be added to the object model.\n\n\n\n  \n\n\nPress \"Tracker Stop\"\n\n\nPress \"Camera Stop\"\n\n\nPress \"Pose Optimize\" (optional): bundle adjustment of all cameras (be patient!)\n\n\nPress \"Object Segment\": The object should already be segmented correctly thanks to the ROI set previously\n\n\n\n  You can check segmentation (\n \n buttons). Highlighted areas are segments that will be clustered to the object model. If some highlighted areas do not belong to the object (e.g. supporting plane), you can click on these wrong segmentations to undo. You can also click on dark areas to add these segments to the object model. These areas should automatically be projected into the other frames.\n\n\nPress \"Object ...Ok\"\n\n\n\n  \n\n\nPress \"Store for Recognizer\": saves the point clouds in a format for object recognition. You will be asked for an model name.\n\n  By default the program will store models in various subfolders of the folder \"./data\", which will be created if not present. This can be changed in the configuration options (see below).\n\n\nPress \"Store for Tracker\": save a different model suitable for tracking\n\n\nIf the 3D point cloud visualization is activated +/- can be used to increase/ decrease the size of dots\n\n\n\n\nThis is a convenient way to model objects with the STRANDS robots. Put the objects on something elevated (a trash can in this case) to bring it within a good distance to the robot's head camera.\n\n\n\n\nConfiguration options:\n\n\n\n\nSet data folder and model name:\n\n  (File -\n Preferences -\n Settings -\n Path and model name)\n\n\nConfigure number of keyfames to be selected using a camera rotation and a camera translation threshold:\n\n  (File -\n Preferences -\n Settings -\n Min. delta angle, Min. delta camera distance)\n\n\n\n\nTrouble shooting\n\n\n\n\nIf you press any of the buttons in the wrong order, just restart. Recovery is futile.\n\n\nIf you do not get an image, there is a problem with the OpenNI device driver.\nCheck the file \n/etc/openni/GlobalDefaults.ini\n, set \nUsbInterface=2\n (i.e. BULK).\n\n\nIf the plane supporting the object is not removed completely, try to increase the inlier distance for dominant plane segmentation in File -\n Preferences -\n Postprocessing.\n\n\n\n\nReferences\n\n\nWhen referencing this work, pleace cite:\n\n\n\n\n\n\nJ. Prankl, A. Aldoma Buchaca, A. Svejda, M. Vincze, RGB-D Object Modelling for Object Recognition and Tracking. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\n\n\n\n\n\n\nThomas F\u00e4ulhammer, Aitor Aldoma, Michael Zillich and Markus Vincze\nTemporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered And Dynamic Environments\nIEEE International Conference on Robotics and Automation (ICRA), Seattle, WA, USA, 2015.\n\n\n\n\n\n\nThomas F\u00e4ulhammer, Michael Zillich and Markus Vincze\nMulti-View Hypotheses Transfer for Enhanced Object Recognition in Clutter,\nIAPR International Conference on Machine Vision Applications (MVA), Tokyo, Japan, 2015.\n\n\n\n\n\n\nA. Aldoma Buchaca, F. Tombari, J. Prankl, A. Richtsfeld, L. di Stefano, M. Vincze, Multimodal Cue Integration through Hypotheses Verification for RGB-D Object Recognition and 6DOF Pose Estimation. IEEE International Conference on Robotics and Automation (ICRA), 2013.\n\n\n\n\n\n\nJ. Prankl, T. M\u00f6rwald, M. Zillich, M. Vincze, Probabilistic Cue Integration for Real-time Object Pose Tracking. Proc. International Conference on Computer Vision Systems (ICVS). 2013.\n\n\n\n\n\n\nFor further information check out \nthis site\n.", 
            "title": "Objectmodeling"
        }, 
        {
            "location": "/v4r/docs/objectmodeling/#rtmt-recognition-modeling-tracking-toolbox", 
            "text": "In this tutorial you will learn how to model objects with an RGB-D camera. These 3D models can used for training an object recognizer or used for object tracking", 
            "title": "RTMT Recognition Modeling &amp; Tracking Toolbox"
        }, 
        {
            "location": "/v4r/docs/objectmodeling/#object-modelling", 
            "text": "Place the object on a flat surface on a newspaper or something similar. This allows you to rotate the object without touching it. The texture on the newspaper also helps view registration. The pictures below were taken with the object on a turn table, which is the most convenient way of rotating the object. You can also model objects without a turntable by putting the object on the floor for instance and moving your camera. In this case just skip the step definining the ROI.  Start the modelling tool: ~/somewhere/v4r/bin/RTMT  Press \"Camera Start\": You should now see the camera image   If you use a turntable like shown in the image, select the region of interest by pressing \"ROI Set\" and clicking on the flat surface next to the object. If you model the object without a turntable, skip this step.   Press Tracker Start: you now see the tracking quality bar top left   Rotate 360 degrees, the program will generate a number of keyframes.  IMPORTANT:  Do not touch the object itself while moving it.  Also, if you selected a ROI, try not to move your fingers above the turntable, otherwise it might be added to the object model.  \n    Press \"Tracker Stop\"  Press \"Camera Stop\"  Press \"Pose Optimize\" (optional): bundle adjustment of all cameras (be patient!)  Press \"Object Segment\": The object should already be segmented correctly thanks to the ROI set previously  \n  You can check segmentation (    buttons). Highlighted areas are segments that will be clustered to the object model. If some highlighted areas do not belong to the object (e.g. supporting plane), you can click on these wrong segmentations to undo. You can also click on dark areas to add these segments to the object model. These areas should automatically be projected into the other frames.  Press \"Object ...Ok\"  \n    Press \"Store for Recognizer\": saves the point clouds in a format for object recognition. You will be asked for an model name. \n  By default the program will store models in various subfolders of the folder \"./data\", which will be created if not present. This can be changed in the configuration options (see below).  Press \"Store for Tracker\": save a different model suitable for tracking  If the 3D point cloud visualization is activated +/- can be used to increase/ decrease the size of dots   This is a convenient way to model objects with the STRANDS robots. Put the objects on something elevated (a trash can in this case) to bring it within a good distance to the robot's head camera.", 
            "title": "Object modelling"
        }, 
        {
            "location": "/v4r/docs/objectmodeling/#configuration-options", 
            "text": "Set data folder and model name: \n  (File -  Preferences -  Settings -  Path and model name)  Configure number of keyfames to be selected using a camera rotation and a camera translation threshold: \n  (File -  Preferences -  Settings -  Min. delta angle, Min. delta camera distance)", 
            "title": "Configuration options:"
        }, 
        {
            "location": "/v4r/docs/objectmodeling/#trouble-shooting", 
            "text": "If you press any of the buttons in the wrong order, just restart. Recovery is futile.  If you do not get an image, there is a problem with the OpenNI device driver.\nCheck the file  /etc/openni/GlobalDefaults.ini , set  UsbInterface=2  (i.e. BULK).  If the plane supporting the object is not removed completely, try to increase the inlier distance for dominant plane segmentation in File -  Preferences -  Postprocessing.", 
            "title": "Trouble shooting"
        }, 
        {
            "location": "/v4r/docs/objectmodeling/#references", 
            "text": "When referencing this work, pleace cite:    J. Prankl, A. Aldoma Buchaca, A. Svejda, M. Vincze, RGB-D Object Modelling for Object Recognition and Tracking. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.    Thomas F\u00e4ulhammer, Aitor Aldoma, Michael Zillich and Markus Vincze\nTemporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered And Dynamic Environments\nIEEE International Conference on Robotics and Automation (ICRA), Seattle, WA, USA, 2015.    Thomas F\u00e4ulhammer, Michael Zillich and Markus Vincze\nMulti-View Hypotheses Transfer for Enhanced Object Recognition in Clutter,\nIAPR International Conference on Machine Vision Applications (MVA), Tokyo, Japan, 2015.    A. Aldoma Buchaca, F. Tombari, J. Prankl, A. Richtsfeld, L. di Stefano, M. Vincze, Multimodal Cue Integration through Hypotheses Verification for RGB-D Object Recognition and 6DOF Pose Estimation. IEEE International Conference on Robotics and Automation (ICRA), 2013.    J. Prankl, T. M\u00f6rwald, M. Zillich, M. Vincze, Probabilistic Cue Integration for Real-time Object Pose Tracking. Proc. International Conference on Computer Vision Systems (ICVS). 2013.    For further information check out  this site .", 
            "title": "References"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/", 
            "text": "1. Naming\n\n\n1.1. Files\n\n\nAll files should be \nunder_scored\n.\n\n\n\n\nHeader files have the extension .h\n\n\nTemplated implementation files have the extension .hpp\n\n\nSource files have the extension .cpp\n\n\n\n\n1.2. Directories\n\n\nAll directories and subdirectories should be under_scored.\n\n\n\n\nHeader files should go under include/v4r/module_name/\n\n\nTemplated implementation files should go under include/v4r/module_name/impl/\n\n\nSource files should go under src/\n\n\n\n\n1.3. Includes\n\n\nAll include statement are made with \n, e.g.:\n\n\n#include \npcl/module_name/file_name.h\n\n#incluce \npcl/module_name/impl/file_name.hpp\n\n\n\n\n\nAlso keep the list of includes clean and orderly by first including system level, then external libraries, then v4r internal. \nI.e. as a general rule: the more general include files go first. Sort includes within a group alphabetically. \n\n\n1.4. Defines \n Macros \n Include guards\n\n\nMacros should all be \nALL_CAPITALS_AND_UNDERSCORED\n. \nTo avoid the problem of double inclusion of header files, guard each header file with a \n#pragma once\n statement placed just past the MIT license.\n\n\n// the license\n#pragma once\n// the code\n\n\n\n\nYou might still encounter include guards in existing files which names are mapped from their include name, e.g.: v4r/filters/bilateral.h becomes V4R_FILTERS_BILATERAL_H_.\n\n\n// the license\n\n#ifndef V4R_MODULE_NAME_FILE_NAME_H_\n#define V4R_MODULE_NAME_FILE_NAME_H_\n\n// the code\n\n#endif\n\n\n\n\n1.5. Namespaces\n\n\nPut all your code into the namespace \nv4r\n and avoid using sub-namespaces whenever possible.\n\n\nnamespace v4r\n{\n    ...\n}\n\n\n\n\n1.6. Classes / Structs\n\n\nClass names (and other type names) should be \nCamelCased\n . Exception: if the class name contains a short acronym, the acronym itself should be all capitals. Class and struct names are preferably nouns: \nPFHEstimation\n instead of \nEstimatePFH\n.\n\n\nCorrect examples:\n\n\nclass ExampleClass;\nclass PFHEstimation;\n\n\n\n\n1.7. Functions / Methods\n\n\nFunctions and class method names should be \ncamelCased\n, and arguments are \nunder_scored\n. Function and method names are preferably verbs, and the name should make clear what it does: \ncheckForErrors()\n instead of \nerrorCheck()\n, \ndumpDataToFile()\n instead of \ndataFile()\n.\n\n\nCorrect usage:\n\n\nint\napplyExample (int example_arg);\n\n\n\n\n1.8. Variables\n\n\nVariable names should be \nunder_scored\n.\n\n\nint my_variable;\n\n\n\n\nGive meaningful names to all your variables.\n\n\n1.8.1. Constants\n\n\nConstants should be \nALL_CAPITALS\n, e.g.:\n\n\nconst static int MY_CONSTANT = 1000;\n\n\n\n\n1.8.2. Member variables\n\n\nVariables that are members of a class are \nunder_scored_\n, with a trailing underscore added, e.g.:\n\n\nint example_int_;\n\n\n\n\n2. Indentation and Formatting\n\n\nV4R uses a variant of the Qt style formatting. The standard indentation for each block is 4 spaces. If possible, apply this measure for your tabs and other spacings.\n\n\n2.1. Namespaces\n\n\nIn a header file, the contents of a namespace should be indented, e.g.:\n\n\nnamespace v4r\n{\n    class Foo\n    {\n        ...\n    };\n}\n\n\n\n\n2.2. Classes\n\n\nThe template parameters of a class should be declared on a different line, e.g.:\n\n\ntemplate \ntypename T\n\nclass Foo\n{\n    ...\n}\n\n\n\n\n2.3. Functions / Methods\n\n\nThe return type of each function declaration must be placed on a different line, e.g.:\n\n\nvoid\nbar ();\n\n\n\n\nSame for the implementation/definition, e.g.:\n\n\nvoid\nbar ()\n{\n  ...\n}\n\n\n\n\nor\n\n\nvoid\nFoo::bar ()\n{\n  ...\n}\n\n\n\n\nor\n\n\ntemplate \ntypename T\n void\nFoo\nT\n::bar ()\n{\n  ...\n}\n\n\n\n\n2.4. Braces\n\n\nBraces, both open and close, go on their own lines, e.g.:\n\n\nif (a \n b)\n{\n    ...\n}\nelse\n{\n    ...\n}\n\n\n\n\nBraces can be omitted if the enclosed block is a single-line statement, e.g.:\n\n\nif (a \n b)\n    x = 2 * a;\n\n\n\n\n3. Structuring\n\n\n3.1. Classes and API\n\n\nFor most classes in V4R, it is preferred that the interface (all public members) does not contain variables and only two types of methods:\n\n\n\n\nThe first method type is the get/set type that allows to manipulate the parameters and input data used by the class.\n\n\nThe second type of methods is actually performing the class functionality and produces output, e.g. compute, filter, segment.\n\n\n\n\n3.2. Passing arguments\n\n\nFor getter/setter methods the following rules apply:\n\n\n\n\nIf large amounts of data needs to be set it is preferred to pass either by a \nconst\n reference or by a \nconst\n boost shared_pointer instead of the actual data.\n\n\nGetters always need to pass exactly the same types as their respective setters and vice versa.\n\n\nFor getters, if only one argument needs to be passed this will be done via the return keyword. If two or more arguments need to be passed they will all be passed by reference instead.\n\n\n\n\nFor the compute, filter, segment, etc. type methods the following rules apply:\n\n\n\n\nThe output arguments are preferably non-pointer type, regardless of data size.\n\n\nThe output arguments will always be passed by reference.\n\n\n\n\n3.3 Use const\n\n\nTo allow clients of your class to immediately see which variable can be altered and which are used for read only access, define input arguments \nconst\n. The same applies for member functions which do not change member variables.\n\n\n3.3 Do not clutter header files\n\n\nTo reduce compile time amongst others, put your definitions into seperate .cpp or .hpp files. Define functions inline in the header only when they are small, say, 10 lines or fewer.\n\n\n3.4 Fix warnings\n\n\nTo keep the compiler output non-verbose and reduce potential conflicts, avoid warnings produced by the compiler. If you encounter warnings from other parts in the library, either try to fix them directly or report them using the issue tracker.\n\n\n3.5 Check input range and handle exceptions\n\n\nTo avoid confusing runtime errors and undefined behaviour, check the input to your interfaces for potential conflicts and provide meaningful error messages. If possible, try to catch these exceptions and return in a well-defined state.\n\n\n3.6 Document\n\n\nV4R uses \nDoxygen\n for documentation of classes and interfaces. Any code in V4R must be documented using Doxygen format.\n\n\n3.7 Template your classes/functions whenever appropriate\n\n\nAs many classes and functions in V4R depend on e.g. templated point clouds, allow your classes to accommodate with the various types by using template classes and functions.", 
            "title": "V4r style guide"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#1-naming", 
            "text": "", 
            "title": "1. Naming"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#11-files", 
            "text": "All files should be  under_scored .   Header files have the extension .h  Templated implementation files have the extension .hpp  Source files have the extension .cpp", 
            "title": "1.1. Files"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#12-directories", 
            "text": "All directories and subdirectories should be under_scored.   Header files should go under include/v4r/module_name/  Templated implementation files should go under include/v4r/module_name/impl/  Source files should go under src/", 
            "title": "1.2. Directories"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#13-includes", 
            "text": "All include statement are made with  , e.g.:  #include  pcl/module_name/file_name.h \n#incluce  pcl/module_name/impl/file_name.hpp   Also keep the list of includes clean and orderly by first including system level, then external libraries, then v4r internal. \nI.e. as a general rule: the more general include files go first. Sort includes within a group alphabetically.", 
            "title": "1.3. Includes"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#14-defines-macros-include-guards", 
            "text": "Macros should all be  ALL_CAPITALS_AND_UNDERSCORED . \nTo avoid the problem of double inclusion of header files, guard each header file with a  #pragma once  statement placed just past the MIT license.  // the license\n#pragma once\n// the code  You might still encounter include guards in existing files which names are mapped from their include name, e.g.: v4r/filters/bilateral.h becomes V4R_FILTERS_BILATERAL_H_.  // the license\n\n#ifndef V4R_MODULE_NAME_FILE_NAME_H_\n#define V4R_MODULE_NAME_FILE_NAME_H_\n\n// the code\n\n#endif", 
            "title": "1.4. Defines &amp; Macros &amp; Include guards"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#15-namespaces", 
            "text": "Put all your code into the namespace  v4r  and avoid using sub-namespaces whenever possible.  namespace v4r\n{\n    ...\n}", 
            "title": "1.5. Namespaces"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#16-classes-structs", 
            "text": "Class names (and other type names) should be  CamelCased  . Exception: if the class name contains a short acronym, the acronym itself should be all capitals. Class and struct names are preferably nouns:  PFHEstimation  instead of  EstimatePFH .  Correct examples:  class ExampleClass;\nclass PFHEstimation;", 
            "title": "1.6. Classes / Structs"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#17-functions-methods", 
            "text": "Functions and class method names should be  camelCased , and arguments are  under_scored . Function and method names are preferably verbs, and the name should make clear what it does:  checkForErrors()  instead of  errorCheck() ,  dumpDataToFile()  instead of  dataFile() .  Correct usage:  int\napplyExample (int example_arg);", 
            "title": "1.7. Functions / Methods"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#18-variables", 
            "text": "Variable names should be  under_scored .  int my_variable;  Give meaningful names to all your variables.", 
            "title": "1.8. Variables"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#181-constants", 
            "text": "Constants should be  ALL_CAPITALS , e.g.:  const static int MY_CONSTANT = 1000;", 
            "title": "1.8.1. Constants"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#182-member-variables", 
            "text": "Variables that are members of a class are  under_scored_ , with a trailing underscore added, e.g.:  int example_int_;", 
            "title": "1.8.2. Member variables"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#2-indentation-and-formatting", 
            "text": "V4R uses a variant of the Qt style formatting. The standard indentation for each block is 4 spaces. If possible, apply this measure for your tabs and other spacings.", 
            "title": "2. Indentation and Formatting"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#21-namespaces", 
            "text": "In a header file, the contents of a namespace should be indented, e.g.:  namespace v4r\n{\n    class Foo\n    {\n        ...\n    };\n}", 
            "title": "2.1. Namespaces"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#22-classes", 
            "text": "The template parameters of a class should be declared on a different line, e.g.:  template  typename T \nclass Foo\n{\n    ...\n}", 
            "title": "2.2. Classes"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#23-functions-methods", 
            "text": "The return type of each function declaration must be placed on a different line, e.g.:  void\nbar ();  Same for the implementation/definition, e.g.:  void\nbar ()\n{\n  ...\n}  or  void\nFoo::bar ()\n{\n  ...\n}  or  template  typename T  void\nFoo T ::bar ()\n{\n  ...\n}", 
            "title": "2.3. Functions / Methods"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#24-braces", 
            "text": "Braces, both open and close, go on their own lines, e.g.:  if (a   b)\n{\n    ...\n}\nelse\n{\n    ...\n}  Braces can be omitted if the enclosed block is a single-line statement, e.g.:  if (a   b)\n    x = 2 * a;", 
            "title": "2.4. Braces"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#3-structuring", 
            "text": "", 
            "title": "3. Structuring"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#31-classes-and-api", 
            "text": "For most classes in V4R, it is preferred that the interface (all public members) does not contain variables and only two types of methods:   The first method type is the get/set type that allows to manipulate the parameters and input data used by the class.  The second type of methods is actually performing the class functionality and produces output, e.g. compute, filter, segment.", 
            "title": "3.1. Classes and API"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#32-passing-arguments", 
            "text": "For getter/setter methods the following rules apply:   If large amounts of data needs to be set it is preferred to pass either by a  const  reference or by a  const  boost shared_pointer instead of the actual data.  Getters always need to pass exactly the same types as their respective setters and vice versa.  For getters, if only one argument needs to be passed this will be done via the return keyword. If two or more arguments need to be passed they will all be passed by reference instead.   For the compute, filter, segment, etc. type methods the following rules apply:   The output arguments are preferably non-pointer type, regardless of data size.  The output arguments will always be passed by reference.", 
            "title": "3.2. Passing arguments"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#33-use-const", 
            "text": "To allow clients of your class to immediately see which variable can be altered and which are used for read only access, define input arguments  const . The same applies for member functions which do not change member variables.", 
            "title": "3.3 Use const"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#33-do-not-clutter-header-files", 
            "text": "To reduce compile time amongst others, put your definitions into seperate .cpp or .hpp files. Define functions inline in the header only when they are small, say, 10 lines or fewer.", 
            "title": "3.3 Do not clutter header files"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#34-fix-warnings", 
            "text": "To keep the compiler output non-verbose and reduce potential conflicts, avoid warnings produced by the compiler. If you encounter warnings from other parts in the library, either try to fix them directly or report them using the issue tracker.", 
            "title": "3.4 Fix warnings"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#35-check-input-range-and-handle-exceptions", 
            "text": "To avoid confusing runtime errors and undefined behaviour, check the input to your interfaces for potential conflicts and provide meaningful error messages. If possible, try to catch these exceptions and return in a well-defined state.", 
            "title": "3.5 Check input range and handle exceptions"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#36-document", 
            "text": "V4R uses  Doxygen  for documentation of classes and interfaces. Any code in V4R must be documented using Doxygen format.", 
            "title": "3.6 Document"
        }, 
        {
            "location": "/v4r/docs/v4r_style_guide/#37-template-your-classesfunctions-whenever-appropriate", 
            "text": "As many classes and functions in V4R depend on e.g. templated point clouds, allow your classes to accommodate with the various types by using template classes and functions.", 
            "title": "3.7 Template your classes/functions whenever appropriate"
        }, 
        {
            "location": "/v4r_ros_wrappers/contributing/", 
            "text": "For developers\n\n\nThis repository is a ROS wrapper only and shall therefore only include ROS interfaces (service calls, test nodes, visualizations, etc.) to objects / functions defined in the \nV4R library\n (e.g. by deriving from classes defined in the V4R library). Please do not commit any essential code into this repository - this should go directly into V4R library (so that people can use it also without ROS). This also means that external dependencies should already be resolved by the V4R library.\n\n\nAs a convention, please create seperate folders for service/message definitions (e.g. do not put your \n*.cpp/*.hpp\n files together with \n*.msg/*.srv\n).\nIn your package.xml file, please fill in the maintainer (and optional author) field with your name and e-mail address.\n\n\nLicense\n\n\nThe V4R libary and v4r_ros_wrappers are published under the MIT license. \n\n\nPlease add this to the top of each of your files. \n\n\n/******************************************************************************\n * Copyright (c) 2015 Author's name\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \nSoftware\n), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \nAS IS\n, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n ******************************************************************************/", 
            "title": "Contributing"
        }, 
        {
            "location": "/v4r_ros_wrappers/contributing/#for-developers", 
            "text": "This repository is a ROS wrapper only and shall therefore only include ROS interfaces (service calls, test nodes, visualizations, etc.) to objects / functions defined in the  V4R library  (e.g. by deriving from classes defined in the V4R library). Please do not commit any essential code into this repository - this should go directly into V4R library (so that people can use it also without ROS). This also means that external dependencies should already be resolved by the V4R library.  As a convention, please create seperate folders for service/message definitions (e.g. do not put your  *.cpp/*.hpp  files together with  *.msg/*.srv ).\nIn your package.xml file, please fill in the maintainer (and optional author) field with your name and e-mail address.", 
            "title": "For developers"
        }, 
        {
            "location": "/v4r_ros_wrappers/contributing/#license", 
            "text": "The V4R libary and v4r_ros_wrappers are published under the MIT license.   Please add this to the top of each of your files.   /******************************************************************************\n * Copyright (c) 2015 Author's name\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the  Software ), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED  AS IS , WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n ******************************************************************************/", 
            "title": "License"
        }, 
        {
            "location": "/v4r_ros_wrappers/", 
            "text": "This repository contains ROS wrappers for the V4R library.\n\n\nInstallation\n\n\nFrom Ubuntu Packages\n\n\nsimply install \nsudo apt-get install ros-indigo-v4r-ros-wrappers\n after enabling the \nSTRANDS repositories\n. \n\n\nFrom Source\n\n\nMake sure you install the V4R library first.\n\n\nThen clone this repository and build it with \ncatkin_make\n. You might get an error regarding V4RModules.cmake. This is easy to fix:\n\n\ncd my_catkin_ws/build\nccmake ../src\n\n\n\n\nLocate the option \nV4R_DIR\n and set it, according to where you build/installed V4R library, e.g.:\n\n\nV4R_DIR   /home/somewhere/v4r/build\n\n\n\n\nThen call catkin again, and all should now compile fine.\n\n\nTutorial\n\n\nA tutorial can be found \nhere\n.\n\n\nTroubleshooting\n\n\nOpenGL not working for SiftGPU\n\n\nA GPU (best: NVIDIA) is required for many components. For this to work, the user running the software needs to be allowed to access the X server. The easiest (but \nvery\n insecure in a not secured network) way to achieve this is to allow access for all users via \nDISPLAY=:0 sudo xhost +\n. \nAlternatively, SIFT can be computed using the non-free library of OpenCV. In this case, link the v4r and v4r_ros_wrappers libraries to the right OpenCV library.", 
            "title": "Home"
        }, 
        {
            "location": "/v4r_ros_wrappers/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/v4r_ros_wrappers/#from-ubuntu-packages", 
            "text": "simply install  sudo apt-get install ros-indigo-v4r-ros-wrappers  after enabling the  STRANDS repositories .", 
            "title": "From Ubuntu Packages"
        }, 
        {
            "location": "/v4r_ros_wrappers/#from-source", 
            "text": "Make sure you install the V4R library first.  Then clone this repository and build it with  catkin_make . You might get an error regarding V4RModules.cmake. This is easy to fix:  cd my_catkin_ws/build\nccmake ../src  Locate the option  V4R_DIR  and set it, according to where you build/installed V4R library, e.g.:  V4R_DIR   /home/somewhere/v4r/build  Then call catkin again, and all should now compile fine.", 
            "title": "From Source"
        }, 
        {
            "location": "/v4r_ros_wrappers/#tutorial", 
            "text": "A tutorial can be found  here .", 
            "title": "Tutorial"
        }, 
        {
            "location": "/v4r_ros_wrappers/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/v4r_ros_wrappers/#opengl-not-working-for-siftgpu", 
            "text": "A GPU (best: NVIDIA) is required for many components. For this to work, the user running the software needs to be allowed to access the X server. The easiest (but  very  insecure in a not secured network) way to achieve this is to allow access for all users via  DISPLAY=:0 sudo xhost + . \nAlternatively, SIFT can be computed using the non-free library of OpenCV. In this case, link the v4r and v4r_ros_wrappers libraries to the right OpenCV library.", 
            "title": "OpenGL not working for SiftGPU"
        }, 
        {
            "location": "/v4r_ros_wrappers/multiview_object_recognizer/", 
            "text": "usage:\n\n\nrosrun multiview_object_recognizer multiview_object_recognizer_node -m /path/to/your/models/ [--optional_parameter p]\n\n\nparams (see extended help output with -h):\n\n\n\n\nmodels_dir [in] - Directory containing the object models (REQUIRED)\n\n\ndo_sift [in] - if true, does SIFT feature matching\n\n\ndo_shot [in] - if true, does SHOT feature matching\n\n\nchop_z [in] - cut of distance in meters with respect to the camera coordinate system\n\n\ncompute_mst [in] - if true, does point cloud registration by SIFT background matching (given scene_to_scene == true), by using given pose (if use_robot_pose == true) and by common object hypotheses (if hyp_to_hyp == true) from all the possible connection a Mimimum Spanning Tree is computed. If false, it only uses the given pose for each point cloud. The given pose can be extracted from the .pcd's header fields sensor_origin and sensor_orientation or from the extracted camera pose e.g. from the camera_tracker package\n\n\nscene_to_scene [in] - if true, estimates relative camera transform between two viewpoints using SIFT background matching\n\n\nuse_go3d [in] - if true, verifies against a reconstructed scene from multiple viewpoints. Otherwise only against the current viewpoint.\n\n\ncg_size_thresh [in] - minimum number of feature corrrespondences neccessary to generate an object hypothesis (low number increases recall and compuation time, high number increases precision)\n\n\nhv_color_sigma_l [in] - maximum allowed standard deviation of the luminance color (L channel in LAB color space) for points for generated object hypotheses and scene (low number increases precision, high number increases recall)\n\n\nhv_color_sigma_ab [in] - maximum allowed standard deviation of the luminance color (AB channels in LAB color space) for points for generated object hypotheses and scene (low number increases precision, high number increases recall)\n\n\nmax_vertices_in_graph [in] - maximum number of view points taken into account by the multi-view recognizer\n\n\ntransfer_feature_matches [in] - defines the method used for transferring information from other viewpoints (1 = keypoint correspondences (Faeulhammer ea., ICRA2015 paper); 0 = full hypotheses only (Faeulhammer ea., MVA2015 paper))\n\n\ndistance_keypoints_get_discarded [in] - defines the squared distance [m\u00b2] for keypoints to be considered the same. This avoids redundant keypoints when transformed from other view points (only used for extension_mode=0)\n\n\n\n\nfurther description of parameters in the example object_recognizer_multiview in V4R/samples\n\n\nTest:\n\n\nrosrun multiview_object_recognizer test_multiview_object_recognizer_node [_optional_parameter:=p]\n\n\nTest params (NOTE THAT THESE ARE ROS PARAMETERS):\n\n\n\n\ninput_method[in] (default: 0) - 0=camera input; 1 = input from disk\n\n\ntopic[in] (default: /camera/depth_registered/points) - camera topic being used when input_method=0\n\n\ndirectory[in] - directory being used to read test .pcd files when input_method=1\n\n\n\n\nObject models and test scenes can be obtained from https://repo.acin.tuwien.ac.at/tmp/permanent/dataset_index.php\nTo model your own objects have a look at \nhttp://www.acin.tuwien.ac.at/forschung/v4r/software-tools/rtm/\n\n\nReferences:\n\n\n\n\n\n\n[\nThomas F\u00e4ulhammer, Aitor Aldoma, Michael Zillich, Markus Vincze\n, \nTemporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered And Dynamic Environments, IEEE Int. Conf. on Robotics and Automation (ICRA), 2015\n]\n\n\n\n\n\n\n[\nThomas F\u00e4ulhammer, Michael Zillich, Markus Vincze\n, \nMulti-View Hypotheses Transfer for Enhanced Object Recognition in Clutter, IAPR Conference on Machine Vision Applications (MVA), 2015\n]", 
            "title": "Multiview object recognizer"
        }, 
        {
            "location": "/v4r_ros_wrappers/multiview_object_recognizer/#usage", 
            "text": "rosrun multiview_object_recognizer multiview_object_recognizer_node -m /path/to/your/models/ [--optional_parameter p]", 
            "title": "usage:"
        }, 
        {
            "location": "/v4r_ros_wrappers/multiview_object_recognizer/#params-see-extended-help-output-with-h", 
            "text": "models_dir [in] - Directory containing the object models (REQUIRED)  do_sift [in] - if true, does SIFT feature matching  do_shot [in] - if true, does SHOT feature matching  chop_z [in] - cut of distance in meters with respect to the camera coordinate system  compute_mst [in] - if true, does point cloud registration by SIFT background matching (given scene_to_scene == true), by using given pose (if use_robot_pose == true) and by common object hypotheses (if hyp_to_hyp == true) from all the possible connection a Mimimum Spanning Tree is computed. If false, it only uses the given pose for each point cloud. The given pose can be extracted from the .pcd's header fields sensor_origin and sensor_orientation or from the extracted camera pose e.g. from the camera_tracker package  scene_to_scene [in] - if true, estimates relative camera transform between two viewpoints using SIFT background matching  use_go3d [in] - if true, verifies against a reconstructed scene from multiple viewpoints. Otherwise only against the current viewpoint.  cg_size_thresh [in] - minimum number of feature corrrespondences neccessary to generate an object hypothesis (low number increases recall and compuation time, high number increases precision)  hv_color_sigma_l [in] - maximum allowed standard deviation of the luminance color (L channel in LAB color space) for points for generated object hypotheses and scene (low number increases precision, high number increases recall)  hv_color_sigma_ab [in] - maximum allowed standard deviation of the luminance color (AB channels in LAB color space) for points for generated object hypotheses and scene (low number increases precision, high number increases recall)  max_vertices_in_graph [in] - maximum number of view points taken into account by the multi-view recognizer  transfer_feature_matches [in] - defines the method used for transferring information from other viewpoints (1 = keypoint correspondences (Faeulhammer ea., ICRA2015 paper); 0 = full hypotheses only (Faeulhammer ea., MVA2015 paper))  distance_keypoints_get_discarded [in] - defines the squared distance [m\u00b2] for keypoints to be considered the same. This avoids redundant keypoints when transformed from other view points (only used for extension_mode=0)   further description of parameters in the example object_recognizer_multiview in V4R/samples", 
            "title": "params (see extended help output with -h):"
        }, 
        {
            "location": "/v4r_ros_wrappers/multiview_object_recognizer/#test", 
            "text": "rosrun multiview_object_recognizer test_multiview_object_recognizer_node [_optional_parameter:=p]", 
            "title": "Test:"
        }, 
        {
            "location": "/v4r_ros_wrappers/multiview_object_recognizer/#test-params-note-that-these-are-ros-parameters", 
            "text": "input_method[in] (default: 0) - 0=camera input; 1 = input from disk  topic[in] (default: /camera/depth_registered/points) - camera topic being used when input_method=0  directory[in] - directory being used to read test .pcd files when input_method=1   Object models and test scenes can be obtained from https://repo.acin.tuwien.ac.at/tmp/permanent/dataset_index.php\nTo model your own objects have a look at  http://www.acin.tuwien.ac.at/forschung/v4r/software-tools/rtm/", 
            "title": "Test params (NOTE THAT THESE ARE ROS PARAMETERS):"
        }, 
        {
            "location": "/v4r_ros_wrappers/multiview_object_recognizer/#references", 
            "text": "[ Thomas F\u00e4ulhammer, Aitor Aldoma, Michael Zillich, Markus Vincze ,  Temporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered And Dynamic Environments, IEEE Int. Conf. on Robotics and Automation (ICRA), 2015 ]    [ Thomas F\u00e4ulhammer, Michael Zillich, Markus Vincze ,  Multi-View Hypotheses Transfer for Enhanced Object Recognition in Clutter, IAPR Conference on Machine Vision Applications (MVA), 2015 ]", 
            "title": "References:"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_classifier/", 
            "text": "This classifier relies on object shape and is based on the approach of [1].\n\n\nIt uses CAD data for training, such as https://repo.acin.tuwien.ac.at/tmp/permanent/Cat200_ModelDatabase.zip (taken from https://repo.acin.tuwien.ac.at/tmp/permanent/3d-net.org).\nIn order to speed up training, please select a subset of the training data you want to use and put it in a folder \nCat200_ModelDatabase__small\n.\n\n\nUsage:\n\n\nThe classifier can then be started by:\n\n\n\n\n\n\nroslaunch openni_launch openni.launch depth_registration:=true\n\n\n\n\n\n\nrun segmentation to get some point cloud clusters (if you want to launch both segmentation and classification at the same time, please refer to the launch file in segment_and_classify and add the classification parameters desrcibed below)\n\n\n\n\n\n\nroslaunch object_classifier classifier.launch models_dir:=your_dataset_dir/Cat200_ModelDatabase__small/ topic:=/camera/depth_registered/points training_dir:=your_training_dir\n\n\n\n\n\n\nparams:\n\n\n\n\nmodels_dir - training directory with the CAD models of the classes, \n\n\ntraining_dir - directory containing the trained data (if they exist - otherwise they will be re-trained)\n\n\n\n\nTest:\n\n\nTo test, you can use the test node in segment_and_classify.\n\n\nReferences:\n\n\n\n\n[1] W. Wohlkinger and M. Vincze\n, \nEnsemble of Shape Functions for 3D Object Classification*", 
            "title": "Object classifier"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_classifier/#usage", 
            "text": "The classifier can then be started by:    roslaunch openni_launch openni.launch depth_registration:=true    run segmentation to get some point cloud clusters (if you want to launch both segmentation and classification at the same time, please refer to the launch file in segment_and_classify and add the classification parameters desrcibed below)    roslaunch object_classifier classifier.launch models_dir:=your_dataset_dir/Cat200_ModelDatabase__small/ topic:=/camera/depth_registered/points training_dir:=your_training_dir", 
            "title": "Usage:"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_classifier/#params", 
            "text": "models_dir - training directory with the CAD models of the classes,   training_dir - directory containing the trained data (if they exist - otherwise they will be re-trained)", 
            "title": "params:"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_classifier/#test", 
            "text": "To test, you can use the test node in segment_and_classify.", 
            "title": "Test:"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_classifier/#references", 
            "text": "[1] W. Wohlkinger and M. Vincze ,  Ensemble of Shape Functions for 3D Object Classification*", 
            "title": "References:"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_gestalt_segmentation/", 
            "text": "object_gestalt_segmentation\n\n\nThe current segmentation is based on the work of Ekaterina Popatova and Andreas Richtsfeld. Segmented objects are discarded if they are too tall or too far away from the robot. A service is provided that returns the segmented object in the current scene. A more sophisticated solution based on attention cues will be provided in the near future.  \n\n\nTechnical Maintainer: [markus](https://github.com/edith-langer (Edith Langer, TU Wien) - langer@acin.tuwien.ac.at\n\n\nContents\n\n\n\n\nInstallation Requirements\n\n\nExecution\n\n\nSoftware architecture\n\n\n\n\n1. Installation Requirements: \n\n\nROS packages\n\n\nThe ROS packages dependencies can be installed with the command:\n\n\nrosdep install --from-path object_gestalt_segmentation -i -y\n\n\n\n\n2. Execution: \n\n\nroslaunch object_gestalt_segmentation startup.launch\n\n\n\n\ntop", 
            "title": "Object gestalt segmentation"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_gestalt_segmentation/#object_gestalt_segmentation", 
            "text": "The current segmentation is based on the work of Ekaterina Popatova and Andreas Richtsfeld. Segmented objects are discarded if they are too tall or too far away from the robot. A service is provided that returns the segmented object in the current scene. A more sophisticated solution based on attention cues will be provided in the near future.", 
            "title": "object_gestalt_segmentation"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_gestalt_segmentation/#technical-maintainer-markushttpsgithubcomedith-langer-edith-langer-tu-wien-langeracintuwienacat", 
            "text": "", 
            "title": "Technical Maintainer: [markus](https://github.com/edith-langer (Edith Langer, TU Wien) - langer@acin.tuwien.ac.at"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_gestalt_segmentation/#contents", 
            "text": "Installation Requirements  Execution  Software architecture", 
            "title": "Contents"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_gestalt_segmentation/#1-installation-requirements", 
            "text": "", 
            "title": "1. Installation Requirements: "
        }, 
        {
            "location": "/v4r_ros_wrappers/object_gestalt_segmentation/#ros-packages", 
            "text": "The ROS packages dependencies can be installed with the command:  rosdep install --from-path object_gestalt_segmentation -i -y", 
            "title": "ROS packages"
        }, 
        {
            "location": "/v4r_ros_wrappers/object_gestalt_segmentation/#2-execution", 
            "text": "roslaunch object_gestalt_segmentation startup.launch  top", 
            "title": "2. Execution: "
        }, 
        {
            "location": "/v4r_ros_wrappers/singleview_object_recognizer/", 
            "text": "usage:\n\n\nrosrun singleview_object_recognizer recognition_service -m /path/to/your/models/ [--optional_parameter p]\n\n\nparams (see extended help output with -h)::\n\n\n\n\nmodels_dir [in] - Directory containing 3D models (saved as *.pcd files)\n\n\nchop_z [in] - cut of distance in meters with respect to the camera\n\n\n\n\nTest:\n\n\nrosrun singleview_object_recognizer test_single_view_recognition [_optional_parameter:=p]\n \n\n\nTest params (NOTE THAT THESE ARE ROS PARAMETERS):\n\n\n\n\ninput_method[in] (default: 0) - 0=camera input; 1 = input from disk\n\n\ntopic[in] (default: /camera/depth_registered/points) - camera topic being used when input_method=0\n\n\ndirectory[in] - directory being used to read test .pcd files when input_method=1\n\n\n\n\nObject models and test scenes can be obtained from https://repo.acin.tuwien.ac.at/tmp/permanent/dataset_index.php\nTo model your own objects have a look at \nhttp://www.acin.tuwien.ac.at/forschung/v4r/software-tools/rtm/", 
            "title": "Singleview object recognizer"
        }, 
        {
            "location": "/v4r_ros_wrappers/singleview_object_recognizer/#usage", 
            "text": "rosrun singleview_object_recognizer recognition_service -m /path/to/your/models/ [--optional_parameter p]", 
            "title": "usage:"
        }, 
        {
            "location": "/v4r_ros_wrappers/singleview_object_recognizer/#params-see-extended-help-output-with-h", 
            "text": "models_dir [in] - Directory containing 3D models (saved as *.pcd files)  chop_z [in] - cut of distance in meters with respect to the camera", 
            "title": "params (see extended help output with -h)::"
        }, 
        {
            "location": "/v4r_ros_wrappers/singleview_object_recognizer/#test", 
            "text": "rosrun singleview_object_recognizer test_single_view_recognition [_optional_parameter:=p]", 
            "title": "Test:"
        }, 
        {
            "location": "/v4r_ros_wrappers/singleview_object_recognizer/#test-params-note-that-these-are-ros-parameters", 
            "text": "input_method[in] (default: 0) - 0=camera input; 1 = input from disk  topic[in] (default: /camera/depth_registered/points) - camera topic being used when input_method=0  directory[in] - directory being used to read test .pcd files when input_method=1   Object models and test scenes can be obtained from https://repo.acin.tuwien.ac.at/tmp/permanent/dataset_index.php\nTo model your own objects have a look at  http://www.acin.tuwien.ac.at/forschung/v4r/software-tools/rtm/", 
            "title": "Test params (NOTE THAT THESE ARE ROS PARAMETERS):"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/", 
            "text": "Overview\n\n\nIn this tutorial you will learn how to recognise and track trained 3D object models. You will first model objects by rotating them in front of an RGB-D camera. These objects can then be recognised by a recognition component via a ROS service call. You will also learn how much you can (or can't) trust recogniser responses.\n\n\nThis tutorial requires installation of a specific vision library, which is part of the STRANDS repositories.\n\n\nThe V4R (Vision for Robotics) library\n\n\nThis is now released as Ubuntu packages as well, so there should be no need to install this from source as detailed below. Just install with \nsudo apt-get install ros-indigo-v4r-ros-wrappers\n and you should be ready to go! Jump straight to [[Tutorial.md#object-modelling]].\n\n\nThis library is part of the STRANDS repositories 'https://github.com/strands-project/v4r'. The library itself is independent of ROS, so it is built outside ROS catkin. There are wrappers for ROS 'https://github.com/strands-project/v4r_ros_wrappers', which can then be placed inside the normal catkin workspace.\n\n\nDependencies\n\n\n\n\nopenni drivers: \nsudo apt-get install libopenni-dev libopenni-sensor-primesense0 libopenni2-dev\n\n\nQt 4 (http://qt-project.org/): \nsudo apt-get install libqt4-dev\n\n\nBoost (http://www.boost.org/): comes with ubuntu\n\n\nPoint Cloud Library 1.7.x (http://pointclouds.org/): comes with ROS\n\n\nEigen3 (http://eigen.tuxfamily.org/): \nsudo apt-get install libeigen3-dev\n\n\nOpenCV 2.x (http://opencv.org/): comes with ROS\n\n\nCeres Solver 1.9.0 (http://ceres-solver.org/): \nsudo apt-get install libceres-dev\n\n\nOpenGL GLSL mathematics (libglm-dev): \nsudo apt-get install libglm-dev\n\n\nGLEW - The OpenGL Extension Wrangler Library (libglew-dev): \nsudo apt-get install libglew-dev\n\n\nlibraries for sparse matrices computations (libsuitesparse-dev): \nsudo apt-get install libsuitesparse-dev\n\n\nGoogle Logging Library \nsudo apt-get install libgoogle-glog-dev\n\n\nX-related libraries \nsudo apt-get install libx11-dev libxinerama-dev libxi-dev libxrandr-de libassimp-dev\n\n\n\n\nInstallation\n\n\nClone 'https://github.com/strands-project/v4r' somewhere on your computer, e.g. `~/somewhere/v4r/') and build using cmake:\n\n\ncd ~/somewhere\ngit clone 'https://github.com/strands-project/v4r'\ncd v4r\nmkdir build\ncd build\ncmake ..\nmake\nsudo make install (optional)\n\n\n\n\nNow you can install the STRANDS ros wrappers. Clone v4r_ros_wrappers into your catkin workspace:\n\n\ncd my_catkin_ws/src\ngit clone https://github.com/strands-project/v4r_ros_wrappers.git\ncd ..\n\n\n\n\nThen call catkin_make once. If you chose not to install the V4R library (optional point above) you will get an error regarding V4RModules.cmake. This is easy to fix by locating the build/install directory of V4R and setting it appropriately within cmake:\n\n\ncatkin_make -DV4R_DIR=/path/to/your/v4r\n\n\n\n\nIt should compile fine now. Eventually you have to source the workspace with \n\n\nsource devel/setup.bash\n\n\n\n\nObject modelling\n\n\nFirst you have to model all the objects you want to recognise later. You use an offline tool for this, the RTM Toolbox.\n\n\n\n\nPlace the object on a flat surface on a newspaper or something similar. This allows you to rotate the object without touching it. The texture on the newspaper also helps view registration. The pictures below were taken with the object on a turn table, which is the most convenient way of rotating the object. You can also model objects without a turntable by putting the object on the floor for instance and moving your camera. In this case just skip the step definining the ROI.\n\n\nStart the modelling tool: ~/somewhere/v4r/bin/RTMT\n\n\nPress \"Camera Start\": You should now see the camera image\n\n\n\n\nPress \"ROI Set\" + click on the flat surface next to the object\n\n\n\n\nPress Tracker Start: you now see the tracking quality bar top left\n\n\n\n\nRotate 360 degrees, the program will generate a number of keyframes.\n\n\nIMPORTANT:\n Do not touch the object itself while moving it.\n\n\n\n  \n\n\nPress \"Tracker Stop\"\n\n\nPress \"Camera Stop\"\n\n\nPress \"Pose Optimize\" (optional): bundle adjustment of all cameras (be patient!)\n\n\nPress \"Object Segment\": The object should already be segmented correctly thanks to the ROI set previously\n\n\n\n  You can check segmentation (\n \n buttons). Highlighted areas are segments that will be clustered to the object model. If some highlighted areas do not belong to the object (e.g. supporting plane), you can click on these wrong segmentations to undo. You can also click on dark areas to add these segments to the object model. These areas should automatically be projected into the other frames.\n\n\nPress \"Object ...Ok\"\n\n\n\n  \n\n\nPress \"Store for Recognizer\": saves the point clouds in a format for object recognition. You will be asked for an model name.\n\n  By default the program will store models in various subfolders of the folder \"./data\", which will be created if not present. This can be changed in the configuration options (see below).\n\n\nPress \"Store for Tracker\": save a different model suitable for tracking\n\n\nIf the 3D point cloud visualization is activated +/- can be used to increase/ decrease the size of dots\n\n\n\n\nThis is a convenient way to model objects with the STRANDS robots. Put the objects on something elevated (a trash can in this case) to bring it within a good distance to the robot's head camera.\n\n\n\n\nConfiguration options:\n\n\n\n\nSet data folder and model name:\n\n  (File -\n Preferences -\n Settings -\n Path and model name)\n\n\nConfigure number of keyfames to be selected using a camera rotation and a camera translation threshold:\n\n  (File -\n Preferences -\n Settings -\n Min. delta angle, Min. delta camera distance)\n\n\n\n\nTrouble shooting\n\n\n\n\nIf you press any of the buttons in the wrong order, just restart. Recovery is futile.\n\n\nIf you do not get an image, there is a problem with the OpenNI device driver.\nCheck the file \n/etc/openni/GlobalDefaults.ini\n, set \nUsbInterface=2\n (i.e. BULK).\n\n\nIf the plane supporting the object is not removed completely, try to increase the inlier distance for dominant plane segmentation in File -\n Preferences -\n Postprocessing.\n\n\n\n\nObject recognition\n\n\nWith the models created above you can now call the object recognition service within the STRANDS system.\n\n\nStart the object recogniser on the side PC with the head camera attached. To do this, you must SSH into the side PC \nwithout\n X forwarding then run:\n\n\nexport DISPLAY=:0\nrosrun singleview_object_recognizer recognition_service -m /path/to/your/model/data\n\n\n\n\nIf you want to get a description about other parameters, just add -h to the command. The ones you might want to change are:\n\n \n-z 2.5\n (will neglect all points further away than 2.5m - this will ensure the noise level of the measured points is not too high. Note that RGB-D data from the Asus or Kinect gets worse with distance)\n\n \n--knn_sift 3\n (this will increase the number of generated hypotheses)\n\n \n--do_shot true\n (this will enable SHOT feature extraction - necessary for objects without visual texture information)\n\n \n-c 5\n (increase speed at the cost of possibly missing objects if they are e.g. half occluded.)\n\n\nThe recogniser offers a service \n/recognition_service/sv_recognition\n, defined in \nv4r_ros_wrappers/recognition_srv_definitions/srv\n:\n\n\nsensor_msgs/PointCloud2 cloud  # input scene point cloud\nfloat32[] transform            # optional transform to world coordinate system\nstd_msgs/String scene_name     # optional name for multiview recognition\nstd_msgs/String view_name      # optional name for multiview recognition\n---\nstd_msgs/String[] ids                 # name of the recognised object model\ngeometry_msgs/Transform[] transforms  # 6D object pose\nfloat32[] confidence                  # ratio of visible points\ngeometry_msgs/Point32[] centroid      # centroid of the cluster\nobject_perception_msgs/BBox[] bbox    # bounding box of the cluster\nsensor_msgs/PointCloud2[] models_cloud  # point cloud of the model transformed into camera coordinates\n\n\n\n\nFor you, all you have to provide is a point cloud. The recogniser will return arrays of ids (the name you gave during modelling), transforms (the 6D object poses), as well as confidences, bounding boxes and the segmented point clouds corresponding to the recognised portions of the scene.\n\n\nThere is a test ROS component for you as an example:\n\n\nrosrun singleview_object_recognizer test_single_view_recognition_from_file _topic:=/camera/depth_registered/points\n\n\n\n\nwhere you have to set the topic to the respective RGB-D source of your robot, e.g. the head_xtion.\n\n\nThe recogniser publishes visualisation information.\n\n\n\n\n/recognition_service/sv_recogniced_object_instances_img\n: displays the original image with overlaid bounding boxes of recognised objects\n\n\n/recognition_service/sv_recogniced_object_instances\n: the model point clouds of the recognised objects, in the camera frame.\nThe following picture shows an example where R2-D2 is detected in a shelf, with the debug picture and recognised model displayed in rviz.\n\n\n\n\n\nRecognition performance\n\n\nThe modelling tool provides full 3D object models from all the views you provided, which in principle allow the recogniser to recogise the object in any condition dfrom any view.\nPractically, however, recognition performance depends on several factors:\n\n Distance: Performance can quite rapidly decline with distance. First, because the object features on which the recogniser depends become too small (no way that it could detect an object just a few pixels large). Second, because the depth data, on which a pose verification step in the recogniser depends, becomes more and more noisy (and close to useless beyond 3 m or so)\n\n Lighting conditions: In principle the object features are lighting invariant. Practically, different characteristics of the camera which was used for modelling and the camera used for recognition can affect the appearance of the object features.\n\n Motion blur: The robot might be moving while it taske a picture. Motion blur will deteriorate object feature extraction.\n\n Occlusions: Objects might not be entirely visible. The recogniser does not need all object features, so it can handle a certain amount of occlusion. How much, depends on the object and how many features it is has.\n* Object specifics: Some objects are harder to detect than others, e.g. because they have few features, are small, have a somewhat shiny surface, or are non-rigid.\n\n\nBefore using the recogniser in any object search scenario it is therefore important to gather some statistics about the recognition performance. The recogniser's confidence value can be a useful measure. But don't trust it too much -it is not an actual probability.\n\n\nUseful aspects to learn are:\n\n How fast the recognition rate (in how many cases is the object found) drops with distance.\n\n How false positive rate and confidence measure are related.\n\n\nTrouble shooting\n\n\n\n\nIf you get an error like this\n\n\n\n\nterminate called after throwing an instance of 'flann::FLANNException'\n  what():  Saved index does not contain the dataset and no dataset was provided.\n[recognition_service-2] process has died [pid 17534, exit code -6, cmd /home/mz/work/STRANDS/code/catkin_ws/devel/lib/singleview_object_recognizer/recognition_service __name:=recognition_service __log:=/home/mz/.ros/log/61fb16b8-4afc-11e5-801d-503f56004d09/recognition_service-2.log].\nlog file: /home/mz/.ros/log/61fb16b8-4afc-11e5-801d-503f56004d09/recognition_service-2*.log\n\n\n\n\nlocate the file \nsift_flann.idx\n, probably right in your catkin workspace or in \n~/.ros\n, and remove it.\nDo the same if you get some \nvector::_M_range_check\n error. In case you enable SHOT, please also remove \nshot_omp_flann.idx\n. \n\n\n\n\n\n\nPlease also make sure that your camera uses VGA resolution for both RGB and depth. You can check the values for \ncolor_mode\n and \ndepth_mode\n in \nrosrun rqt_reconfigure rqt_reconfigure\n (camera -\n driver).\n\n\n\n\n\n\nAlso ensure you turned on depth_registration (i.e. start the camera with \nroslaunch openni2_launch openni2.launch depth_registration:=true\n)\n\n\n\n\n\n\nObject tracker\n\n\nIf you stored you object model for tracking, you can track single objects in real-time using the object-tracker module. You can start the service by \n\n\nrosrun object_tracker object_tracker_service -m /path/to/your/model/data/object_name/tracking_model.ao\n\n\n\n\nThe tracker will start as soon as you call the service \n\n\nrosservice call /object_tracker/start_recording\n\n\n\n\nThis will publish topics for the 3D object pose and the confidence of this estimate\n\n\nrostopic echo /object_tracker/object_pose\nrostopic echo /object_tracker/object_tracker_confidence\n\n\n\n\nTo visualize the pose, you can use RVIZ and check out the image topic \n/object_tracker/debug_images\n.\n\n\nTo stop the tracker call\n\n\nrosservice call /object_tracker/stop_recording\nrosservice call /object_tracker/cleanup\n\n\n\n\nReferences\n\n\nWhen referencing this work, pleace cite:\n\n\n\n\n\n\nJ. Prankl, A. Aldoma Buchaca, A. Svejda, M. Vincze, RGB-D Object Modelling for Object Recognition and Tracking. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\n\n\n\n\n\n\nThomas F\u00e4ulhammer, Aitor Aldoma, Michael Zillich and Markus Vincze\nTemporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered And Dynamic Environments\nIEEE International Conference on Robotics and Automation (ICRA), Seattle, WA, USA, 2015.\n\n\n\n\n\n\nThomas F\u00e4ulhammer, Michael Zillich and Markus Vincze\nMulti-View Hypotheses Transfer for Enhanced Object Recognition in Clutter,\nIAPR International Conference on Machine Vision Applications (MVA), Tokyo, Japan, 2015.\n\n\n\n\n\n\nA. Aldoma Buchaca, F. Tombari, J. Prankl, A. Richtsfeld, L. di Stefano, M. Vincze, Multimodal Cue Integration through Hypotheses Verification for RGB-D Object Recognition and 6DOF Pose Estimation. IEEE International Conference on Robotics and Automation (ICRA), 2013.\n\n\n\n\n\n\nJ. Prankl, T. M\u00f6rwald, M. Zillich, M. Vincze, Probabilistic Cue Integration for Real-time Object Pose Tracking. Proc. International Conference on Computer Vision Systems (ICVS). 2013.\n\n\n\n\n\n\nFor further information check out \nthis site\n.", 
            "title": "Tutorial"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#overview", 
            "text": "In this tutorial you will learn how to recognise and track trained 3D object models. You will first model objects by rotating them in front of an RGB-D camera. These objects can then be recognised by a recognition component via a ROS service call. You will also learn how much you can (or can't) trust recogniser responses.  This tutorial requires installation of a specific vision library, which is part of the STRANDS repositories.", 
            "title": "Overview"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#the-v4r-vision-for-robotics-library", 
            "text": "This is now released as Ubuntu packages as well, so there should be no need to install this from source as detailed below. Just install with  sudo apt-get install ros-indigo-v4r-ros-wrappers  and you should be ready to go! Jump straight to [[Tutorial.md#object-modelling]].  This library is part of the STRANDS repositories 'https://github.com/strands-project/v4r'. The library itself is independent of ROS, so it is built outside ROS catkin. There are wrappers for ROS 'https://github.com/strands-project/v4r_ros_wrappers', which can then be placed inside the normal catkin workspace.", 
            "title": "The V4R (Vision for Robotics) library"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#dependencies", 
            "text": "openni drivers:  sudo apt-get install libopenni-dev libopenni-sensor-primesense0 libopenni2-dev  Qt 4 (http://qt-project.org/):  sudo apt-get install libqt4-dev  Boost (http://www.boost.org/): comes with ubuntu  Point Cloud Library 1.7.x (http://pointclouds.org/): comes with ROS  Eigen3 (http://eigen.tuxfamily.org/):  sudo apt-get install libeigen3-dev  OpenCV 2.x (http://opencv.org/): comes with ROS  Ceres Solver 1.9.0 (http://ceres-solver.org/):  sudo apt-get install libceres-dev  OpenGL GLSL mathematics (libglm-dev):  sudo apt-get install libglm-dev  GLEW - The OpenGL Extension Wrangler Library (libglew-dev):  sudo apt-get install libglew-dev  libraries for sparse matrices computations (libsuitesparse-dev):  sudo apt-get install libsuitesparse-dev  Google Logging Library  sudo apt-get install libgoogle-glog-dev  X-related libraries  sudo apt-get install libx11-dev libxinerama-dev libxi-dev libxrandr-de libassimp-dev", 
            "title": "Dependencies"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#installation", 
            "text": "Clone 'https://github.com/strands-project/v4r' somewhere on your computer, e.g. `~/somewhere/v4r/') and build using cmake:  cd ~/somewhere\ngit clone 'https://github.com/strands-project/v4r'\ncd v4r\nmkdir build\ncd build\ncmake ..\nmake\nsudo make install (optional)  Now you can install the STRANDS ros wrappers. Clone v4r_ros_wrappers into your catkin workspace:  cd my_catkin_ws/src\ngit clone https://github.com/strands-project/v4r_ros_wrappers.git\ncd ..  Then call catkin_make once. If you chose not to install the V4R library (optional point above) you will get an error regarding V4RModules.cmake. This is easy to fix by locating the build/install directory of V4R and setting it appropriately within cmake:  catkin_make -DV4R_DIR=/path/to/your/v4r  It should compile fine now. Eventually you have to source the workspace with   source devel/setup.bash", 
            "title": "Installation"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#object-modelling", 
            "text": "First you have to model all the objects you want to recognise later. You use an offline tool for this, the RTM Toolbox.   Place the object on a flat surface on a newspaper or something similar. This allows you to rotate the object without touching it. The texture on the newspaper also helps view registration. The pictures below were taken with the object on a turn table, which is the most convenient way of rotating the object. You can also model objects without a turntable by putting the object on the floor for instance and moving your camera. In this case just skip the step definining the ROI.  Start the modelling tool: ~/somewhere/v4r/bin/RTMT  Press \"Camera Start\": You should now see the camera image   Press \"ROI Set\" + click on the flat surface next to the object   Press Tracker Start: you now see the tracking quality bar top left   Rotate 360 degrees, the program will generate a number of keyframes.  IMPORTANT:  Do not touch the object itself while moving it.  \n    Press \"Tracker Stop\"  Press \"Camera Stop\"  Press \"Pose Optimize\" (optional): bundle adjustment of all cameras (be patient!)  Press \"Object Segment\": The object should already be segmented correctly thanks to the ROI set previously  \n  You can check segmentation (    buttons). Highlighted areas are segments that will be clustered to the object model. If some highlighted areas do not belong to the object (e.g. supporting plane), you can click on these wrong segmentations to undo. You can also click on dark areas to add these segments to the object model. These areas should automatically be projected into the other frames.  Press \"Object ...Ok\"  \n    Press \"Store for Recognizer\": saves the point clouds in a format for object recognition. You will be asked for an model name. \n  By default the program will store models in various subfolders of the folder \"./data\", which will be created if not present. This can be changed in the configuration options (see below).  Press \"Store for Tracker\": save a different model suitable for tracking  If the 3D point cloud visualization is activated +/- can be used to increase/ decrease the size of dots   This is a convenient way to model objects with the STRANDS robots. Put the objects on something elevated (a trash can in this case) to bring it within a good distance to the robot's head camera.", 
            "title": "Object modelling"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#configuration-options", 
            "text": "Set data folder and model name: \n  (File -  Preferences -  Settings -  Path and model name)  Configure number of keyfames to be selected using a camera rotation and a camera translation threshold: \n  (File -  Preferences -  Settings -  Min. delta angle, Min. delta camera distance)", 
            "title": "Configuration options:"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#trouble-shooting", 
            "text": "If you press any of the buttons in the wrong order, just restart. Recovery is futile.  If you do not get an image, there is a problem with the OpenNI device driver.\nCheck the file  /etc/openni/GlobalDefaults.ini , set  UsbInterface=2  (i.e. BULK).  If the plane supporting the object is not removed completely, try to increase the inlier distance for dominant plane segmentation in File -  Preferences -  Postprocessing.", 
            "title": "Trouble shooting"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#object-recognition", 
            "text": "With the models created above you can now call the object recognition service within the STRANDS system.  Start the object recogniser on the side PC with the head camera attached. To do this, you must SSH into the side PC  without  X forwarding then run:  export DISPLAY=:0\nrosrun singleview_object_recognizer recognition_service -m /path/to/your/model/data  If you want to get a description about other parameters, just add -h to the command. The ones you might want to change are:   -z 2.5  (will neglect all points further away than 2.5m - this will ensure the noise level of the measured points is not too high. Note that RGB-D data from the Asus or Kinect gets worse with distance)   --knn_sift 3  (this will increase the number of generated hypotheses)   --do_shot true  (this will enable SHOT feature extraction - necessary for objects without visual texture information)   -c 5  (increase speed at the cost of possibly missing objects if they are e.g. half occluded.)  The recogniser offers a service  /recognition_service/sv_recognition , defined in  v4r_ros_wrappers/recognition_srv_definitions/srv :  sensor_msgs/PointCloud2 cloud  # input scene point cloud\nfloat32[] transform            # optional transform to world coordinate system\nstd_msgs/String scene_name     # optional name for multiview recognition\nstd_msgs/String view_name      # optional name for multiview recognition\n---\nstd_msgs/String[] ids                 # name of the recognised object model\ngeometry_msgs/Transform[] transforms  # 6D object pose\nfloat32[] confidence                  # ratio of visible points\ngeometry_msgs/Point32[] centroid      # centroid of the cluster\nobject_perception_msgs/BBox[] bbox    # bounding box of the cluster\nsensor_msgs/PointCloud2[] models_cloud  # point cloud of the model transformed into camera coordinates  For you, all you have to provide is a point cloud. The recogniser will return arrays of ids (the name you gave during modelling), transforms (the 6D object poses), as well as confidences, bounding boxes and the segmented point clouds corresponding to the recognised portions of the scene.  There is a test ROS component for you as an example:  rosrun singleview_object_recognizer test_single_view_recognition_from_file _topic:=/camera/depth_registered/points  where you have to set the topic to the respective RGB-D source of your robot, e.g. the head_xtion.  The recogniser publishes visualisation information.   /recognition_service/sv_recogniced_object_instances_img : displays the original image with overlaid bounding boxes of recognised objects  /recognition_service/sv_recogniced_object_instances : the model point clouds of the recognised objects, in the camera frame.\nThe following picture shows an example where R2-D2 is detected in a shelf, with the debug picture and recognised model displayed in rviz.", 
            "title": "Object recognition"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#recognition-performance", 
            "text": "The modelling tool provides full 3D object models from all the views you provided, which in principle allow the recogniser to recogise the object in any condition dfrom any view.\nPractically, however, recognition performance depends on several factors:  Distance: Performance can quite rapidly decline with distance. First, because the object features on which the recogniser depends become too small (no way that it could detect an object just a few pixels large). Second, because the depth data, on which a pose verification step in the recogniser depends, becomes more and more noisy (and close to useless beyond 3 m or so)  Lighting conditions: In principle the object features are lighting invariant. Practically, different characteristics of the camera which was used for modelling and the camera used for recognition can affect the appearance of the object features.  Motion blur: The robot might be moving while it taske a picture. Motion blur will deteriorate object feature extraction.  Occlusions: Objects might not be entirely visible. The recogniser does not need all object features, so it can handle a certain amount of occlusion. How much, depends on the object and how many features it is has.\n* Object specifics: Some objects are harder to detect than others, e.g. because they have few features, are small, have a somewhat shiny surface, or are non-rigid.  Before using the recogniser in any object search scenario it is therefore important to gather some statistics about the recognition performance. The recogniser's confidence value can be a useful measure. But don't trust it too much -it is not an actual probability.  Useful aspects to learn are:  How fast the recognition rate (in how many cases is the object found) drops with distance.  How false positive rate and confidence measure are related.", 
            "title": "Recognition performance"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#trouble-shooting_1", 
            "text": "If you get an error like this   terminate called after throwing an instance of 'flann::FLANNException'\n  what():  Saved index does not contain the dataset and no dataset was provided.\n[recognition_service-2] process has died [pid 17534, exit code -6, cmd /home/mz/work/STRANDS/code/catkin_ws/devel/lib/singleview_object_recognizer/recognition_service __name:=recognition_service __log:=/home/mz/.ros/log/61fb16b8-4afc-11e5-801d-503f56004d09/recognition_service-2.log].\nlog file: /home/mz/.ros/log/61fb16b8-4afc-11e5-801d-503f56004d09/recognition_service-2*.log  locate the file  sift_flann.idx , probably right in your catkin workspace or in  ~/.ros , and remove it.\nDo the same if you get some  vector::_M_range_check  error. In case you enable SHOT, please also remove  shot_omp_flann.idx .     Please also make sure that your camera uses VGA resolution for both RGB and depth. You can check the values for  color_mode  and  depth_mode  in  rosrun rqt_reconfigure rqt_reconfigure  (camera -  driver).    Also ensure you turned on depth_registration (i.e. start the camera with  roslaunch openni2_launch openni2.launch depth_registration:=true )", 
            "title": "Trouble shooting"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#object-tracker", 
            "text": "If you stored you object model for tracking, you can track single objects in real-time using the object-tracker module. You can start the service by   rosrun object_tracker object_tracker_service -m /path/to/your/model/data/object_name/tracking_model.ao  The tracker will start as soon as you call the service   rosservice call /object_tracker/start_recording  This will publish topics for the 3D object pose and the confidence of this estimate  rostopic echo /object_tracker/object_pose\nrostopic echo /object_tracker/object_tracker_confidence  To visualize the pose, you can use RVIZ and check out the image topic  /object_tracker/debug_images .  To stop the tracker call  rosservice call /object_tracker/stop_recording\nrosservice call /object_tracker/cleanup", 
            "title": "Object tracker"
        }, 
        {
            "location": "/v4r_ros_wrappers/tutorial/#references", 
            "text": "When referencing this work, pleace cite:    J. Prankl, A. Aldoma Buchaca, A. Svejda, M. Vincze, RGB-D Object Modelling for Object Recognition and Tracking. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.    Thomas F\u00e4ulhammer, Aitor Aldoma, Michael Zillich and Markus Vincze\nTemporal Integration of Feature Correspondences For Enhanced Recognition in Cluttered And Dynamic Environments\nIEEE International Conference on Robotics and Automation (ICRA), Seattle, WA, USA, 2015.    Thomas F\u00e4ulhammer, Michael Zillich and Markus Vincze\nMulti-View Hypotheses Transfer for Enhanced Object Recognition in Clutter,\nIAPR International Conference on Machine Vision Applications (MVA), Tokyo, Japan, 2015.    A. Aldoma Buchaca, F. Tombari, J. Prankl, A. Richtsfeld, L. di Stefano, M. Vincze, Multimodal Cue Integration through Hypotheses Verification for RGB-D Object Recognition and 6DOF Pose Estimation. IEEE International Conference on Robotics and Automation (ICRA), 2013.    J. Prankl, T. M\u00f6rwald, M. Zillich, M. Vincze, Probabilistic Cue Integration for Real-time Object Pose Tracking. Proc. International Conference on Computer Vision Systems (ICVS). 2013.    For further information check out  this site .", 
            "title": "References"
        }, 
        {
            "location": "/viper/", 
            "text": "viper\n\n\nA view planning enviorment for robots", 
            "title": "Home"
        }, 
        {
            "location": "/viper/#viper", 
            "text": "A view planning enviorment for robots", 
            "title": "viper"
        }
    ]
}