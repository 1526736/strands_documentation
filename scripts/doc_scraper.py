#!/usr/bin/env python

# Scrapes all strands-project repositories for any readme files and wikis, and
# puts them into directories by repository

import requests
import getpass
import os
import json
import argparse
import rospkg
import subprocess
import shutil
import base64
import sys
import fnmatch
import yaml
import pypandoc
import urlparse
import re
import xml.etree.ElementTree as ET

def path_to_arr(path):
    arr = []
    while path:
        arr.append(os.path.basename(path))
        path = os.path.dirname(path)

    return list(reversed(arr))

def create_package_file():
    if os.path.isfile("docs/package.md"):
        done = False
        while not done:
            resp = raw_input("docs/package.md already exists. Overwrite? (y/n)\n")
            if resp == "y":
                done = True
                os.remove("docs/package.md")
            else:
                print("Will not overwrite index.md. Exiting.")
                sys.exit(0)

    link_dict = {}
    desc_dict = {}
    # walk over the directory tree, and look for files with index.md, which we
    # will link to
    for subdir, dirs, files in os.walk("docs"):
        # take all chars after the 5th in the joined path, to remove docs/ from the string
        split = subdir.split('/')
        if len(split) == 1: # skip the top level directory
            continue
        dirpath = os.path.join(*split[1:])

        for file in files:
            if fnmatch.fnmatch(file, "*.xml"):
                fname, ext = os.path.splitext(file)
                with open(os.path.join(subdir, file), 'r') as f:
                    if fname == "package":
                        dict_key = dirpath
                    else:
                        dict_key = os.path.join(dirpath, fname)
                    desc_dict[dict_key] = get_package_xml_description(f.read())
                    
            if fnmatch.fnmatch(file, "index.md"):
                if not split[1] in link_dict:
                    link_dict[split[1]] = []
                # This will add links to the toplevel index and subpackage indexes
                link_dict[split[1]].append((dirpath, "[{0}]({1})".format(dirpath, os.path.join(dirpath, file))))

    with open("docs/packages.md", 'w') as f:
        f.write("Here you can find all the documentation generated by the STRANDS project, aggregated from the github repositories.\n")
        for pkg_name in sorted(link_dict.keys()):
            link_list = link_dict[pkg_name]
            # The first entry in the list is the main package link
            f.write("## {0}\n".format(link_list[0][1]))
            print(link_list[0][0])
            # check base path and repeated subpackage path. e.g. in
            # aaf_deployment, the readme for info_termianl is at
            # aaf_deployment/info_terminal/readme.md, and package xml is at
            # aaf_deployment/info_terminal/info_terminal/package.xml
            if link_list[0][0] in desc_dict:
                f.write("{0}\n".format(desc_dict[link_list[0][0]]))
            elif os.path.join(link_list[0][0], os.path.basename(link_list[0][0])) in desc_dict:
                f.write("{0}\n".format(desc_dict[os.path.join(link_list[0][0], os.path.basename(link_list[0][0]))]))

            # Subsequent entries are subpackages
            for link in link_list[1:]:
                f.write("### {0}\n".format(link[1]))
                if link[0] in desc_dict:
                    f.write("{0}\n".format(desc_dict[link[0]]))
                elif os.path.join(link[0], os.path.basename(link[0])) in desc_dict:
                    f.write("{0}\n".format(desc_dict[os.path.join(link[0], os.path.basename(link[0]))]))
                


def get_oauth_header(private=False):
    # The first thing to do is get an OAuth token - we will use this in place of the
    # username and password in order to access public and private repositories in
    # the organisation. This allows for many more API requrests to be made

    # Check if we already have a token in the config file
    conf_file = os.path.join(os.path.expanduser("~"), ".strands_doc_oauth.tok")
    if not os.path.isfile(conf_file) or private:
        print("Couldn't find the token file, or private token requested. Will generate a new token.")
        
        user = raw_input("Enter your github username: ")
        password = getpass.getpass("Enter your github password:")

        scopes = ["repo"] if private else ["public_repo"]
        auth_data = json.dumps({"scopes": scopes, "note": "strands_documentation scraper"})
        auth = (requests.post("https://api.github.com/authorizations", data=auth_data, auth=(user, password))).json()

        # If we already got a token recently, we will receive an error message
        if "token" not in auth:
            print("Couldn't get Github auth token: {0}".format(auth["message"]))
            token = None
        else: # otherwise, save the token to a file so we can use it again later
            with os.fdopen(os.open(os.path.join(os.path.expanduser("~"), ".strands_doc_oauth.tok"), os.O_WRONLY | os.O_CREAT, 0o600), 'w') as handle:
                handle.write(auth["token"])
                token = auth["token"]
    else:
        with open(conf_file, 'r') as f:
            token = f.read()
        print("Found config file with token.")


    if token:
        header = {"Authorization": "token {0}".format(token)}
    else:
        print("Proceeding without auth token.")
        header = ""

    return header

def get_org_repo_dict(org, header=None):
    """get a list of all the repositories in the given organisation
    """

    print("https://api.github.com/orgs/{0}/repos?type=all".format(org))
    repo_rq = requests.get("https://api.github.com/orgs/{0}/repos?type=all".format(org), headers=header)
    repos = {repo_data["name"]: repo_data for repo_data in json.loads(repo_rq.text)}
    # If there are more than 30 repos, there will be multiple pages
    if "link" in repo_rq.headers:
        # keep getting the data until we reach the final page - we can deduce this
        # by there not being a link to the last page, because we are on it.
        while "last" in repo_rq.headers["link"]:
            print(repo_rq.headers["link"])
            # Get the URL for the next page by splitting the links up
            next_pg = repo_rq.headers["link"].split(',')[0].split(';')[0][1:-1]
            repo_rq = requests.get(next_pg, headers=header)
            repos.update({repo_data["name"]: repo_data for repo_data in json.loads(repo_rq.text)})

    return repos

def get_wiki(org_name, repo_name):
    """Check if a wiki exists, and if it does, clone it to the docs/repo_name/wiki
    """
    # to devnull so there's no output
    FNULL = open(os.devnull, 'w')
    # We can check if a wiki exists by calling git ls-remote. If it returns an
    # OK, then there is a wiki
    if subprocess.call(["git", "ls-remote", "https://github.com/{0}/{1}.wiki.git".format(org_name, repo_name)], stdout=FNULL, stderr=FNULL) == 0:
        wiki_dir = "docs/{0}/wiki".format(repo_name)
        if not os.path.isdir(wiki_dir):
            print("Wiki exists. Cloning...")
            subprocess.call(["git", "clone", "https://github.com/{0}/{1}.wiki.git".format(org_name, repo_name), wiki_dir])
            # delete the .git directory cloned along with the wiki
            shutil.rmtree(os.path.join(wiki_dir, ".git"))
            # rename the Home.md file to index.md so it works properly with mkdocs
            #os.rename(os.path.join(wiki_dir, "Home.md"), os.path.join(wiki_dir, "index.md"))

def get_repo_files(org_name, repo_name, match_ext=[], match_filename=[], match_full=[]):
    """Get files in the given repository which have extensions matching any in the
    given match_ext list, or filenames (without extensions) which match any in
    the given match_filename list. Full filenames (filename + extension) are
    compared to entries in match_full.

    A dictionary where the file path in the repository is the key, and the item
    returned by the github api is the value will be returned.

    """
    if not match_ext and not match_filename and not match_full:
        return {}
    global org
    # The main readme file in the repo is easily retrieved, but just do this using the tree instead
    #readme_rq = requests.get("https://api.github.com/repos/{0}/{1}/readme".format(org, repo_name), headers=header)

    # We also need to look at the whole repository to find the readmes for
    # subdirectories, since there are many such cases. First, get the current
    # commit sha on the default branch
    sha_rq = requests.get("https://api.github.com/repos/{0}/{1}/commits".format(org_name, repo_name), headers=header)
    latest_sha = json.loads(sha_rq.text)[0]["sha"]
    # Use that sha to get the commit tree
    tree_rq = requests.get("https://api.github.com/repos/{0}/{1}/git/trees/{2}?recursive=1".format(org_name, repo_name, latest_sha), headers=header)
    repo_tree = json.loads(tree_rq.text)

    # Look through the tree and try to find things which are likely to be readme-type files
    print("Looking for files matching strings {0}".format(match_ext + match_filename + match_full))
    # We gather readmes here so we can remap any links in them, which we need to
    # do because we will change the filenames to make the documentation appear
    # in a nicer way. Gather them in a dict which will group multiple readmes in
    # the same subdirectory, which we want to handle differently.
    matching = {}
    for item in repo_tree["tree"]:
        lower_fname, lower_ext = os.path.splitext(os.path.basename(item["path"].lower()))
        fname_matches = map(lambda x: lower_fname == x.lower(), match_filename)
        ext_matches = map(lambda x: lower_ext == x.lower(), match_ext)
        full_matches = map(lambda x: lower_fname + lower_ext == x.lower(), match_full)
        if any(fname_matches) or any(ext_matches) or any(full_matches):
            matching[item["path"]] = item

    return matching

def files_to_subpackages(file_dict):
    """Converts a dict of path-item pairs received from get_repo_files to a dict
    where files that are in the same subpackage can be found in a list under the
    key with the subpackage name.

    """
    new_dict = {}
    for file_path in file_dict.keys():
        # The first directory in the path we get is a subdirectory in the
        # repo. Populate a new dict with readmes from each subdirectory
        # under the same key
        split_path = path_to_arr(os.path.dirname(file_path))
        key = split_path[0] if split_path else "index"
        if key not in new_dict:
            new_dict[key] = []
        # path, item pair
        new_dict[key].append((file_path, file_dict[file_path]))

    return new_dict

def get_package_xml_description(xml):
    root = ET.fromstring(xml)
    return root.findall("description")[0].text

def create_dataset_docs(dataset_conf):
    """Creates dataset docs from a configuration provided, which should be found in datasets/datasets.yaml
    Will convert html pages to markdown files.
    """
    link_re =  re.compile("\[[a-zA-Z0-9\/\.\\\_\s:\-]*\]\(([a-zA-Z0-9\/\.\\_\s:\-]*)\)")
    for dataset in datasets.keys():
        # verify=false is dangerous as it ignores ssl certificates, but we're
        # not doing anything which has security risks associated.
        with open("datasets/{0}.md".format(dataset), 'w') as f:
            url = datasets[dataset]["url"]
            url_split = urlparse.urlparse(url)
            orig_path = url_split.path
            # trim the path to get the base path for the page, to replace
            # relative paths in the html
            trimmed_path = os.path.dirname(orig_path)
            base_url = url.replace(orig_path, trimmed_path)
            pandoc_args = ["--no-wrap"]
            if "pandoc_extra_args" in datasets[dataset] and datasets[dataset]["pandoc_extra_args"]:
                pandoc_args.extend(datasets[dataset]["pandoc_extra_args"])
                    
            md_text = pypandoc.convert_text(requests.get(url, verify=False).text, "md", format="html", extra_args=pandoc_args)

            # we don't want to replace the whole match, just the link itself
            def link_replace(match):
                link = match.group(1)
                if link.startswith("http") or link.startswith("www"):
                    # non-relative links stay the same
                    return match.group(0)
                else:
                    # change the link in relative links
                    return match.group(0).replace(link, "{0}/{1}".format(base_url, link))

            fixed_links = link_re.sub(link_replace, md_text)

            f.write(fixed_links)

def copy_dataset_docs():
    """copies docs from the dataset directory to the docs directory, into a dataset directory
    """
    if not os.path.isdir("docs/datasets"):
        os.makedirs("docs/datasets")

    for subdir, dirs, files in os.walk("datasets/"):
        for f in files:
            if fnmatch.fnmatch(f, "*.md"):
                shutil.copyfile(os.path.abspath(os.path.join(subdir, f)), "docs/datasets/{0}".format(f))

def write_readme_files(repo_name):
    readmes = get_repo_files(org, repo_name, match_ext=[".md"], match_filename=["readme"])
    subpkg_readmes = files_to_subpackages(readmes)

    for subpkg in subpkg_readmes.keys():
        print("processing {0}".format(subpkg))

        # The path we get in each item is something like
        # strands_navigation/topological_rviz_tools/readme.md. When using
        # mkdocs, this will generate the documentation in subheadings for each
        # subdirectory, whereas we would prefer it to be grouped under
        # strands_navigation. So, we will save the data in readme.md to
        # strands_navigation/topological_rviz_tools.md. In the case of packages
        # with multiple readmes, we will create a separate directory for them so
        # they are in their own section.
        base_path = os.path.join("docs", repo_name)

        multiple = False
        if len(subpkg_readmes[subpkg]) > 1:
            # sometimes the top level may have multiple files, but we don't want
            # to put them in a subdirectory
            if subpkg != "index":
                base_path = os.path.join(base_path, subpkg)
            multiple = True

        for readme in subpkg_readmes[subpkg]:
            # # Get a filename for the new readme file based on where it was in the directory tree.
            split_path = path_to_arr(os.path.dirname(readme[0]))
            if multiple:
                # There is more than one file in the subpackage
                lower_fname = os.path.splitext(os.path.basename(readme[0]))[0].lower()
                if len(split_path) <= 1:
                    # The file was at level 0 or 1 in the directory tree. If
                    # it was called readme, then we rename it to index.md so
                    # that it is used as a base page in the documentation.
                    # Otherwise, we keep its current name in lowercase.
                    if lower_fname == "readme":
                        fname = "index.md"
                    else:
                        fname = lower_fname + ".md"
                else:
                    # The path is long, so the file was nested deeper than
                    # level 1 in the tree. We will rename it to the name of
                    # the directory that it was in.
                    print("path is long: {0}".format(split_path))
                    fname = split_path[-1] + ".md"
            else:
                # There is only one file in the subpackage. If the split
                # path length is zero, that means it was a toplevel readme,
                # so rename it to index so it's parsed differently by the
                # documentation code.
                if len(split_path) == 0:
                    fname = "index.md"
                else:
                    # Otherwise, rename it to the name of the directory it
                    # was in.
                    fname = split_path[-1] + ".md"

            # make sure a directory exists for the files
            path = os.path.join(base_path, fname)
            print("Saving {0} to {1}".format(readme[1]["path"], path))
            if not os.path.isdir(os.path.dirname(path)):
                os.makedirs(os.path.dirname(path))

            # Get the contents of the readme file from github and output them to a file
            file_rq = json.loads(requests.get(readme[1]["url"], headers=header).text)
            # decode and output the base64 string to file
            with open(path, 'w') as f:
                f.write(base64.b64decode(file_rq["content"]))

if __name__ == '__main__':
    org = "strands-project"

    parser = argparse.ArgumentParser(description="Scrape documentation from the strands project repositories. This script should be run from the top level directory of strands_documentation.")
    parser.add_argument("--private", action="store_true", help="Include private repositories in the scrape. This requires the generation of an OAuth token for github.")
    parser.add_argument("--pkgxml", action="store_true", help="Get descriptions of packages from the package.xml in each subdirectory of a repository. If set, the readme data will not be gathered.")
    parser.add_argument("--nowiki", action="store_true", help="Skip cloning wikis for each package.")
    #parser.add_argument("--ignore-file", nargs=1, help="File containing the names of repos to ignore in the documentation scrape. One repo per line.")
    #parser.add_argument("--repos", action="append", help="Repositories for which docs should be fetched.")
    parser.add_argument("--package-index", action="store_true", help="Generate a readme in the docs directory, populating it with links to all the toplevel readmes in each directory in the docs directory. Basically a list of packages along with a description scraped from the package xml. Does not generate other docs.")
    parser.add_argument("--conf", default="./conf/conf.yaml", help="Config file to use for this docs generation. Can specify repositories to ignore.")

    args = parser.parse_args()

    ignore_repos = []
    with open(args.conf, 'r') as f:
        ignore_repos = yaml.safe_load(f.read())["ignore_repos"]

    datasets = {}
    with open("datasets/datasets.yaml") as f:
        datasets = yaml.safe_load(f.read())["datasets"]


    create_dataset_docs(datasets)
    copy_dataset_docs()

    sys.exit(0)

    get_readmes = True if not args.pkgxml else False
    
    if args.package_index:
        create_package_file()
        sys.exit(0)

    header = get_oauth_header(args.private)
    repos = get_org_repo_dict(org, header)

    # This is where the bulk of the work is done. We check each repository for
    # readme files and see if it has a wiki. If we find files there, we copy them
    # and put them in directories corresponding to the name of the repository
    for repo_name in sorted(repos.keys()):
    #for repo_name in ["rosbridge_suite", "v4r", "v4r_ros_wrappers", "strands_executive", "aaf_deployment"]:
        print("-------------------- {0} --------------------".format(repo_name))
        if repo_name in ignore_repos:
            print("ignoring repo".format(repo_name))
            continue

        if not args.nowiki:
            get_wiki(org, repo_name)

        if get_readmes:
            write_readme_files(repo_name)

        if get_readmes or args.pkgxml:
            package_xml = get_repo_files(org, repo_name, match_full=["package.xml".format(repo_name)])
            subpkg_xml = files_to_subpackages(package_xml)

            base_path = os.path.join("docs", repo_name)
            for subpkg in subpkg_xml.keys():
                multiple = len(subpkg_xml[subpkg]) > 1
                for pkg_xml in subpkg_xml[subpkg]:
                    split_path = path_to_arr(os.path.dirname(pkg_xml[0]))
                    if multiple:
                        # There is more than one file in the subpackage
                        if len(split_path) <= 1:
                            fname = "package.xml"
                        else:
                            # The path is long, so the file was nested deeper than
                            # level 1 in the tree. We will rename it to the name of
                            # the directory that it was in.
                            print("path is long: {0}".format(split_path))
                            fname = split_path[-1] + ".xml"
                    else:
                        # There is only one file in the subpackage. If the split
                        # path length is zero, that means it was a toplevel readme,
                        # so rename it to index so it's parsed differently by the
                        # documentation code.
                        if len(split_path) == 0:
                            fname = "package.xml"
                        else:
                            # Otherwise, rename it to the name of the directory it
                            # was in.
                            fname = split_path[-1] + ".xml"

                    if len(split_path) > 1:
                        path = os.path.join(base_path, os.path.join(*split_path[:-1]), fname)
                    else:
                        path = os.path.join(base_path, fname)

                    print("Saving {0} to {1}".format(pkg_xml[1]["path"], path))
                    if not os.path.isdir(os.path.dirname(path)):
                        os.makedirs(os.path.dirname(path))

                    # Get the contents of the package.xml file from github and output them to a file
                    file_rq = json.loads(requests.get(pkg_xml[1]["url"], headers=header).text)
                    with open(path, 'w') as f:
                        f.write(base64.b64decode(file_rq["content"]))

    # Copy the readme and setup into docs so that it's set up as the first page
    shutil.copy2("README.md", "docs/index.md")
    shutil.copy2("setup.md", "docs/setup.md")
